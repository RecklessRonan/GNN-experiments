nohup: ignoring input
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.4973, Train: 86.87%, Valid: 86.88%, Test: 87.01%
Epoch: 25, Loss: 44892.4648, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 42528.3828, Train: 83.90%, Valid: 83.69%, Test: 84.03%
Epoch: 75, Loss: 7769.6553, Train: 84.16%, Valid: 84.01%, Test: 84.29%
Epoch: 100, Loss: 17.0503, Train: 16.10%, Valid: 16.27%, Test: 15.97%
Epoch: 125, Loss: 1528.6608, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 372.0215, Train: 84.72%, Valid: 84.67%, Test: 84.86%
Epoch: 175, Loss: 14738.5576, Train: 85.03%, Valid: 84.98%, Test: 85.15%
Epoch: 200, Loss: 6.1811, Train: 12.80%, Valid: 12.81%, Test: 12.72%
Epoch: 225, Loss: 3130.0173, Train: 15.54%, Valid: 15.64%, Test: 15.41%
Epoch: 250, Loss: 132.5509, Train: 85.00%, Valid: 84.98%, Test: 85.10%
Epoch: 275, Loss: 56.4201, Train: 84.93%, Valid: 84.90%, Test: 85.05%
Epoch: 300, Loss: 737.6693, Train: 85.42%, Valid: 85.41%, Test: 85.50%
Epoch: 325, Loss: 232.3308, Train: 85.43%, Valid: 85.42%, Test: 85.51%
Epoch: 350, Loss: 4.5614, Train: 15.04%, Valid: 15.08%, Test: 14.92%
Epoch: 375, Loss: 16.2324, Train: 87.57%, Valid: 87.45%, Test: 87.61%
Epoch: 400, Loss: 73.1171, Train: 87.57%, Valid: 87.44%, Test: 87.58%
Epoch: 425, Loss: 5.5434, Train: 15.07%, Valid: 15.10%, Test: 14.94%
Epoch: 450, Loss: 110.9513, Train: 15.03%, Valid: 15.06%, Test: 14.91%
Epoch: 475, Loss: 459.3216, Train: 87.13%, Valid: 86.99%, Test: 87.18%
Run 01:
Highest Train: 87.96
Highest Valid: 87.97
  Final Train: 87.96
   Final Test: 88.00
All runs:
Highest Train: 87.96, nan
Highest Valid: 87.97, nan
  Final Train: 87.96, nan
   Final Test: 88.00, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.7890, Train: 16.27%, Valid: 16.42%, Test: 16.12%
Epoch: 25, Loss: 1894.0242, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 10389.6807, Train: 85.02%, Valid: 84.98%, Test: 85.11%
Epoch: 75, Loss: 2263.7883, Train: 85.98%, Valid: 85.95%, Test: 86.01%
Epoch: 100, Loss: 1448.4487, Train: 85.98%, Valid: 85.95%, Test: 86.01%
Epoch: 125, Loss: 136.2375, Train: 85.98%, Valid: 85.95%, Test: 86.01%
Epoch: 150, Loss: 1.0186, Train: 85.98%, Valid: 85.95%, Test: 86.01%
Epoch: 175, Loss: 18.5222, Train: 85.97%, Valid: 85.94%, Test: 86.00%
Epoch: 200, Loss: 14.8529, Train: 16.42%, Valid: 16.55%, Test: 16.25%
Epoch: 225, Loss: 18.0800, Train: 16.40%, Valid: 16.54%, Test: 16.23%
Epoch: 250, Loss: 9.9885, Train: 85.97%, Valid: 85.95%, Test: 86.01%
Epoch: 275, Loss: 288.4186, Train: 16.44%, Valid: 16.57%, Test: 16.27%
Epoch: 300, Loss: 1.6446, Train: 16.44%, Valid: 16.56%, Test: 16.27%
Epoch: 325, Loss: 65.2573, Train: 16.43%, Valid: 16.56%, Test: 16.27%
Epoch: 350, Loss: 1216.3820, Train: 85.97%, Valid: 85.95%, Test: 86.01%
Epoch: 375, Loss: 2295.7188, Train: 85.97%, Valid: 85.94%, Test: 86.00%
Epoch: 400, Loss: 1695.1078, Train: 85.97%, Valid: 85.94%, Test: 86.01%
Epoch: 425, Loss: 732.0161, Train: 16.44%, Valid: 16.56%, Test: 16.27%
Epoch: 450, Loss: 117.5691, Train: 85.97%, Valid: 85.94%, Test: 86.00%
Epoch: 475, Loss: 394.6673, Train: 85.97%, Valid: 85.94%, Test: 86.01%
Run 01:
Highest Train: 87.21
Highest Valid: 87.22
  Final Train: 87.21
   Final Test: 87.21
All runs:
Highest Train: 87.21, nan
Highest Valid: 87.22, nan
  Final Train: 87.21, nan
   Final Test: 87.21, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.4691, Train: 14.75%, Valid: 14.82%, Test: 14.59%
Epoch: 25, Loss: 20051.7891, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 434102.6250, Train: 84.04%, Valid: 84.06%, Test: 84.19%
Epoch: 75, Loss: 96.8269, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 274.6865, Train: 13.17%, Valid: 13.15%, Test: 13.12%
Epoch: 125, Loss: 179.5363, Train: 16.18%, Valid: 16.19%, Test: 16.09%
Epoch: 150, Loss: 179.2286, Train: 16.19%, Valid: 16.19%, Test: 16.09%
Epoch: 175, Loss: 146.9128, Train: 16.20%, Valid: 16.20%, Test: 16.11%
Epoch: 200, Loss: 194.3003, Train: 16.18%, Valid: 16.19%, Test: 16.09%
Epoch: 225, Loss: 191.7788, Train: 16.19%, Valid: 16.19%, Test: 16.10%
Epoch: 250, Loss: 159.1404, Train: 16.19%, Valid: 16.20%, Test: 16.10%
Epoch: 275, Loss: 147.2774, Train: 16.19%, Valid: 16.19%, Test: 16.09%
Epoch: 300, Loss: 144.1064, Train: 16.19%, Valid: 16.19%, Test: 16.10%
Epoch: 325, Loss: 145.2206, Train: 16.21%, Valid: 16.21%, Test: 16.12%
Epoch: 350, Loss: 177.1624, Train: 16.20%, Valid: 16.21%, Test: 16.11%
Epoch: 375, Loss: 154.4608, Train: 16.21%, Valid: 16.21%, Test: 16.11%
Epoch: 400, Loss: 182.6413, Train: 16.21%, Valid: 16.21%, Test: 16.11%
Epoch: 425, Loss: 198.0268, Train: 16.22%, Valid: 16.22%, Test: 16.13%
Epoch: 450, Loss: 138.3460, Train: 16.22%, Valid: 16.22%, Test: 16.13%
Epoch: 475, Loss: 187.1362, Train: 16.21%, Valid: 16.21%, Test: 16.12%
Run 01:
Highest Train: 87.16
Highest Valid: 87.19
  Final Train: 87.15
   Final Test: 87.16
All runs:
Highest Train: 87.16, nan
Highest Valid: 87.19, nan
  Final Train: 87.15, nan
   Final Test: 87.16, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.3658, Train: 85.67%, Valid: 85.51%, Test: 85.66%
Epoch: 25, Loss: 60247792.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 17469845760538480476160.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 7476405475199437242368.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.07
Highest Valid: 85.98
  Final Train: 86.07
   Final Test: 86.21
All runs:
Highest Train: 86.07, nan
Highest Valid: 85.98, nan
  Final Train: 86.07, nan
   Final Test: 86.21, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.5047, Train: 16.02%, Valid: 16.06%, Test: 15.76%
Epoch: 25, Loss: 121565871907194657570816.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 1941172784385043646644224.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 125455960.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 12381483892152991744.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 4748312313856.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 12316835840000.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 19372250272628736.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 207495745585020928.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 126637694976.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 251143831766958080.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 541862316832980992.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 4106957518536704.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 4422852608.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 19660699336704.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 987923021824.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 249648195305472.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 1013369470976.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 2764092997632.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 3224826806272.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.74
Highest Valid: 86.70
  Final Train: 86.74
   Final Test: 86.74
All runs:
Highest Train: 86.74, nan
Highest Valid: 86.70, nan
  Final Train: 86.74, nan
   Final Test: 86.74, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.6907, Train: 85.82%, Valid: 85.77%, Test: 85.91%
Epoch: 25, Loss: 218226272.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 12933444608.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 95139012608.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 258137886031872.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 12625701888.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 1212489793536.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 216283873280.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 1480796274688.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 87236884299776.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 52671315968.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 44222320640.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 1134656808288256.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 6429692919808.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 4155117404160.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 1289754378240.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 378565689344.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 8709485363200.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 135367008.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 3232866560.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.71
Highest Valid: 86.72
  Final Train: 86.71
   Final Test: 86.78
All runs:
Highest Train: 86.71, nan
Highest Valid: 86.72, nan
  Final Train: 86.71, nan
   Final Test: 86.78, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.6693, Train: 87.73%, Valid: 87.75%, Test: 87.72%
Epoch: 25, Loss: 3505789335502848.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 87.73
Highest Valid: 87.75
  Final Train: 87.73
   Final Test: 87.72
All runs:
Highest Train: 87.73, nan
Highest Valid: 87.75, nan
  Final Train: 87.73, nan
   Final Test: 87.72, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6126, Train: 85.48%, Valid: 85.31%, Test: 85.57%
Epoch: 25, Loss: 894972972061604257464320.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 15733307710401274998549381120.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 1610122759437045530099712.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 35021924998472932524032.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: -inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: -inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 321300488359478390947840.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 19245319291111245025504636436480.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 23343065065265823744.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 77320463403813004146921963520.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 85.97
Highest Valid: 85.94
  Final Train: 85.97
   Final Test: 86.01
All runs:
Highest Train: 85.97, nan
Highest Valid: 85.94, nan
  Final Train: 85.97, nan
   Final Test: 86.01, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.5878, Train: 84.71%, Valid: 84.72%, Test: 84.80%
Epoch: 25, Loss: 1.1861, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.62
Highest Valid: 86.65
  Final Train: 86.62
   Final Test: 86.76
All runs:
Highest Train: 86.62, nan
Highest Valid: 86.65, nan
  Final Train: 86.62, nan
   Final Test: 86.76, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.0152, Train: 86.50%, Valid: 86.46%, Test: 86.62%
Epoch: 25, Loss: 30695.7812, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 353.6640, Train: 15.59%, Valid: 15.62%, Test: 15.47%
Epoch: 75, Loss: 382.9234, Train: 86.59%, Valid: 86.67%, Test: 86.68%
Epoch: 100, Loss: 230.3772, Train: 16.49%, Valid: 16.67%, Test: 16.40%
Epoch: 125, Loss: 409.1014, Train: 16.48%, Valid: 16.67%, Test: 16.39%
Epoch: 150, Loss: 273.7654, Train: 16.49%, Valid: 16.67%, Test: 16.39%
Epoch: 175, Loss: 198.3784, Train: 16.49%, Valid: 16.67%, Test: 16.39%
Epoch: 200, Loss: 111.9617, Train: 16.47%, Valid: 16.65%, Test: 16.38%
Epoch: 225, Loss: 191.2163, Train: 16.47%, Valid: 16.65%, Test: 16.38%
Epoch: 250, Loss: 306.9819, Train: 16.48%, Valid: 16.66%, Test: 16.39%
Epoch: 275, Loss: 251.9116, Train: 16.48%, Valid: 16.66%, Test: 16.39%
Epoch: 300, Loss: 298.7377, Train: 16.48%, Valid: 16.66%, Test: 16.39%
Epoch: 325, Loss: 250.3763, Train: 16.48%, Valid: 16.66%, Test: 16.39%
Epoch: 350, Loss: 295.4209, Train: 16.48%, Valid: 16.67%, Test: 16.39%
Epoch: 375, Loss: 264.6707, Train: 16.48%, Valid: 16.66%, Test: 16.39%
Epoch: 400, Loss: 264.7466, Train: 16.48%, Valid: 16.66%, Test: 16.39%
Epoch: 425, Loss: 262.1046, Train: 16.48%, Valid: 16.66%, Test: 16.39%
Epoch: 450, Loss: 270.4785, Train: 16.48%, Valid: 16.66%, Test: 16.39%
Epoch: 475, Loss: 280.5893, Train: 16.48%, Valid: 16.66%, Test: 16.39%
Run 01:
Highest Train: 88.26
Highest Valid: 88.31
  Final Train: 88.26
   Final Test: 88.26
All runs:
Highest Train: 88.26, nan
Highest Valid: 88.31, nan
  Final Train: 88.26, nan
   Final Test: 88.26, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 3.1849, Train: 85.74%, Valid: 85.61%, Test: 85.83%
Epoch: 25, Loss: 3992.1870, Train: 86.18%, Valid: 86.09%, Test: 86.28%
Epoch: 50, Loss: 365.5757, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 59.8696, Train: 86.55%, Valid: 86.45%, Test: 86.65%
Epoch: 100, Loss: 75.0941, Train: 86.54%, Valid: 86.44%, Test: 86.63%
Epoch: 125, Loss: 80.2220, Train: 86.20%, Valid: 86.14%, Test: 86.27%
Epoch: 150, Loss: 82.0149, Train: 86.64%, Valid: 86.51%, Test: 86.72%
Epoch: 175, Loss: 80.4333, Train: 86.67%, Valid: 86.56%, Test: 86.76%
Epoch: 200, Loss: 77.6553, Train: 86.54%, Valid: 86.43%, Test: 86.63%
Epoch: 225, Loss: 75.5208, Train: 85.36%, Valid: 85.19%, Test: 85.49%
Epoch: 250, Loss: 80.0225, Train: 86.21%, Valid: 86.14%, Test: 86.28%
Epoch: 275, Loss: 77.9390, Train: 85.40%, Valid: 85.22%, Test: 85.53%
Epoch: 300, Loss: 77.8635, Train: 85.37%, Valid: 85.20%, Test: 85.50%
Epoch: 325, Loss: 77.1664, Train: 86.42%, Valid: 86.31%, Test: 86.50%
Epoch: 350, Loss: 80.8299, Train: 86.25%, Valid: 86.16%, Test: 86.31%
Epoch: 375, Loss: 78.7187, Train: 86.25%, Valid: 86.17%, Test: 86.32%
Epoch: 400, Loss: 78.1010, Train: 86.21%, Valid: 86.14%, Test: 86.27%
Epoch: 425, Loss: 71.1740, Train: 85.36%, Valid: 85.19%, Test: 85.49%
Epoch: 450, Loss: 66.3082, Train: 86.53%, Valid: 86.42%, Test: 86.62%
Epoch: 475, Loss: 75.8333, Train: 86.61%, Valid: 86.50%, Test: 86.71%
Run 01:
Highest Train: 86.68
Highest Valid: 86.56
  Final Train: 86.68
   Final Test: 86.76
All runs:
Highest Train: 86.68, nan
Highest Valid: 86.56, nan
  Final Train: 86.68, nan
   Final Test: 86.76, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.4919, Train: 86.23%, Valid: 86.10%, Test: 86.19%
Epoch: 25, Loss: 3049.1528, Train: 87.14%, Valid: 87.22%, Test: 87.21%
Epoch: 50, Loss: 1.1050, Train: 87.16%, Valid: 87.24%, Test: 87.23%
Epoch: 75, Loss: 1.1892, Train: 87.17%, Valid: 87.26%, Test: 87.26%
Epoch: 100, Loss: 1.2057, Train: 87.17%, Valid: 87.26%, Test: 87.26%
Epoch: 125, Loss: 1.2084, Train: 87.17%, Valid: 87.27%, Test: 87.26%
Epoch: 150, Loss: 1.2057, Train: 87.18%, Valid: 87.27%, Test: 87.27%
Epoch: 175, Loss: 1.2036, Train: 87.19%, Valid: 87.28%, Test: 87.28%
Epoch: 200, Loss: 1.2042, Train: 87.20%, Valid: 87.29%, Test: 87.30%
Epoch: 225, Loss: 1.2038, Train: 87.21%, Valid: 87.30%, Test: 87.31%
Epoch: 250, Loss: 1.2014, Train: 87.22%, Valid: 87.33%, Test: 87.33%
Epoch: 275, Loss: 1.2004, Train: 85.19%, Valid: 85.01%, Test: 85.28%
Epoch: 300, Loss: 1.1997, Train: 85.11%, Valid: 84.88%, Test: 85.17%
Epoch: 325, Loss: 1.1991, Train: 85.12%, Valid: 84.88%, Test: 85.16%
Epoch: 350, Loss: 1.1967, Train: 85.12%, Valid: 84.88%, Test: 85.17%
Epoch: 375, Loss: 1.2000, Train: 85.12%, Valid: 84.88%, Test: 85.16%
Epoch: 400, Loss: 1.1955, Train: 85.12%, Valid: 84.88%, Test: 85.16%
Epoch: 425, Loss: 1.1985, Train: 85.11%, Valid: 84.88%, Test: 85.16%
Epoch: 450, Loss: 1.1980, Train: 85.11%, Valid: 84.88%, Test: 85.16%
Epoch: 475, Loss: 1.1954, Train: 85.11%, Valid: 84.88%, Test: 85.16%
Run 01:
Highest Train: 88.35
Highest Valid: 88.30
  Final Train: 88.34
   Final Test: 88.32
All runs:
Highest Train: 88.35, nan
Highest Valid: 88.30, nan
  Final Train: 88.34, nan
   Final Test: 88.32, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 5.0121, Train: 13.46%, Valid: 13.38%, Test: 13.41%
Epoch: 25, Loss: 4.7893, Train: 84.45%, Valid: 84.45%, Test: 84.59%
Epoch: 50, Loss: 303100.5625, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 56.2549, Train: 49.96%, Valid: 49.96%, Test: 49.97%
Epoch: 100, Loss: 14123355.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 2.4870, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 44564172.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 60894.3594, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 6112352.5000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 469747.3125, Train: 85.59%, Valid: 85.43%, Test: 85.71%
Epoch: 250, Loss: 22347.7930, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 801.2239, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 17522816.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 404035.0312, Train: 84.70%, Valid: 84.55%, Test: 84.84%
Epoch: 350, Loss: 5768233.5000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 24.7445, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 112.2257, Train: 50.04%, Valid: 50.04%, Test: 50.03%
Epoch: 425, Loss: 739.3743, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 530066.3750, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 2852817.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 87.28
Highest Valid: 87.27
  Final Train: 87.28
   Final Test: 87.31
All runs:
Highest Train: 87.28, nan
Highest Valid: 87.27, nan
  Final Train: 87.28, nan
   Final Test: 87.31, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 3.4757, Train: 85.71%, Valid: 85.64%, Test: 85.84%
Epoch: 25, Loss: 718719483904.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 22985155018752.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 13568550912.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 20193201881088.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 75031224778752.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 21871519659458560.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 9903991647371264.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 5733224960.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 14425142658596864.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 49857032638955520.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 99389745922048.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 14976939982848.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 38737851935883264.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 3717765267456.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 980514525675520.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 10592638113677312.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 832341844951040.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 1548145657905152.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 14866167365632.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.54
Highest Valid: 86.52
  Final Train: 86.54
   Final Test: 86.63
All runs:
Highest Train: 86.54, nan
Highest Valid: 86.52, nan
  Final Train: 86.54, nan
   Final Test: 86.63, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 9.8259, Train: 19.46%, Valid: 19.23%, Test: 19.21%
Epoch: 25, Loss: 2.1506, Train: 84.85%, Valid: 84.63%, Test: 84.94%
Epoch: 50, Loss: 1144315547931407024128.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 31565165887488.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 21227234131968.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 5988420681728.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 2488282382336.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 366267831681024.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 93941861160321024.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 178505316827136.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 156351439306752.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 2758935861788672.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 502889332080640.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 856816179216384.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 2719252243021824.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 216910444625920.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 2840859141734400.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 62889005056.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 1329362842419200.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 3878518408609792.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 85.83
Highest Valid: 85.65
  Final Train: 85.83
   Final Test: 85.89
All runs:
Highest Train: 85.83, nan
Highest Valid: 85.65, nan
  Final Train: 85.83, nan
   Final Test: 85.89, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 16.4220, Train: 85.08%, Valid: 84.93%, Test: 85.20%
Epoch: 25, Loss: 2.0081, Train: 85.58%, Valid: 85.39%, Test: 85.68%
Epoch: 50, Loss: 2.5256, Train: 85.57%, Valid: 85.36%, Test: 85.66%
Epoch: 75, Loss: 1.9394, Train: 85.56%, Valid: 85.36%, Test: 85.66%
Epoch: 100, Loss: 2.3488, Train: 86.21%, Valid: 86.31%, Test: 86.30%
Epoch: 125, Loss: 2.2504, Train: 85.56%, Valid: 85.36%, Test: 85.66%
Epoch: 150, Loss: 2.2737, Train: 85.56%, Valid: 85.36%, Test: 85.66%
Epoch: 175, Loss: 2.2924, Train: 85.56%, Valid: 85.36%, Test: 85.65%
Epoch: 200, Loss: 2.3246, Train: 85.56%, Valid: 85.36%, Test: 85.66%
Epoch: 225, Loss: 2.2396, Train: 13.82%, Valid: 13.76%, Test: 13.81%
Epoch: 250, Loss: 2.3003, Train: 85.56%, Valid: 85.36%, Test: 85.66%
Epoch: 275, Loss: 2.3034, Train: 85.56%, Valid: 85.36%, Test: 85.65%
Epoch: 300, Loss: 2.3119, Train: 85.56%, Valid: 85.36%, Test: 85.66%
Epoch: 325, Loss: 2.2862, Train: 85.56%, Valid: 85.35%, Test: 85.65%
Epoch: 350, Loss: 2.2918, Train: 85.56%, Valid: 85.36%, Test: 85.65%
Epoch: 375, Loss: 2.1514, Train: 15.24%, Valid: 15.23%, Test: 15.18%
Epoch: 400, Loss: 2.4093, Train: 85.57%, Valid: 85.37%, Test: 85.67%
Epoch: 425, Loss: 2.3662, Train: 85.55%, Valid: 85.36%, Test: 85.64%
Epoch: 450, Loss: 2.2411, Train: 85.56%, Valid: 85.36%, Test: 85.66%
Epoch: 475, Loss: 2.2958, Train: 85.55%, Valid: 85.35%, Test: 85.64%
Run 01:
Highest Train: 87.18
Highest Valid: 87.13
  Final Train: 87.18
   Final Test: 87.26
All runs:
Highest Train: 87.18, nan
Highest Valid: 87.13, nan
  Final Train: 87.18, nan
   Final Test: 87.26, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.4850, Train: 86.00%, Valid: 86.01%, Test: 86.07%
Epoch: 25, Loss: 39569077784571194611114363060224.0000, Train: 85.62%, Valid: 85.64%, Test: 85.78%
Epoch: 50, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 350856.7812, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.75
Highest Valid: 86.66
  Final Train: 86.75
   Final Test: 86.88
All runs:
Highest Train: 86.75, nan
Highest Valid: 86.66, nan
  Final Train: 86.75, nan
   Final Test: 86.88, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.3397, Train: 86.33%, Valid: 86.21%, Test: 86.37%
Epoch: 25, Loss: nan, Train: 50.01%, Valid: 50.01%, Test: 50.01%
Epoch: 50, Loss: 12204348213821440.0000, Train: 50.04%, Valid: 50.06%, Test: 50.05%
Epoch: 75, Loss: 329569349402624.0000, Train: 49.96%, Valid: 49.94%, Test: 49.95%
Epoch: 100, Loss: 4169956635705344.0000, Train: 50.04%, Valid: 50.06%, Test: 50.05%
Epoch: 125, Loss: 3145728880468295680.0000, Train: 49.96%, Valid: 49.94%, Test: 49.95%
Epoch: 150, Loss: 17758985256884502528.0000, Train: 84.76%, Valid: 84.51%, Test: 84.84%
Epoch: 175, Loss: 1515.0314, Train: 50.04%, Valid: 50.05%, Test: 50.04%
Epoch: 200, Loss: 4.1355, Train: 50.04%, Valid: 50.06%, Test: 50.05%
Epoch: 225, Loss: 11058321016487936.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 553868330973265920.0000, Train: 50.03%, Valid: 50.03%, Test: 50.04%
Epoch: 275, Loss: 2.4670, Train: 49.96%, Valid: 49.94%, Test: 49.96%
Epoch: 300, Loss: 20795793342464.0000, Train: 50.04%, Valid: 50.06%, Test: 50.05%
Epoch: 325, Loss: 28992417381430067200.0000, Train: 85.36%, Valid: 85.16%, Test: 85.40%
Epoch: 350, Loss: 763846602015113216.0000, Train: 50.04%, Valid: 50.06%, Test: 50.05%
Epoch: 375, Loss: 1058199756550409551872.0000, Train: 50.04%, Valid: 50.06%, Test: 50.05%
Epoch: 400, Loss: 166963004516669063168.0000, Train: 49.96%, Valid: 49.95%, Test: 49.96%
Epoch: 425, Loss: 132914544.0000, Train: 49.96%, Valid: 49.94%, Test: 49.96%
Epoch: 450, Loss: 21573.7910, Train: 50.04%, Valid: 50.06%, Test: 50.05%
Epoch: 475, Loss: 296920331942100992.0000, Train: 50.04%, Valid: 50.06%, Test: 50.05%
Run 01:
Highest Train: 87.07
Highest Valid: 87.13
  Final Train: 87.07
   Final Test: 87.12
All runs:
Highest Train: 87.07, nan
Highest Valid: 87.13, nan
  Final Train: 87.07, nan
   Final Test: 87.12, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 5.9835, Train: 84.47%, Valid: 84.24%, Test: 84.55%
Epoch: 25, Loss: 27.7018, Train: 86.18%, Valid: 86.14%, Test: 86.25%
Epoch: 50, Loss: 335.9927, Train: 15.21%, Valid: 15.19%, Test: 15.08%
Epoch: 75, Loss: 479.9531, Train: 15.87%, Valid: 16.01%, Test: 15.76%
Epoch: 100, Loss: 493.7004, Train: 15.90%, Valid: 16.03%, Test: 15.78%
Epoch: 125, Loss: 484.2334, Train: 15.90%, Valid: 16.03%, Test: 15.78%
Epoch: 150, Loss: 481.1444, Train: 15.89%, Valid: 16.03%, Test: 15.78%
Epoch: 175, Loss: 484.3875, Train: 15.87%, Valid: 16.01%, Test: 15.75%
Epoch: 200, Loss: 465.7189, Train: 15.89%, Valid: 16.03%, Test: 15.78%
Epoch: 225, Loss: 480.8009, Train: 15.89%, Valid: 16.03%, Test: 15.78%
Epoch: 250, Loss: 480.9363, Train: 15.89%, Valid: 16.03%, Test: 15.78%
Epoch: 275, Loss: 482.4563, Train: 16.12%, Valid: 16.28%, Test: 15.99%
Epoch: 300, Loss: 484.7076, Train: 16.13%, Valid: 16.29%, Test: 16.00%
Epoch: 325, Loss: 479.6978, Train: 15.89%, Valid: 16.03%, Test: 15.78%
Epoch: 350, Loss: 483.9711, Train: 15.89%, Valid: 16.03%, Test: 15.78%
Epoch: 375, Loss: 481.1538, Train: 15.89%, Valid: 16.03%, Test: 15.78%
Epoch: 400, Loss: 481.4538, Train: 15.89%, Valid: 16.03%, Test: 15.78%
Epoch: 425, Loss: 480.4763, Train: 15.87%, Valid: 16.00%, Test: 15.75%
Epoch: 450, Loss: 480.9412, Train: 15.87%, Valid: 16.00%, Test: 15.75%
Epoch: 475, Loss: 481.8654, Train: 15.87%, Valid: 16.00%, Test: 15.75%
Run 01:
Highest Train: 87.26
Highest Valid: 87.28
  Final Train: 87.26
   Final Test: 87.27
All runs:
Highest Train: 87.26, nan
Highest Valid: 87.28, nan
  Final Train: 87.26, nan
   Final Test: 87.27, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 7.4489, Train: 86.65%, Valid: 86.61%, Test: 86.81%
Epoch: 25, Loss: 148.6924, Train: 85.83%, Valid: 85.83%, Test: 85.88%
Epoch: 50, Loss: 23.8911, Train: 85.88%, Valid: 85.83%, Test: 85.91%
Epoch: 75, Loss: 3.3080, Train: 85.54%, Valid: 85.49%, Test: 85.71%
Epoch: 100, Loss: 3.2289, Train: 85.58%, Valid: 85.51%, Test: 85.74%
Epoch: 125, Loss: 3.2115, Train: 85.44%, Valid: 85.36%, Test: 85.58%
Epoch: 150, Loss: 3.1911, Train: 85.43%, Valid: 85.35%, Test: 85.57%
Epoch: 175, Loss: 3.1674, Train: 85.43%, Valid: 85.35%, Test: 85.57%
Epoch: 200, Loss: 3.1409, Train: 85.43%, Valid: 85.35%, Test: 85.57%
Epoch: 225, Loss: 3.1081, Train: 85.48%, Valid: 85.39%, Test: 85.60%
Epoch: 250, Loss: 3.0735, Train: 85.49%, Valid: 85.40%, Test: 85.61%
Epoch: 275, Loss: 3.0239, Train: 85.49%, Valid: 85.39%, Test: 85.60%
Epoch: 300, Loss: 2.9597, Train: 85.52%, Valid: 85.42%, Test: 85.64%
Epoch: 325, Loss: 2.8800, Train: 85.56%, Valid: 85.46%, Test: 85.69%
Epoch: 350, Loss: 0.9931, Train: 84.69%, Valid: 84.60%, Test: 84.78%
Epoch: 375, Loss: 2.4777, Train: 85.71%, Valid: 85.61%, Test: 85.84%
Epoch: 400, Loss: 2.3868, Train: 85.60%, Valid: 85.49%, Test: 85.73%
Epoch: 425, Loss: 2.0898, Train: 85.23%, Valid: 85.15%, Test: 85.38%
Epoch: 450, Loss: 1.7432, Train: 85.13%, Valid: 85.06%, Test: 85.28%
Epoch: 475, Loss: 1.3509, Train: 85.08%, Valid: 85.01%, Test: 85.24%
Run 01:
Highest Train: 87.28
Highest Valid: 87.20
  Final Train: 87.28
   Final Test: 87.25
All runs:
Highest Train: 87.28, nan
Highest Valid: 87.20, nan
  Final Train: 87.28, nan
   Final Test: 87.25, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 7.6764, Train: 87.04%, Valid: 87.03%, Test: 87.03%
Epoch: 25, Loss: 2.5874, Train: 85.99%, Valid: 85.81%, Test: 85.99%
Epoch: 50, Loss: 2.5217, Train: 86.62%, Valid: 86.69%, Test: 86.68%
Epoch: 75, Loss: 2.4971, Train: 86.70%, Valid: 86.76%, Test: 86.78%
Epoch: 100, Loss: 2.5185, Train: 86.95%, Valid: 86.91%, Test: 86.96%
Epoch: 125, Loss: 2.4551, Train: 86.88%, Valid: 86.92%, Test: 86.94%
Epoch: 150, Loss: 2.4355, Train: 86.90%, Valid: 86.95%, Test: 86.98%
Epoch: 175, Loss: 2.4161, Train: 86.95%, Valid: 87.02%, Test: 87.04%
Epoch: 200, Loss: 2.3955, Train: 86.94%, Valid: 87.01%, Test: 87.03%
Epoch: 225, Loss: 2.3761, Train: 87.00%, Valid: 87.09%, Test: 87.09%
Epoch: 250, Loss: 2.3563, Train: 86.97%, Valid: 87.05%, Test: 87.05%
Epoch: 275, Loss: 1.6375, Train: 85.63%, Valid: 85.47%, Test: 85.69%
Epoch: 300, Loss: 0.7406, Train: 87.14%, Valid: 87.09%, Test: 87.10%
Epoch: 325, Loss: 2.2524, Train: 88.07%, Valid: 87.89%, Test: 87.98%
Epoch: 350, Loss: 1.7104, Train: 87.12%, Valid: 87.10%, Test: 87.11%
Epoch: 375, Loss: 1.1195, Train: 87.11%, Valid: 87.08%, Test: 87.11%
Epoch: 400, Loss: 0.6947, Train: 87.05%, Valid: 87.05%, Test: 87.06%
Epoch: 425, Loss: 0.5284, Train: 87.01%, Valid: 87.04%, Test: 87.03%
Epoch: 450, Loss: 0.4696, Train: 87.04%, Valid: 87.05%, Test: 87.06%
Epoch: 475, Loss: 0.4127, Train: 87.14%, Valid: 87.14%, Test: 87.16%
Run 01:
Highest Train: 88.17
Highest Valid: 87.98
  Final Train: 88.17
   Final Test: 88.03
All runs:
Highest Train: 88.17, nan
Highest Valid: 87.98, nan
  Final Train: 88.17, nan
   Final Test: 88.03, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 10.5098, Train: 85.82%, Valid: 85.71%, Test: 85.88%
Epoch: 25, Loss: 7.9149, Train: 85.85%, Valid: 85.93%, Test: 85.96%
Epoch: 50, Loss: 2.7797, Train: 85.47%, Valid: 85.52%, Test: 85.58%
Epoch: 75, Loss: 9.2263, Train: 86.59%, Valid: 86.57%, Test: 86.61%
Epoch: 100, Loss: 61308.6641, Train: 14.77%, Valid: 14.81%, Test: 14.67%
Epoch: 125, Loss: 15.9969, Train: 85.13%, Valid: 85.10%, Test: 85.23%
Epoch: 150, Loss: 676109188333568.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 591731990265475593404416.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 50882749453942432727040.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 243580788726035172556800.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 705877387464193081344.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 173785342225623810048.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 706514717180210184192.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 13585147858381701120.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 33785153382533562368.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 16502148908336545792.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 875080688703897600.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 628679377005051904.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 46150324920176869376.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 1568515085595479900160.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 88.33
Highest Valid: 88.35
  Final Train: 88.32
   Final Test: 88.31
All runs:
Highest Train: 88.33, nan
Highest Valid: 88.35, nan
  Final Train: 88.32, nan
   Final Test: 88.31, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.7935, Train: 85.90%, Valid: 85.81%, Test: 85.93%
Epoch: 25, Loss: 10.6968, Train: 85.89%, Valid: 85.81%, Test: 85.87%
Epoch: 50, Loss: 3.2097, Train: 85.89%, Valid: 85.79%, Test: 85.91%
Epoch: 75, Loss: 6.7230, Train: 85.73%, Valid: 85.62%, Test: 85.73%
Epoch: 100, Loss: 3.2916, Train: 86.27%, Valid: 86.20%, Test: 86.33%
Epoch: 125, Loss: 3.2516, Train: 86.28%, Valid: 86.19%, Test: 86.33%
Epoch: 150, Loss: 3.2363, Train: 86.33%, Valid: 86.23%, Test: 86.42%
Epoch: 175, Loss: 3.2232, Train: 86.13%, Valid: 85.98%, Test: 86.23%
Epoch: 200, Loss: 3.2113, Train: 86.02%, Valid: 85.86%, Test: 86.13%
Epoch: 225, Loss: 3.1988, Train: 86.19%, Valid: 86.03%, Test: 86.29%
Epoch: 250, Loss: 3.1872, Train: 86.05%, Valid: 85.89%, Test: 86.16%
Epoch: 275, Loss: 3.1740, Train: 86.05%, Valid: 85.89%, Test: 86.17%
Epoch: 300, Loss: 3.1614, Train: 85.94%, Valid: 85.78%, Test: 86.06%
Epoch: 325, Loss: 3.1508, Train: 85.92%, Valid: 85.76%, Test: 86.04%
Epoch: 350, Loss: 3.1406, Train: 85.91%, Valid: 85.76%, Test: 86.04%
Epoch: 375, Loss: 3.1300, Train: 85.89%, Valid: 85.74%, Test: 86.02%
Epoch: 400, Loss: 3.1185, Train: 85.88%, Valid: 85.72%, Test: 86.00%
Epoch: 425, Loss: 3.1084, Train: 85.87%, Valid: 85.72%, Test: 86.00%
Epoch: 450, Loss: 3.0968, Train: 85.87%, Valid: 85.71%, Test: 85.99%
Epoch: 475, Loss: 3.0857, Train: 85.88%, Valid: 85.72%, Test: 86.01%
Run 01:
Highest Train: 86.52
Highest Valid: 86.45
  Final Train: 86.52
   Final Test: 86.64
All runs:
Highest Train: 86.52, nan
Highest Valid: 86.45, nan
  Final Train: 86.52, nan
   Final Test: 86.64, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 4.0626, Train: 84.28%, Valid: 84.37%, Test: 84.30%
Epoch: 25, Loss: 2.9539, Train: 86.60%, Valid: 86.64%, Test: 86.72%
Epoch: 50, Loss: 2.2357, Train: 86.69%, Valid: 86.71%, Test: 86.79%
Epoch: 75, Loss: 0.5235, Train: 85.67%, Valid: 85.53%, Test: 85.76%
Epoch: 100, Loss: 0.3704, Train: 85.54%, Valid: 85.45%, Test: 85.71%
Epoch: 125, Loss: 0.3394, Train: 85.59%, Valid: 85.37%, Test: 85.71%
Epoch: 150, Loss: 1.5417, Train: 86.98%, Valid: 87.01%, Test: 87.05%
Epoch: 175, Loss: 2.2863, Train: 87.05%, Valid: 87.00%, Test: 87.08%
Epoch: 200, Loss: 0.5574, Train: 86.02%, Valid: 85.97%, Test: 86.16%
Epoch: 225, Loss: 0.3675, Train: 85.79%, Valid: 85.57%, Test: 85.86%
Epoch: 250, Loss: 0.3452, Train: 85.84%, Valid: 85.66%, Test: 85.91%
Epoch: 275, Loss: 0.3336, Train: 85.51%, Valid: 85.30%, Test: 85.58%
Epoch: 300, Loss: 0.3310, Train: 85.76%, Valid: 85.55%, Test: 85.81%
Epoch: 325, Loss: 0.3398, Train: 85.74%, Valid: 85.53%, Test: 85.81%
Epoch: 350, Loss: 0.4638, Train: 87.36%, Valid: 87.28%, Test: 87.46%
Epoch: 375, Loss: 0.3627, Train: 86.13%, Valid: 86.18%, Test: 86.40%
Epoch: 400, Loss: 0.3397, Train: 85.80%, Valid: 85.62%, Test: 85.88%
Epoch: 425, Loss: 0.3304, Train: 85.75%, Valid: 85.54%, Test: 85.81%
Epoch: 450, Loss: 0.3302, Train: 85.98%, Valid: 85.76%, Test: 86.03%
Epoch: 475, Loss: 0.3290, Train: 85.81%, Valid: 85.61%, Test: 85.88%
Run 01:
Highest Train: 87.79
Highest Valid: 87.69
  Final Train: 87.79
   Final Test: 87.70
All runs:
Highest Train: 87.79, nan
Highest Valid: 87.69, nan
  Final Train: 87.79, nan
   Final Test: 87.70, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 10.3409, Train: 84.50%, Valid: 84.33%, Test: 84.62%
Epoch: 25, Loss: 2.3125, Train: 84.69%, Valid: 84.51%, Test: 84.79%
Epoch: 50, Loss: 2.4182, Train: 87.02%, Valid: 87.08%, Test: 87.06%
Epoch: 75, Loss: 2.2507, Train: 86.29%, Valid: 86.31%, Test: 86.35%
Epoch: 100, Loss: 1.3165, Train: 85.65%, Valid: 85.71%, Test: 85.72%
Epoch: 125, Loss: 0.3562, Train: 87.46%, Valid: 87.25%, Test: 87.53%
Epoch: 150, Loss: 0.3500, Train: 87.27%, Valid: 87.04%, Test: 87.32%
Epoch: 175, Loss: 0.3396, Train: 85.94%, Valid: 85.92%, Test: 86.03%
Epoch: 200, Loss: 0.3295, Train: 86.27%, Valid: 86.14%, Test: 86.33%
Epoch: 225, Loss: 0.3376, Train: 86.91%, Valid: 86.72%, Test: 86.93%
Epoch: 250, Loss: 0.3280, Train: 85.98%, Valid: 85.99%, Test: 86.08%
Epoch: 275, Loss: 0.3264, Train: 85.84%, Valid: 85.86%, Test: 85.88%
Epoch: 300, Loss: 0.3365, Train: 87.66%, Valid: 87.46%, Test: 87.64%
Epoch: 325, Loss: 0.3267, Train: 86.06%, Valid: 86.03%, Test: 86.16%
Epoch: 350, Loss: 0.3244, Train: 85.79%, Valid: 85.77%, Test: 85.89%
Epoch: 375, Loss: 0.3942, Train: 85.79%, Valid: 85.83%, Test: 85.89%
Epoch: 400, Loss: 0.3537, Train: 85.65%, Valid: 85.70%, Test: 85.74%
Epoch: 425, Loss: 0.3446, Train: 85.92%, Valid: 85.94%, Test: 86.02%
Epoch: 450, Loss: 0.3382, Train: 87.49%, Valid: 87.35%, Test: 87.53%
Epoch: 475, Loss: 0.3307, Train: 86.11%, Valid: 86.09%, Test: 86.21%
Run 01:
Highest Train: 88.51
Highest Valid: 88.52
  Final Train: 88.51
   Final Test: 88.53
All runs:
Highest Train: 88.51, nan
Highest Valid: 88.52, nan
  Final Train: 88.51, nan
   Final Test: 88.53, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 13.0160, Train: 85.75%, Valid: 85.72%, Test: 85.79%
Epoch: 25, Loss: 3.6178, Train: 85.49%, Valid: 85.33%, Test: 85.59%
Epoch: 50, Loss: 3.9660, Train: 85.52%, Valid: 85.38%, Test: 85.63%
Epoch: 75, Loss: 4.0007, Train: 85.52%, Valid: 85.38%, Test: 85.63%
Epoch: 100, Loss: 4.0030, Train: 85.52%, Valid: 85.39%, Test: 85.63%
Epoch: 125, Loss: 4.0028, Train: 85.52%, Valid: 85.39%, Test: 85.63%
Epoch: 150, Loss: 4.0034, Train: 85.52%, Valid: 85.39%, Test: 85.63%
Epoch: 175, Loss: 4.0032, Train: 85.52%, Valid: 85.39%, Test: 85.63%
Epoch: 200, Loss: 4.0034, Train: 85.52%, Valid: 85.39%, Test: 85.63%
Epoch: 225, Loss: 4.0031, Train: 85.52%, Valid: 85.39%, Test: 85.63%
Epoch: 250, Loss: 4.0025, Train: 85.52%, Valid: 85.39%, Test: 85.64%
Epoch: 275, Loss: 4.0022, Train: 85.52%, Valid: 85.39%, Test: 85.64%
Epoch: 300, Loss: 4.0022, Train: 85.52%, Valid: 85.39%, Test: 85.64%
Epoch: 325, Loss: 4.0024, Train: 85.52%, Valid: 85.39%, Test: 85.64%
Epoch: 350, Loss: 4.0015, Train: 85.52%, Valid: 85.39%, Test: 85.64%
Epoch: 375, Loss: 4.0017, Train: 85.52%, Valid: 85.39%, Test: 85.64%
Epoch: 400, Loss: 4.0014, Train: 85.52%, Valid: 85.39%, Test: 85.64%
Epoch: 425, Loss: 4.0020, Train: 85.52%, Valid: 85.39%, Test: 85.64%
Epoch: 450, Loss: 4.0011, Train: 85.52%, Valid: 85.39%, Test: 85.64%
Epoch: 475, Loss: 4.0006, Train: 85.52%, Valid: 85.39%, Test: 85.64%
Run 01:
Highest Train: 86.64
Highest Valid: 86.59
  Final Train: 86.64
   Final Test: 86.68
All runs:
Highest Train: 86.64, nan
Highest Valid: 86.59, nan
  Final Train: 86.64, nan
   Final Test: 86.68, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 6.5758, Train: 86.66%, Valid: 86.70%, Test: 86.76%
Epoch: 25, Loss: 4.5316, Train: 85.55%, Valid: 85.36%, Test: 85.59%
Epoch: 50, Loss: 21817390186627072.0000, Train: 50.01%, Valid: 50.01%, Test: 50.03%
Epoch: 75, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: nan, Train: 50.00%, Valid: 50.01%, Test: 49.99%
Epoch: 125, Loss: 250440354991661183646058938368.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 439044736.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 5779065048021540298821730304.0000, Train: 49.88%, Valid: 49.89%, Test: 49.86%
Epoch: 200, Loss: 215728344320401280556887393173504.0000, Train: 49.97%, Valid: 49.97%, Test: 49.94%
Epoch: 225, Loss: 3662953959532945494857022963712.0000, Train: 50.03%, Valid: 50.03%, Test: 50.05%
Epoch: 250, Loss: 470587066644917118425890816.0000, Train: 50.00%, Valid: 50.01%, Test: 50.01%
Epoch: 275, Loss: 6736929535619110380145181982720.0000, Train: 49.99%, Valid: 49.99%, Test: 49.97%
Epoch: 300, Loss: 15028683407492295236213800960.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 42123443745202976116192885866496.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 6693153406976.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 25510244896663666700396349358080.0000, Train: 50.02%, Valid: 50.02%, Test: 50.04%
Epoch: 400, Loss: 139168306270865107833585664.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 57157639684427520678136119296.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: inf, Train: 50.11%, Valid: 50.10%, Test: 50.13%
Run 01:
Highest Train: 86.66
Highest Valid: 86.70
  Final Train: 86.66
   Final Test: 86.76
All runs:
Highest Train: 86.66, nan
Highest Valid: 86.70, nan
  Final Train: 86.66, nan
   Final Test: 86.76, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.1724, Train: 86.25%, Valid: 86.12%, Test: 86.33%
Epoch: 25, Loss: 25.3104, Train: 87.18%, Valid: 87.07%, Test: 87.22%
Epoch: 50, Loss: 19156.3242, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 105.3061, Train: 87.27%, Valid: 87.15%, Test: 87.27%
Epoch: 100, Loss: 86.1942, Train: 15.42%, Valid: 15.45%, Test: 15.28%
Epoch: 125, Loss: 14.2163, Train: 85.09%, Valid: 85.06%, Test: 85.21%
Epoch: 150, Loss: 103.2977, Train: 16.00%, Valid: 16.15%, Test: 15.88%
Epoch: 175, Loss: 123.7041, Train: 15.48%, Valid: 15.50%, Test: 15.34%
Epoch: 200, Loss: 101.4022, Train: 15.48%, Valid: 15.50%, Test: 15.34%
Epoch: 225, Loss: 127.0664, Train: 15.48%, Valid: 15.50%, Test: 15.34%
Epoch: 250, Loss: 109.7206, Train: 13.47%, Valid: 13.49%, Test: 13.40%
Epoch: 275, Loss: 126.4531, Train: 15.46%, Valid: 15.49%, Test: 15.33%
Epoch: 300, Loss: 125.4446, Train: 15.48%, Valid: 15.50%, Test: 15.35%
Epoch: 325, Loss: 124.1435, Train: 15.49%, Valid: 15.51%, Test: 15.36%
Epoch: 350, Loss: 123.6654, Train: 84.97%, Valid: 84.94%, Test: 85.11%
Epoch: 375, Loss: 17.0136, Train: 12.53%, Valid: 12.64%, Test: 12.50%
Epoch: 400, Loss: 362.1177, Train: 15.54%, Valid: 15.55%, Test: 15.41%
Epoch: 425, Loss: 288.2137, Train: 15.49%, Valid: 15.51%, Test: 15.35%
Epoch: 450, Loss: 296.5442, Train: 16.01%, Valid: 16.14%, Test: 15.89%
Epoch: 475, Loss: 159.3261, Train: 14.15%, Valid: 14.27%, Test: 14.08%
Run 01:
Highest Train: 88.03
Highest Valid: 88.09
  Final Train: 88.03
   Final Test: 88.07
All runs:
Highest Train: 88.03, nan
Highest Valid: 88.09, nan
  Final Train: 88.03, nan
   Final Test: 88.07, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.5044, Train: 15.37%, Valid: 15.44%, Test: 15.33%
Epoch: 25, Loss: 348.4973, Train: 85.22%, Valid: 85.15%, Test: 85.30%
Epoch: 50, Loss: 17985.8008, Train: 84.73%, Valid: 84.70%, Test: 84.84%
Epoch: 75, Loss: 318295.6250, Train: 85.36%, Valid: 85.23%, Test: 85.46%
Epoch: 100, Loss: 2.6086, Train: 86.77%, Valid: 86.75%, Test: 86.83%
Epoch: 125, Loss: 94.2751, Train: 86.04%, Valid: 85.95%, Test: 86.23%
Epoch: 150, Loss: 3.3232, Train: 86.75%, Valid: 86.74%, Test: 86.89%
Epoch: 175, Loss: 3.2086, Train: 86.79%, Valid: 86.77%, Test: 86.93%
Epoch: 200, Loss: 3.1967, Train: 86.03%, Valid: 85.94%, Test: 86.21%
Epoch: 225, Loss: 3.2433, Train: 86.87%, Valid: 86.81%, Test: 87.03%
Epoch: 250, Loss: 3.4695, Train: 86.53%, Valid: 86.47%, Test: 86.76%
Epoch: 275, Loss: 3.0440, Train: 86.23%, Valid: 86.16%, Test: 86.45%
Epoch: 300, Loss: 3.2209, Train: 85.30%, Valid: 85.15%, Test: 85.48%
Epoch: 325, Loss: 3.1595, Train: 86.33%, Valid: 86.26%, Test: 86.55%
Epoch: 350, Loss: 3.1606, Train: 86.78%, Valid: 86.77%, Test: 86.94%
Epoch: 375, Loss: 3.1393, Train: 86.59%, Valid: 86.62%, Test: 86.69%
Epoch: 400, Loss: 3.1642, Train: 86.61%, Valid: 86.62%, Test: 86.70%
Epoch: 425, Loss: 3.0602, Train: 86.80%, Valid: 86.78%, Test: 86.95%
Epoch: 450, Loss: 3.1354, Train: 85.66%, Valid: 85.53%, Test: 85.81%
Epoch: 475, Loss: 3.1548, Train: 86.79%, Valid: 86.78%, Test: 86.93%
Run 01:
Highest Train: 87.02
Highest Valid: 86.97
  Final Train: 87.02
   Final Test: 87.23
All runs:
Highest Train: 87.02, nan
Highest Valid: 86.97, nan
  Final Train: 87.02, nan
   Final Test: 87.23, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.6884, Train: 22.07%, Valid: 21.90%, Test: 21.73%
Epoch: 25, Loss: 1388.1254, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 31824.0156, Train: 84.15%, Valid: 84.18%, Test: 84.25%
Epoch: 75, Loss: 19.2194, Train: 15.16%, Valid: 15.38%, Test: 15.05%
Epoch: 100, Loss: 65.6185, Train: 15.14%, Valid: 15.35%, Test: 15.03%
Epoch: 125, Loss: 61.1454, Train: 15.14%, Valid: 15.33%, Test: 15.02%
Epoch: 150, Loss: 52.7829, Train: 13.05%, Valid: 12.97%, Test: 12.96%
Epoch: 175, Loss: 59.2985, Train: 12.83%, Valid: 12.77%, Test: 12.80%
Epoch: 200, Loss: 63.6547, Train: 12.81%, Valid: 12.75%, Test: 12.79%
Epoch: 225, Loss: 60.7273, Train: 12.83%, Valid: 12.77%, Test: 12.82%
Epoch: 250, Loss: 59.6569, Train: 12.86%, Valid: 12.80%, Test: 12.85%
Epoch: 275, Loss: 59.4295, Train: 12.87%, Valid: 12.82%, Test: 12.86%
Epoch: 300, Loss: 56.3448, Train: 12.87%, Valid: 12.82%, Test: 12.86%
Epoch: 325, Loss: 55.2418, Train: 12.91%, Valid: 12.88%, Test: 12.90%
Epoch: 350, Loss: 59.5208, Train: 12.92%, Valid: 12.89%, Test: 12.92%
Epoch: 375, Loss: 54.8520, Train: 12.90%, Valid: 12.86%, Test: 12.90%
Epoch: 400, Loss: 55.5938, Train: 12.94%, Valid: 12.91%, Test: 12.93%
Epoch: 425, Loss: 56.8464, Train: 12.92%, Valid: 12.89%, Test: 12.91%
Epoch: 450, Loss: 58.5303, Train: 12.92%, Valid: 12.89%, Test: 12.91%
Epoch: 475, Loss: 57.8481, Train: 12.93%, Valid: 12.90%, Test: 12.92%
Run 01:
Highest Train: 87.28
Highest Valid: 87.35
  Final Train: 87.28
   Final Test: 87.31
All runs:
Highest Train: 87.28, nan
Highest Valid: 87.35, nan
  Final Train: 87.28, nan
   Final Test: 87.31, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.5630, Train: 85.29%, Valid: 85.27%, Test: 85.44%
Epoch: 25, Loss: 10.8047, Train: 85.50%, Valid: 85.35%, Test: 85.67%
Epoch: 50, Loss: 9.2244, Train: 84.72%, Valid: 84.51%, Test: 84.85%
Epoch: 75, Loss: 9.2974, Train: 84.71%, Valid: 84.50%, Test: 84.84%
Epoch: 100, Loss: 8.9432, Train: 84.72%, Valid: 84.50%, Test: 84.84%
Epoch: 125, Loss: 8.9466, Train: 84.71%, Valid: 84.50%, Test: 84.83%
Epoch: 150, Loss: 9.8568, Train: 84.72%, Valid: 84.50%, Test: 84.84%
Epoch: 175, Loss: 9.4661, Train: 84.70%, Valid: 84.49%, Test: 84.82%
Epoch: 200, Loss: 7.8582, Train: 84.71%, Valid: 84.49%, Test: 84.83%
Epoch: 225, Loss: 9.3700, Train: 84.96%, Valid: 84.77%, Test: 85.10%
Epoch: 250, Loss: 7.7836, Train: 84.96%, Valid: 84.76%, Test: 85.09%
Epoch: 275, Loss: 8.1543, Train: 84.96%, Valid: 84.76%, Test: 85.09%
Epoch: 300, Loss: 10.0679, Train: 84.96%, Valid: 84.75%, Test: 85.08%
Epoch: 325, Loss: 8.4501, Train: 84.96%, Valid: 84.75%, Test: 85.08%
Epoch: 350, Loss: 8.5920, Train: 84.96%, Valid: 84.75%, Test: 85.09%
Epoch: 375, Loss: 8.5800, Train: 84.94%, Valid: 84.74%, Test: 85.06%
Epoch: 400, Loss: 7.4814, Train: 84.95%, Valid: 84.75%, Test: 85.07%
Epoch: 425, Loss: 9.3908, Train: 84.96%, Valid: 84.75%, Test: 85.08%
Epoch: 450, Loss: 9.1310, Train: 84.96%, Valid: 84.76%, Test: 85.09%
Epoch: 475, Loss: 8.3988, Train: 84.96%, Valid: 84.76%, Test: 85.09%
Run 01:
Highest Train: 87.23
Highest Valid: 87.28
  Final Train: 87.23
   Final Test: 87.34
All runs:
Highest Train: 87.23, nan
Highest Valid: 87.28, nan
  Final Train: 87.23, nan
   Final Test: 87.34, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.9472, Train: 14.25%, Valid: 14.28%, Test: 14.24%
Epoch: 25, Loss: 0.6688, Train: 86.67%, Valid: 86.62%, Test: 86.68%
Epoch: 50, Loss: 1.7897, Train: 86.63%, Valid: 86.57%, Test: 86.64%
Epoch: 75, Loss: 1.1980, Train: 86.39%, Valid: 86.36%, Test: 86.43%
Epoch: 100, Loss: 1.4350, Train: 86.46%, Valid: 86.42%, Test: 86.53%
Epoch: 125, Loss: 3.3619, Train: 85.82%, Valid: 85.73%, Test: 85.88%
Epoch: 150, Loss: 48370532352.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 394572704.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 4793471860736.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 10168441373720576.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 59703000351899648.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 12.7298, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 251001492115713694249391612755968.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 4420682671362273347035597897728.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 122533142655712753845860869079040.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.86
Highest Valid: 86.84
  Final Train: 86.86
   Final Test: 86.88
All runs:
Highest Train: 86.86, nan
Highest Valid: 86.84, nan
  Final Train: 86.86, nan
   Final Test: 86.88, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.2672, Train: 84.16%, Valid: 84.12%, Test: 84.16%
Epoch: 25, Loss: 43.5517, Train: 83.18%, Valid: 83.14%, Test: 83.31%
Epoch: 50, Loss: 24009930440704.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 72153189976893292544.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 17598683058624528384.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 15737098822125355008.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 938856692441939968.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 1912040825097814016.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 559285809446912.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 10577068762740031488.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 39866201014272.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 7439100252520448.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 250806951712129024.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 1911181694199660544.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 3736802966938583040.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 129444747474068570112.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 315403259843969024.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 6168954023660486656.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 23311401002991616.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 4426833924542431232.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.97
Highest Valid: 86.74
  Final Train: 86.97
   Final Test: 86.99
All runs:
Highest Train: 86.97, nan
Highest Valid: 86.74, nan
  Final Train: 86.97, nan
   Final Test: 86.99, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.8312, Train: 86.07%, Valid: 86.00%, Test: 86.08%
Epoch: 25, Loss: 24.7382, Train: 17.15%, Valid: 17.01%, Test: 17.06%
Epoch: 50, Loss: 24.8244, Train: 83.54%, Valid: 83.39%, Test: 83.56%
Epoch: 75, Loss: 23.8706, Train: 83.37%, Valid: 83.23%, Test: 83.39%
Epoch: 100, Loss: 22.0224, Train: 32.27%, Valid: 31.89%, Test: 31.86%
Epoch: 125, Loss: 23.7425, Train: 17.38%, Valid: 17.22%, Test: 17.26%
Epoch: 150, Loss: 23.2894, Train: 14.72%, Valid: 14.54%, Test: 14.60%
Epoch: 175, Loss: 24.0826, Train: 13.78%, Valid: 13.62%, Test: 13.65%
Epoch: 200, Loss: 25.1433, Train: 22.20%, Valid: 21.96%, Test: 21.89%
Epoch: 225, Loss: 21.1331, Train: 83.18%, Valid: 83.02%, Test: 83.19%
Epoch: 250, Loss: 23.4434, Train: 83.17%, Valid: 82.98%, Test: 83.15%
Epoch: 275, Loss: 25.5271, Train: 14.14%, Valid: 13.96%, Test: 14.02%
Epoch: 300, Loss: 29.8795, Train: 14.56%, Valid: 14.38%, Test: 14.42%
Epoch: 325, Loss: 22.6531, Train: 83.58%, Valid: 83.42%, Test: 83.59%
Epoch: 350, Loss: 32.1378, Train: 16.05%, Valid: 15.85%, Test: 15.89%
Epoch: 375, Loss: 21.6902, Train: 14.39%, Valid: 14.20%, Test: 14.25%
Epoch: 400, Loss: 29.4863, Train: 13.99%, Valid: 13.81%, Test: 13.87%
Epoch: 425, Loss: 25.5981, Train: 18.56%, Valid: 18.41%, Test: 18.42%
Epoch: 450, Loss: 25.4016, Train: 83.25%, Valid: 83.08%, Test: 83.27%
Epoch: 475, Loss: 23.9382, Train: 17.76%, Valid: 17.59%, Test: 17.62%
Run 01:
Highest Train: 86.07
Highest Valid: 86.00
  Final Train: 86.07
   Final Test: 86.08
All runs:
Highest Train: 86.07, nan
Highest Valid: 86.00, nan
  Final Train: 86.07, nan
   Final Test: 86.08, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.1537, Train: 86.03%, Valid: 85.97%, Test: 86.02%
Epoch: 25, Loss: 352901.7188, Train: 85.78%, Valid: 85.65%, Test: 85.89%
Epoch: 50, Loss: 48205176.0000, Train: 49.99%, Valid: 49.99%, Test: 49.99%
Epoch: 75, Loss: 1.5305, Train: 86.81%, Valid: 86.82%, Test: 86.86%
Epoch: 100, Loss: 1.3563, Train: 85.67%, Valid: 85.50%, Test: 85.74%
Epoch: 125, Loss: 1.0713, Train: 85.67%, Valid: 85.50%, Test: 85.74%
Epoch: 150, Loss: 0.9517, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 1.3219, Train: 85.67%, Valid: 85.50%, Test: 85.74%
Epoch: 200, Loss: 1.2046, Train: 85.67%, Valid: 85.50%, Test: 85.74%
Epoch: 225, Loss: 1.2524, Train: 85.68%, Valid: 85.52%, Test: 85.77%
Epoch: 250, Loss: 2359.4167, Train: 86.89%, Valid: 86.89%, Test: 86.95%
Epoch: 275, Loss: 1.4085, Train: 85.67%, Valid: 85.50%, Test: 85.74%
Epoch: 300, Loss: 1.2064, Train: 85.69%, Valid: 85.52%, Test: 85.79%
Epoch: 325, Loss: 1.2494, Train: 16.07%, Valid: 16.11%, Test: 15.97%
Epoch: 350, Loss: 1.2930, Train: 85.67%, Valid: 85.50%, Test: 85.74%
Epoch: 375, Loss: 1.0609, Train: 85.67%, Valid: 85.50%, Test: 85.74%
Epoch: 400, Loss: 1.1644, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 15.1369, Train: 85.67%, Valid: 85.50%, Test: 85.74%
Epoch: 450, Loss: 1.3026, Train: 85.67%, Valid: 85.50%, Test: 85.74%
Epoch: 475, Loss: 1.3226, Train: 85.67%, Valid: 85.50%, Test: 85.74%
Run 01:
Highest Train: 87.43
Highest Valid: 87.32
  Final Train: 87.43
   Final Test: 87.51
All runs:
Highest Train: 87.43, nan
Highest Valid: 87.32, nan
  Final Train: 87.43, nan
   Final Test: 87.51, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.4528, Train: 85.63%, Valid: 85.48%, Test: 85.71%
Epoch: 25, Loss: 3.5063, Train: 85.37%, Valid: 85.20%, Test: 85.43%
Epoch: 50, Loss: nan, Train: 50.01%, Valid: 50.01%, Test: 50.00%
Epoch: 75, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 85.63
Highest Valid: 85.48
  Final Train: 85.63
   Final Test: 85.71
All runs:
Highest Train: 85.63, nan
Highest Valid: 85.48, nan
  Final Train: 85.63, nan
   Final Test: 85.71, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.5443, Train: 86.12%, Valid: 86.06%, Test: 86.22%
Epoch: 25, Loss: 20842.8457, Train: 16.05%, Valid: 16.16%, Test: 15.94%
Epoch: 50, Loss: 185128.7500, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 15.1299, Train: 15.74%, Valid: 15.91%, Test: 15.63%
Epoch: 100, Loss: 20661.1348, Train: 85.11%, Valid: 85.11%, Test: 85.25%
Epoch: 125, Loss: 7.2613, Train: 84.19%, Valid: 84.01%, Test: 84.29%
Epoch: 150, Loss: 4.9415, Train: 85.08%, Valid: 85.02%, Test: 85.18%
Epoch: 175, Loss: 5.8514, Train: 84.19%, Valid: 84.01%, Test: 84.29%
Epoch: 200, Loss: 5.6474, Train: 84.27%, Valid: 84.06%, Test: 84.34%
Epoch: 225, Loss: 5.7659, Train: 84.10%, Valid: 83.90%, Test: 84.21%
Epoch: 250, Loss: 6.2681, Train: 84.10%, Valid: 83.89%, Test: 84.20%
Epoch: 275, Loss: 5.2177, Train: 84.46%, Valid: 84.23%, Test: 84.52%
Epoch: 300, Loss: 6.1826, Train: 85.04%, Valid: 84.99%, Test: 85.15%
Epoch: 325, Loss: 5.7486, Train: 84.61%, Valid: 84.37%, Test: 84.66%
Epoch: 350, Loss: 6.2983, Train: 84.13%, Valid: 83.89%, Test: 84.23%
Epoch: 375, Loss: 5.4647, Train: 84.27%, Valid: 84.06%, Test: 84.34%
Epoch: 400, Loss: 5.4820, Train: 84.63%, Valid: 84.39%, Test: 84.68%
Epoch: 425, Loss: 5.0812, Train: 85.03%, Valid: 84.99%, Test: 85.15%
Epoch: 450, Loss: 5.4456, Train: 84.72%, Valid: 84.46%, Test: 84.78%
Epoch: 475, Loss: 5.1521, Train: 84.62%, Valid: 84.38%, Test: 84.68%
Run 01:
Highest Train: 88.44
Highest Valid: 88.50
  Final Train: 88.44
   Final Test: 88.47
All runs:
Highest Train: 88.44, nan
Highest Valid: 88.50, nan
  Final Train: 88.44, nan
   Final Test: 88.47, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6070, Train: 86.44%, Valid: 86.47%, Test: 86.45%
Epoch: 25, Loss: 161.4445, Train: 85.56%, Valid: 85.52%, Test: 85.64%
Epoch: 50, Loss: 193.8584, Train: 16.42%, Valid: 16.60%, Test: 16.43%
Epoch: 75, Loss: 206.0799, Train: 15.71%, Valid: 15.81%, Test: 15.68%
Epoch: 100, Loss: 208.7972, Train: 15.72%, Valid: 15.81%, Test: 15.68%
Epoch: 125, Loss: 207.3309, Train: 15.70%, Valid: 15.79%, Test: 15.65%
Epoch: 150, Loss: 207.0498, Train: 15.71%, Valid: 15.81%, Test: 15.67%
Epoch: 175, Loss: 207.8265, Train: 15.71%, Valid: 15.81%, Test: 15.67%
Epoch: 200, Loss: 208.3721, Train: 15.71%, Valid: 15.80%, Test: 15.67%
Epoch: 225, Loss: 206.8146, Train: 15.71%, Valid: 15.80%, Test: 15.66%
Epoch: 250, Loss: 208.4967, Train: 15.71%, Valid: 15.80%, Test: 15.66%
Epoch: 275, Loss: 207.4029, Train: 15.71%, Valid: 15.80%, Test: 15.66%
Epoch: 300, Loss: 207.5828, Train: 15.71%, Valid: 15.81%, Test: 15.67%
Epoch: 325, Loss: 207.5484, Train: 15.72%, Valid: 15.81%, Test: 15.68%
Epoch: 350, Loss: 207.2659, Train: 15.73%, Valid: 15.83%, Test: 15.69%
Epoch: 375, Loss: 207.3880, Train: 15.74%, Valid: 15.84%, Test: 15.70%
Epoch: 400, Loss: 207.3013, Train: 15.77%, Valid: 15.87%, Test: 15.73%
Epoch: 425, Loss: 207.3171, Train: 15.78%, Valid: 15.88%, Test: 15.75%
Epoch: 450, Loss: 207.2821, Train: 15.79%, Valid: 15.90%, Test: 15.76%
Epoch: 475, Loss: 202.4197, Train: 15.78%, Valid: 15.79%, Test: 15.72%
Run 01:
Highest Train: 87.24
Highest Valid: 87.26
  Final Train: 87.24
   Final Test: 87.29
All runs:
Highest Train: 87.24, nan
Highest Valid: 87.26, nan
  Final Train: 87.24, nan
   Final Test: 87.29, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 5.7149, Train: 81.86%, Valid: 81.89%, Test: 82.08%
Epoch: 25, Loss: 170.6117, Train: 14.95%, Valid: 15.02%, Test: 14.79%
Epoch: 50, Loss: 124.0560, Train: 11.59%, Valid: 11.64%, Test: 11.66%
Epoch: 75, Loss: 4260.0024, Train: 15.45%, Valid: 15.45%, Test: 15.32%
Epoch: 100, Loss: 296.0771, Train: 15.59%, Valid: 15.58%, Test: 15.47%
Epoch: 125, Loss: 18.8979, Train: 85.09%, Valid: 84.90%, Test: 85.20%
Epoch: 150, Loss: 248609.1875, Train: 83.19%, Valid: 83.13%, Test: 83.36%
Epoch: 175, Loss: 6996.6440, Train: 84.34%, Valid: 84.32%, Test: 84.45%
Epoch: 200, Loss: 63.3020, Train: 15.05%, Valid: 15.19%, Test: 14.88%
Epoch: 225, Loss: 1783.0181, Train: 15.83%, Valid: 15.98%, Test: 15.71%
Epoch: 250, Loss: 1174.0399, Train: 16.72%, Valid: 16.72%, Test: 16.61%
Epoch: 275, Loss: 2313.8884, Train: 16.43%, Valid: 16.47%, Test: 16.33%
Epoch: 300, Loss: 1562.7152, Train: 15.75%, Valid: 15.91%, Test: 15.64%
Epoch: 325, Loss: 1330.7931, Train: 84.81%, Valid: 84.64%, Test: 84.95%
Epoch: 350, Loss: 1211.5548, Train: 16.64%, Valid: 16.66%, Test: 16.55%
Epoch: 375, Loss: 8719.5977, Train: 16.82%, Valid: 16.88%, Test: 16.68%
Epoch: 400, Loss: 1308.5558, Train: 30.17%, Valid: 30.00%, Test: 29.82%
Epoch: 425, Loss: 35.0828, Train: 16.31%, Valid: 16.34%, Test: 16.20%
Epoch: 450, Loss: 1585.0472, Train: 15.68%, Valid: 15.82%, Test: 15.58%
Epoch: 475, Loss: 1120.9731, Train: 14.87%, Valid: 14.88%, Test: 14.79%
Run 01:
Highest Train: 88.27
Highest Valid: 88.21
  Final Train: 88.27
   Final Test: 88.25
All runs:
Highest Train: 88.27, nan
Highest Valid: 88.21, nan
  Final Train: 88.27, nan
   Final Test: 88.25, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 9.4654, Train: 85.19%, Valid: 84.98%, Test: 85.28%
Epoch: 25, Loss: 5.1488, Train: 85.79%, Valid: 85.83%, Test: 85.88%
Epoch: 50, Loss: 2.3649, Train: 86.51%, Valid: 86.53%, Test: 86.57%
Epoch: 75, Loss: 9.2451, Train: 86.03%, Valid: 86.11%, Test: 86.13%
Epoch: 100, Loss: 16.6513, Train: 85.97%, Valid: 86.07%, Test: 86.08%
Epoch: 125, Loss: 24830488793367707648.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 20323.9980, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 7954733530212796511289344.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 2857664519244839886744715264.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 98270475354968435244144401580032.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 88.11
Highest Valid: 88.17
  Final Train: 88.11
   Final Test: 88.14
All runs:
Highest Train: 88.11, nan
Highest Valid: 88.17, nan
  Final Train: 88.11, nan
   Final Test: 88.14, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 4.2729, Train: 85.64%, Valid: 85.56%, Test: 85.65%
Epoch: 25, Loss: 2.4471, Train: 86.94%, Valid: 86.96%, Test: 86.98%
Epoch: 50, Loss: 68124512.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 128.9721, Train: 84.12%, Valid: 83.88%, Test: 84.18%
Epoch: 100, Loss: 395941.8750, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 3393.6003, Train: 85.43%, Valid: 85.34%, Test: 85.57%
Epoch: 150, Loss: 2.7470, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 677.0894, Train: 15.53%, Valid: 15.64%, Test: 15.40%
Epoch: 200, Loss: 3.6098, Train: 85.01%, Valid: 84.89%, Test: 85.14%
Epoch: 225, Loss: 21797688.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 422390568357497667584.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 133712494592.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 248719589376.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 567359176704.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 29745301422080.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 1278883659776.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 2911781519360.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 26162891849728.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 177129635840.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 416265764864.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 87.11
Highest Valid: 87.08
  Final Train: 87.02
   Final Test: 87.08
All runs:
Highest Train: 87.11, nan
Highest Valid: 87.08, nan
  Final Train: 87.02, nan
   Final Test: 87.08, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.8244, Train: 85.14%, Valid: 84.97%, Test: 85.20%
Epoch: 25, Loss: 47749.6484, Train: 50.01%, Valid: 50.01%, Test: 50.00%
Epoch: 50, Loss: 284.7071, Train: 15.78%, Valid: 15.95%, Test: 15.69%
Epoch: 75, Loss: 336.2100, Train: 15.76%, Valid: 15.93%, Test: 15.69%
Epoch: 100, Loss: 332.9807, Train: 15.76%, Valid: 15.93%, Test: 15.69%
Epoch: 125, Loss: 360.3425, Train: 15.76%, Valid: 15.93%, Test: 15.69%
Epoch: 150, Loss: 359.8922, Train: 15.76%, Valid: 15.93%, Test: 15.69%
Epoch: 175, Loss: 330.5023, Train: 15.76%, Valid: 15.93%, Test: 15.69%
Epoch: 200, Loss: 347.0092, Train: 15.76%, Valid: 15.93%, Test: 15.69%
Epoch: 225, Loss: 355.3031, Train: 15.75%, Valid: 15.92%, Test: 15.67%
Epoch: 250, Loss: 359.6546, Train: 15.76%, Valid: 15.93%, Test: 15.69%
Epoch: 275, Loss: 332.3128, Train: 15.75%, Valid: 15.92%, Test: 15.68%
Epoch: 300, Loss: 316.6891, Train: 15.76%, Valid: 15.93%, Test: 15.68%
Epoch: 325, Loss: 361.4498, Train: 15.76%, Valid: 15.93%, Test: 15.68%
Epoch: 350, Loss: 336.5200, Train: 15.75%, Valid: 15.92%, Test: 15.68%
Epoch: 375, Loss: 314.4647, Train: 15.76%, Valid: 15.93%, Test: 15.69%
Epoch: 400, Loss: 359.2536, Train: 15.75%, Valid: 15.92%, Test: 15.68%
Epoch: 425, Loss: 317.3120, Train: 15.76%, Valid: 15.93%, Test: 15.69%
Epoch: 450, Loss: 352.5120, Train: 15.75%, Valid: 15.92%, Test: 15.68%
Epoch: 475, Loss: 339.7541, Train: 15.75%, Valid: 15.92%, Test: 15.68%
Run 01:
Highest Train: 87.29
Highest Valid: 87.22
  Final Train: 87.29
   Final Test: 87.36
All runs:
Highest Train: 87.29, nan
Highest Valid: 87.22, nan
  Final Train: 87.29, nan
   Final Test: 87.36, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.4431, Train: 86.76%, Valid: 86.80%, Test: 86.75%
Epoch: 25, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 974013136896.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 5062996564944985849331712.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 6703411028349668554954506240.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 21908080506375825424524509184.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 20.8492, Train: 87.38%, Valid: 87.47%, Test: 87.39%
Epoch: 175, Loss: 10.2771, Train: 86.87%, Valid: 86.96%, Test: 86.87%
Epoch: 200, Loss: 9.2866, Train: 86.83%, Valid: 86.92%, Test: 86.84%
Epoch: 225, Loss: 9.4598, Train: 86.83%, Valid: 86.92%, Test: 86.84%
Epoch: 250, Loss: 9.2969, Train: 86.83%, Valid: 86.92%, Test: 86.84%
Epoch: 275, Loss: 9.6288, Train: 86.83%, Valid: 86.92%, Test: 86.84%
Epoch: 300, Loss: 9.4684, Train: 86.83%, Valid: 86.92%, Test: 86.84%
Epoch: 325, Loss: 9.4188, Train: 86.83%, Valid: 86.92%, Test: 86.84%
Epoch: 350, Loss: 9.0665, Train: 86.83%, Valid: 86.92%, Test: 86.84%
Epoch: 375, Loss: 9.1587, Train: 86.83%, Valid: 86.92%, Test: 86.84%
Epoch: 400, Loss: 9.3262, Train: 86.83%, Valid: 86.92%, Test: 86.84%
Epoch: 425, Loss: 9.9744, Train: 86.83%, Valid: 86.92%, Test: 86.84%
Epoch: 450, Loss: 9.1884, Train: 86.83%, Valid: 86.92%, Test: 86.84%
Epoch: 475, Loss: 9.2093, Train: 86.83%, Valid: 86.92%, Test: 86.84%
Run 01:
Highest Train: 87.47
Highest Valid: 87.52
  Final Train: 87.47
   Final Test: 87.49
All runs:
Highest Train: 87.47, nan
Highest Valid: 87.52, nan
  Final Train: 87.47, nan
   Final Test: 87.49, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.3349, Train: 86.06%, Valid: 85.95%, Test: 86.12%
Epoch: 25, Loss: 140133.7656, Train: 49.98%, Valid: 49.98%, Test: 49.98%
Epoch: 50, Loss: 362855571316667691772019736576.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 4973.5674, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.64
Highest Valid: 86.58
  Final Train: 86.64
   Final Test: 86.72
All runs:
Highest Train: 86.64, nan
Highest Valid: 86.58, nan
  Final Train: 86.64, nan
   Final Test: 86.72, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 5.3769, Train: 84.77%, Valid: 84.72%, Test: 84.87%
Epoch: 25, Loss: 41721669338312250256271035334656.0000, Train: 50.00%, Valid: 49.99%, Test: 50.01%
Epoch: 50, Loss: 4432074746716225536.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 274320.0000, Train: 85.39%, Valid: 85.13%, Test: 85.41%
Epoch: 100, Loss: 2212349473919598592.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 6854649769643734990848.0000, Train: 84.14%, Valid: 83.98%, Test: 84.16%
Epoch: 150, Loss: 176883983450112.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 2107109881675776.0000, Train: 16.35%, Valid: 16.30%, Test: 16.20%
Epoch: 200, Loss: 238501949865984.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 46991306179890642944.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 37375333725129146368.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 1509526515463716077568.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 584894417731584.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 197589.0938, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 959314289900979748864.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 15299027927040.0000, Train: 84.86%, Valid: 84.67%, Test: 84.89%
Epoch: 400, Loss: 246.6469, Train: 84.64%, Valid: 84.45%, Test: 84.64%
Epoch: 425, Loss: 781981712384.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 614581713791614976.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 158495.2812, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.40
Highest Valid: 86.29
  Final Train: 86.40
   Final Test: 86.48
All runs:
Highest Train: 86.40, nan
Highest Valid: 86.29, nan
  Final Train: 86.40, nan
   Final Test: 86.48, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 6.2713, Train: 88.17%, Valid: 88.26%, Test: 88.22%
Epoch: 25, Loss: 2656.2065, Train: 15.07%, Valid: 15.08%, Test: 14.92%
Epoch: 50, Loss: 111.1806, Train: 88.03%, Valid: 88.08%, Test: 88.05%
Epoch: 75, Loss: 194.9928, Train: 88.07%, Valid: 88.10%, Test: 88.07%
Epoch: 100, Loss: 189.4734, Train: 88.07%, Valid: 88.10%, Test: 88.08%
Epoch: 125, Loss: 193.6659, Train: 88.08%, Valid: 88.13%, Test: 88.09%
Epoch: 150, Loss: 263.1122, Train: 87.99%, Valid: 88.00%, Test: 87.99%
Epoch: 175, Loss: 294.1523, Train: 87.94%, Valid: 87.95%, Test: 87.93%
Epoch: 200, Loss: 306.5989, Train: 87.91%, Valid: 87.92%, Test: 87.90%
Epoch: 225, Loss: 300.9107, Train: 87.91%, Valid: 87.92%, Test: 87.90%
Epoch: 250, Loss: 296.3236, Train: 87.91%, Valid: 87.92%, Test: 87.89%
Epoch: 275, Loss: 303.3731, Train: 87.90%, Valid: 87.91%, Test: 87.89%
Epoch: 300, Loss: 303.6010, Train: 87.91%, Valid: 87.92%, Test: 87.90%
Epoch: 325, Loss: 295.6020, Train: 87.90%, Valid: 87.92%, Test: 87.90%
Epoch: 350, Loss: 299.8876, Train: 87.90%, Valid: 87.91%, Test: 87.89%
Epoch: 375, Loss: 299.5536, Train: 87.90%, Valid: 87.92%, Test: 87.89%
Epoch: 400, Loss: 299.0840, Train: 87.90%, Valid: 87.91%, Test: 87.89%
Epoch: 425, Loss: 302.7660, Train: 87.90%, Valid: 87.91%, Test: 87.89%
Epoch: 450, Loss: 299.7640, Train: 87.92%, Valid: 87.93%, Test: 87.92%
Epoch: 475, Loss: 304.5825, Train: 87.91%, Valid: 87.93%, Test: 87.92%
Run 01:
Highest Train: 88.29
Highest Valid: 88.34
  Final Train: 88.28
   Final Test: 88.30
All runs:
Highest Train: 88.29, nan
Highest Valid: 88.34, nan
  Final Train: 88.28, nan
   Final Test: 88.30, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 3.0443, Train: 85.33%, Valid: 85.29%, Test: 85.45%
Epoch: 25, Loss: 5.8443, Train: 85.57%, Valid: 85.40%, Test: 85.66%
Epoch: 50, Loss: 16.4804, Train: 16.85%, Valid: 16.88%, Test: 16.76%
Epoch: 75, Loss: 15.7827, Train: 84.77%, Valid: 84.62%, Test: 84.95%
Epoch: 100, Loss: 5438.3750, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 976.7056, Train: 83.14%, Valid: 83.07%, Test: 83.27%
Epoch: 150, Loss: 45.8402, Train: 83.96%, Valid: 83.78%, Test: 84.12%
Epoch: 175, Loss: 59.5469, Train: 83.37%, Valid: 83.27%, Test: 83.52%
Epoch: 200, Loss: 40.2299, Train: 83.36%, Valid: 83.26%, Test: 83.51%
Epoch: 225, Loss: 44.7104, Train: 83.45%, Valid: 83.33%, Test: 83.61%
Epoch: 250, Loss: 43.1795, Train: 84.08%, Valid: 83.90%, Test: 84.26%
Epoch: 275, Loss: 48.2565, Train: 83.46%, Valid: 83.34%, Test: 83.62%
Epoch: 300, Loss: 48.5243, Train: 83.54%, Valid: 83.42%, Test: 83.72%
Epoch: 325, Loss: 47.3703, Train: 83.42%, Valid: 83.31%, Test: 83.57%
Epoch: 350, Loss: 52.7059, Train: 83.36%, Valid: 83.26%, Test: 83.52%
Epoch: 375, Loss: 41.9326, Train: 83.35%, Valid: 83.26%, Test: 83.51%
Epoch: 400, Loss: 42.3719, Train: 83.14%, Valid: 83.05%, Test: 83.26%
Epoch: 425, Loss: 49.3453, Train: 83.58%, Valid: 83.45%, Test: 83.76%
Epoch: 450, Loss: 49.3370, Train: 83.09%, Valid: 83.00%, Test: 83.21%
Epoch: 475, Loss: 48.1683, Train: 83.44%, Valid: 83.32%, Test: 83.59%
Run 01:
Highest Train: 86.41
Highest Valid: 86.36
  Final Train: 86.41
   Final Test: 86.50
All runs:
Highest Train: 86.41, nan
Highest Valid: 86.36, nan
  Final Train: 86.41, nan
   Final Test: 86.50, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 5.5408, Train: 84.75%, Valid: 84.64%, Test: 84.80%
Epoch: 25, Loss: 3.6942, Train: 86.31%, Valid: 86.19%, Test: 86.30%
Epoch: 50, Loss: 9.9313, Train: 85.16%, Valid: 84.95%, Test: 85.23%
Epoch: 75, Loss: 16.3313, Train: 84.84%, Valid: 84.72%, Test: 84.93%
Epoch: 100, Loss: 16.0603, Train: 84.77%, Valid: 84.63%, Test: 84.86%
Epoch: 125, Loss: 15.9604, Train: 84.76%, Valid: 84.63%, Test: 84.85%
Epoch: 150, Loss: 15.9801, Train: 84.76%, Valid: 84.62%, Test: 84.85%
Epoch: 175, Loss: 16.0212, Train: 84.74%, Valid: 84.59%, Test: 84.82%
Epoch: 200, Loss: 16.0095, Train: 84.73%, Valid: 84.58%, Test: 84.82%
Epoch: 225, Loss: 15.9433, Train: 84.74%, Valid: 84.59%, Test: 84.83%
Epoch: 250, Loss: 15.8387, Train: 84.85%, Valid: 84.70%, Test: 84.95%
Epoch: 275, Loss: 15.4938, Train: 84.86%, Valid: 84.72%, Test: 84.96%
Epoch: 300, Loss: 15.9355, Train: 84.78%, Valid: 84.64%, Test: 84.87%
Epoch: 325, Loss: 15.9194, Train: 84.78%, Valid: 84.63%, Test: 84.86%
Epoch: 350, Loss: 15.8914, Train: 84.78%, Valid: 84.63%, Test: 84.87%
Epoch: 375, Loss: 15.8902, Train: 84.79%, Valid: 84.64%, Test: 84.87%
Epoch: 400, Loss: 15.8684, Train: 84.80%, Valid: 84.65%, Test: 84.87%
Epoch: 425, Loss: 15.8508, Train: 84.80%, Valid: 84.65%, Test: 84.88%
Epoch: 450, Loss: 15.8332, Train: 84.79%, Valid: 84.64%, Test: 84.87%
Epoch: 475, Loss: 15.4577, Train: 83.72%, Valid: 83.78%, Test: 83.88%
Run 01:
Highest Train: 86.97
Highest Valid: 86.98
  Final Train: 86.97
   Final Test: 87.06
All runs:
Highest Train: 86.97, nan
Highest Valid: 86.98, nan
  Final Train: 86.97, nan
   Final Test: 87.06, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 6.9457, Train: 85.87%, Valid: 85.70%, Test: 85.94%
Epoch: 25, Loss: 2.2609, Train: 86.08%, Valid: 86.13%, Test: 86.11%
Epoch: 50, Loss: 7.0505, Train: 85.17%, Valid: 85.18%, Test: 85.28%
Epoch: 75, Loss: 18.1801, Train: 84.64%, Valid: 84.65%, Test: 84.77%
Epoch: 100, Loss: 25.1977, Train: 84.55%, Valid: 84.57%, Test: 84.69%
Epoch: 125, Loss: 1736.2594, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 20.7829, Train: 84.43%, Valid: 84.38%, Test: 84.55%
Epoch: 175, Loss: 44.7641, Train: 83.49%, Valid: 83.33%, Test: 83.63%
Epoch: 200, Loss: 169.9675, Train: 83.50%, Valid: 83.32%, Test: 83.63%
Epoch: 225, Loss: 197.0604, Train: 83.51%, Valid: 83.33%, Test: 83.63%
Epoch: 250, Loss: 204.1081, Train: 83.51%, Valid: 83.33%, Test: 83.63%
Epoch: 275, Loss: 199.1189, Train: 83.51%, Valid: 83.32%, Test: 83.63%
Epoch: 300, Loss: 199.3968, Train: 83.51%, Valid: 83.32%, Test: 83.63%
Epoch: 325, Loss: 200.4409, Train: 83.51%, Valid: 83.33%, Test: 83.63%
Epoch: 350, Loss: 200.2935, Train: 83.51%, Valid: 83.32%, Test: 83.63%
Epoch: 375, Loss: 201.7337, Train: 83.51%, Valid: 83.33%, Test: 83.63%
Epoch: 400, Loss: 200.3669, Train: 83.51%, Valid: 83.32%, Test: 83.63%
Epoch: 425, Loss: 200.4000, Train: 83.51%, Valid: 83.32%, Test: 83.63%
Epoch: 450, Loss: 198.3757, Train: 83.51%, Valid: 83.33%, Test: 83.63%
Epoch: 475, Loss: 200.9694, Train: 83.51%, Valid: 83.33%, Test: 83.63%
Run 01:
Highest Train: 86.80
Highest Valid: 86.87
  Final Train: 86.80
   Final Test: 86.84
All runs:
Highest Train: 86.80, nan
Highest Valid: 86.87, nan
  Final Train: 86.80, nan
   Final Test: 86.84, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 13.1221, Train: 86.31%, Valid: 86.20%, Test: 86.41%
Epoch: 25, Loss: 1.7124, Train: 85.53%, Valid: 85.51%, Test: 85.59%
Epoch: 50, Loss: 5.0575, Train: 85.99%, Valid: 85.86%, Test: 85.94%
Epoch: 75, Loss: 6.1014, Train: 85.96%, Valid: 85.84%, Test: 85.94%
Epoch: 100, Loss: 3.6626, Train: 85.44%, Valid: 85.32%, Test: 85.52%
Epoch: 125, Loss: 1.0216, Train: 86.24%, Valid: 86.16%, Test: 86.28%
Epoch: 150, Loss: 0.4621, Train: 85.63%, Valid: 85.53%, Test: 85.63%
Epoch: 175, Loss: 0.3969, Train: 85.90%, Valid: 85.79%, Test: 85.99%
Epoch: 200, Loss: 0.3767, Train: 85.97%, Valid: 85.86%, Test: 86.05%
Epoch: 225, Loss: 0.9238, Train: 85.69%, Valid: 85.60%, Test: 85.79%
Epoch: 250, Loss: 0.5240, Train: 85.96%, Valid: 85.85%, Test: 86.08%
Epoch: 275, Loss: 0.3942, Train: 85.92%, Valid: 85.80%, Test: 86.03%
Epoch: 300, Loss: 0.8321, Train: 85.88%, Valid: 85.76%, Test: 85.99%
Epoch: 325, Loss: 0.6842, Train: 85.74%, Valid: 85.60%, Test: 85.84%
Epoch: 350, Loss: 1.3669, Train: 85.86%, Valid: 85.73%, Test: 85.97%
Epoch: 375, Loss: 0.4017, Train: 86.17%, Valid: 86.01%, Test: 86.29%
Epoch: 400, Loss: 0.3716, Train: 86.07%, Valid: 85.91%, Test: 86.19%
Epoch: 425, Loss: 0.3463, Train: 85.90%, Valid: 85.73%, Test: 86.02%
Epoch: 450, Loss: 2.0195, Train: 86.16%, Valid: 86.21%, Test: 86.27%
Epoch: 475, Loss: 1.2535, Train: 86.04%, Valid: 86.08%, Test: 86.16%
Run 01:
Highest Train: 87.49
Highest Valid: 87.42
  Final Train: 87.49
   Final Test: 87.55
All runs:
Highest Train: 87.49, nan
Highest Valid: 87.42, nan
  Final Train: 87.49, nan
   Final Test: 87.55, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.9329, Train: 85.86%, Valid: 85.75%, Test: 85.99%
Epoch: 25, Loss: 2.1401, Train: 85.90%, Valid: 85.72%, Test: 85.96%
Epoch: 50, Loss: 2.8132, Train: 86.00%, Valid: 85.88%, Test: 86.09%
Epoch: 75, Loss: 1.2481, Train: 86.01%, Valid: 85.89%, Test: 86.07%
Epoch: 100, Loss: 0.4037, Train: 85.46%, Valid: 85.29%, Test: 85.54%
Epoch: 125, Loss: 0.3446, Train: 85.72%, Valid: 85.53%, Test: 85.78%
Epoch: 150, Loss: 0.3998, Train: 85.61%, Valid: 85.43%, Test: 85.67%
Epoch: 175, Loss: 0.3412, Train: 85.76%, Valid: 85.58%, Test: 85.84%
Epoch: 200, Loss: 0.3295, Train: 85.70%, Valid: 85.51%, Test: 85.79%
Epoch: 225, Loss: 0.3607, Train: 85.78%, Valid: 85.59%, Test: 85.86%
Epoch: 250, Loss: 0.3437, Train: 85.97%, Valid: 85.77%, Test: 86.05%
Epoch: 275, Loss: 0.3290, Train: 87.04%, Valid: 86.91%, Test: 87.17%
Epoch: 300, Loss: 0.3538, Train: 86.52%, Valid: 86.48%, Test: 86.64%
Epoch: 325, Loss: 0.3385, Train: 85.59%, Valid: 85.38%, Test: 85.67%
Epoch: 350, Loss: 0.4621, Train: 87.34%, Valid: 87.24%, Test: 87.31%
Epoch: 375, Loss: 0.3503, Train: 86.04%, Valid: 85.85%, Test: 86.13%
Epoch: 400, Loss: 0.3352, Train: 86.05%, Valid: 85.85%, Test: 86.14%
Epoch: 425, Loss: 0.3263, Train: 86.21%, Valid: 86.04%, Test: 86.33%
Epoch: 450, Loss: 0.3393, Train: 87.12%, Valid: 87.00%, Test: 87.29%
Epoch: 475, Loss: 0.3351, Train: 86.11%, Valid: 85.90%, Test: 86.19%
Run 01:
Highest Train: 87.38
Highest Valid: 87.24
  Final Train: 87.34
   Final Test: 87.31
All runs:
Highest Train: 87.38, nan
Highest Valid: 87.24, nan
  Final Train: 87.34, nan
   Final Test: 87.31, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 3.0958, Train: 86.63%, Valid: 86.40%, Test: 86.73%
Epoch: 25, Loss: 3.0329, Train: 88.54%, Valid: 88.54%, Test: 88.53%
Epoch: 50, Loss: 5.3302, Train: 88.41%, Valid: 88.42%, Test: 88.39%
Epoch: 75, Loss: 5.5684, Train: 88.42%, Valid: 88.43%, Test: 88.39%
Epoch: 100, Loss: 5.5525, Train: 88.39%, Valid: 88.43%, Test: 88.39%
Epoch: 125, Loss: 5.5404, Train: 88.39%, Valid: 88.42%, Test: 88.38%
Epoch: 150, Loss: 5.5383, Train: 88.39%, Valid: 88.42%, Test: 88.38%
Epoch: 175, Loss: 5.5332, Train: 88.39%, Valid: 88.42%, Test: 88.39%
Epoch: 200, Loss: 5.5202, Train: 88.39%, Valid: 88.42%, Test: 88.39%
Epoch: 225, Loss: 5.5103, Train: 88.39%, Valid: 88.43%, Test: 88.39%
Epoch: 250, Loss: 5.5103, Train: 88.39%, Valid: 88.42%, Test: 88.39%
Epoch: 275, Loss: 5.5086, Train: 88.39%, Valid: 88.42%, Test: 88.39%
Epoch: 300, Loss: 5.5067, Train: 88.39%, Valid: 88.42%, Test: 88.39%
Epoch: 325, Loss: 5.5121, Train: 88.39%, Valid: 88.43%, Test: 88.39%
Epoch: 350, Loss: 5.5131, Train: 88.39%, Valid: 88.42%, Test: 88.39%
Epoch: 375, Loss: 5.5052, Train: 88.39%, Valid: 88.43%, Test: 88.39%
Epoch: 400, Loss: 5.5068, Train: 88.39%, Valid: 88.43%, Test: 88.39%
Epoch: 425, Loss: 5.5101, Train: 88.39%, Valid: 88.43%, Test: 88.39%
Epoch: 450, Loss: 5.5136, Train: 88.39%, Valid: 88.43%, Test: 88.39%
Epoch: 475, Loss: 5.5183, Train: 88.39%, Valid: 88.43%, Test: 88.39%
Run 01:
Highest Train: 88.62
Highest Valid: 88.62
  Final Train: 88.62
   Final Test: 88.61
All runs:
Highest Train: 88.62, nan
Highest Valid: 88.62, nan
  Final Train: 88.62, nan
   Final Test: 88.61, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 6.8439, Train: 86.81%, Valid: 86.81%, Test: 86.81%
Epoch: 25, Loss: 2.6522, Train: 86.00%, Valid: 85.88%, Test: 86.10%
Epoch: 50, Loss: 7.4995, Train: 85.87%, Valid: 85.69%, Test: 85.95%
Epoch: 75, Loss: 3.1248, Train: 85.50%, Valid: 85.34%, Test: 85.61%
Epoch: 100, Loss: 0.6771, Train: 86.34%, Valid: 86.31%, Test: 86.36%
Epoch: 125, Loss: 0.3570, Train: 85.85%, Valid: 85.64%, Test: 86.00%
Epoch: 150, Loss: 0.3442, Train: 85.59%, Valid: 85.33%, Test: 85.75%
Epoch: 175, Loss: 0.3361, Train: 85.58%, Valid: 85.36%, Test: 85.73%
Epoch: 200, Loss: 1.5356, Train: 86.30%, Valid: 86.26%, Test: 86.38%
Epoch: 225, Loss: 0.3824, Train: 86.79%, Valid: 86.78%, Test: 86.87%
Epoch: 250, Loss: 0.3521, Train: 85.89%, Valid: 85.78%, Test: 86.01%
Epoch: 275, Loss: 0.3392, Train: 85.59%, Valid: 85.39%, Test: 85.75%
Epoch: 300, Loss: 1.4320, Train: 86.83%, Valid: 86.85%, Test: 86.86%
Epoch: 325, Loss: 0.4467, Train: 86.30%, Valid: 86.20%, Test: 86.42%
Epoch: 350, Loss: 0.3533, Train: 87.17%, Valid: 86.99%, Test: 87.16%
Epoch: 375, Loss: 0.3412, Train: 86.16%, Valid: 85.98%, Test: 86.28%
Epoch: 400, Loss: 0.7217, Train: 86.37%, Valid: 86.22%, Test: 86.46%
Epoch: 425, Loss: 0.5720, Train: 86.33%, Valid: 86.19%, Test: 86.39%
Epoch: 450, Loss: 0.3614, Train: 87.25%, Valid: 87.10%, Test: 87.35%
Epoch: 475, Loss: 0.3445, Train: 87.72%, Valid: 87.52%, Test: 87.78%
Run 01:
Highest Train: 87.78
Highest Valid: 87.61
  Final Train: 87.73
   Final Test: 87.68
All runs:
Highest Train: 87.78, nan
Highest Valid: 87.61, nan
  Final Train: 87.73, nan
   Final Test: 87.68, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.6557, Train: 24.81%, Valid: 24.54%, Test: 24.47%
Epoch: 25, Loss: 3.4811, Train: 84.97%, Valid: 84.81%, Test: 85.07%
Epoch: 50, Loss: 1.0807, Train: 85.91%, Valid: 85.84%, Test: 85.93%
Epoch: 75, Loss: 0.8024, Train: 85.48%, Valid: 85.35%, Test: 85.49%
Epoch: 100, Loss: 0.6055, Train: 85.45%, Valid: 85.29%, Test: 85.48%
Epoch: 125, Loss: 0.5225, Train: 86.04%, Valid: 86.01%, Test: 86.10%
Epoch: 150, Loss: 0.4803, Train: 85.23%, Valid: 85.06%, Test: 85.29%
Epoch: 175, Loss: 0.4367, Train: 85.13%, Valid: 84.95%, Test: 85.21%
Epoch: 200, Loss: 0.4055, Train: 84.93%, Valid: 84.73%, Test: 85.02%
Epoch: 225, Loss: 0.3829, Train: 84.95%, Valid: 84.74%, Test: 85.03%
Epoch: 250, Loss: 0.3628, Train: 85.23%, Valid: 85.04%, Test: 85.30%
Epoch: 275, Loss: 0.3342, Train: 85.57%, Valid: 85.49%, Test: 85.68%
Epoch: 300, Loss: 0.3258, Train: 86.57%, Valid: 86.48%, Test: 86.67%
Epoch: 325, Loss: 0.3234, Train: 85.89%, Valid: 85.79%, Test: 85.92%
Epoch: 350, Loss: 0.3227, Train: 87.10%, Valid: 87.04%, Test: 87.12%
Epoch: 375, Loss: 0.3225, Train: 86.15%, Valid: 86.09%, Test: 86.19%
Epoch: 400, Loss: 0.3220, Train: 88.00%, Valid: 87.79%, Test: 88.08%
Epoch: 425, Loss: 0.3219, Train: 87.58%, Valid: 87.42%, Test: 87.65%
Epoch: 450, Loss: 0.3216, Train: 87.91%, Valid: 87.70%, Test: 88.00%
Epoch: 475, Loss: 0.3212, Train: 88.05%, Valid: 87.84%, Test: 88.12%
Run 01:
Highest Train: 88.12
Highest Valid: 87.91
  Final Train: 88.12
   Final Test: 88.15
All runs:
Highest Train: 88.12, nan
Highest Valid: 87.91, nan
  Final Train: 88.12, nan
   Final Test: 88.15, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.7791, Train: 86.47%, Valid: 86.46%, Test: 86.58%
Epoch: 25, Loss: 689.0073, Train: 16.13%, Valid: 16.28%, Test: 16.05%
Epoch: 50, Loss: 469.5779, Train: 14.39%, Valid: 14.59%, Test: 14.34%
Epoch: 75, Loss: 66.6695, Train: 14.97%, Valid: 15.13%, Test: 14.88%
Epoch: 100, Loss: 72.7974, Train: 14.69%, Valid: 14.86%, Test: 14.63%
Epoch: 125, Loss: 67.7865, Train: 14.41%, Valid: 14.57%, Test: 14.36%
Epoch: 150, Loss: 69.7197, Train: 14.28%, Valid: 14.47%, Test: 14.24%
Epoch: 175, Loss: 76.9233, Train: 15.88%, Valid: 16.08%, Test: 15.78%
Epoch: 200, Loss: 52.5374, Train: 16.00%, Valid: 16.22%, Test: 15.90%
Epoch: 225, Loss: 160.6432, Train: 15.90%, Valid: 16.11%, Test: 15.80%
Epoch: 250, Loss: 68.4667, Train: 14.36%, Valid: 14.54%, Test: 14.33%
Epoch: 275, Loss: 76.9508, Train: 14.35%, Valid: 14.53%, Test: 14.31%
Epoch: 300, Loss: 72.4446, Train: 14.35%, Valid: 14.54%, Test: 14.32%
Epoch: 325, Loss: 71.4063, Train: 14.43%, Valid: 14.59%, Test: 14.38%
Epoch: 350, Loss: 72.4289, Train: 14.36%, Valid: 14.54%, Test: 14.33%
Epoch: 375, Loss: 72.0554, Train: 14.43%, Valid: 14.59%, Test: 14.37%
Epoch: 400, Loss: 72.0680, Train: 14.66%, Valid: 14.83%, Test: 14.60%
Epoch: 425, Loss: 72.0640, Train: 14.36%, Valid: 14.55%, Test: 14.33%
Epoch: 450, Loss: 72.0636, Train: 14.36%, Valid: 14.55%, Test: 14.33%
Epoch: 475, Loss: 72.0636, Train: 14.88%, Valid: 15.02%, Test: 14.79%
Run 01:
Highest Train: 88.42
Highest Valid: 88.46
  Final Train: 88.42
   Final Test: 88.44
All runs:
Highest Train: 88.42, nan
Highest Valid: 88.46, nan
  Final Train: 88.42, nan
   Final Test: 88.44, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.5635, Train: 85.81%, Valid: 85.71%, Test: 85.88%
Epoch: 25, Loss: 7.6604, Train: 15.77%, Valid: 15.81%, Test: 15.67%
Epoch: 50, Loss: 5.0363, Train: 86.15%, Valid: 86.13%, Test: 86.22%
Epoch: 75, Loss: 1.4212, Train: 87.02%, Valid: 87.03%, Test: 87.06%
Epoch: 100, Loss: 0.9688, Train: 87.01%, Valid: 87.04%, Test: 87.06%
Epoch: 125, Loss: 1.2297, Train: 87.00%, Valid: 87.02%, Test: 87.04%
Epoch: 150, Loss: 1.1344, Train: 87.00%, Valid: 87.02%, Test: 87.04%
Epoch: 175, Loss: 1.1362, Train: 87.00%, Valid: 87.02%, Test: 87.04%
Epoch: 200, Loss: 1.0267, Train: 87.00%, Valid: 87.02%, Test: 87.04%
Epoch: 225, Loss: 0.9391, Train: 87.00%, Valid: 87.02%, Test: 87.04%
Epoch: 250, Loss: 1.0274, Train: 87.00%, Valid: 87.02%, Test: 87.04%
Epoch: 275, Loss: 0.9432, Train: 87.00%, Valid: 87.02%, Test: 87.03%
Epoch: 300, Loss: 1.0006, Train: 86.99%, Valid: 87.01%, Test: 87.03%
Epoch: 325, Loss: 1.0010, Train: 86.99%, Valid: 87.01%, Test: 87.02%
Epoch: 350, Loss: 1.1516, Train: 86.99%, Valid: 87.01%, Test: 87.03%
Epoch: 375, Loss: 1.0719, Train: 87.00%, Valid: 87.02%, Test: 87.03%
Epoch: 400, Loss: 0.8457, Train: 86.99%, Valid: 87.01%, Test: 87.03%
Epoch: 425, Loss: 0.8959, Train: 86.99%, Valid: 87.01%, Test: 87.03%
Epoch: 450, Loss: 0.8919, Train: 87.00%, Valid: 87.01%, Test: 87.03%
Epoch: 475, Loss: 0.8832, Train: 86.99%, Valid: 87.01%, Test: 87.03%
Run 01:
Highest Train: 87.06
Highest Valid: 87.07
  Final Train: 87.06
   Final Test: 87.10
All runs:
Highest Train: 87.06, nan
Highest Valid: 87.07, nan
  Final Train: 87.06, nan
   Final Test: 87.10, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 3.4262, Train: 82.06%, Valid: 82.24%, Test: 82.30%
Epoch: 25, Loss: 0.4372, Train: 85.39%, Valid: 85.22%, Test: 85.45%
Epoch: 50, Loss: 0.3759, Train: 85.51%, Valid: 85.37%, Test: 85.58%
Epoch: 75, Loss: 0.3688, Train: 85.54%, Valid: 85.39%, Test: 85.60%
Epoch: 100, Loss: 0.3643, Train: 85.46%, Valid: 85.30%, Test: 85.53%
Epoch: 125, Loss: 0.3601, Train: 85.43%, Valid: 85.27%, Test: 85.50%
Epoch: 150, Loss: 0.3544, Train: 85.36%, Valid: 85.19%, Test: 85.43%
Epoch: 175, Loss: 0.3914, Train: 86.09%, Valid: 85.96%, Test: 86.23%
Epoch: 200, Loss: 0.3541, Train: 85.57%, Valid: 85.38%, Test: 85.61%
Epoch: 225, Loss: 0.3474, Train: 85.52%, Valid: 85.32%, Test: 85.56%
Epoch: 250, Loss: 0.3420, Train: 85.47%, Valid: 85.26%, Test: 85.53%
Epoch: 275, Loss: 0.4175, Train: 85.56%, Valid: 85.37%, Test: 85.58%
Epoch: 300, Loss: 0.3461, Train: 85.62%, Valid: 85.43%, Test: 85.69%
Epoch: 325, Loss: 0.3346, Train: 85.50%, Valid: 85.31%, Test: 85.56%
Epoch: 350, Loss: 0.3337, Train: 85.57%, Valid: 85.37%, Test: 85.64%
Epoch: 375, Loss: 1.3154, Train: 85.55%, Valid: 85.36%, Test: 85.62%
Epoch: 400, Loss: 24.4466, Train: 86.31%, Valid: 86.17%, Test: 86.30%
Epoch: 425, Loss: 39786.3633, Train: 83.76%, Valid: 83.76%, Test: 83.89%
Epoch: 450, Loss: 47.9585, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 18415.9941, Train: 84.32%, Valid: 84.20%, Test: 84.44%
Run 01:
Highest Train: 87.50
Highest Valid: 87.37
  Final Train: 87.50
   Final Test: 87.47
All runs:
Highest Train: 87.50, nan
Highest Valid: 87.37, nan
  Final Train: 87.50, nan
   Final Test: 87.47, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.7762, Train: 86.01%, Valid: 85.89%, Test: 86.10%
Epoch: 25, Loss: 34.0431, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 91.4269, Train: 13.42%, Valid: 13.42%, Test: 13.40%
Epoch: 75, Loss: 112.5057, Train: 13.45%, Valid: 13.46%, Test: 13.44%
Epoch: 100, Loss: 111.0378, Train: 15.31%, Valid: 15.33%, Test: 15.25%
Epoch: 125, Loss: 115.9810, Train: 15.30%, Valid: 15.32%, Test: 15.24%
Epoch: 150, Loss: 119.1891, Train: 15.32%, Valid: 15.33%, Test: 15.26%
Epoch: 175, Loss: 128.0266, Train: 15.31%, Valid: 15.32%, Test: 15.25%
Epoch: 200, Loss: 134.2597, Train: 15.31%, Valid: 15.34%, Test: 15.26%
Epoch: 225, Loss: 117.2180, Train: 15.31%, Valid: 15.33%, Test: 15.25%
Epoch: 250, Loss: 88.6552, Train: 15.32%, Valid: 15.34%, Test: 15.26%
Epoch: 275, Loss: 148.3256, Train: 15.32%, Valid: 15.34%, Test: 15.26%
Epoch: 300, Loss: 110.9196, Train: 13.40%, Valid: 13.41%, Test: 13.39%
Epoch: 325, Loss: 102.1425, Train: 15.30%, Valid: 15.32%, Test: 15.24%
Epoch: 350, Loss: 122.9717, Train: 15.32%, Valid: 15.34%, Test: 15.26%
Epoch: 375, Loss: 103.0919, Train: 15.32%, Valid: 15.34%, Test: 15.26%
Epoch: 400, Loss: 97.5478, Train: 15.33%, Valid: 15.35%, Test: 15.26%
Epoch: 425, Loss: 102.9012, Train: 15.32%, Valid: 15.34%, Test: 15.26%
Epoch: 450, Loss: 109.8150, Train: 15.31%, Valid: 15.33%, Test: 15.26%
Epoch: 475, Loss: 92.4126, Train: 13.43%, Valid: 13.43%, Test: 13.42%
Run 01:
Highest Train: 86.05
Highest Valid: 85.89
  Final Train: 86.05
   Final Test: 86.16
All runs:
Highest Train: 86.05, nan
Highest Valid: 85.89, nan
  Final Train: 86.05, nan
   Final Test: 86.16, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.6787, Train: 84.26%, Valid: 84.17%, Test: 84.41%
Epoch: 25, Loss: 1.0941, Train: 85.02%, Valid: 84.88%, Test: 85.12%
Epoch: 50, Loss: 2818544370712576.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 11818872832.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 14511623168.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 400597024768.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 333131579392.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 42186600448.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 15757377536.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 7309429248.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 38133824.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 18152200192.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 1539639040.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 13086190592.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 1243044608.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 14133462016.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 15553082368.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 3675226624.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 71658384.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 1951232640.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 85.97
Highest Valid: 85.92
  Final Train: 85.97
   Final Test: 86.15
All runs:
Highest Train: 85.97, nan
Highest Valid: 85.92, nan
  Final Train: 85.97, nan
   Final Test: 86.15, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 7.5790, Train: 15.85%, Valid: 15.95%, Test: 15.85%
Epoch: 25, Loss: 67.5462, Train: 14.88%, Valid: 15.06%, Test: 14.80%
Epoch: 50, Loss: 73.0874, Train: 14.63%, Valid: 14.79%, Test: 14.58%
Epoch: 75, Loss: 82.6421, Train: 18.61%, Valid: 18.59%, Test: 18.36%
Epoch: 100, Loss: 68.1305, Train: 83.57%, Valid: 83.55%, Test: 83.72%
Epoch: 125, Loss: 15065.3535, Train: 15.10%, Valid: 15.27%, Test: 14.99%
Epoch: 150, Loss: 26377.7285, Train: 83.54%, Valid: 83.53%, Test: 83.67%
Epoch: 175, Loss: 15.4025, Train: 85.27%, Valid: 85.25%, Test: 85.35%
Epoch: 200, Loss: 4.3629, Train: 86.64%, Valid: 86.70%, Test: 86.64%
Epoch: 225, Loss: 74.2681, Train: 15.08%, Valid: 15.24%, Test: 14.97%
Epoch: 250, Loss: 4831.3506, Train: 15.08%, Valid: 15.26%, Test: 14.98%
Epoch: 275, Loss: 58741.9688, Train: 14.68%, Valid: 14.85%, Test: 14.63%
Epoch: 300, Loss: 1.3567, Train: 14.63%, Valid: 14.81%, Test: 14.58%
Epoch: 325, Loss: 109.9446, Train: 14.59%, Valid: 14.77%, Test: 14.54%
Epoch: 350, Loss: 76.5226, Train: 86.49%, Valid: 86.47%, Test: 86.56%
Epoch: 375, Loss: 88.6334, Train: 14.64%, Valid: 14.82%, Test: 14.58%
Epoch: 400, Loss: 123.1143, Train: 14.68%, Valid: 14.86%, Test: 14.62%
Epoch: 425, Loss: 110.8256, Train: 14.67%, Valid: 14.85%, Test: 14.61%
Epoch: 450, Loss: 246.8197, Train: 14.64%, Valid: 14.82%, Test: 14.59%
Epoch: 475, Loss: 75.1393, Train: 14.60%, Valid: 14.73%, Test: 14.53%
Run 01:
Highest Train: 86.67
Highest Valid: 86.70
  Final Train: 86.64
   Final Test: 86.64
All runs:
Highest Train: 86.67, nan
Highest Valid: 86.70, nan
  Final Train: 86.64, nan
   Final Test: 86.64, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.6111, Train: 86.79%, Valid: 86.74%, Test: 86.79%
Epoch: 25, Loss: 15.9290, Train: 84.57%, Valid: 84.39%, Test: 84.69%
Epoch: 50, Loss: 28.7931, Train: 84.15%, Valid: 84.01%, Test: 84.26%
Epoch: 75, Loss: 27.3480, Train: 84.58%, Valid: 84.41%, Test: 84.70%
Epoch: 100, Loss: 23.6781, Train: 84.33%, Valid: 84.13%, Test: 84.45%
Epoch: 125, Loss: 17.5530, Train: 84.33%, Valid: 84.13%, Test: 84.45%
Epoch: 150, Loss: 19.0338, Train: 84.31%, Valid: 84.11%, Test: 84.42%
Epoch: 175, Loss: 18.6675, Train: 84.31%, Valid: 84.11%, Test: 84.42%
Epoch: 200, Loss: 18.8140, Train: 84.31%, Valid: 84.11%, Test: 84.42%
Epoch: 225, Loss: 17.5486, Train: 84.31%, Valid: 84.11%, Test: 84.42%
Epoch: 250, Loss: 18.9584, Train: 84.31%, Valid: 84.11%, Test: 84.42%
Epoch: 275, Loss: 20.0497, Train: 84.31%, Valid: 84.11%, Test: 84.42%
Epoch: 300, Loss: 18.6113, Train: 84.31%, Valid: 84.11%, Test: 84.42%
Epoch: 325, Loss: 18.8249, Train: 84.31%, Valid: 84.11%, Test: 84.42%
Epoch: 350, Loss: 19.2122, Train: 84.31%, Valid: 84.11%, Test: 84.42%
Epoch: 375, Loss: 17.9521, Train: 84.31%, Valid: 84.11%, Test: 84.42%
Epoch: 400, Loss: 18.2468, Train: 84.31%, Valid: 84.11%, Test: 84.42%
Epoch: 425, Loss: 18.4558, Train: 84.31%, Valid: 84.11%, Test: 84.42%
Epoch: 450, Loss: 17.9046, Train: 84.31%, Valid: 84.11%, Test: 84.42%
Epoch: 475, Loss: 18.8599, Train: 84.31%, Valid: 84.11%, Test: 84.42%
Run 01:
Highest Train: 86.79
Highest Valid: 86.74
  Final Train: 86.79
   Final Test: 86.79
All runs:
Highest Train: 86.79, nan
Highest Valid: 86.74, nan
  Final Train: 86.79, nan
   Final Test: 86.79, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.7046, Train: 85.76%, Valid: 85.75%, Test: 85.86%
Epoch: 25, Loss: 70.2813, Train: 14.07%, Valid: 14.18%, Test: 13.99%
Epoch: 50, Loss: 90.1520, Train: 14.26%, Valid: 14.34%, Test: 14.17%
Epoch: 75, Loss: 1514551508992.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.67
Highest Valid: 86.60
  Final Train: 86.67
   Final Test: 86.71
All runs:
Highest Train: 86.67, nan
Highest Valid: 86.60, nan
  Final Train: 86.67, nan
   Final Test: 86.71, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.5853, Train: 84.81%, Valid: 84.67%, Test: 84.85%
Epoch: 25, Loss: 34.3465, Train: 16.34%, Valid: 16.23%, Test: 16.18%
Epoch: 50, Loss: 55.5433, Train: 16.10%, Valid: 15.99%, Test: 15.96%
Epoch: 75, Loss: 1.2656, Train: 86.12%, Valid: 86.12%, Test: 86.19%
Epoch: 100, Loss: 620017811456.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 2.3937, Train: 85.64%, Valid: 85.44%, Test: 85.65%
Epoch: 150, Loss: 2.4887, Train: 85.63%, Valid: 85.44%, Test: 85.65%
Epoch: 175, Loss: 158.7805, Train: 50.06%, Valid: 50.07%, Test: 50.07%
Epoch: 200, Loss: 18684.0312, Train: 86.22%, Valid: 86.11%, Test: 86.30%
Epoch: 225, Loss: 2.4436, Train: 85.72%, Valid: 85.55%, Test: 85.75%
Epoch: 250, Loss: 2.4976, Train: 85.63%, Valid: 85.44%, Test: 85.64%
Epoch: 275, Loss: 2.5167, Train: 85.66%, Valid: 85.47%, Test: 85.67%
Epoch: 300, Loss: 3.0559, Train: 85.66%, Valid: 85.47%, Test: 85.67%
Epoch: 325, Loss: 2.5599, Train: 85.33%, Valid: 85.12%, Test: 85.38%
Epoch: 350, Loss: 2.4722, Train: 85.75%, Valid: 85.58%, Test: 85.77%
Epoch: 375, Loss: 244590.9531, Train: 85.61%, Valid: 85.42%, Test: 85.63%
Epoch: 400, Loss: 2.4962, Train: 86.50%, Valid: 86.49%, Test: 86.57%
Epoch: 425, Loss: 2.4416, Train: 85.64%, Valid: 85.45%, Test: 85.65%
Epoch: 450, Loss: 15.1405, Train: 85.63%, Valid: 85.44%, Test: 85.64%
Epoch: 475, Loss: 2.5476, Train: 14.82%, Valid: 14.97%, Test: 14.61%
Run 01:
Highest Train: 86.80
Highest Valid: 86.85
  Final Train: 86.80
   Final Test: 86.88
All runs:
Highest Train: 86.80, nan
Highest Valid: 86.85, nan
  Final Train: 86.80, nan
   Final Test: 86.88, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 4.9219, Train: 85.42%, Valid: 85.30%, Test: 85.51%
Epoch: 25, Loss: 1.9404, Train: 88.10%, Valid: 88.17%, Test: 88.12%
Epoch: 50, Loss: 12.5321, Train: 87.96%, Valid: 88.00%, Test: 87.96%
Epoch: 75, Loss: 230.3917, Train: 84.48%, Valid: 84.31%, Test: 84.60%
Epoch: 100, Loss: 228.2371, Train: 84.48%, Valid: 84.32%, Test: 84.62%
Epoch: 125, Loss: 263.4232, Train: 88.06%, Valid: 88.08%, Test: 88.05%
Epoch: 150, Loss: 255.9684, Train: 88.06%, Valid: 88.08%, Test: 88.04%
Epoch: 175, Loss: 266.0679, Train: 84.47%, Valid: 84.30%, Test: 84.61%
Epoch: 200, Loss: 253.5614, Train: 88.05%, Valid: 88.07%, Test: 88.03%
Epoch: 225, Loss: 245.1830, Train: 88.05%, Valid: 88.07%, Test: 88.03%
Epoch: 250, Loss: 272.3528, Train: 88.06%, Valid: 88.08%, Test: 88.04%
Epoch: 275, Loss: 270.0041, Train: 87.76%, Valid: 87.79%, Test: 87.75%
Epoch: 300, Loss: 258.4161, Train: 84.48%, Valid: 84.31%, Test: 84.62%
Epoch: 325, Loss: 2.7747, Train: 84.52%, Valid: 84.34%, Test: 84.64%
Epoch: 350, Loss: 189.1765, Train: 15.46%, Valid: 15.45%, Test: 15.34%
Epoch: 375, Loss: 339.7224, Train: 84.71%, Valid: 84.53%, Test: 84.85%
Epoch: 400, Loss: 348.3027, Train: 16.19%, Valid: 16.16%, Test: 16.10%
Epoch: 425, Loss: 366.8300, Train: 15.63%, Valid: 15.60%, Test: 15.52%
Epoch: 450, Loss: 369.4310, Train: 15.63%, Valid: 15.60%, Test: 15.52%
Epoch: 475, Loss: 368.7583, Train: 15.62%, Valid: 15.59%, Test: 15.51%
Run 01:
Highest Train: 88.32
Highest Valid: 88.41
  Final Train: 88.31
   Final Test: 88.35
All runs:
Highest Train: 88.32, nan
Highest Valid: 88.41, nan
  Final Train: 88.31, nan
   Final Test: 88.35, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 8.2291, Train: 86.83%, Valid: 86.88%, Test: 86.94%
Epoch: 25, Loss: 20.4903, Train: 86.13%, Valid: 86.04%, Test: 86.31%
Epoch: 50, Loss: 2138.7734, Train: 16.03%, Valid: 16.19%, Test: 15.88%
Epoch: 75, Loss: 93.1188, Train: 86.79%, Valid: 86.80%, Test: 86.83%
Epoch: 100, Loss: 119.2532, Train: 85.95%, Valid: 85.91%, Test: 86.00%
Epoch: 125, Loss: 106.3391, Train: 86.02%, Valid: 85.98%, Test: 86.06%
Epoch: 150, Loss: 114.8914, Train: 86.01%, Valid: 85.97%, Test: 86.05%
Epoch: 175, Loss: 104.0681, Train: 86.02%, Valid: 85.97%, Test: 86.06%
Epoch: 200, Loss: 101.4638, Train: 86.06%, Valid: 86.01%, Test: 86.09%
Epoch: 225, Loss: 91.9222, Train: 86.06%, Valid: 86.01%, Test: 86.09%
Epoch: 250, Loss: 101.3329, Train: 86.08%, Valid: 86.03%, Test: 86.10%
Epoch: 275, Loss: 99.9780, Train: 86.09%, Valid: 86.04%, Test: 86.11%
Epoch: 300, Loss: 94.4582, Train: 86.07%, Valid: 86.02%, Test: 86.10%
Epoch: 325, Loss: 104.5614, Train: 86.03%, Valid: 85.99%, Test: 86.07%
Epoch: 350, Loss: 110.7108, Train: 86.02%, Valid: 85.98%, Test: 86.06%
Epoch: 375, Loss: 98.1805, Train: 86.03%, Valid: 85.98%, Test: 86.07%
Epoch: 400, Loss: 103.2813, Train: 86.04%, Valid: 86.00%, Test: 86.08%
Epoch: 425, Loss: 101.1326, Train: 86.05%, Valid: 86.00%, Test: 86.08%
Epoch: 450, Loss: 98.9010, Train: 86.04%, Valid: 85.99%, Test: 86.07%
Epoch: 475, Loss: 99.4343, Train: 86.05%, Valid: 86.00%, Test: 86.08%
Run 01:
Highest Train: 86.92
Highest Valid: 86.93
  Final Train: 86.92
   Final Test: 86.96
All runs:
Highest Train: 86.92, nan
Highest Valid: 86.93, nan
  Final Train: 86.92, nan
   Final Test: 86.96, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 65.3901, Train: 21.26%, Valid: 21.25%, Test: 20.91%
Epoch: 25, Loss: 2.1814, Train: 86.28%, Valid: 86.23%, Test: 86.41%
Epoch: 50, Loss: 2.0357, Train: 86.59%, Valid: 86.47%, Test: 86.52%
Epoch: 75, Loss: 1.6483, Train: 87.86%, Valid: 87.66%, Test: 87.80%
Epoch: 100, Loss: 0.9530, Train: 87.25%, Valid: 87.07%, Test: 87.11%
Epoch: 125, Loss: 0.3915, Train: 87.08%, Valid: 86.88%, Test: 86.96%
Epoch: 150, Loss: 0.3641, Train: 87.09%, Valid: 86.93%, Test: 86.98%
Epoch: 175, Loss: 0.3603, Train: 87.12%, Valid: 86.96%, Test: 87.02%
Epoch: 200, Loss: 0.3564, Train: 87.06%, Valid: 86.87%, Test: 86.95%
Epoch: 225, Loss: 0.3520, Train: 87.11%, Valid: 86.91%, Test: 87.00%
Epoch: 250, Loss: 0.3498, Train: 87.06%, Valid: 86.84%, Test: 86.97%
Epoch: 275, Loss: 0.3463, Train: 87.10%, Valid: 86.92%, Test: 87.02%
Epoch: 300, Loss: 0.3429, Train: 87.14%, Valid: 86.98%, Test: 87.07%
Epoch: 325, Loss: 0.3400, Train: 87.16%, Valid: 86.99%, Test: 87.11%
Epoch: 350, Loss: 0.3439, Train: 86.74%, Valid: 86.57%, Test: 86.74%
Epoch: 375, Loss: 0.3378, Train: 86.88%, Valid: 86.71%, Test: 86.90%
Epoch: 400, Loss: 0.3351, Train: 86.90%, Valid: 86.73%, Test: 86.92%
Epoch: 425, Loss: 0.3328, Train: 86.64%, Valid: 86.48%, Test: 86.67%
Epoch: 450, Loss: 0.3309, Train: 86.49%, Valid: 86.31%, Test: 86.52%
Epoch: 475, Loss: 0.3294, Train: 86.31%, Valid: 86.14%, Test: 86.32%
Run 01:
Highest Train: 87.92
Highest Valid: 87.73
  Final Train: 87.92
   Final Test: 87.82
All runs:
Highest Train: 87.92, nan
Highest Valid: 87.73, nan
  Final Train: 87.92, nan
   Final Test: 87.82, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.6282, Train: 85.77%, Valid: 85.61%, Test: 85.76%
Epoch: 25, Loss: 75.4968, Train: 86.77%, Valid: 86.62%, Test: 86.80%
Epoch: 50, Loss: 98.8480, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 10.5239, Train: 84.24%, Valid: 84.04%, Test: 84.36%
Epoch: 100, Loss: 222943.5469, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 5.1569, Train: 84.19%, Valid: 83.97%, Test: 84.26%
Epoch: 150, Loss: 508.1184, Train: 83.93%, Valid: 83.72%, Test: 84.05%
Epoch: 175, Loss: 4466.3535, Train: 15.16%, Valid: 15.15%, Test: 15.02%
Epoch: 200, Loss: 328636.2500, Train: 15.13%, Valid: 15.13%, Test: 14.98%
Epoch: 225, Loss: 7.1681, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 1611933.0000, Train: 84.19%, Valid: 83.98%, Test: 84.27%
Epoch: 275, Loss: 6.8652, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 3778274.2500, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 48139360.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 392330.5938, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 2596763.7500, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 5619427.5000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 158992832.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 10932.0137, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 400927.3125, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 88.43
Highest Valid: 88.51
  Final Train: 88.43
   Final Test: 88.52
All runs:
Highest Train: 88.43, nan
Highest Valid: 88.51, nan
  Final Train: 88.43, nan
   Final Test: 88.52, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.8113, Train: 86.11%, Valid: 86.05%, Test: 86.16%
Epoch: 25, Loss: 1.9220, Train: 85.88%, Valid: 85.78%, Test: 86.00%
Epoch: 50, Loss: 1.7751, Train: 85.76%, Valid: 85.65%, Test: 85.89%
Epoch: 75, Loss: 1.1621, Train: 85.68%, Valid: 85.54%, Test: 85.78%
Epoch: 100, Loss: 0.3665, Train: 85.67%, Valid: 85.52%, Test: 85.75%
Epoch: 125, Loss: 0.3514, Train: 86.12%, Valid: 85.90%, Test: 86.24%
Epoch: 150, Loss: 0.3360, Train: 86.01%, Valid: 85.83%, Test: 86.14%
Epoch: 175, Loss: 0.3586, Train: 86.17%, Valid: 85.99%, Test: 86.29%
Epoch: 200, Loss: 0.3349, Train: 86.18%, Valid: 85.98%, Test: 86.29%
Epoch: 225, Loss: 0.3274, Train: 86.04%, Valid: 85.85%, Test: 86.17%
Epoch: 250, Loss: 0.3253, Train: 86.04%, Valid: 85.87%, Test: 86.16%
Epoch: 275, Loss: 0.3243, Train: 85.92%, Valid: 85.73%, Test: 86.05%
Epoch: 300, Loss: 0.3236, Train: 85.92%, Valid: 85.73%, Test: 86.06%
Epoch: 325, Loss: 0.3236, Train: 85.93%, Valid: 85.74%, Test: 86.06%
Epoch: 350, Loss: 0.3235, Train: 85.92%, Valid: 85.71%, Test: 86.05%
Epoch: 375, Loss: 0.3255, Train: 86.11%, Valid: 85.92%, Test: 86.22%
Epoch: 400, Loss: 0.3209, Train: 86.33%, Valid: 86.14%, Test: 86.43%
Epoch: 425, Loss: 0.3233, Train: 86.59%, Valid: 86.40%, Test: 86.66%
Epoch: 450, Loss: 0.3213, Train: 86.15%, Valid: 85.95%, Test: 86.25%
Epoch: 475, Loss: 0.3208, Train: 86.00%, Valid: 85.78%, Test: 86.10%
Run 01:
Highest Train: 87.75
Highest Valid: 87.55
  Final Train: 87.75
   Final Test: 87.80
All runs:
Highest Train: 87.75, nan
Highest Valid: 87.55, nan
  Final Train: 87.75, nan
   Final Test: 87.80, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.8656, Train: 85.06%, Valid: 84.91%, Test: 85.11%
Epoch: 25, Loss: 2.4498, Train: 85.28%, Valid: 85.10%, Test: 85.34%
Epoch: 50, Loss: 3.1694, Train: 85.33%, Valid: 85.12%, Test: 85.38%
Epoch: 75, Loss: 3.1608, Train: 85.36%, Valid: 85.14%, Test: 85.41%
Epoch: 100, Loss: 3.1137, Train: 85.37%, Valid: 85.15%, Test: 85.42%
Epoch: 125, Loss: 3.0603, Train: 85.38%, Valid: 85.16%, Test: 85.42%
Epoch: 150, Loss: 3.0057, Train: 85.39%, Valid: 85.17%, Test: 85.43%
Epoch: 175, Loss: 2.9498, Train: 85.39%, Valid: 85.17%, Test: 85.43%
Epoch: 200, Loss: 2.8927, Train: 85.40%, Valid: 85.18%, Test: 85.44%
Epoch: 225, Loss: 2.8359, Train: 85.41%, Valid: 85.19%, Test: 85.45%
Epoch: 250, Loss: 2.7796, Train: 85.42%, Valid: 85.19%, Test: 85.46%
Epoch: 275, Loss: 2.7226, Train: 85.43%, Valid: 85.21%, Test: 85.47%
Epoch: 300, Loss: 2.6652, Train: 85.49%, Valid: 85.29%, Test: 85.54%
Epoch: 325, Loss: 2.6073, Train: 85.83%, Valid: 85.66%, Test: 85.87%
Epoch: 350, Loss: 2.5137, Train: 85.25%, Valid: 85.19%, Test: 85.38%
Epoch: 375, Loss: 0.4729, Train: 85.39%, Valid: 85.18%, Test: 85.47%
Epoch: 400, Loss: 0.3893, Train: 85.40%, Valid: 85.18%, Test: 85.49%
Epoch: 425, Loss: 0.3757, Train: 85.50%, Valid: 85.29%, Test: 85.59%
Epoch: 450, Loss: 0.3550, Train: 85.62%, Valid: 85.44%, Test: 85.70%
Epoch: 475, Loss: 0.3423, Train: 85.54%, Valid: 85.35%, Test: 85.61%
Run 01:
Highest Train: 86.23
Highest Valid: 86.11
  Final Train: 86.23
   Final Test: 86.32
All runs:
Highest Train: 86.23, nan
Highest Valid: 86.11, nan
  Final Train: 86.23, nan
   Final Test: 86.32, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 8.9328, Train: 85.56%, Valid: 85.48%, Test: 85.61%
Epoch: 25, Loss: 37.4066, Train: 88.21%, Valid: 88.28%, Test: 88.21%
Epoch: 50, Loss: 8664001536.0000, Train: 87.40%, Valid: 87.29%, Test: 87.51%
Epoch: 75, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 3067490772847040218349436928.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 10111500844793856.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 9293836658323721835748937170944.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 88.28
Highest Valid: 88.33
  Final Train: 88.28
   Final Test: 88.30
All runs:
Highest Train: 88.28, nan
Highest Valid: 88.33, nan
  Final Train: 88.28, nan
   Final Test: 88.30, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 5.0553, Train: 85.13%, Valid: 85.00%, Test: 85.21%
Epoch: 25, Loss: 0.7753, Train: 85.65%, Valid: 85.53%, Test: 85.73%
Epoch: 50, Loss: 0.7911, Train: 85.77%, Valid: 85.65%, Test: 85.83%
Epoch: 75, Loss: 0.8428, Train: 85.81%, Valid: 85.69%, Test: 85.90%
Epoch: 100, Loss: 0.4904, Train: 85.83%, Valid: 85.69%, Test: 85.93%
Epoch: 125, Loss: 0.3787, Train: 86.08%, Valid: 85.93%, Test: 86.21%
Epoch: 150, Loss: 0.3429, Train: 86.14%, Valid: 85.98%, Test: 86.27%
Epoch: 175, Loss: 0.3736, Train: 87.55%, Valid: 87.40%, Test: 87.65%
Epoch: 200, Loss: 0.3604, Train: 86.18%, Valid: 86.10%, Test: 86.25%
Epoch: 225, Loss: 0.3398, Train: 87.50%, Valid: 87.32%, Test: 87.59%
Epoch: 250, Loss: 0.6882, Train: 86.68%, Valid: 86.53%, Test: 86.77%
Epoch: 275, Loss: 0.6614, Train: 86.98%, Valid: 86.95%, Test: 87.10%
Epoch: 300, Loss: 0.9625, Train: 86.78%, Valid: 86.77%, Test: 86.85%
Epoch: 325, Loss: 0.3565, Train: 86.79%, Valid: 86.74%, Test: 86.87%
Epoch: 350, Loss: 0.3464, Train: 85.99%, Valid: 85.81%, Test: 86.10%
Epoch: 375, Loss: 0.3378, Train: 86.22%, Valid: 86.05%, Test: 86.33%
Epoch: 400, Loss: 1.1170, Train: 86.16%, Valid: 85.98%, Test: 86.25%
Epoch: 425, Loss: 0.6069, Train: 86.30%, Valid: 86.30%, Test: 86.35%
Epoch: 450, Loss: 1.5016, Train: 86.60%, Valid: 86.59%, Test: 86.66%
Epoch: 475, Loss: 0.5065, Train: 85.76%, Valid: 85.59%, Test: 85.88%
Run 01:
Highest Train: 88.15
Highest Valid: 88.01
  Final Train: 88.15
   Final Test: 88.25
All runs:
Highest Train: 88.15, nan
Highest Valid: 88.01, nan
  Final Train: 88.15, nan
   Final Test: 88.25, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 7.9163, Train: 83.87%, Valid: 83.96%, Test: 84.06%
Epoch: 25, Loss: 18.9989, Train: 85.74%, Valid: 85.73%, Test: 85.83%
Epoch: 50, Loss: 411473671094272.0000, Train: 49.99%, Valid: 49.98%, Test: 50.00%
Epoch: 75, Loss: 54764084.0000, Train: 85.48%, Valid: 85.31%, Test: 85.54%
Epoch: 100, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 610264756263059456.0000, Train: 50.01%, Valid: 50.00%, Test: 50.02%
Epoch: 150, Loss: 58964845797572608.0000, Train: 50.00%, Valid: 49.99%, Test: 50.00%
Epoch: 175, Loss: 369685325287299032809472.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 59757873839591687595346624512.0000, Train: 84.85%, Valid: 84.67%, Test: 84.96%
Epoch: 225, Loss: 4837093598702862336.0000, Train: 49.98%, Valid: 49.98%, Test: 49.95%
Epoch: 250, Loss: 377.1393, Train: 49.99%, Valid: 50.00%, Test: 49.98%
Epoch: 275, Loss: 72960426151627271962624.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 619904045402330956866973501227008.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 4819225015074259495858871468032.0000, Train: 50.02%, Valid: 50.01%, Test: 50.04%
Epoch: 350, Loss: 25171752920726488219253668315136.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: -inf, Train: 50.01%, Valid: 50.00%, Test: 50.01%
Epoch: 400, Loss: 6961300052274990238780096512.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 7076926019738799267295784861696.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 20331477891679576282628096.0000, Train: 50.02%, Valid: 50.02%, Test: 50.04%
Epoch: 475, Loss: 2734601688763577532162529820672.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.64
Highest Valid: 86.61
  Final Train: 86.64
   Final Test: 86.68
All runs:
Highest Train: 86.64, nan
Highest Valid: 86.61, nan
  Final Train: 86.64, nan
   Final Test: 86.68, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.5054, Train: 87.91%, Valid: 87.98%, Test: 87.93%
Epoch: 25, Loss: 2.8706, Train: 85.32%, Valid: 85.35%, Test: 85.40%
Epoch: 50, Loss: 2.7342, Train: 85.17%, Valid: 85.20%, Test: 85.25%
Epoch: 75, Loss: 0.5405, Train: 86.90%, Valid: 86.92%, Test: 86.96%
Epoch: 100, Loss: 0.3680, Train: 85.30%, Valid: 85.32%, Test: 85.40%
Epoch: 125, Loss: 0.3468, Train: 85.44%, Valid: 85.48%, Test: 85.57%
Epoch: 150, Loss: 2.7765, Train: 85.39%, Valid: 85.41%, Test: 85.47%
Epoch: 175, Loss: 1.0090, Train: 84.83%, Valid: 84.83%, Test: 84.97%
Epoch: 200, Loss: 1.2706, Train: 85.11%, Valid: 85.10%, Test: 85.25%
Epoch: 225, Loss: 0.7714, Train: 85.63%, Valid: 85.57%, Test: 85.75%
Epoch: 250, Loss: 0.3720, Train: 86.15%, Valid: 86.03%, Test: 86.21%
Epoch: 275, Loss: 0.6315, Train: 86.37%, Valid: 86.21%, Test: 86.45%
Epoch: 300, Loss: 1.8258, Train: 86.25%, Valid: 86.20%, Test: 86.40%
Epoch: 325, Loss: 0.4371, Train: 85.62%, Valid: 85.40%, Test: 85.68%
Epoch: 350, Loss: 0.3650, Train: 85.77%, Valid: 85.72%, Test: 85.84%
Epoch: 375, Loss: 0.3391, Train: 85.51%, Valid: 85.47%, Test: 85.60%
Epoch: 400, Loss: 1.5778, Train: 87.41%, Valid: 87.28%, Test: 87.44%
Epoch: 425, Loss: 1.2149, Train: 86.91%, Valid: 86.79%, Test: 86.95%
Epoch: 450, Loss: 0.8449, Train: 87.40%, Valid: 87.25%, Test: 87.43%
Epoch: 475, Loss: 0.5980, Train: 87.38%, Valid: 87.24%, Test: 87.44%
Run 01:
Highest Train: 87.99
Highest Valid: 88.06
  Final Train: 87.99
   Final Test: 87.99
All runs:
Highest Train: 87.99, nan
Highest Valid: 88.06, nan
  Final Train: 87.99, nan
   Final Test: 87.99, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 5.0463, Train: 86.24%, Valid: 86.14%, Test: 86.20%
Epoch: 25, Loss: 1.3570, Train: 86.63%, Valid: 86.53%, Test: 86.60%
Epoch: 50, Loss: 1.5293, Train: 86.07%, Valid: 85.95%, Test: 86.13%
Epoch: 75, Loss: 1.0900, Train: 85.68%, Valid: 85.52%, Test: 85.83%
Epoch: 100, Loss: 0.7021, Train: 85.90%, Valid: 85.89%, Test: 85.98%
Epoch: 125, Loss: 0.3523, Train: 86.05%, Valid: 85.90%, Test: 86.14%
Epoch: 150, Loss: 1.1533, Train: 87.32%, Valid: 87.30%, Test: 87.38%
Epoch: 175, Loss: 1.9805, Train: 87.21%, Valid: 87.22%, Test: 87.24%
Epoch: 200, Loss: 3.8059, Train: 86.32%, Valid: 86.20%, Test: 86.39%
Epoch: 225, Loss: 1.5924, Train: 85.41%, Valid: 85.19%, Test: 85.51%
Epoch: 250, Loss: 0.4370, Train: 86.60%, Valid: 86.61%, Test: 86.64%
Epoch: 275, Loss: 0.3506, Train: 86.15%, Valid: 85.93%, Test: 86.23%
Epoch: 300, Loss: 0.3377, Train: 87.27%, Valid: 87.11%, Test: 87.34%
Epoch: 325, Loss: 0.3486, Train: 86.34%, Valid: 86.14%, Test: 86.46%
Epoch: 350, Loss: 0.3361, Train: 86.29%, Valid: 86.09%, Test: 86.41%
Epoch: 375, Loss: 0.5133, Train: 86.25%, Valid: 86.08%, Test: 86.34%
Epoch: 400, Loss: 0.3514, Train: 86.03%, Valid: 85.85%, Test: 86.16%
Epoch: 425, Loss: 0.3432, Train: 86.86%, Valid: 86.70%, Test: 86.95%
Epoch: 450, Loss: 0.3327, Train: 86.23%, Valid: 86.08%, Test: 86.31%
Epoch: 475, Loss: 0.3258, Train: 87.59%, Valid: 87.46%, Test: 87.67%
Run 01:
Highest Train: 87.74
Highest Valid: 87.76
  Final Train: 87.70
   Final Test: 87.76
All runs:
Highest Train: 87.74, nan
Highest Valid: 87.76, nan
  Final Train: 87.70, nan
   Final Test: 87.76, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 280.5548, Train: 16.03%, Valid: 16.05%, Test: 15.83%
Epoch: 25, Loss: 2.4122, Train: 85.07%, Valid: 84.99%, Test: 85.24%
Epoch: 50, Loss: 0.6104, Train: 74.72%, Valid: 74.61%, Test: 74.92%
Epoch: 75, Loss: 5.4109, Train: 84.94%, Valid: 84.92%, Test: 85.11%
Epoch: 100, Loss: 4.6370, Train: 84.64%, Valid: 84.61%, Test: 84.79%
Epoch: 125, Loss: 3.2780, Train: 84.68%, Valid: 84.62%, Test: 84.86%
Epoch: 150, Loss: 1.8720, Train: 84.98%, Valid: 84.90%, Test: 85.15%
Epoch: 175, Loss: 1.1122, Train: 86.22%, Valid: 86.10%, Test: 86.31%
Epoch: 200, Loss: 0.4981, Train: 85.57%, Valid: 85.43%, Test: 85.62%
Epoch: 225, Loss: 0.4144, Train: 85.41%, Valid: 85.27%, Test: 85.48%
Epoch: 250, Loss: 0.3923, Train: 85.44%, Valid: 85.29%, Test: 85.52%
Epoch: 275, Loss: 0.3865, Train: 85.47%, Valid: 85.32%, Test: 85.55%
Epoch: 300, Loss: 0.3853, Train: 85.51%, Valid: 85.34%, Test: 85.58%
Epoch: 325, Loss: 0.3816, Train: 85.49%, Valid: 85.31%, Test: 85.57%
Epoch: 350, Loss: 0.3777, Train: 85.45%, Valid: 85.27%, Test: 85.53%
Epoch: 375, Loss: 1.2837, Train: 86.16%, Valid: 86.17%, Test: 86.34%
Epoch: 400, Loss: 0.5119, Train: 85.75%, Valid: 85.59%, Test: 85.82%
Epoch: 425, Loss: 0.4144, Train: 85.51%, Valid: 85.36%, Test: 85.59%
Epoch: 450, Loss: 0.4031, Train: 85.51%, Valid: 85.35%, Test: 85.59%
Epoch: 475, Loss: 0.3753, Train: 85.37%, Valid: 85.22%, Test: 85.45%
Run 01:
Highest Train: 87.85
Highest Valid: 87.73
  Final Train: 87.85
   Final Test: 87.84
All runs:
Highest Train: 87.85, nan
Highest Valid: 87.73, nan
  Final Train: 87.85, nan
   Final Test: 87.84, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.3966, Train: 85.95%, Valid: 85.94%, Test: 85.98%
Epoch: 25, Loss: 1.4847, Train: 86.03%, Valid: 86.07%, Test: 86.13%
Epoch: 50, Loss: 0.7803, Train: 85.42%, Valid: 85.48%, Test: 85.52%
Epoch: 75, Loss: 0.6771, Train: 85.85%, Valid: 85.91%, Test: 85.93%
Epoch: 100, Loss: 0.3429, Train: 85.35%, Valid: 85.26%, Test: 85.40%
Epoch: 125, Loss: 1.0325, Train: 85.61%, Valid: 85.65%, Test: 85.72%
Epoch: 150, Loss: 0.4089, Train: 85.48%, Valid: 85.50%, Test: 85.63%
Epoch: 175, Loss: 0.3470, Train: 85.80%, Valid: 85.77%, Test: 85.91%
Epoch: 200, Loss: 0.6507, Train: 85.54%, Valid: 85.57%, Test: 85.64%
Epoch: 225, Loss: 0.3590, Train: 85.80%, Valid: 85.85%, Test: 85.96%
Epoch: 250, Loss: 0.3413, Train: 85.95%, Valid: 86.00%, Test: 86.02%
Epoch: 275, Loss: 0.9073, Train: 85.43%, Valid: 85.40%, Test: 85.53%
Epoch: 300, Loss: 0.4283, Train: 87.60%, Valid: 87.60%, Test: 87.66%
Epoch: 325, Loss: 0.3551, Train: 85.91%, Valid: 85.92%, Test: 86.03%
Epoch: 350, Loss: 0.3478, Train: 86.94%, Valid: 86.94%, Test: 87.05%
Epoch: 375, Loss: 0.3394, Train: 86.58%, Valid: 86.60%, Test: 86.70%
Epoch: 400, Loss: 0.4006, Train: 86.93%, Valid: 86.95%, Test: 87.05%
Epoch: 425, Loss: 0.3393, Train: 85.78%, Valid: 85.81%, Test: 85.87%
Epoch: 450, Loss: 0.3267, Train: 86.52%, Valid: 86.53%, Test: 86.58%
Epoch: 475, Loss: 0.3264, Train: 86.35%, Valid: 86.37%, Test: 86.43%
Run 01:
Highest Train: 87.99
Highest Valid: 88.01
  Final Train: 87.99
   Final Test: 88.03
All runs:
Highest Train: 87.99, nan
Highest Valid: 88.01, nan
  Final Train: 87.99, nan
   Final Test: 88.03, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 12.2164, Train: 87.22%, Valid: 87.05%, Test: 87.21%
Epoch: 25, Loss: 1.5895, Train: 87.01%, Valid: 86.95%, Test: 86.98%
Epoch: 50, Loss: 2.0362, Train: 86.19%, Valid: 86.13%, Test: 86.24%
Epoch: 75, Loss: 0.8954, Train: 85.69%, Valid: 85.56%, Test: 85.75%
Epoch: 100, Loss: 0.5168, Train: 85.78%, Valid: 85.68%, Test: 85.88%
Epoch: 125, Loss: 0.3622, Train: 85.91%, Valid: 85.74%, Test: 86.03%
Epoch: 150, Loss: 0.3432, Train: 85.99%, Valid: 85.82%, Test: 86.12%
Epoch: 175, Loss: 1.2374, Train: 86.76%, Valid: 86.82%, Test: 86.78%
Epoch: 200, Loss: 1.6344, Train: 86.42%, Valid: 86.39%, Test: 86.49%
Epoch: 225, Loss: 1.4919, Train: 86.16%, Valid: 86.24%, Test: 86.21%
Epoch: 250, Loss: 1.3093, Train: 86.28%, Valid: 86.28%, Test: 86.30%
Epoch: 275, Loss: 1.1032, Train: 86.38%, Valid: 86.37%, Test: 86.39%
Epoch: 300, Loss: 0.5591, Train: 87.14%, Valid: 87.03%, Test: 87.21%
Epoch: 325, Loss: 0.5282, Train: 85.49%, Valid: 85.43%, Test: 85.63%
Epoch: 350, Loss: 0.3601, Train: 85.56%, Valid: 85.40%, Test: 85.71%
Epoch: 375, Loss: 0.3495, Train: 85.60%, Valid: 85.42%, Test: 85.74%
Epoch: 400, Loss: 0.3369, Train: 85.38%, Valid: 85.19%, Test: 85.53%
Epoch: 425, Loss: 0.3324, Train: 85.61%, Valid: 85.42%, Test: 85.74%
Epoch: 450, Loss: 0.3295, Train: 85.70%, Valid: 85.53%, Test: 85.83%
Epoch: 475, Loss: 0.7848, Train: 85.84%, Valid: 85.66%, Test: 85.96%
Run 01:
Highest Train: 87.54
Highest Valid: 87.50
  Final Train: 87.54
   Final Test: 87.52
All runs:
Highest Train: 87.54, nan
Highest Valid: 87.50, nan
  Final Train: 87.54, nan
   Final Test: 87.52, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.7880, Train: 84.54%, Valid: 84.42%, Test: 84.63%
Epoch: 25, Loss: 3.8661, Train: 85.94%, Valid: 85.83%, Test: 86.02%
Epoch: 50, Loss: 5.0574, Train: 85.44%, Valid: 85.27%, Test: 85.52%
Epoch: 75, Loss: 89.3281, Train: 86.07%, Valid: 86.07%, Test: 86.10%
Epoch: 100, Loss: 11.2480, Train: 85.93%, Valid: 85.93%, Test: 86.07%
Epoch: 125, Loss: 16.0790, Train: 83.50%, Valid: 83.44%, Test: 83.58%
Epoch: 150, Loss: 30.7609, Train: 83.29%, Valid: 83.20%, Test: 83.44%
Epoch: 175, Loss: 390.7317, Train: 83.20%, Valid: 83.14%, Test: 83.34%
Epoch: 200, Loss: 437.4529, Train: 83.21%, Valid: 83.16%, Test: 83.35%
Epoch: 225, Loss: 411.9868, Train: 83.24%, Valid: 83.18%, Test: 83.38%
Epoch: 250, Loss: 450.5514, Train: 83.25%, Valid: 83.18%, Test: 83.39%
Epoch: 275, Loss: 453.1480, Train: 83.26%, Valid: 83.19%, Test: 83.41%
Epoch: 300, Loss: 437.3452, Train: 83.25%, Valid: 83.19%, Test: 83.40%
Epoch: 325, Loss: 442.1549, Train: 83.28%, Valid: 83.21%, Test: 83.43%
Epoch: 350, Loss: 425.7557, Train: 83.35%, Valid: 83.27%, Test: 83.50%
Epoch: 375, Loss: 383.9431, Train: 84.08%, Valid: 83.90%, Test: 84.23%
Epoch: 400, Loss: 368.6431, Train: 84.30%, Valid: 84.13%, Test: 84.44%
Epoch: 425, Loss: 397.5465, Train: 84.34%, Valid: 84.18%, Test: 84.50%
Epoch: 450, Loss: 401.1468, Train: 84.31%, Valid: 84.16%, Test: 84.46%
Epoch: 475, Loss: 401.0200, Train: 84.33%, Valid: 84.17%, Test: 84.48%
Run 01:
Highest Train: 87.34
Highest Valid: 87.10
  Final Train: 87.34
   Final Test: 87.33
All runs:
Highest Train: 87.34, nan
Highest Valid: 87.10, nan
  Final Train: 87.34, nan
   Final Test: 87.33, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.5056, Train: 86.38%, Valid: 86.36%, Test: 86.34%
Epoch: 25, Loss: 41.5739, Train: 85.82%, Valid: 85.88%, Test: 85.93%
Epoch: 50, Loss: 123.3399, Train: 86.46%, Valid: 86.46%, Test: 86.52%
Epoch: 75, Loss: 250.1751, Train: 84.49%, Valid: 84.31%, Test: 84.65%
Epoch: 100, Loss: 43678716.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 129882112.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 2402.1628, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 15128996.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 1262884224.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 34.4084, Train: 83.96%, Valid: 83.77%, Test: 84.04%
Epoch: 250, Loss: 200.6044, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 47343844.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 53662.7539, Train: 17.19%, Valid: 17.31%, Test: 17.04%
Epoch: 325, Loss: 13754603.0000, Train: 85.12%, Valid: 85.08%, Test: 85.19%
Epoch: 350, Loss: 716274466816.0000, Train: 86.06%, Valid: 86.10%, Test: 86.13%
Epoch: 375, Loss: 182944480.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 386931584.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 1044.7877, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 833819.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 115.5204, Train: 83.49%, Valid: 83.35%, Test: 83.66%
Run 01:
Highest Train: 87.45
Highest Valid: 87.32
  Final Train: 87.45
   Final Test: 87.49
All runs:
Highest Train: 87.45, nan
Highest Valid: 87.32, nan
  Final Train: 87.45, nan
   Final Test: 87.49, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 7.1807, Train: 86.36%, Valid: 86.32%, Test: 86.41%
Epoch: 25, Loss: 2.6589, Train: 86.43%, Valid: 86.35%, Test: 86.47%
Epoch: 50, Loss: 4.1099, Train: 86.29%, Valid: 86.20%, Test: 86.28%
Epoch: 75, Loss: 2.8053, Train: 86.05%, Valid: 85.93%, Test: 86.08%
Epoch: 100, Loss: 2.2242, Train: 85.90%, Valid: 85.75%, Test: 85.97%
Epoch: 125, Loss: 1.4586, Train: 85.78%, Valid: 85.64%, Test: 85.88%
Epoch: 150, Loss: 0.5398, Train: 85.90%, Valid: 85.74%, Test: 86.01%
Epoch: 175, Loss: 1.3642, Train: 85.74%, Valid: 85.74%, Test: 85.80%
Epoch: 200, Loss: 0.9708, Train: 85.76%, Valid: 85.83%, Test: 85.82%
Epoch: 225, Loss: 1.8936, Train: 85.76%, Valid: 85.79%, Test: 85.83%
Epoch: 250, Loss: 0.4286, Train: 42.78%, Valid: 42.76%, Test: 42.41%
Epoch: 275, Loss: 0.4123, Train: 85.69%, Valid: 85.77%, Test: 85.84%
Epoch: 300, Loss: 0.3825, Train: 85.73%, Valid: 85.68%, Test: 85.86%
Epoch: 325, Loss: 0.3685, Train: 85.95%, Valid: 85.83%, Test: 86.07%
Epoch: 350, Loss: 0.3568, Train: 86.03%, Valid: 85.86%, Test: 86.15%
Epoch: 375, Loss: 0.3471, Train: 85.94%, Valid: 85.79%, Test: 86.09%
Epoch: 400, Loss: 3.3599, Train: 86.20%, Valid: 86.17%, Test: 86.26%
Epoch: 425, Loss: 1.9008, Train: 86.30%, Valid: 86.27%, Test: 86.36%
Epoch: 450, Loss: 0.8735, Train: 85.88%, Valid: 85.87%, Test: 85.92%
Epoch: 475, Loss: 1.0260, Train: 86.09%, Valid: 86.14%, Test: 86.11%
Run 01:
Highest Train: 87.24
Highest Valid: 87.30
  Final Train: 87.24
   Final Test: 87.27
All runs:
Highest Train: 87.24, nan
Highest Valid: 87.30, nan
  Final Train: 87.24, nan
   Final Test: 87.27, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 14.2714, Train: 85.30%, Valid: 85.17%, Test: 85.37%
Epoch: 25, Loss: 3.1360, Train: 85.89%, Valid: 85.74%, Test: 85.96%
Epoch: 50, Loss: 1.8366, Train: 85.94%, Valid: 85.77%, Test: 86.00%
Epoch: 75, Loss: 0.4802, Train: 86.90%, Valid: 86.91%, Test: 86.96%
Epoch: 100, Loss: 0.3679, Train: 87.09%, Valid: 87.11%, Test: 87.18%
Epoch: 125, Loss: 0.3513, Train: 86.03%, Valid: 85.85%, Test: 86.09%
Epoch: 150, Loss: 0.3429, Train: 85.94%, Valid: 85.77%, Test: 86.04%
Epoch: 175, Loss: 0.3345, Train: 85.86%, Valid: 85.67%, Test: 85.91%
Epoch: 200, Loss: 0.3285, Train: 86.88%, Valid: 86.79%, Test: 87.00%
Epoch: 225, Loss: 0.3488, Train: 87.54%, Valid: 87.52%, Test: 87.55%
Epoch: 250, Loss: 0.3597, Train: 87.35%, Valid: 87.35%, Test: 87.43%
Epoch: 275, Loss: 0.3455, Train: 86.07%, Valid: 85.88%, Test: 86.14%
Epoch: 300, Loss: 0.3399, Train: 86.30%, Valid: 86.13%, Test: 86.38%
Epoch: 325, Loss: 0.3328, Train: 87.43%, Valid: 87.41%, Test: 87.50%
Epoch: 350, Loss: 0.3888, Train: 87.48%, Valid: 87.45%, Test: 87.50%
Epoch: 375, Loss: 0.3427, Train: 86.12%, Valid: 85.93%, Test: 86.20%
Epoch: 400, Loss: 0.3366, Train: 86.19%, Valid: 86.00%, Test: 86.27%
Epoch: 425, Loss: 0.3329, Train: 86.96%, Valid: 86.80%, Test: 87.13%
Epoch: 450, Loss: 0.3299, Train: 86.66%, Valid: 86.50%, Test: 86.78%
Epoch: 475, Loss: 0.3278, Train: 86.20%, Valid: 86.01%, Test: 86.27%
Run 01:
Highest Train: 87.57
Highest Valid: 87.55
  Final Train: 87.57
   Final Test: 87.61
All runs:
Highest Train: 87.57, nan
Highest Valid: 87.55, nan
  Final Train: 87.57, nan
   Final Test: 87.61, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.7810, Train: 84.04%, Valid: 83.86%, Test: 84.16%
Epoch: 25, Loss: 1323782.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 124099000.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 919.8641, Train: 84.79%, Valid: 84.83%, Test: 84.95%
Epoch: 100, Loss: 2005301.7500, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 547605.1875, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 125604.0391, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 655874.4375, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 9471.6094, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 495376.5938, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 242145.8750, Train: 84.00%, Valid: 83.87%, Test: 84.11%
Epoch: 275, Loss: 10467.5459, Train: 83.95%, Valid: 83.82%, Test: 84.06%
Epoch: 300, Loss: 5.9588, Train: 83.88%, Valid: 83.77%, Test: 84.00%
Epoch: 325, Loss: 2959.2036, Train: 16.27%, Valid: 16.42%, Test: 16.13%
Epoch: 350, Loss: 10.7036, Train: 85.30%, Valid: 85.32%, Test: 85.37%
Epoch: 375, Loss: 136.9111, Train: 85.30%, Valid: 85.32%, Test: 85.38%
Epoch: 400, Loss: 8.9014, Train: 86.53%, Valid: 86.58%, Test: 86.57%
Epoch: 425, Loss: 9.0092, Train: 85.30%, Valid: 85.33%, Test: 85.37%
Epoch: 450, Loss: 9.3953, Train: 85.36%, Valid: 85.39%, Test: 85.43%
Epoch: 475, Loss: 9.0194, Train: 87.41%, Valid: 87.32%, Test: 87.45%
Run 01:
Highest Train: 87.43
Highest Valid: 87.35
  Final Train: 87.43
   Final Test: 87.47
All runs:
Highest Train: 87.43, nan
Highest Valid: 87.35, nan
  Final Train: 87.43, nan
   Final Test: 87.47, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.5408, Train: 15.54%, Valid: 15.64%, Test: 15.41%
Epoch: 25, Loss: 12478421.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 1161422.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 1493218.2500, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 2158.1001, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 7475.0703, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 29680.9023, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 897.0245, Train: 84.71%, Valid: 84.68%, Test: 84.80%
Epoch: 200, Loss: 650.2069, Train: 86.95%, Valid: 86.95%, Test: 86.95%
Epoch: 225, Loss: 8030.5303, Train: 84.43%, Valid: 84.44%, Test: 84.53%
Epoch: 250, Loss: 793.7994, Train: 15.98%, Valid: 16.18%, Test: 15.84%
Epoch: 275, Loss: 30.7773, Train: 84.49%, Valid: 84.49%, Test: 84.60%
Epoch: 300, Loss: 114.4072, Train: 15.83%, Valid: 16.00%, Test: 15.67%
Epoch: 325, Loss: 8143.1343, Train: 84.89%, Valid: 84.84%, Test: 84.99%
Epoch: 350, Loss: 8.0588, Train: 84.23%, Valid: 84.25%, Test: 84.32%
Epoch: 375, Loss: 114.1016, Train: 15.62%, Valid: 15.82%, Test: 15.46%
Epoch: 400, Loss: 11283.6475, Train: 84.25%, Valid: 84.27%, Test: 84.33%
Epoch: 425, Loss: 249.3246, Train: 84.55%, Valid: 84.55%, Test: 84.66%
Epoch: 450, Loss: 10286.2793, Train: 84.27%, Valid: 84.29%, Test: 84.35%
Epoch: 475, Loss: 158.9442, Train: 15.92%, Valid: 16.09%, Test: 15.78%
Run 01:
Highest Train: 86.95
Highest Valid: 86.97
  Final Train: 86.95
   Final Test: 86.97
All runs:
Highest Train: 86.95, nan
Highest Valid: 86.97, nan
  Final Train: 86.95, nan
   Final Test: 86.97, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 4.6889, Train: 83.84%, Valid: 83.72%, Test: 83.94%
Epoch: 25, Loss: 45413.1328, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 22248096.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 100178.5156, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 153516.1562, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 304595.6875, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 7.0261, Train: 16.07%, Valid: 16.08%, Test: 15.93%
Epoch: 175, Loss: 796.4528, Train: 86.94%, Valid: 86.94%, Test: 87.01%
Epoch: 200, Loss: 10.0500, Train: 86.95%, Valid: 86.96%, Test: 87.03%
Epoch: 225, Loss: 1981.5601, Train: 16.14%, Valid: 16.13%, Test: 16.00%
Epoch: 250, Loss: 198.1543, Train: 16.10%, Valid: 16.10%, Test: 15.96%
Epoch: 275, Loss: 172.4051, Train: 85.59%, Valid: 85.43%, Test: 85.65%
Epoch: 300, Loss: 200.8617, Train: 85.59%, Valid: 85.44%, Test: 85.65%
Epoch: 325, Loss: 192.9701, Train: 85.59%, Valid: 85.44%, Test: 85.65%
Epoch: 350, Loss: 165.0547, Train: 16.15%, Valid: 16.15%, Test: 16.01%
Epoch: 375, Loss: 277.9309, Train: 86.95%, Valid: 86.96%, Test: 87.02%
Epoch: 400, Loss: 189.5662, Train: 85.59%, Valid: 85.44%, Test: 85.65%
Epoch: 425, Loss: 201.3981, Train: 85.59%, Valid: 85.44%, Test: 85.65%
Epoch: 450, Loss: 184.6327, Train: 85.59%, Valid: 85.44%, Test: 85.65%
Epoch: 475, Loss: 185.4854, Train: 86.95%, Valid: 86.96%, Test: 87.02%
Run 01:
Highest Train: 86.97
Highest Valid: 86.98
  Final Train: 86.97
   Final Test: 87.03
All runs:
Highest Train: 86.97, nan
Highest Valid: 86.98, nan
  Final Train: 86.97, nan
   Final Test: 87.03, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.5045, Train: 85.20%, Valid: 85.14%, Test: 85.30%
Epoch: 25, Loss: 160287904608813056.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 700278243328.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 1420548636672.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 133170569674752.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 2895305728.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 38487781376.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 2600421294080.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 2057595781120.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 20659.5645, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 3542834176.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 15503305211904.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 3195862515712.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 4980403200.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 823272576.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 81994712.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 17964427264.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 11144372224.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 989189767168.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 5382770524160.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 85.20
Highest Valid: 85.14
  Final Train: 85.20
   Final Test: 85.30
All runs:
Highest Train: 85.20, nan
Highest Valid: 85.14, nan
  Final Train: 85.20, nan
   Final Test: 85.30, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.2642, Train: 84.38%, Valid: 84.23%, Test: 84.53%
Epoch: 25, Loss: 347857756356608.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 301486181222921514516480.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 328778697526328590073856.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 1683880754901155840.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 183225590732034146304.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 157987506381180108800.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 1421922214037128478720.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 15768356838191398912.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 5399123260891725824.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 106660470350073112297472.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 714289407880679391232.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 6147714669585807441920.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 5736012480150199336960.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 653326150049801961472.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 2296938751735103488.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 9659612746385657656508416.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 328164658738734387167232.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 26571229731035394152071168.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 87.02
Highest Valid: 86.99
  Final Train: 87.02
   Final Test: 87.02
All runs:
Highest Train: 87.02, nan
Highest Valid: 86.99, nan
  Final Train: 87.02, nan
   Final Test: 87.02, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 3.6752, Train: 85.03%, Valid: 85.14%, Test: 85.17%
Epoch: 25, Loss: 5963261149184.0000, Train: 50.06%, Valid: 50.07%, Test: 50.06%
Epoch: 50, Loss: 8819253.0000, Train: 50.03%, Valid: 50.05%, Test: 50.04%
Epoch: 75, Loss: 9430971926806790144.0000, Train: 50.03%, Valid: 50.05%, Test: 50.04%
Epoch: 100, Loss: 232489544401249636431110864896.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 223758342121606017916927475712.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 42338708743561130935320576.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 5282971004263118329832538112.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 81753451754114671408316416.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 2072673858278996005879808.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 388004977923639377104207872.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 1124561849202807385536593920.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 320687016521034078458216448.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 48112166092064727972708352.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 24162156526214204882944.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 85.53
Highest Valid: 85.67
  Final Train: 85.53
   Final Test: 85.60
All runs:
Highest Train: 85.53, nan
Highest Valid: 85.67, nan
  Final Train: 85.53, nan
   Final Test: 85.60, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.9907, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 25, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 50.00
Highest Valid: 50.00
  Final Train: 50.00
   Final Test: 50.00
All runs:
Highest Train: 50.00, nan
Highest Valid: 50.00, nan
  Final Train: 50.00, nan
   Final Test: 50.00, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.5478, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 25, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 85.13
Highest Valid: 85.08
  Final Train: 85.13
   Final Test: 85.16
All runs:
Highest Train: 85.13, nan
Highest Valid: 85.08, nan
  Final Train: 85.13, nan
   Final Test: 85.16, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.7615, Train: 15.67%, Valid: 15.78%, Test: 15.56%
Epoch: 25, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 50.00
Highest Valid: 50.00
  Final Train: 50.00
   Final Test: 50.00
All runs:
Highest Train: 50.00, nan
Highest Valid: 50.00, nan
  Final Train: 50.00, nan
   Final Test: 50.00, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 12.0012, Train: 84.16%, Valid: 83.95%, Test: 84.27%
Epoch: 25, Loss: 136857.6250, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 875263.1250, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 315028.0938, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 175679.5312, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 41174.7344, Train: 84.74%, Valid: 84.68%, Test: 84.88%
Epoch: 150, Loss: 286.7206, Train: 84.80%, Valid: 84.73%, Test: 84.94%
Epoch: 175, Loss: 757.2271, Train: 15.05%, Valid: 14.94%, Test: 14.98%
Epoch: 200, Loss: 48.4587, Train: 84.23%, Valid: 84.00%, Test: 84.32%
Epoch: 225, Loss: 12.3871, Train: 16.50%, Valid: 16.62%, Test: 16.42%
Epoch: 250, Loss: 144.0727, Train: 83.74%, Valid: 83.57%, Test: 83.88%
Epoch: 275, Loss: 48.9599, Train: 84.45%, Valid: 84.25%, Test: 84.58%
Epoch: 300, Loss: 232.8818, Train: 85.59%, Valid: 85.38%, Test: 85.56%
Epoch: 325, Loss: 534.9073, Train: 86.27%, Valid: 86.10%, Test: 86.32%
Epoch: 350, Loss: 24.7249, Train: 83.81%, Valid: 83.66%, Test: 83.96%
Epoch: 375, Loss: 388.6714, Train: 84.68%, Valid: 84.51%, Test: 84.79%
Epoch: 400, Loss: 987.5085, Train: 14.96%, Valid: 14.87%, Test: 14.91%
Epoch: 425, Loss: 12.2946, Train: 16.10%, Valid: 16.22%, Test: 15.99%
Epoch: 450, Loss: 12.5993, Train: 16.53%, Valid: 16.65%, Test: 16.45%
Epoch: 475, Loss: 648.6201, Train: 83.52%, Valid: 83.30%, Test: 83.58%
Run 01:
Highest Train: 87.25
Highest Valid: 87.08
  Final Train: 87.25
   Final Test: 87.31
All runs:
Highest Train: 87.25, nan
Highest Valid: 87.08, nan
  Final Train: 87.25, nan
   Final Test: 87.31, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 11.7844, Train: 14.92%, Valid: 15.05%, Test: 14.89%
Epoch: 25, Loss: 1192949.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 375.3925, Train: 84.30%, Valid: 84.26%, Test: 84.42%
Epoch: 75, Loss: 175.7699, Train: 84.38%, Valid: 84.25%, Test: 84.63%
Epoch: 100, Loss: 715.8913, Train: 15.09%, Valid: 15.22%, Test: 14.91%
Epoch: 125, Loss: 1051.6000, Train: 85.40%, Valid: 85.25%, Test: 85.60%
Epoch: 150, Loss: 11.8326, Train: 15.27%, Valid: 15.48%, Test: 15.14%
Epoch: 175, Loss: 1712.2341, Train: 86.51%, Valid: 86.44%, Test: 86.71%
Epoch: 200, Loss: 1791.6636, Train: 85.87%, Valid: 85.95%, Test: 85.89%
Epoch: 225, Loss: 1858.5455, Train: 86.47%, Valid: 86.45%, Test: 86.60%
Epoch: 250, Loss: 1682.3136, Train: 85.25%, Valid: 85.11%, Test: 85.43%
Epoch: 275, Loss: 19.1930, Train: 83.71%, Valid: 83.71%, Test: 83.82%
Epoch: 300, Loss: 570.3930, Train: 74.75%, Valid: 75.01%, Test: 74.93%
Epoch: 325, Loss: 1111.5266, Train: 85.21%, Valid: 85.07%, Test: 85.38%
Epoch: 350, Loss: 1482.7002, Train: 85.23%, Valid: 85.09%, Test: 85.42%
Epoch: 375, Loss: 2241.9260, Train: 85.23%, Valid: 85.10%, Test: 85.43%
Epoch: 400, Loss: 1929.1055, Train: 86.77%, Valid: 86.73%, Test: 86.90%
Epoch: 425, Loss: 1571.6368, Train: 15.30%, Valid: 15.51%, Test: 15.17%
Epoch: 450, Loss: 651.1750, Train: 85.33%, Valid: 85.44%, Test: 85.37%
Epoch: 475, Loss: 1306.5054, Train: 16.48%, Valid: 16.45%, Test: 16.44%
Run 01:
Highest Train: 86.84
Highest Valid: 86.80
  Final Train: 86.84
   Final Test: 86.93
All runs:
Highest Train: 86.84, nan
Highest Valid: 86.80, nan
  Final Train: 86.84, nan
   Final Test: 86.93, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 3.9543, Train: 85.89%, Valid: 85.76%, Test: 86.07%
Epoch: 25, Loss: 5750140.5000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 37826.7773, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 12871949.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 2870389.2500, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 374968.5000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 4351.1353, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 1245.3977, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 227305.5000, Train: 84.88%, Valid: 84.69%, Test: 85.01%
Epoch: 225, Loss: 158404.3281, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 1783.8915, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 32888.4492, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 70394.2734, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 713513.8125, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 758769.3750, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 716682.9375, Train: 83.93%, Valid: 83.91%, Test: 84.06%
Epoch: 400, Loss: 307942.6562, Train: 83.98%, Valid: 83.97%, Test: 84.12%
Epoch: 425, Loss: 54426.6133, Train: 84.76%, Valid: 84.58%, Test: 84.88%
Epoch: 450, Loss: 755981.3125, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 59068.0430, Train: 83.93%, Valid: 83.91%, Test: 84.06%
Run 01:
Highest Train: 86.18
Highest Valid: 85.90
  Final Train: 86.18
   Final Test: 86.20
All runs:
Highest Train: 86.18, nan
Highest Valid: 85.90, nan
  Final Train: 86.18, nan
   Final Test: 86.20, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 15.1686, Train: 15.57%, Valid: 15.55%, Test: 15.44%
Epoch: 25, Loss: 211293765880240221273980928.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 1886652672.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 647211264.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 6922726.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 5190396.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 3101388288.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 869993.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 261637456.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 49625214976.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 481.4962, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 13044786.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 271756.3438, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 163346176.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 46662352.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 1641986176.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 1882884.5000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 789574784.0000, Train: 83.88%, Valid: 83.67%, Test: 83.99%
Epoch: 450, Loss: 9430437.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 5459612.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 88.12
Highest Valid: 88.17
  Final Train: 88.12
   Final Test: 88.15
All runs:
Highest Train: 88.12, nan
Highest Valid: 88.17, nan
  Final Train: 88.12, nan
   Final Test: 88.15, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 6.0022, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 25, Loss: 1557390336.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 131333528611717120.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 494303241625966401814528.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 1360037655956379773435904.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 5834370301674130338207632457728.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: -inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.04
Highest Valid: 85.99
  Final Train: 86.04
   Final Test: 86.09
All runs:
Highest Train: 86.04, nan
Highest Valid: 85.99, nan
  Final Train: 86.04, nan
   Final Test: 86.09, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 4.1025, Train: 49.99%, Valid: 49.99%, Test: 49.99%
Epoch: 25, Loss: 654479005033955328.0000, Train: 50.00%, Valid: 49.99%, Test: 50.00%
Epoch: 50, Loss: 251462.0469, Train: 16.50%, Valid: 16.59%, Test: 16.37%
Epoch: 75, Loss: 77221928960.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 18974797824.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 218992230400.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 282004979712.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 2637394944.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 68162720.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 96358277120.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 1868026478592.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 135676576.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 1119185928192.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 12768718.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 337339424.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 31365.3262, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 8430945792.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 2823435008.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 18950604.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 583576192.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.15
Highest Valid: 86.06
  Final Train: 86.15
   Final Test: 86.18
All runs:
Highest Train: 86.15, nan
Highest Valid: 86.06, nan
  Final Train: 86.15, nan
   Final Test: 86.18, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 14.4881, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 25, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 416099170845400231837696.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 74302834311242870746223229272064.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 85.68
Highest Valid: 85.53
  Final Train: 85.68
   Final Test: 85.80
All runs:
Highest Train: 85.68, nan
Highest Valid: 85.53, nan
  Final Train: 85.68, nan
   Final Test: 85.80, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 6.7969, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 25, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 85.00
Highest Valid: 84.95
  Final Train: 85.00
   Final Test: 85.16
All runs:
Highest Train: 85.00, nan
Highest Valid: 84.95, nan
  Final Train: 85.00, nan
   Final Test: 85.16, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.6067, Train: 86.20%, Valid: 86.16%, Test: 86.28%
Epoch: 25, Loss: 274.4224, Train: 17.49%, Valid: 17.41%, Test: 17.33%
Epoch: 50, Loss: 325.8914, Train: 16.11%, Valid: 15.98%, Test: 16.01%
Epoch: 75, Loss: 312.7930, Train: 17.63%, Valid: 17.52%, Test: 17.49%
Epoch: 100, Loss: 305.6870, Train: 14.82%, Valid: 14.70%, Test: 14.76%
Epoch: 125, Loss: 291.8327, Train: 15.25%, Valid: 15.13%, Test: 15.17%
Epoch: 150, Loss: 1437.0697, Train: 15.11%, Valid: 14.99%, Test: 15.06%
Epoch: 175, Loss: 422.8392, Train: 15.67%, Valid: 15.58%, Test: 15.60%
Epoch: 200, Loss: 391.8151, Train: 17.63%, Valid: 17.53%, Test: 17.49%
Epoch: 225, Loss: 312.8798, Train: 14.92%, Valid: 14.80%, Test: 14.87%
Epoch: 250, Loss: 288.5689, Train: 15.80%, Valid: 15.70%, Test: 15.72%
Epoch: 275, Loss: 316.9903, Train: 16.10%, Valid: 15.96%, Test: 15.99%
Epoch: 300, Loss: 347.4248, Train: 16.01%, Valid: 15.88%, Test: 15.92%
Epoch: 325, Loss: 325.8974, Train: 15.13%, Valid: 15.02%, Test: 15.05%
Epoch: 350, Loss: 320.5048, Train: 15.24%, Valid: 15.12%, Test: 15.16%
Epoch: 375, Loss: 322.9881, Train: 15.26%, Valid: 15.14%, Test: 15.18%
Epoch: 400, Loss: 334.0477, Train: 16.10%, Valid: 15.98%, Test: 16.01%
Epoch: 425, Loss: 324.6664, Train: 16.14%, Valid: 16.02%, Test: 16.05%
Epoch: 450, Loss: 301.2845, Train: 14.90%, Valid: 14.78%, Test: 14.85%
Epoch: 475, Loss: 323.6251, Train: 15.49%, Valid: 15.39%, Test: 15.39%
Run 01:
Highest Train: 86.20
Highest Valid: 86.16
  Final Train: 86.20
   Final Test: 86.28
All runs:
Highest Train: 86.20, nan
Highest Valid: 86.16, nan
  Final Train: 86.20, nan
   Final Test: 86.28, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 11.0870, Train: 86.46%, Valid: 86.52%, Test: 86.50%
Epoch: 25, Loss: 3.9654, Train: 84.49%, Valid: 84.51%, Test: 84.60%
Epoch: 50, Loss: 21.0811, Train: 87.45%, Valid: 87.34%, Test: 87.50%
Epoch: 75, Loss: 14.6715, Train: 84.05%, Valid: 83.86%, Test: 84.17%
Epoch: 100, Loss: 14.2062, Train: 83.98%, Valid: 83.80%, Test: 84.11%
Epoch: 125, Loss: 14.0559, Train: 83.97%, Valid: 83.79%, Test: 84.11%
Epoch: 150, Loss: 14.4313, Train: 83.99%, Valid: 83.81%, Test: 84.13%
Epoch: 175, Loss: 14.5118, Train: 83.96%, Valid: 83.79%, Test: 84.12%
Epoch: 200, Loss: 14.4022, Train: 84.06%, Valid: 83.87%, Test: 84.19%
Epoch: 225, Loss: 14.5213, Train: 84.29%, Valid: 84.13%, Test: 84.40%
Epoch: 250, Loss: 14.4165, Train: 84.26%, Valid: 84.11%, Test: 84.37%
Epoch: 275, Loss: 14.4400, Train: 84.24%, Valid: 84.08%, Test: 84.36%
Epoch: 300, Loss: 14.6619, Train: 84.22%, Valid: 84.07%, Test: 84.34%
Epoch: 325, Loss: 14.4931, Train: 84.27%, Valid: 84.12%, Test: 84.38%
Epoch: 350, Loss: 14.4977, Train: 84.36%, Valid: 84.20%, Test: 84.46%
Epoch: 375, Loss: 14.0300, Train: 85.10%, Valid: 85.09%, Test: 85.24%
Epoch: 400, Loss: 14.4493, Train: 85.06%, Valid: 85.05%, Test: 85.21%
Epoch: 425, Loss: 14.5864, Train: 85.08%, Valid: 85.09%, Test: 85.23%
Epoch: 450, Loss: 14.5860, Train: 85.10%, Valid: 85.10%, Test: 85.24%
Epoch: 475, Loss: 14.5649, Train: 85.10%, Valid: 85.10%, Test: 85.24%
Run 01:
Highest Train: 87.85
Highest Valid: 87.92
  Final Train: 87.85
   Final Test: 87.86
All runs:
Highest Train: 87.85, nan
Highest Valid: 87.92, nan
  Final Train: 87.85, nan
   Final Test: 87.86, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 5.2062, Train: 86.73%, Valid: 86.62%, Test: 86.76%
Epoch: 25, Loss: 31.0351, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 167913.2500, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 2420.4124, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 71222.9297, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 26738.1016, Train: 15.45%, Valid: 15.66%, Test: 15.29%
Epoch: 150, Loss: 17449.6191, Train: 15.55%, Valid: 15.75%, Test: 15.37%
Epoch: 175, Loss: 8924.7695, Train: 16.08%, Valid: 16.24%, Test: 15.93%
Epoch: 200, Loss: 6039.3442, Train: 15.51%, Valid: 15.72%, Test: 15.34%
Epoch: 225, Loss: 6024.9243, Train: 15.42%, Valid: 15.63%, Test: 15.25%
Epoch: 250, Loss: 6038.6797, Train: 16.70%, Valid: 16.79%, Test: 16.57%
Epoch: 275, Loss: 6041.4287, Train: 16.65%, Valid: 16.75%, Test: 16.52%
Epoch: 300, Loss: 5826.4595, Train: 16.91%, Valid: 16.98%, Test: 16.76%
Epoch: 325, Loss: 6427.3867, Train: 16.92%, Valid: 16.99%, Test: 16.79%
Epoch: 350, Loss: 5852.4868, Train: 15.48%, Valid: 15.69%, Test: 15.30%
Epoch: 375, Loss: 6024.6333, Train: 16.68%, Valid: 16.78%, Test: 16.55%
Epoch: 400, Loss: 6018.7446, Train: 15.35%, Valid: 15.56%, Test: 15.18%
Epoch: 425, Loss: 6888.6089, Train: 16.67%, Valid: 16.77%, Test: 16.54%
Epoch: 450, Loss: 8611.5000, Train: 16.79%, Valid: 16.78%, Test: 16.74%
Epoch: 475, Loss: 5887.8169, Train: 16.67%, Valid: 16.65%, Test: 16.60%
Run 01:
Highest Train: 87.18
Highest Valid: 87.16
  Final Train: 87.18
   Final Test: 87.21
All runs:
Highest Train: 87.18, nan
Highest Valid: 87.16, nan
  Final Train: 87.18, nan
   Final Test: 87.21, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 9.4613, Train: 85.46%, Valid: 85.26%, Test: 85.49%
Epoch: 25, Loss: 25.0950, Train: 16.51%, Valid: 16.51%, Test: 16.28%
Epoch: 50, Loss: 20118.7891, Train: 16.39%, Valid: 16.48%, Test: 16.21%
Epoch: 75, Loss: 796.5100, Train: 84.84%, Valid: 84.66%, Test: 84.93%
Epoch: 100, Loss: 49.6663, Train: 83.66%, Valid: 83.70%, Test: 83.82%
Epoch: 125, Loss: 28.2542, Train: 83.66%, Valid: 83.70%, Test: 83.82%
Epoch: 150, Loss: 28.9102, Train: 83.65%, Valid: 83.68%, Test: 83.82%
Epoch: 175, Loss: 27.9182, Train: 83.68%, Valid: 83.71%, Test: 83.83%
Epoch: 200, Loss: 32.4854, Train: 83.67%, Valid: 83.70%, Test: 83.82%
Epoch: 225, Loss: 31.1413, Train: 83.64%, Valid: 83.66%, Test: 83.80%
Epoch: 250, Loss: 35.5123, Train: 83.66%, Valid: 83.69%, Test: 83.82%
Epoch: 275, Loss: 34.3623, Train: 83.65%, Valid: 83.68%, Test: 83.81%
Epoch: 300, Loss: 30.9668, Train: 83.62%, Valid: 83.65%, Test: 83.79%
Epoch: 325, Loss: 30.2103, Train: 83.63%, Valid: 83.66%, Test: 83.79%
Epoch: 350, Loss: 33.5650, Train: 83.62%, Valid: 83.66%, Test: 83.79%
Epoch: 375, Loss: 32.6943, Train: 83.58%, Valid: 83.59%, Test: 83.75%
Epoch: 400, Loss: 31.0485, Train: 83.57%, Valid: 83.58%, Test: 83.74%
Epoch: 425, Loss: 34.8830, Train: 83.57%, Valid: 83.59%, Test: 83.74%
Epoch: 450, Loss: 33.3795, Train: 83.56%, Valid: 83.57%, Test: 83.73%
Epoch: 475, Loss: 27.8992, Train: 83.53%, Valid: 83.54%, Test: 83.71%
Run 01:
Highest Train: 87.06
Highest Valid: 87.16
  Final Train: 87.06
   Final Test: 87.11
All runs:
Highest Train: 87.06, nan
Highest Valid: 87.16, nan
  Final Train: 87.06, nan
   Final Test: 87.11, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 12.5457, Train: 84.89%, Valid: 84.72%, Test: 84.99%
Epoch: 25, Loss: 1294761.8750, Train: 87.40%, Valid: 87.27%, Test: 87.44%
Epoch: 50, Loss: 528669.3750, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 3906670.5000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 593109.3750, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 95924096.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 2961.2847, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 36.2620, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 3113.5652, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 30.0506, Train: 85.14%, Valid: 85.11%, Test: 85.24%
Epoch: 250, Loss: 21004508.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 239141792.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 5572528128.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 11076475.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 2189724.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 533.2937, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 30210966.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 156064912.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 106052256.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 5.8902, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 88.41
Highest Valid: 88.45
  Final Train: 88.41
   Final Test: 88.41
All runs:
Highest Train: 88.41, nan
Highest Valid: 88.45, nan
  Final Train: 88.41, nan
   Final Test: 88.41, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.8906, Train: 86.54%, Valid: 86.56%, Test: 86.59%
Epoch: 25, Loss: 4849865326592.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 110061133824.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 323145420505088.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 3299161948553216.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 115094906208256.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 272910979694592.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 288392155758592.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 149282460008448.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 33024415855804416.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 500424933376.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 208896136314880.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 11272442609664.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 4879597170589696.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 12182665071427584.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 286556124348416.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 9169926975651840.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 244416856064.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 55057932353536.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 296493549631832064.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.54
Highest Valid: 86.56
  Final Train: 86.54
   Final Test: 86.59
All runs:
Highest Train: 86.54, nan
Highest Valid: 86.56, nan
  Final Train: 86.54, nan
   Final Test: 86.59, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.4682, Train: 86.18%, Valid: 86.16%, Test: 86.20%
Epoch: 25, Loss: 52554650091520.0000, Train: 50.04%, Valid: 50.05%, Test: 50.04%
Epoch: 50, Loss: 1000409.6250, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 17257553985536.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 23433064742912.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 22253144113152.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 5796619804076933120.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 21211971596582912.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 38287880.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 46163457736704.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 150501495207362560.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 3046861344654491648.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 5115645351952384.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 288614353207296.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 413020933062656.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 102704128000.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 1003410554880.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 147473732242243584.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 28748212224.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 8572241474224128.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.18
Highest Valid: 86.16
  Final Train: 86.18
   Final Test: 86.20
All runs:
Highest Train: 86.18, nan
Highest Valid: 86.16, nan
  Final Train: 86.18, nan
   Final Test: 86.20, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 6.5454, Train: 87.98%, Valid: 88.05%, Test: 88.00%
Epoch: 25, Loss: 10.1402, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: -inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 355718704639172214864640963772416.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 47642019275413874006441328640.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 2488427670768268237831170686976.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 40745780895389815459797920645120.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 94520629247687778470040784863232.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 10956336333885014693940186578944.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 87.98
Highest Valid: 88.05
  Final Train: 87.98
   Final Test: 88.00
All runs:
Highest Train: 87.98, nan
Highest Valid: 88.05, nan
  Final Train: 87.98, nan
   Final Test: 88.00, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.1132, Train: 86.17%, Valid: 86.11%, Test: 86.21%
Epoch: 25, Loss: 223.4774, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 104933231689728.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 587.1976, Train: 85.49%, Valid: 85.36%, Test: 85.59%
Epoch: 100, Loss: 115.8180, Train: 84.03%, Valid: 83.85%, Test: 84.18%
Epoch: 125, Loss: 17.4292, Train: 83.93%, Valid: 83.75%, Test: 84.08%
Epoch: 150, Loss: 15.8483, Train: 83.92%, Valid: 83.75%, Test: 84.08%
Epoch: 175, Loss: 17.5751, Train: 15.44%, Valid: 15.46%, Test: 15.36%
Epoch: 200, Loss: 17.4365, Train: 83.91%, Valid: 83.74%, Test: 84.07%
Epoch: 225, Loss: 16.4513, Train: 83.92%, Valid: 83.74%, Test: 84.07%
Epoch: 250, Loss: 17.4694, Train: 83.92%, Valid: 83.74%, Test: 84.07%
Epoch: 275, Loss: 15.8697, Train: 83.92%, Valid: 83.74%, Test: 84.07%
Epoch: 300, Loss: 17.3614, Train: 83.92%, Valid: 83.74%, Test: 84.08%
Epoch: 325, Loss: 15.3689, Train: 83.92%, Valid: 83.74%, Test: 84.07%
Epoch: 350, Loss: 17.4519, Train: 83.92%, Valid: 83.75%, Test: 84.08%
Epoch: 375, Loss: 15.4140, Train: 83.92%, Valid: 83.74%, Test: 84.07%
Epoch: 400, Loss: 18.4995, Train: 83.92%, Valid: 83.74%, Test: 84.07%
Epoch: 425, Loss: 16.2897, Train: 83.91%, Valid: 83.74%, Test: 84.07%
Epoch: 450, Loss: 17.6650, Train: 83.92%, Valid: 83.74%, Test: 84.07%
Epoch: 475, Loss: 20.2950, Train: 83.92%, Valid: 83.74%, Test: 84.07%
Run 01:
Highest Train: 86.36
Highest Valid: 86.23
  Final Train: 86.36
   Final Test: 86.40
All runs:
Highest Train: 86.36, nan
Highest Valid: 86.23, nan
  Final Train: 86.36, nan
   Final Test: 86.40, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 4.3305, Train: 86.25%, Valid: 86.24%, Test: 86.34%
Epoch: 25, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.43
Highest Valid: 86.48
  Final Train: 86.43
   Final Test: 86.51
All runs:
Highest Train: 86.43, nan
Highest Valid: 86.48, nan
  Final Train: 86.43, nan
   Final Test: 86.51, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 3.1539, Train: 86.22%, Valid: 86.28%, Test: 86.27%
Epoch: 25, Loss: 26809.1953, Train: 84.62%, Valid: 84.64%, Test: 84.74%
Epoch: 50, Loss: 1406710.8750, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 127.5782, Train: 84.20%, Valid: 84.00%, Test: 84.30%
Epoch: 100, Loss: 1283.6534, Train: 15.22%, Valid: 15.21%, Test: 15.08%
Epoch: 125, Loss: 87.0045, Train: 15.25%, Valid: 15.23%, Test: 15.10%
Epoch: 150, Loss: 573.3151, Train: 84.20%, Valid: 84.00%, Test: 84.31%
Epoch: 175, Loss: 1250.9957, Train: 15.26%, Valid: 15.23%, Test: 15.12%
Epoch: 200, Loss: 96.8628, Train: 15.26%, Valid: 15.23%, Test: 15.12%
Epoch: 225, Loss: 449.4368, Train: 15.27%, Valid: 15.24%, Test: 15.12%
Epoch: 250, Loss: 38.4771, Train: 13.24%, Valid: 13.22%, Test: 13.17%
Epoch: 275, Loss: 346.8894, Train: 11.66%, Valid: 11.60%, Test: 11.63%
Epoch: 300, Loss: 443.9983, Train: 11.80%, Valid: 11.77%, Test: 11.76%
Epoch: 325, Loss: 84.0538, Train: 15.27%, Valid: 15.25%, Test: 15.13%
Epoch: 350, Loss: 230.5220, Train: 15.26%, Valid: 15.23%, Test: 15.12%
Epoch: 375, Loss: 59.5314, Train: 11.69%, Valid: 11.64%, Test: 11.68%
Epoch: 400, Loss: 105.2937, Train: 15.28%, Valid: 15.25%, Test: 15.13%
Epoch: 425, Loss: 237.6497, Train: 11.70%, Valid: 11.65%, Test: 11.69%
Epoch: 450, Loss: 281.4970, Train: 13.21%, Valid: 13.18%, Test: 13.14%
Epoch: 475, Loss: 299.5965, Train: 15.26%, Valid: 15.24%, Test: 15.12%
Run 01:
Highest Train: 87.57
Highest Valid: 87.58
  Final Train: 87.57
   Final Test: 87.59
All runs:
Highest Train: 87.57, nan
Highest Valid: 87.58, nan
  Final Train: 87.57, nan
   Final Test: 87.59, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.4230, Train: 14.93%, Valid: 14.97%, Test: 14.82%
Epoch: 25, Loss: 22321532.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 48919932.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 15960.5234, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 38457.7148, Train: 83.74%, Valid: 83.77%, Test: 83.81%
Epoch: 125, Loss: 242.5384, Train: 85.89%, Valid: 85.72%, Test: 86.00%
Epoch: 150, Loss: 4.4286, Train: 84.71%, Valid: 84.52%, Test: 84.88%
Epoch: 175, Loss: 5.2127, Train: 86.73%, Valid: 86.73%, Test: 86.76%
Epoch: 200, Loss: 5.1788, Train: 86.80%, Valid: 86.83%, Test: 86.83%
Epoch: 225, Loss: 4.3804, Train: 85.95%, Valid: 85.78%, Test: 86.08%
Epoch: 250, Loss: 4.3097, Train: 86.74%, Valid: 86.76%, Test: 86.75%
Epoch: 275, Loss: 5.1088, Train: 85.86%, Valid: 85.69%, Test: 85.97%
Epoch: 300, Loss: 5.5448, Train: 86.73%, Valid: 86.76%, Test: 86.75%
Epoch: 325, Loss: 3.8013, Train: 86.74%, Valid: 86.77%, Test: 86.75%
Epoch: 350, Loss: 5.2284, Train: 86.73%, Valid: 86.75%, Test: 86.74%
Epoch: 375, Loss: 6.2162, Train: 86.73%, Valid: 86.75%, Test: 86.74%
Epoch: 400, Loss: 5.6205, Train: 86.73%, Valid: 86.76%, Test: 86.75%
Epoch: 425, Loss: 4.8211, Train: 85.93%, Valid: 85.76%, Test: 86.04%
Epoch: 450, Loss: 5.5766, Train: 86.74%, Valid: 86.76%, Test: 86.75%
Epoch: 475, Loss: 6.2571, Train: 85.86%, Valid: 85.69%, Test: 85.96%
Run 01:
Highest Train: 87.15
Highest Valid: 87.17
  Final Train: 87.15
   Final Test: 87.16
All runs:
Highest Train: 87.15, nan
Highest Valid: 87.17, nan
  Final Train: 87.15, nan
   Final Test: 87.16, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 5.2497, Train: 84.55%, Valid: 84.48%, Test: 84.68%
Epoch: 25, Loss: 92.4373, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 198824.5156, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 3648120320.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 11233.2646, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 64658660.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 20222644.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 2523766.2500, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 284192.5312, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 40687.9492, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 623.6866, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 11376.6094, Train: 49.99%, Valid: 49.99%, Test: 50.00%
Epoch: 300, Loss: 233561.9688, Train: 83.99%, Valid: 83.97%, Test: 84.13%
Epoch: 325, Loss: 15809.2100, Train: 50.01%, Valid: 50.01%, Test: 50.03%
Epoch: 350, Loss: 1651983.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 30115.7539, Train: 50.00%, Valid: 49.99%, Test: 50.01%
Epoch: 400, Loss: 775287.1875, Train: 49.99%, Valid: 49.99%, Test: 50.00%
Epoch: 425, Loss: 106724.3516, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 223.2101, Train: 50.00%, Valid: 49.99%, Test: 50.00%
Epoch: 475, Loss: 875440.8125, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.26
Highest Valid: 86.15
  Final Train: 86.26
   Final Test: 86.29
All runs:
Highest Train: 86.26, nan
Highest Valid: 86.15, nan
  Final Train: 86.26, nan
   Final Test: 86.29, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.4936, Train: 85.49%, Valid: 85.31%, Test: 85.60%
Epoch: 25, Loss: 40767610530299904.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 7153893135734227861504.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 1214130067879927742464.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 753234171379525210865664.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 1139025225744130244608.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 273073230405697536.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 29908383135134908416.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 44393566794059813486592.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 5114321513839140536320.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 14336634369152647168.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 58225869701901189120.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 119806686792777728.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 307893080030183424.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 91349053312839188480.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 3304855975676084224.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 26305683753145466880.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 6967493650756990402560.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 91552418983512637440.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 11510339729954439168.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 88.27
Highest Valid: 88.32
  Final Train: 88.27
   Final Test: 88.29
All runs:
Highest Train: 88.27, nan
Highest Valid: 88.32, nan
  Final Train: 88.27, nan
   Final Test: 88.29, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.5025, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 25, Loss: 158015170023366208782336.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 1344805421905289269968109568.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 276155195773953336055066917863424.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 61279816319136074800168515403776.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 558448910660834368543889096179712.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 39184078432353441345620735426560.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 32445816056018205836810439884800.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: -inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 87.56
Highest Valid: 87.53
  Final Train: 87.56
   Final Test: 87.55
All runs:
Highest Train: 87.56, nan
Highest Valid: 87.53, nan
  Final Train: 87.56, nan
   Final Test: 87.55, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.6678, Train: 85.34%, Valid: 85.16%, Test: 85.40%
Epoch: 25, Loss: 305059119409228083424657408.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 4022612.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 524115305807478784.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 2322765988500980367360.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 732793572849637916672.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 896300094888869888.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 1490421958871875584.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 279328362969867617304576.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 1589131165011279872.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 5216067399570648203264.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 111561542692012818432.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 2462985422161672404992.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 54965747356801171456.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 24815678371741564928.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 32669648258619932672.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 751851962047739723776.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 796412349723443200.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 4248647465758132438827008.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 38973489146119048921088.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 85.70
Highest Valid: 85.51
  Final Train: 85.70
   Final Test: 85.78
All runs:
Highest Train: 85.70, nan
Highest Valid: 85.51, nan
  Final Train: 85.70, nan
   Final Test: 85.78, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.5585, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 25, Loss: 5959055.5000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 55107050755590335151152178397184.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 47.2963, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 518058602313444038646621779525632.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 361459753121127013518336327680.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 10394334752313615904833273856.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 87.95
Highest Valid: 87.97
  Final Train: 87.94
   Final Test: 88.00
All runs:
Highest Train: 87.95, nan
Highest Valid: 87.97, nan
  Final Train: 87.94, nan
   Final Test: 88.00, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.6400, Train: 86.25%, Valid: 86.11%, Test: 86.28%
Epoch: 25, Loss: nan, Train: 50.06%, Valid: 50.08%, Test: 50.07%
Epoch: 50, Loss: 526911690916795209786327040.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 175039612314032537600.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.25
Highest Valid: 86.11
  Final Train: 86.25
   Final Test: 86.28
All runs:
Highest Train: 86.25, nan
Highest Valid: 86.11, nan
  Final Train: 86.25, nan
   Final Test: 86.28, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.9317, Train: 14.88%, Valid: 15.01%, Test: 14.83%
Epoch: 25, Loss: 8393430174531584.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.50
Highest Valid: 86.56
  Final Train: 86.50
   Final Test: 86.55
All runs:
Highest Train: 86.50, nan
Highest Valid: 86.56, nan
  Final Train: 86.50, nan
   Final Test: 86.55, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 4.1562, Train: 86.48%, Valid: 86.50%, Test: 86.53%
Epoch: 25, Loss: 8586.2920, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 47350.0273, Train: 84.53%, Valid: 84.53%, Test: 84.64%
Epoch: 75, Loss: 13602.1680, Train: 15.86%, Valid: 16.04%, Test: 15.71%
Epoch: 100, Loss: 85469.1016, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 12003.7266, Train: 15.85%, Valid: 16.03%, Test: 15.72%
Epoch: 150, Loss: 537.5693, Train: 86.86%, Valid: 86.86%, Test: 86.91%
Epoch: 175, Loss: 159.0937, Train: 16.04%, Valid: 16.21%, Test: 15.91%
Epoch: 200, Loss: 25.1134, Train: 86.80%, Valid: 86.82%, Test: 86.86%
Epoch: 225, Loss: 515.8989, Train: 86.99%, Valid: 87.00%, Test: 87.02%
Epoch: 250, Loss: 106.8955, Train: 87.37%, Valid: 87.39%, Test: 87.40%
Epoch: 275, Loss: 632.3327, Train: 15.77%, Valid: 15.86%, Test: 15.75%
Epoch: 300, Loss: 6.2686, Train: 87.12%, Valid: 87.14%, Test: 87.15%
Epoch: 325, Loss: 489.5584, Train: 87.40%, Valid: 87.46%, Test: 87.42%
Epoch: 350, Loss: 13.2654, Train: 86.99%, Valid: 87.01%, Test: 87.03%
Epoch: 375, Loss: 903.3445, Train: 87.41%, Valid: 87.47%, Test: 87.43%
Epoch: 400, Loss: 566.6794, Train: 87.61%, Valid: 87.67%, Test: 87.65%
Epoch: 425, Loss: 828.0367, Train: 86.87%, Valid: 86.88%, Test: 86.92%
Epoch: 450, Loss: 921.6191, Train: 86.81%, Valid: 86.82%, Test: 86.87%
Epoch: 475, Loss: 501.3299, Train: 87.34%, Valid: 87.37%, Test: 87.39%
Run 01:
Highest Train: 88.27
Highest Valid: 88.36
  Final Train: 88.27
   Final Test: 88.32
All runs:
Highest Train: 88.27, nan
Highest Valid: 88.36, nan
  Final Train: 88.27, nan
   Final Test: 88.32, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 5.1373, Train: 14.96%, Valid: 15.20%, Test: 14.83%
Epoch: 25, Loss: 377391.4688, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 1278.7772, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 37814.5156, Train: 83.56%, Valid: 83.44%, Test: 83.72%
Epoch: 100, Loss: 113598.7578, Train: 83.65%, Valid: 83.51%, Test: 83.82%
Epoch: 125, Loss: 404659.1250, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 605857.7500, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 142668.3281, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 16972.0430, Train: 84.11%, Valid: 83.92%, Test: 84.28%
Epoch: 225, Loss: 59403.2227, Train: 84.11%, Valid: 83.92%, Test: 84.28%
Epoch: 250, Loss: 2574.8403, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 27111.0312, Train: 83.76%, Valid: 83.62%, Test: 83.93%
Epoch: 300, Loss: 5839.5229, Train: 14.97%, Valid: 15.03%, Test: 14.87%
Epoch: 325, Loss: 24379.9395, Train: 14.88%, Valid: 14.95%, Test: 14.78%
Epoch: 350, Loss: 2271.1418, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 7369.3335, Train: 14.91%, Valid: 14.98%, Test: 14.81%
Epoch: 400, Loss: 8278.6748, Train: 83.69%, Valid: 83.56%, Test: 83.87%
Epoch: 425, Loss: 2241.2424, Train: 14.90%, Valid: 14.97%, Test: 14.80%
Epoch: 450, Loss: 3722.0559, Train: 83.73%, Valid: 83.60%, Test: 83.91%
Epoch: 475, Loss: 95.5600, Train: 85.73%, Valid: 85.60%, Test: 85.79%
Run 01:
Highest Train: 86.21
Highest Valid: 86.13
  Final Train: 86.21
   Final Test: 86.28
All runs:
Highest Train: 86.21, nan
Highest Valid: 86.13, nan
  Final Train: 86.21, nan
   Final Test: 86.28, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 6.9430, Train: 15.04%, Valid: 15.01%, Test: 15.01%
Epoch: 25, Loss: 59665.0391, Train: 49.99%, Valid: 49.99%, Test: 50.00%
Epoch: 50, Loss: 11286.6221, Train: 15.87%, Valid: 15.81%, Test: 15.72%
Epoch: 75, Loss: 1973.2206, Train: 85.12%, Valid: 84.89%, Test: 85.18%
Epoch: 100, Loss: 556.2999, Train: 84.20%, Valid: 84.02%, Test: 84.23%
Epoch: 125, Loss: 1131.9878, Train: 83.58%, Valid: 83.40%, Test: 83.62%
Epoch: 150, Loss: 1207.4985, Train: 16.08%, Valid: 16.04%, Test: 15.94%
Epoch: 175, Loss: 825.9164, Train: 85.24%, Valid: 85.02%, Test: 85.29%
Epoch: 200, Loss: 470.7840, Train: 15.95%, Valid: 15.89%, Test: 15.80%
Epoch: 225, Loss: 901.2216, Train: 16.05%, Valid: 16.01%, Test: 15.91%
Epoch: 250, Loss: 859.4852, Train: 15.63%, Valid: 15.59%, Test: 15.46%
Epoch: 275, Loss: 834.3418, Train: 15.97%, Valid: 15.92%, Test: 15.82%
Epoch: 300, Loss: 832.9835, Train: 15.96%, Valid: 15.89%, Test: 15.80%
Epoch: 325, Loss: 836.9000, Train: 15.95%, Valid: 15.88%, Test: 15.80%
Epoch: 350, Loss: 834.6663, Train: 15.95%, Valid: 15.89%, Test: 15.80%
Epoch: 375, Loss: 833.8397, Train: 15.91%, Valid: 15.85%, Test: 15.75%
Epoch: 400, Loss: 833.8528, Train: 15.94%, Valid: 15.88%, Test: 15.79%
Epoch: 425, Loss: 833.6722, Train: 15.95%, Valid: 15.89%, Test: 15.80%
Epoch: 450, Loss: 833.4203, Train: 15.93%, Valid: 15.86%, Test: 15.79%
Epoch: 475, Loss: 834.0355, Train: 15.93%, Valid: 15.86%, Test: 15.78%
Run 01:
Highest Train: 86.87
Highest Valid: 86.68
  Final Train: 86.87
   Final Test: 86.81
All runs:
Highest Train: 86.87, nan
Highest Valid: 86.68, nan
  Final Train: 86.87, nan
   Final Test: 86.81, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.3948, Train: 85.19%, Valid: 85.05%, Test: 85.27%
Epoch: 25, Loss: 69595952.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 440.2425, Train: 16.67%, Valid: 16.62%, Test: 16.60%
Epoch: 75, Loss: 91042.4609, Train: 16.69%, Valid: 16.63%, Test: 16.61%
Epoch: 100, Loss: 7627.4077, Train: 84.49%, Valid: 84.30%, Test: 84.61%
Epoch: 125, Loss: 68935.4609, Train: 84.60%, Valid: 84.61%, Test: 84.72%
Epoch: 150, Loss: 2.5967, Train: 16.78%, Valid: 16.72%, Test: 16.70%
Epoch: 175, Loss: 4537.5332, Train: 16.14%, Valid: 16.37%, Test: 16.03%
Epoch: 200, Loss: 9509.2910, Train: 84.48%, Valid: 84.30%, Test: 84.60%
Epoch: 225, Loss: 4427.9780, Train: 84.18%, Valid: 84.01%, Test: 84.28%
Epoch: 250, Loss: 512.7433, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 205790.8125, Train: 84.47%, Valid: 84.28%, Test: 84.57%
Epoch: 300, Loss: 1253.2324, Train: 84.20%, Valid: 84.02%, Test: 84.34%
Epoch: 325, Loss: 176487.6406, Train: 84.18%, Valid: 84.00%, Test: 84.32%
Epoch: 350, Loss: 569.7985, Train: 16.73%, Valid: 16.68%, Test: 16.66%
Epoch: 375, Loss: 3967.1196, Train: 84.18%, Valid: 84.01%, Test: 84.28%
Epoch: 400, Loss: 526.2811, Train: 84.05%, Valid: 83.88%, Test: 84.19%
Epoch: 425, Loss: 556.4197, Train: 84.00%, Valid: 83.84%, Test: 84.15%
Epoch: 450, Loss: 918.6483, Train: 16.52%, Valid: 16.49%, Test: 16.45%
Epoch: 475, Loss: 247.3420, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.06
Highest Valid: 85.90
  Final Train: 86.06
   Final Test: 86.12
All runs:
Highest Train: 86.06, nan
Highest Valid: 85.90, nan
  Final Train: 86.06, nan
   Final Test: 86.12, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 11.6538, Train: 86.90%, Valid: 86.90%, Test: 87.00%
Epoch: 25, Loss: 2165689.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 239022480918503751680.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 9106929469095936.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 20062928258465792.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 12566281886629888.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 6437548.5000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 174866.3906, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 524078.0938, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 128.3600, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 360867.7188, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 21435.3496, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 4527.1421, Train: 15.83%, Valid: 15.88%, Test: 15.70%
Epoch: 325, Loss: 13179.5488, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 237494.0312, Train: 85.84%, Valid: 85.70%, Test: 85.92%
Epoch: 375, Loss: 21.7091, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 450.4455, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 76.1480, Train: 85.85%, Valid: 85.71%, Test: 85.92%
Epoch: 450, Loss: 72466.4609, Train: 85.85%, Valid: 85.70%, Test: 85.92%
Epoch: 475, Loss: 79247.5156, Train: 84.25%, Valid: 84.08%, Test: 84.34%
Run 01:
Highest Train: 86.90
Highest Valid: 86.90
  Final Train: 86.90
   Final Test: 87.00
All runs:
Highest Train: 86.90, nan
Highest Valid: 86.90, nan
  Final Train: 86.90, nan
   Final Test: 87.00, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 8.9926, Train: 84.96%, Valid: 84.97%, Test: 85.05%
Epoch: 25, Loss: 3.8155, Train: 84.94%, Valid: 85.04%, Test: 85.00%
Epoch: 50, Loss: 4.9742, Train: 84.69%, Valid: 84.86%, Test: 84.84%
Epoch: 75, Loss: 5.1561, Train: 84.70%, Valid: 84.86%, Test: 84.85%
Epoch: 100, Loss: 5.1003, Train: 84.70%, Valid: 84.86%, Test: 84.85%
Epoch: 125, Loss: 5.1053, Train: 84.70%, Valid: 84.86%, Test: 84.85%
Epoch: 150, Loss: 5.0721, Train: 84.69%, Valid: 84.86%, Test: 84.85%
Epoch: 175, Loss: 5.0892, Train: 84.69%, Valid: 84.86%, Test: 84.85%
Epoch: 200, Loss: 5.1556, Train: 84.69%, Valid: 84.86%, Test: 84.85%
Epoch: 225, Loss: 5.1116, Train: 84.69%, Valid: 84.85%, Test: 84.85%
Epoch: 250, Loss: 5.0974, Train: 84.69%, Valid: 84.85%, Test: 84.85%
Epoch: 275, Loss: 5.1166, Train: 84.69%, Valid: 84.85%, Test: 84.85%
Epoch: 300, Loss: 5.1148, Train: 84.69%, Valid: 84.86%, Test: 84.85%
Epoch: 325, Loss: 5.2107, Train: 84.69%, Valid: 84.85%, Test: 84.85%
Epoch: 350, Loss: 5.2412, Train: 84.69%, Valid: 84.85%, Test: 84.85%
Epoch: 375, Loss: 5.1821, Train: 84.70%, Valid: 84.88%, Test: 84.85%
Epoch: 400, Loss: 5.0612, Train: 84.73%, Valid: 84.92%, Test: 84.87%
Epoch: 425, Loss: 5.1836, Train: 84.73%, Valid: 84.92%, Test: 84.87%
Epoch: 450, Loss: 5.3468, Train: 84.73%, Valid: 84.92%, Test: 84.86%
Epoch: 475, Loss: 5.0645, Train: 84.73%, Valid: 84.92%, Test: 84.87%
Run 01:
Highest Train: 86.38
Highest Valid: 86.28
  Final Train: 86.38
   Final Test: 86.43
All runs:
Highest Train: 86.38, nan
Highest Valid: 86.28, nan
  Final Train: 86.38, nan
   Final Test: 86.43, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.7981, Train: 83.73%, Valid: 83.53%, Test: 83.91%
Epoch: 25, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 88.21
Highest Valid: 88.21
  Final Train: 88.21
   Final Test: 88.21
All runs:
Highest Train: 88.21, nan
Highest Valid: 88.21, nan
  Final Train: 88.21, nan
   Final Test: 88.21, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 7.4526, Train: 85.25%, Valid: 85.09%, Test: 85.36%
Epoch: 25, Loss: 286998409292119551053242323435520.0000, Train: 86.27%, Valid: 86.15%, Test: 86.30%
Epoch: 50, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 59433379954688.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 291067969432812767177175203840.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.67
Highest Valid: 86.53
  Final Train: 86.67
   Final Test: 86.64
All runs:
Highest Train: 86.67, nan
Highest Valid: 86.53, nan
  Final Train: 86.67, nan
   Final Test: 86.64, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 7.6919, Train: 85.62%, Valid: 85.52%, Test: 85.67%
Epoch: 25, Loss: 693.6227, Train: 16.63%, Valid: 16.73%, Test: 16.53%
Epoch: 50, Loss: 37531.2383, Train: 88.22%, Valid: 88.27%, Test: 88.26%
Epoch: 75, Loss: 60518309698732032.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 88.22
Highest Valid: 88.27
  Final Train: 88.22
   Final Test: 88.26
All runs:
Highest Train: 88.22, nan
Highest Valid: 88.27, nan
  Final Train: 88.22, nan
   Final Test: 88.26, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 11.5234, Train: 86.68%, Valid: 86.76%, Test: 86.74%
Epoch: 25, Loss: 9.0895, Train: 84.87%, Valid: 84.90%, Test: 85.02%
Epoch: 50, Loss: 1640.4302, Train: 16.31%, Valid: 16.51%, Test: 16.20%
Epoch: 75, Loss: 1791.7850, Train: 16.28%, Valid: 16.48%, Test: 16.17%
Epoch: 100, Loss: 1800.1970, Train: 16.28%, Valid: 16.48%, Test: 16.18%
Epoch: 125, Loss: 1801.4840, Train: 16.28%, Valid: 16.48%, Test: 16.18%
Epoch: 150, Loss: 1796.7336, Train: 16.28%, Valid: 16.49%, Test: 16.19%
Epoch: 175, Loss: 1688.7982, Train: 16.26%, Valid: 16.48%, Test: 16.17%
Epoch: 200, Loss: 245.6954, Train: 16.83%, Valid: 16.72%, Test: 16.87%
Epoch: 225, Loss: 14.4406, Train: 84.70%, Valid: 84.71%, Test: 84.84%
Epoch: 250, Loss: 14.1464, Train: 84.68%, Valid: 84.69%, Test: 84.82%
Epoch: 275, Loss: 12.6920, Train: 88.34%, Valid: 88.39%, Test: 88.37%
Epoch: 300, Loss: 11.4602, Train: 88.34%, Valid: 88.37%, Test: 88.36%
Epoch: 325, Loss: 10.2765, Train: 88.32%, Valid: 88.37%, Test: 88.35%
Epoch: 350, Loss: 9.1465, Train: 88.32%, Valid: 88.37%, Test: 88.35%
Epoch: 375, Loss: 8.0536, Train: 88.34%, Valid: 88.39%, Test: 88.36%
Epoch: 400, Loss: 6.9584, Train: 88.34%, Valid: 88.38%, Test: 88.36%
Epoch: 425, Loss: 5.9169, Train: 88.34%, Valid: 88.38%, Test: 88.36%
Epoch: 450, Loss: 4.9340, Train: 88.35%, Valid: 88.38%, Test: 88.37%
Epoch: 475, Loss: 4.0068, Train: 88.36%, Valid: 88.39%, Test: 88.38%
Run 01:
Highest Train: 88.49
Highest Valid: 88.50
  Final Train: 88.49
   Final Test: 88.52
All runs:
Highest Train: 88.49, nan
Highest Valid: 88.50, nan
  Final Train: 88.49, nan
   Final Test: 88.52, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 15.2221, Train: 86.00%, Valid: 85.99%, Test: 86.11%
Epoch: 25, Loss: 1649.8638, Train: 16.24%, Valid: 16.38%, Test: 16.05%
Epoch: 50, Loss: 27.3194, Train: 83.25%, Valid: 83.13%, Test: 83.39%
Epoch: 75, Loss: 201.8768, Train: 86.22%, Valid: 86.11%, Test: 86.34%
Epoch: 100, Loss: 212.1616, Train: 86.29%, Valid: 86.18%, Test: 86.40%
Epoch: 125, Loss: 340.5734, Train: 86.44%, Valid: 86.34%, Test: 86.53%
Epoch: 150, Loss: 286.1816, Train: 83.61%, Valid: 83.50%, Test: 83.81%
Epoch: 175, Loss: 376.5549, Train: 83.57%, Valid: 83.47%, Test: 83.78%
Epoch: 200, Loss: 315.2307, Train: 85.60%, Valid: 85.49%, Test: 85.73%
Epoch: 225, Loss: 304.6481, Train: 83.62%, Valid: 83.50%, Test: 83.82%
Epoch: 250, Loss: 337.0414, Train: 86.18%, Valid: 86.06%, Test: 86.31%
Epoch: 275, Loss: 351.9961, Train: 86.17%, Valid: 86.05%, Test: 86.29%
Epoch: 300, Loss: 256.9743, Train: 83.59%, Valid: 83.47%, Test: 83.79%
Epoch: 325, Loss: 235.5536, Train: 85.49%, Valid: 85.34%, Test: 85.63%
Epoch: 350, Loss: 311.2553, Train: 83.74%, Valid: 83.62%, Test: 83.93%
Epoch: 375, Loss: 213.6254, Train: 85.28%, Valid: 85.10%, Test: 85.40%
Epoch: 400, Loss: 355.4420, Train: 84.11%, Valid: 83.94%, Test: 84.29%
Epoch: 425, Loss: 227.7468, Train: 84.11%, Valid: 83.94%, Test: 84.30%
Epoch: 450, Loss: 221.6469, Train: 85.37%, Valid: 85.19%, Test: 85.48%
Epoch: 475, Loss: 282.6995, Train: 86.17%, Valid: 86.02%, Test: 86.28%
Run 01:
Highest Train: 87.43
Highest Valid: 87.41
  Final Train: 87.43
   Final Test: 87.41
All runs:
Highest Train: 87.43, nan
Highest Valid: 87.41, nan
  Final Train: 87.43, nan
   Final Test: 87.41, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 5.0132, Train: 85.00%, Valid: 84.89%, Test: 85.04%
Epoch: 25, Loss: 8.4948, Train: 87.76%, Valid: 87.61%, Test: 87.72%
Epoch: 50, Loss: 8.5389, Train: 88.01%, Valid: 87.88%, Test: 87.99%
Epoch: 75, Loss: 8.3319, Train: 87.55%, Valid: 87.52%, Test: 87.56%
Epoch: 100, Loss: 8.0456, Train: 87.65%, Valid: 87.62%, Test: 87.64%
Epoch: 125, Loss: 7.6515, Train: 86.47%, Valid: 86.53%, Test: 86.58%
Epoch: 150, Loss: 7.0648, Train: 87.63%, Valid: 87.54%, Test: 87.62%
Epoch: 175, Loss: 1.9616, Train: 83.35%, Valid: 83.27%, Test: 83.41%
Epoch: 200, Loss: 8.4661, Train: 86.76%, Valid: 86.68%, Test: 86.77%
Epoch: 225, Loss: 5.6026, Train: 87.46%, Valid: 87.47%, Test: 87.49%
Epoch: 250, Loss: 4.1544, Train: 86.46%, Valid: 86.35%, Test: 86.46%
Epoch: 275, Loss: 0.8244, Train: 85.14%, Valid: 84.96%, Test: 85.22%
Epoch: 300, Loss: 2.3166, Train: 87.19%, Valid: 87.20%, Test: 87.20%
Epoch: 325, Loss: 3.3320, Train: 87.23%, Valid: 87.24%, Test: 87.22%
Epoch: 350, Loss: 8.6630, Train: 87.22%, Valid: 87.22%, Test: 87.21%
Epoch: 375, Loss: 5.3856, Train: 87.24%, Valid: 87.26%, Test: 87.26%
Epoch: 400, Loss: 0.8969, Train: 86.84%, Valid: 86.87%, Test: 86.89%
Epoch: 425, Loss: 0.6509, Train: 85.63%, Valid: 85.46%, Test: 85.64%
Epoch: 450, Loss: 0.7014, Train: 85.42%, Valid: 85.23%, Test: 85.49%
Epoch: 475, Loss: 0.5005, Train: 85.95%, Valid: 85.78%, Test: 86.03%
Run 01:
Highest Train: 88.11
Highest Valid: 88.02
  Final Train: 88.10
   Final Test: 88.10
All runs:
Highest Train: 88.11, nan
Highest Valid: 88.02, nan
  Final Train: 88.10, nan
   Final Test: 88.10, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.4715, Train: 87.93%, Valid: 87.98%, Test: 88.02%
Epoch: 25, Loss: 827.0287, Train: 16.46%, Valid: 16.64%, Test: 16.31%
Epoch: 50, Loss: 967.8287, Train: 16.45%, Valid: 16.63%, Test: 16.29%
Epoch: 75, Loss: 981.0728, Train: 16.45%, Valid: 16.63%, Test: 16.29%
Epoch: 100, Loss: 984.3629, Train: 16.45%, Valid: 16.63%, Test: 16.29%
Epoch: 125, Loss: 981.4207, Train: 16.45%, Valid: 16.63%, Test: 16.29%
Epoch: 150, Loss: 983.5911, Train: 16.45%, Valid: 16.63%, Test: 16.29%
Epoch: 175, Loss: 979.1296, Train: 16.45%, Valid: 16.63%, Test: 16.29%
Epoch: 200, Loss: 978.2434, Train: 16.45%, Valid: 16.63%, Test: 16.29%
Epoch: 225, Loss: 982.6918, Train: 16.45%, Valid: 16.63%, Test: 16.29%
Epoch: 250, Loss: 978.4143, Train: 16.45%, Valid: 16.63%, Test: 16.29%
Epoch: 275, Loss: 982.1854, Train: 16.45%, Valid: 16.63%, Test: 16.29%
Epoch: 300, Loss: 981.6563, Train: 16.45%, Valid: 16.63%, Test: 16.29%
Epoch: 325, Loss: 984.0341, Train: 16.45%, Valid: 16.63%, Test: 16.29%
Epoch: 350, Loss: 979.6643, Train: 16.45%, Valid: 16.62%, Test: 16.29%
Epoch: 375, Loss: 983.9631, Train: 16.45%, Valid: 16.63%, Test: 16.29%
Epoch: 400, Loss: 982.2335, Train: 16.45%, Valid: 16.63%, Test: 16.29%
Epoch: 425, Loss: 979.4512, Train: 16.45%, Valid: 16.63%, Test: 16.29%
Epoch: 450, Loss: 984.1116, Train: 16.45%, Valid: 16.63%, Test: 16.29%
Epoch: 475, Loss: 979.9512, Train: 16.45%, Valid: 16.63%, Test: 16.29%
Run 01:
Highest Train: 87.93
Highest Valid: 87.98
  Final Train: 87.93
   Final Test: 88.02
All runs:
Highest Train: 87.93, nan
Highest Valid: 87.98, nan
  Final Train: 87.93, nan
   Final Test: 88.02, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 6.1334, Train: 85.45%, Valid: 85.31%, Test: 85.57%
Epoch: 25, Loss: 71.0755, Train: 85.71%, Valid: 85.61%, Test: 85.71%
Epoch: 50, Loss: 7.7054, Train: 85.98%, Valid: 85.84%, Test: 85.94%
Epoch: 75, Loss: 6.0066, Train: 86.14%, Valid: 86.01%, Test: 86.12%
Epoch: 100, Loss: 5.3893, Train: 86.15%, Valid: 86.01%, Test: 86.17%
Epoch: 125, Loss: 7.4156, Train: 86.51%, Valid: 86.50%, Test: 86.63%
Epoch: 150, Loss: 5.3419, Train: 86.78%, Valid: 86.79%, Test: 86.85%
Epoch: 175, Loss: 2.3091, Train: 87.34%, Valid: 87.27%, Test: 87.31%
Epoch: 200, Loss: 7.0521, Train: 87.05%, Valid: 87.08%, Test: 87.14%
Epoch: 225, Loss: 18.5861, Train: 84.50%, Valid: 84.39%, Test: 84.51%
Epoch: 250, Loss: 55.1966, Train: 83.66%, Valid: 83.56%, Test: 83.83%
Epoch: 275, Loss: 3235.0952, Train: 50.07%, Valid: 50.06%, Test: 50.07%
Epoch: 300, Loss: 1457932.8750, Train: 50.10%, Valid: 50.08%, Test: 50.10%
Epoch: 325, Loss: 3572251.2500, Train: 84.82%, Valid: 84.80%, Test: 84.97%
Epoch: 350, Loss: 5542.3843, Train: 84.93%, Valid: 84.76%, Test: 85.07%
Epoch: 375, Loss: 6681.0713, Train: 15.93%, Valid: 15.99%, Test: 15.81%
Epoch: 400, Loss: 4832.6128, Train: 84.92%, Valid: 84.77%, Test: 85.06%
Epoch: 425, Loss: 341027.9062, Train: 85.02%, Valid: 84.92%, Test: 85.13%
Epoch: 450, Loss: 124470.8750, Train: 86.47%, Valid: 86.39%, Test: 86.54%
Epoch: 475, Loss: 4627.8599, Train: 84.85%, Valid: 84.82%, Test: 85.00%
Run 01:
Highest Train: 87.57
Highest Valid: 87.46
  Final Train: 87.57
   Final Test: 87.40
All runs:
Highest Train: 87.57, nan
Highest Valid: 87.46, nan
  Final Train: 87.57, nan
   Final Test: 87.40, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 5.6971, Train: 86.29%, Valid: 86.18%, Test: 86.43%
Epoch: 25, Loss: 120.6991, Train: 49.99%, Valid: 49.99%, Test: 50.00%
Epoch: 50, Loss: 103.3434, Train: 84.08%, Valid: 84.12%, Test: 84.22%
Epoch: 75, Loss: 2058403.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 292662944.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 6046205.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 3699594240.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 6412571.5000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 132099808.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 196911.4062, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 115471832.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 9341797.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 39416984.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 2109057.7500, Train: 85.06%, Valid: 84.88%, Test: 85.22%
Epoch: 350, Loss: 23203.6074, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 3077529600.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 363547.1250, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 245.5144, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 9231272.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 185128464.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 87.18
Highest Valid: 87.28
  Final Train: 87.18
   Final Test: 87.21
All runs:
Highest Train: 87.18, nan
Highest Valid: 87.28, nan
  Final Train: 87.18, nan
   Final Test: 87.21, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.3680, Train: 85.54%, Valid: 85.37%, Test: 85.65%
Epoch: 25, Loss: 0.9538, Train: 86.76%, Valid: 86.78%, Test: 86.79%
Epoch: 50, Loss: 1.4770, Train: 87.13%, Valid: 87.13%, Test: 87.16%
Epoch: 75, Loss: 0.3584, Train: 86.93%, Valid: 86.70%, Test: 87.03%
Epoch: 100, Loss: 0.3713, Train: 87.51%, Valid: 87.38%, Test: 87.52%
Epoch: 125, Loss: 0.9285, Train: 86.19%, Valid: 86.15%, Test: 86.20%
Epoch: 150, Loss: 1.3871, Train: 87.56%, Valid: 87.59%, Test: 87.60%
Epoch: 175, Loss: 1.8138, Train: 87.51%, Valid: 87.54%, Test: 87.61%
Epoch: 200, Loss: 2.0052, Train: 86.12%, Valid: 86.05%, Test: 86.12%
Epoch: 225, Loss: 2.2309, Train: 85.99%, Valid: 86.00%, Test: 86.16%
Epoch: 250, Loss: 1484022.6250, Train: 17.00%, Valid: 17.18%, Test: 16.95%
Epoch: 275, Loss: 6556.9561, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 7791.4834, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 7577.9746, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 6863.4658, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 5973.6426, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 7273.1045, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 5363.1694, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 14512.3701, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 7631.0400, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 88.44
Highest Valid: 88.48
  Final Train: 88.44
   Final Test: 88.53
All runs:
Highest Train: 88.44, nan
Highest Valid: 88.48, nan
  Final Train: 88.44, nan
   Final Test: 88.53, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 8.9206, Train: 85.89%, Valid: 85.75%, Test: 85.86%
Epoch: 25, Loss: 349874815255594438936311694884864.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 91.0657, Train: 85.89%, Valid: 85.87%, Test: 85.94%
Epoch: 75, Loss: 2615.4502, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 277501920.0000, Train: 85.82%, Valid: 85.78%, Test: 85.87%
Epoch: 125, Loss: 3590.0486, Train: 85.82%, Valid: 85.78%, Test: 85.87%
Epoch: 150, Loss: 220.6012, Train: 83.92%, Valid: 83.76%, Test: 84.05%
Epoch: 175, Loss: 24772.7031, Train: 85.82%, Valid: 85.78%, Test: 85.87%
Epoch: 200, Loss: 198519864426496.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 1556822.0000, Train: 85.79%, Valid: 85.75%, Test: 85.85%
Epoch: 250, Loss: 13171549079076864.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 145.4065, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 216.3081, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 166.3519, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 61112287232.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 810798336.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 1312059817984.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 381.4095, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 165.6598, Train: 85.75%, Valid: 85.72%, Test: 85.78%
Epoch: 475, Loss: 187.9880, Train: 85.73%, Valid: 85.71%, Test: 85.78%
Run 01:
Highest Train: 86.98
Highest Valid: 86.87
  Final Train: 86.98
   Final Test: 86.97
All runs:
Highest Train: 86.98, nan
Highest Valid: 86.87, nan
  Final Train: 86.98, nan
   Final Test: 86.97, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 8.9617, Train: 85.21%, Valid: 85.08%, Test: 85.24%
Epoch: 25, Loss: 12.1165, Train: 85.68%, Valid: 85.51%, Test: 85.70%
Epoch: 50, Loss: 87748.9922, Train: 83.24%, Valid: 83.33%, Test: 83.35%
Epoch: 75, Loss: 1621.2593, Train: 82.82%, Valid: 82.89%, Test: 82.96%
Epoch: 100, Loss: 1187.8882, Train: 83.53%, Valid: 83.58%, Test: 83.69%
Epoch: 125, Loss: 699.2779, Train: 83.61%, Valid: 83.69%, Test: 83.78%
Epoch: 150, Loss: 673.1941, Train: 83.61%, Valid: 83.69%, Test: 83.78%
Epoch: 175, Loss: 668.0939, Train: 83.61%, Valid: 83.69%, Test: 83.78%
Epoch: 200, Loss: 666.5976, Train: 83.61%, Valid: 83.69%, Test: 83.78%
Epoch: 225, Loss: 668.2460, Train: 83.61%, Valid: 83.69%, Test: 83.78%
Epoch: 250, Loss: 675.8617, Train: 83.61%, Valid: 83.69%, Test: 83.78%
Epoch: 275, Loss: 668.5298, Train: 83.61%, Valid: 83.69%, Test: 83.78%
Epoch: 300, Loss: 668.5665, Train: 83.61%, Valid: 83.69%, Test: 83.78%
Epoch: 325, Loss: 668.4553, Train: 83.61%, Valid: 83.69%, Test: 83.79%
Epoch: 350, Loss: 671.0621, Train: 83.61%, Valid: 83.69%, Test: 83.78%
Epoch: 375, Loss: 671.6031, Train: 83.61%, Valid: 83.69%, Test: 83.79%
Epoch: 400, Loss: 668.5242, Train: 83.61%, Valid: 83.69%, Test: 83.79%
Epoch: 425, Loss: 671.2902, Train: 83.61%, Valid: 83.69%, Test: 83.78%
Epoch: 450, Loss: 675.2415, Train: 83.61%, Valid: 83.69%, Test: 83.78%
Epoch: 475, Loss: 668.1174, Train: 83.61%, Valid: 83.69%, Test: 83.78%
Run 01:
Highest Train: 86.21
Highest Valid: 86.06
  Final Train: 86.21
   Final Test: 86.29
All runs:
Highest Train: 86.21, nan
Highest Valid: 86.06, nan
  Final Train: 86.21, nan
   Final Test: 86.29, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.9302, Train: 85.65%, Valid: 85.70%, Test: 85.76%
Epoch: 25, Loss: 26898.2734, Train: 83.94%, Valid: 83.73%, Test: 84.06%
Epoch: 50, Loss: 111.9858, Train: 84.55%, Valid: 84.53%, Test: 84.71%
Epoch: 75, Loss: 20.0975, Train: 84.22%, Valid: 84.01%, Test: 84.32%
Epoch: 100, Loss: 151.7783, Train: 84.22%, Valid: 84.01%, Test: 84.32%
Epoch: 125, Loss: 134.5675, Train: 11.74%, Valid: 11.69%, Test: 11.70%
Epoch: 150, Loss: 133.1537, Train: 15.31%, Valid: 15.28%, Test: 15.16%
Epoch: 175, Loss: 111.9179, Train: 15.31%, Valid: 15.28%, Test: 15.16%
Epoch: 200, Loss: 112.2934, Train: 84.20%, Valid: 83.98%, Test: 84.29%
Epoch: 225, Loss: 77.9351, Train: 15.31%, Valid: 15.28%, Test: 15.17%
Epoch: 250, Loss: 112.0947, Train: 15.30%, Valid: 15.27%, Test: 15.16%
Epoch: 275, Loss: 238.8592, Train: 15.31%, Valid: 15.28%, Test: 15.17%
Epoch: 300, Loss: 170.0482, Train: 15.30%, Valid: 15.27%, Test: 15.16%
Epoch: 325, Loss: 72.9617, Train: 11.70%, Valid: 11.63%, Test: 11.67%
Epoch: 350, Loss: 90.9070, Train: 15.29%, Valid: 15.26%, Test: 15.15%
Epoch: 375, Loss: 114.4716, Train: 11.70%, Valid: 11.64%, Test: 11.67%
Epoch: 400, Loss: 119.0923, Train: 15.30%, Valid: 15.27%, Test: 15.15%
Epoch: 425, Loss: 134.7074, Train: 15.31%, Valid: 15.27%, Test: 15.17%
Epoch: 450, Loss: 120.0056, Train: 15.31%, Valid: 15.27%, Test: 15.16%
Epoch: 475, Loss: 103.3674, Train: 15.31%, Valid: 15.28%, Test: 15.16%
Run 01:
Highest Train: 86.54
Highest Valid: 86.58
  Final Train: 86.54
   Final Test: 86.62
All runs:
Highest Train: 86.54, nan
Highest Valid: 86.58, nan
  Final Train: 86.54, nan
   Final Test: 86.62, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 5.2739, Train: 86.73%, Valid: 86.72%, Test: 86.82%
Epoch: 25, Loss: 312247.3750, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 7445.7173, Train: 85.71%, Valid: 85.64%, Test: 85.72%
Epoch: 75, Loss: 3914.0940, Train: 16.13%, Valid: 16.31%, Test: 15.97%
Epoch: 100, Loss: 206.5880, Train: 85.90%, Valid: 85.82%, Test: 85.90%
Epoch: 125, Loss: 105.0179, Train: 85.82%, Valid: 85.74%, Test: 85.81%
Epoch: 150, Loss: 7.3591, Train: 85.78%, Valid: 85.69%, Test: 85.77%
Epoch: 175, Loss: 517.0676, Train: 85.78%, Valid: 85.69%, Test: 85.77%
Epoch: 200, Loss: 1018.1328, Train: 16.20%, Valid: 16.37%, Test: 16.04%
Epoch: 225, Loss: 8.5928, Train: 85.88%, Valid: 85.79%, Test: 85.88%
Epoch: 250, Loss: 782.4573, Train: 85.82%, Valid: 85.73%, Test: 85.81%
Epoch: 275, Loss: 228.8666, Train: 85.90%, Valid: 85.81%, Test: 85.90%
Epoch: 300, Loss: 81.1453, Train: 85.91%, Valid: 85.82%, Test: 85.90%
Epoch: 325, Loss: 65.2473, Train: 85.87%, Valid: 85.79%, Test: 85.87%
Epoch: 350, Loss: 74.5800, Train: 85.86%, Valid: 85.79%, Test: 85.86%
Epoch: 375, Loss: 127.9826, Train: 85.84%, Valid: 85.76%, Test: 85.83%
Epoch: 400, Loss: 6.3910, Train: 85.91%, Valid: 85.81%, Test: 85.90%
Epoch: 425, Loss: 2.2587, Train: 85.86%, Valid: 85.78%, Test: 85.85%
Epoch: 450, Loss: 103.7233, Train: 85.87%, Valid: 85.78%, Test: 85.86%
Epoch: 475, Loss: 23.3211, Train: 85.86%, Valid: 85.78%, Test: 85.85%
Run 01:
Highest Train: 86.73
Highest Valid: 86.72
  Final Train: 86.73
   Final Test: 86.82
All runs:
Highest Train: 86.73, nan
Highest Valid: 86.72, nan
  Final Train: 86.73, nan
   Final Test: 86.82, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 3.2348, Train: 85.07%, Valid: 84.94%, Test: 85.14%
Epoch: 25, Loss: 11542780.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 9308.8896, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 207834.0781, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 94.4251, Train: 84.46%, Valid: 84.28%, Test: 84.59%
Epoch: 125, Loss: 8.5142, Train: 14.93%, Valid: 15.08%, Test: 14.79%
Epoch: 150, Loss: 3.4284, Train: 86.75%, Valid: 86.69%, Test: 86.83%
Epoch: 175, Loss: 20.8474, Train: 16.49%, Valid: 16.54%, Test: 16.41%
Epoch: 200, Loss: 2.6249, Train: 86.78%, Valid: 86.72%, Test: 86.86%
Epoch: 225, Loss: 2.4757, Train: 86.80%, Valid: 86.74%, Test: 86.87%
Epoch: 250, Loss: 2.9056, Train: 86.70%, Valid: 86.68%, Test: 86.79%
Epoch: 275, Loss: 2.6764, Train: 86.19%, Valid: 86.18%, Test: 86.27%
Epoch: 300, Loss: 2.7218, Train: 86.56%, Valid: 86.46%, Test: 86.59%
Epoch: 325, Loss: 2.7994, Train: 86.54%, Valid: 86.44%, Test: 86.58%
Epoch: 350, Loss: 2.7623, Train: 86.76%, Valid: 86.71%, Test: 86.84%
Epoch: 375, Loss: 2.6612, Train: 86.78%, Valid: 86.71%, Test: 86.85%
Epoch: 400, Loss: 2.7693, Train: 86.82%, Valid: 86.75%, Test: 86.88%
Epoch: 425, Loss: 2.6265, Train: 86.79%, Valid: 86.72%, Test: 86.86%
Epoch: 450, Loss: 2.6302, Train: 86.61%, Valid: 86.51%, Test: 86.63%
Epoch: 475, Loss: 2.7033, Train: 86.69%, Valid: 86.58%, Test: 86.72%
Run 01:
Highest Train: 86.83
Highest Valid: 86.76
  Final Train: 86.83
   Final Test: 86.89
All runs:
Highest Train: 86.83, nan
Highest Valid: 86.76, nan
  Final Train: 86.83, nan
   Final Test: 86.89, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.4788, Train: 84.61%, Valid: 84.42%, Test: 84.73%
Epoch: 25, Loss: 19600.7812, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 5781.0396, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 283138317901155532800.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 9746654.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 1012554399744.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 842855744.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 1221494833152.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 5015386193920.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 7159147921408.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 32291129344.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 8007780925440.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 842350329856.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 7048717139968.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 28223100420096.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 56810999808.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 37935394816.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 29820186624.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 1152917307392.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 146011635712.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.55
Highest Valid: 86.60
  Final Train: 86.55
   Final Test: 86.51
All runs:
Highest Train: 86.55, nan
Highest Valid: 86.60, nan
  Final Train: 86.55, nan
   Final Test: 86.51, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 3.0793, Train: 86.39%, Valid: 86.28%, Test: 86.37%
Epoch: 25, Loss: 0.6095, Train: 85.92%, Valid: 85.82%, Test: 85.97%
Epoch: 50, Loss: 0.3774, Train: 85.72%, Valid: 85.65%, Test: 85.80%
Epoch: 75, Loss: 0.3645, Train: 85.78%, Valid: 85.67%, Test: 85.86%
Epoch: 100, Loss: 0.3609, Train: 85.87%, Valid: 85.76%, Test: 85.97%
Epoch: 125, Loss: 0.3538, Train: 85.71%, Valid: 85.57%, Test: 85.83%
Epoch: 150, Loss: 0.3434, Train: 85.55%, Valid: 85.38%, Test: 85.65%
Epoch: 175, Loss: 0.3340, Train: 85.73%, Valid: 85.58%, Test: 85.86%
Epoch: 200, Loss: 0.6559, Train: 85.87%, Valid: 85.75%, Test: 86.01%
Epoch: 225, Loss: 4.7213, Train: 86.53%, Valid: 86.47%, Test: 86.57%
Epoch: 250, Loss: 25.3002, Train: 85.04%, Valid: 84.98%, Test: 85.13%
Epoch: 275, Loss: 28064000.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 1467329920.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 72238680.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 34016148.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 504835.3125, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 232233424.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 87153590272.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 4987696.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 277996128.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 87.24
Highest Valid: 87.22
  Final Train: 87.24
   Final Test: 87.28
All runs:
Highest Train: 87.24, nan
Highest Valid: 87.22, nan
  Final Train: 87.24, nan
   Final Test: 87.28, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 5.6837, Train: 79.09%, Valid: 79.17%, Test: 79.47%
Epoch: 25, Loss: 480699648.0000, Train: 16.12%, Valid: 16.20%, Test: 16.00%
Epoch: 50, Loss: 1433809977344.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 1864369024.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 244802512.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 597717120.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 9465507.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 33976704.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 27569270784.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 2872998656.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 264679392.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 4808515584.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 854348660736.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 622922825728.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 61607538688.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 13340122112.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 6496369664.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 3626334.7500, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 20113496064.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 27876225024.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.32
Highest Valid: 86.40
  Final Train: 86.32
   Final Test: 86.36
All runs:
Highest Train: 86.32, nan
Highest Valid: 86.40, nan
  Final Train: 86.32, nan
   Final Test: 86.36, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.8835, Train: 86.56%, Valid: 86.59%, Test: 86.61%
Epoch: 25, Loss: 83.8552, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 11875321847404008509789056270336.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 87.31
Highest Valid: 87.27
  Final Train: 87.31
   Final Test: 87.34
All runs:
Highest Train: 87.31, nan
Highest Valid: 87.27, nan
  Final Train: 87.31, nan
   Final Test: 87.34, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.3807, Train: 85.36%, Valid: 85.17%, Test: 85.43%
Epoch: 25, Loss: 134771838415545689268289536.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.42
Highest Valid: 86.39
  Final Train: 86.42
   Final Test: 86.58
All runs:
Highest Train: 86.42, nan
Highest Valid: 86.39, nan
  Final Train: 86.42, nan
   Final Test: 86.58, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.8799, Train: 86.02%, Valid: 86.10%, Test: 86.02%
Epoch: 25, Loss: inf, Train: 50.01%, Valid: 50.01%, Test: 50.03%
Epoch: 50, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 27.6165, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 9222263808.0000, Train: 86.41%, Valid: 86.48%, Test: 86.52%
Epoch: 125, Loss: 17561935872.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 2718328160256.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 12.0240, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 1122686465998848.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 44.4651, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 33027619028992.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 543874816.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 118065264.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 1832881792.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 37339918336.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 36820565650046976.0000, Train: 86.47%, Valid: 86.57%, Test: 86.59%
Epoch: 400, Loss: 2757542543360.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 10467033481216.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 1451000.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 15361.2354, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.85
Highest Valid: 86.88
  Final Train: 86.85
   Final Test: 86.91
All runs:
Highest Train: 86.85, nan
Highest Valid: 86.88, nan
  Final Train: 86.85, nan
   Final Test: 86.91, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 8.1954, Train: 85.61%, Valid: 85.40%, Test: 85.69%
Epoch: 25, Loss: 5009.2793, Train: 84.66%, Valid: 84.66%, Test: 84.79%
Epoch: 50, Loss: 780.5249, Train: 84.98%, Valid: 84.98%, Test: 85.13%
Epoch: 75, Loss: 57.9805, Train: 88.43%, Valid: 88.48%, Test: 88.44%
Epoch: 100, Loss: 30.1266, Train: 87.99%, Valid: 88.03%, Test: 87.99%
Epoch: 125, Loss: 26.7245, Train: 88.00%, Valid: 88.04%, Test: 87.99%
Epoch: 150, Loss: 29.2561, Train: 88.00%, Valid: 88.04%, Test: 88.00%
Epoch: 175, Loss: 26.1621, Train: 88.00%, Valid: 88.04%, Test: 88.00%
Epoch: 200, Loss: 29.0274, Train: 88.00%, Valid: 88.04%, Test: 88.00%
Epoch: 225, Loss: 28.3786, Train: 88.00%, Valid: 88.04%, Test: 88.00%
Epoch: 250, Loss: 28.3181, Train: 88.00%, Valid: 88.04%, Test: 88.00%
Epoch: 275, Loss: 29.9227, Train: 84.49%, Valid: 84.31%, Test: 84.62%
Epoch: 300, Loss: 24.5443, Train: 84.49%, Valid: 84.31%, Test: 84.62%
Epoch: 325, Loss: 29.9089, Train: 84.48%, Valid: 84.30%, Test: 84.62%
Epoch: 350, Loss: 31.1059, Train: 84.48%, Valid: 84.30%, Test: 84.62%
Epoch: 375, Loss: 28.1649, Train: 84.48%, Valid: 84.30%, Test: 84.61%
Epoch: 400, Loss: 29.6925, Train: 84.48%, Valid: 84.30%, Test: 84.61%
Epoch: 425, Loss: 26.9645, Train: 84.48%, Valid: 84.30%, Test: 84.62%
Epoch: 450, Loss: 28.8212, Train: 84.48%, Valid: 84.30%, Test: 84.62%
Epoch: 475, Loss: 31.3168, Train: 84.48%, Valid: 84.30%, Test: 84.62%
Run 01:
Highest Train: 88.47
Highest Valid: 88.50
  Final Train: 88.47
   Final Test: 88.49
All runs:
Highest Train: 88.47, nan
Highest Valid: 88.50, nan
  Final Train: 88.47, nan
   Final Test: 88.49, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 6.5636, Train: 85.95%, Valid: 85.90%, Test: 86.02%
Epoch: 25, Loss: 7559.7134, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 6994253.5000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 1851976.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 1890.5254, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 358934.0000, Train: 84.73%, Valid: 84.72%, Test: 84.80%
Epoch: 150, Loss: 191.9405, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 917.9164, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 78202.7109, Train: 16.33%, Valid: 16.47%, Test: 16.17%
Epoch: 225, Loss: 570.3377, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 711.7118, Train: 85.00%, Valid: 84.94%, Test: 85.07%
Epoch: 275, Loss: 74487.6875, Train: 15.55%, Valid: 15.65%, Test: 15.42%
Epoch: 300, Loss: 38899.8984, Train: 85.00%, Valid: 84.96%, Test: 85.08%
Epoch: 325, Loss: 385.6937, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 35670.3047, Train: 16.38%, Valid: 16.52%, Test: 16.22%
Epoch: 375, Loss: 619.8745, Train: 84.98%, Valid: 84.94%, Test: 85.07%
Epoch: 400, Loss: 398.9629, Train: 16.36%, Valid: 16.51%, Test: 16.20%
Epoch: 425, Loss: 111.4436, Train: 84.98%, Valid: 84.94%, Test: 85.07%
Epoch: 450, Loss: 108.2041, Train: 16.37%, Valid: 16.50%, Test: 16.20%
Epoch: 475, Loss: 4204.6885, Train: 84.84%, Valid: 84.81%, Test: 84.90%
Run 01:
Highest Train: 86.29
Highest Valid: 86.26
  Final Train: 86.29
   Final Test: 86.35
All runs:
Highest Train: 86.29, nan
Highest Valid: 86.26, nan
  Final Train: 86.29, nan
   Final Test: 86.35, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.0337, Train: 83.74%, Valid: 83.61%, Test: 83.78%
Epoch: 25, Loss: 8.7553, Train: 86.94%, Valid: 86.92%, Test: 87.00%
Epoch: 50, Loss: 12.4549, Train: 86.53%, Valid: 86.43%, Test: 86.56%
Epoch: 75, Loss: 12.8204, Train: 85.54%, Valid: 85.37%, Test: 85.62%
Epoch: 100, Loss: 62.7105, Train: 83.51%, Valid: 83.57%, Test: 83.66%
Epoch: 125, Loss: 17.6836, Train: 83.64%, Valid: 83.65%, Test: 83.76%
Epoch: 150, Loss: 15.9967, Train: 83.71%, Valid: 83.72%, Test: 83.82%
Epoch: 175, Loss: 13.6629, Train: 87.00%, Valid: 87.03%, Test: 87.03%
Epoch: 200, Loss: 10.9780, Train: 86.97%, Valid: 86.99%, Test: 87.00%
Epoch: 225, Loss: 3.9905, Train: 86.89%, Valid: 86.90%, Test: 86.91%
Epoch: 250, Loss: 0.9711, Train: 86.94%, Valid: 86.87%, Test: 86.91%
Epoch: 275, Loss: 0.6449, Train: 86.92%, Valid: 86.83%, Test: 86.86%
Epoch: 300, Loss: 0.5876, Train: 86.98%, Valid: 86.86%, Test: 87.04%
Epoch: 325, Loss: 0.5886, Train: 87.15%, Valid: 87.17%, Test: 87.17%
Epoch: 350, Loss: 1.3900, Train: 87.19%, Valid: 87.25%, Test: 87.24%
Epoch: 375, Loss: 1.6556, Train: 87.17%, Valid: 87.23%, Test: 87.19%
Epoch: 400, Loss: 1.8302, Train: 87.26%, Valid: 87.31%, Test: 87.30%
Epoch: 425, Loss: 0.7191, Train: 87.21%, Valid: 87.25%, Test: 87.27%
Epoch: 450, Loss: 0.4258, Train: 86.81%, Valid: 86.80%, Test: 86.88%
Epoch: 475, Loss: 0.3797, Train: 87.43%, Valid: 87.45%, Test: 87.50%
Run 01:
Highest Train: 87.51
Highest Valid: 87.50
  Final Train: 87.47
   Final Test: 87.54
All runs:
Highest Train: 87.51, nan
Highest Valid: 87.50, nan
  Final Train: 87.47, nan
   Final Test: 87.54, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.4720, Train: 88.27%, Valid: 88.34%, Test: 88.29%
Epoch: 25, Loss: 5.6639, Train: 84.46%, Valid: 84.28%, Test: 84.60%
Epoch: 50, Loss: 8.7280, Train: 84.47%, Valid: 84.29%, Test: 84.60%
Epoch: 75, Loss: 8.3962, Train: 84.45%, Valid: 84.28%, Test: 84.59%
Epoch: 100, Loss: 8.3683, Train: 84.45%, Valid: 84.28%, Test: 84.59%
Epoch: 125, Loss: 7.9549, Train: 84.45%, Valid: 84.28%, Test: 84.59%
Epoch: 150, Loss: 8.3623, Train: 84.45%, Valid: 84.28%, Test: 84.59%
Epoch: 175, Loss: 8.9222, Train: 84.45%, Valid: 84.28%, Test: 84.59%
Epoch: 200, Loss: 8.3924, Train: 84.45%, Valid: 84.28%, Test: 84.59%
Epoch: 225, Loss: 8.2236, Train: 84.45%, Valid: 84.28%, Test: 84.59%
Epoch: 250, Loss: 8.4419, Train: 84.45%, Valid: 84.28%, Test: 84.59%
Epoch: 275, Loss: 8.2910, Train: 84.45%, Valid: 84.28%, Test: 84.59%
Epoch: 300, Loss: 8.7275, Train: 84.45%, Valid: 84.28%, Test: 84.59%
Epoch: 325, Loss: 8.2421, Train: 84.45%, Valid: 84.28%, Test: 84.59%
Epoch: 350, Loss: 9.2972, Train: 84.45%, Valid: 84.28%, Test: 84.59%
Epoch: 375, Loss: 8.2593, Train: 84.45%, Valid: 84.28%, Test: 84.59%
Epoch: 400, Loss: 7.8395, Train: 84.45%, Valid: 84.28%, Test: 84.59%
Epoch: 425, Loss: 8.5232, Train: 84.45%, Valid: 84.28%, Test: 84.59%
Epoch: 450, Loss: 8.6176, Train: 84.45%, Valid: 84.28%, Test: 84.59%
Epoch: 475, Loss: 7.8016, Train: 84.45%, Valid: 84.28%, Test: 84.59%
Run 01:
Highest Train: 88.27
Highest Valid: 88.34
  Final Train: 88.27
   Final Test: 88.29
All runs:
Highest Train: 88.27, nan
Highest Valid: 88.34, nan
  Final Train: 88.27, nan
   Final Test: 88.29, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 4.3129, Train: 85.92%, Valid: 85.73%, Test: 85.91%
Epoch: 25, Loss: 725.8420, Train: 16.84%, Valid: 16.97%, Test: 16.80%
Epoch: 50, Loss: 557.3066, Train: 16.93%, Valid: 17.02%, Test: 16.83%
Epoch: 75, Loss: 570.8366, Train: 16.93%, Valid: 17.02%, Test: 16.82%
Epoch: 100, Loss: 573.6510, Train: 16.93%, Valid: 17.02%, Test: 16.82%
Epoch: 125, Loss: 571.6110, Train: 16.93%, Valid: 17.02%, Test: 16.82%
Epoch: 150, Loss: 573.9581, Train: 16.93%, Valid: 17.02%, Test: 16.82%
Epoch: 175, Loss: 570.6900, Train: 16.93%, Valid: 17.02%, Test: 16.82%
Epoch: 200, Loss: 572.8429, Train: 16.93%, Valid: 17.02%, Test: 16.82%
Epoch: 225, Loss: 573.0186, Train: 16.93%, Valid: 17.02%, Test: 16.82%
Epoch: 250, Loss: 572.5392, Train: 16.93%, Valid: 17.02%, Test: 16.82%
Epoch: 275, Loss: 572.6543, Train: 16.93%, Valid: 17.02%, Test: 16.82%
Epoch: 300, Loss: 572.4431, Train: 16.93%, Valid: 17.02%, Test: 16.82%
Epoch: 325, Loss: 572.0541, Train: 16.93%, Valid: 17.02%, Test: 16.82%
Epoch: 350, Loss: 574.0503, Train: 16.93%, Valid: 17.02%, Test: 16.82%
Epoch: 375, Loss: 573.6042, Train: 16.93%, Valid: 17.02%, Test: 16.82%
Epoch: 400, Loss: 570.9199, Train: 16.93%, Valid: 17.02%, Test: 16.82%
Epoch: 425, Loss: 572.7396, Train: 16.93%, Valid: 17.02%, Test: 16.82%
Epoch: 450, Loss: 574.0258, Train: 16.93%, Valid: 17.02%, Test: 16.82%
Epoch: 475, Loss: 572.5175, Train: 16.93%, Valid: 17.02%, Test: 16.82%
Run 01:
Highest Train: 87.63
Highest Valid: 87.55
  Final Train: 87.63
   Final Test: 87.61
All runs:
Highest Train: 87.63, nan
Highest Valid: 87.55, nan
  Final Train: 87.63, nan
   Final Test: 87.61, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 12.2658, Train: 82.40%, Valid: 82.52%, Test: 82.67%
Epoch: 25, Loss: 114.0217, Train: 14.45%, Valid: 14.50%, Test: 14.40%
Epoch: 50, Loss: 560.0244, Train: 18.04%, Valid: 18.16%, Test: 17.93%
Epoch: 75, Loss: 525.1262, Train: 15.23%, Valid: 15.45%, Test: 15.13%
Epoch: 100, Loss: 518.5737, Train: 17.11%, Valid: 17.24%, Test: 17.04%
Epoch: 125, Loss: 519.2768, Train: 17.13%, Valid: 17.25%, Test: 17.07%
Epoch: 150, Loss: 519.2527, Train: 17.11%, Valid: 17.24%, Test: 17.03%
Epoch: 175, Loss: 520.6931, Train: 17.11%, Valid: 17.25%, Test: 17.03%
Epoch: 200, Loss: 519.9422, Train: 17.11%, Valid: 17.24%, Test: 17.04%
Epoch: 225, Loss: 520.4067, Train: 17.17%, Valid: 17.28%, Test: 17.11%
Epoch: 250, Loss: 519.8871, Train: 17.14%, Valid: 17.26%, Test: 17.08%
Epoch: 275, Loss: 519.9801, Train: 17.11%, Valid: 17.24%, Test: 17.03%
Epoch: 300, Loss: 519.8790, Train: 17.11%, Valid: 17.24%, Test: 17.04%
Epoch: 325, Loss: 519.6375, Train: 17.11%, Valid: 17.23%, Test: 17.05%
Epoch: 350, Loss: 519.9496, Train: 17.11%, Valid: 17.25%, Test: 17.03%
Epoch: 375, Loss: 519.9135, Train: 17.11%, Valid: 17.24%, Test: 17.03%
Epoch: 400, Loss: 519.0266, Train: 17.11%, Valid: 17.24%, Test: 17.04%
Epoch: 425, Loss: 520.1385, Train: 17.11%, Valid: 17.24%, Test: 17.03%
Epoch: 450, Loss: 520.5523, Train: 17.11%, Valid: 17.24%, Test: 17.03%
Epoch: 475, Loss: 520.6720, Train: 17.15%, Valid: 17.27%, Test: 17.10%
Run 01:
Highest Train: 86.84
Highest Valid: 86.87
  Final Train: 86.84
   Final Test: 86.83
All runs:
Highest Train: 86.84, nan
Highest Valid: 86.87, nan
  Final Train: 86.84, nan
   Final Test: 86.83, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 7.0116, Train: 87.66%, Valid: 87.69%, Test: 87.76%
Epoch: 25, Loss: 132888414370269390675058066718720.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 3747847786725376.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 1193522176.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 51142497042673412869319557120.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 34377049079067705344.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 8077408989988983456923648.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 889827431675708574784131432448.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 13803196407332470784.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 192302279121650581504.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 8967034900614887161332760576.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 24253278740180319093129216.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 57995420555053975826562285568.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 119345647226813671326875648.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 1040140546310404507080589312.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 26527428860188819456.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 4535775232.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 1449078815621913188520427520.0000, Train: 14.79%, Valid: 14.81%, Test: 14.69%
Epoch: 450, Loss: 83728039422660062342641876992.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 431595679705238339584.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 87.66
Highest Valid: 87.69
  Final Train: 87.66
   Final Test: 87.76
All runs:
Highest Train: 87.66, nan
Highest Valid: 87.69, nan
  Final Train: 87.66, nan
   Final Test: 87.76, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 6.5141, Train: 85.56%, Valid: 85.47%, Test: 85.71%
Epoch: 25, Loss: 130.9839, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 580773101677601217702839451648.0000, Train: 84.40%, Valid: 84.29%, Test: 84.53%
Epoch: 100, Loss: 1617110555663861605826906030080.0000, Train: 15.61%, Valid: 15.73%, Test: 15.48%
Epoch: 125, Loss: 14218412305022478576408403640320.0000, Train: 84.39%, Valid: 84.27%, Test: 84.52%
Epoch: 150, Loss: 3140770685040780296658616320.0000, Train: 15.61%, Valid: 15.73%, Test: 15.48%
Epoch: 175, Loss: 18747855870695449821184.0000, Train: 84.39%, Valid: 84.27%, Test: 84.52%
Epoch: 200, Loss: 242345.8281, Train: 84.39%, Valid: 84.27%, Test: 84.52%
Epoch: 225, Loss: 63286744400871763776064840531968.0000, Train: 84.39%, Valid: 84.27%, Test: 84.52%
Epoch: 250, Loss: 1986396130403247219277824.0000, Train: 15.61%, Valid: 15.73%, Test: 15.48%
Epoch: 275, Loss: 23704564278267585813955011936256.0000, Train: 15.61%, Valid: 15.73%, Test: 15.48%
Epoch: 300, Loss: 2708812576225736876080758784.0000, Train: 84.39%, Valid: 84.27%, Test: 84.52%
Epoch: 325, Loss: 514022413663306919650084782080.0000, Train: 15.61%, Valid: 15.73%, Test: 15.48%
Epoch: 350, Loss: 21191852427978267951104.0000, Train: 15.61%, Valid: 15.73%, Test: 15.48%
Epoch: 375, Loss: inf, Train: 84.39%, Valid: 84.27%, Test: 84.52%
Epoch: 400, Loss: 978666468606023303168.0000, Train: 15.61%, Valid: 15.73%, Test: 15.48%
Epoch: 425, Loss: 1526773271561703537089693351936.0000, Train: 15.61%, Valid: 15.73%, Test: 15.48%
Epoch: 450, Loss: inf, Train: 15.61%, Valid: 15.73%, Test: 15.48%
Epoch: 475, Loss: 20334977589518496385051131904.0000, Train: 15.61%, Valid: 15.73%, Test: 15.48%
Run 01:
Highest Train: 85.98
Highest Valid: 85.94
  Final Train: 85.98
   Final Test: 86.07
All runs:
Highest Train: 85.98, nan
Highest Valid: 85.94, nan
  Final Train: 85.98, nan
   Final Test: 86.07, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 12.9412, Train: 82.19%, Valid: 82.33%, Test: 82.50%
Epoch: 25, Loss: 3.5225, Train: 85.92%, Valid: 85.76%, Test: 85.97%
Epoch: 50, Loss: 2.7565, Train: 85.82%, Valid: 85.65%, Test: 85.87%
Epoch: 75, Loss: 0.4632, Train: 85.61%, Valid: 85.43%, Test: 85.66%
Epoch: 100, Loss: 0.3600, Train: 85.54%, Valid: 85.37%, Test: 85.61%
Epoch: 125, Loss: 0.3534, Train: 85.71%, Valid: 85.52%, Test: 85.78%
Epoch: 150, Loss: 0.3557, Train: 85.75%, Valid: 85.56%, Test: 85.83%
Epoch: 175, Loss: 0.6871, Train: 85.57%, Valid: 85.41%, Test: 85.62%
Epoch: 200, Loss: 1.8511, Train: 87.89%, Valid: 87.75%, Test: 87.88%
Epoch: 225, Loss: 3.4374, Train: 86.29%, Valid: 86.11%, Test: 86.30%
Epoch: 250, Loss: 2.2378, Train: 86.20%, Valid: 86.08%, Test: 86.15%
Epoch: 275, Loss: 3.0621, Train: 85.56%, Valid: 85.35%, Test: 85.60%
Epoch: 300, Loss: 1.4475, Train: 85.63%, Valid: 85.46%, Test: 85.65%
Epoch: 325, Loss: 0.9510, Train: 85.13%, Valid: 84.92%, Test: 85.23%
Epoch: 350, Loss: 0.3908, Train: 85.53%, Valid: 85.31%, Test: 85.62%
Epoch: 375, Loss: 0.3739, Train: 85.54%, Valid: 85.33%, Test: 85.64%
Epoch: 400, Loss: 0.3651, Train: 85.52%, Valid: 85.32%, Test: 85.62%
Epoch: 425, Loss: 0.3545, Train: 85.53%, Valid: 85.32%, Test: 85.61%
Epoch: 450, Loss: 0.3488, Train: 85.56%, Valid: 85.36%, Test: 85.64%
Epoch: 475, Loss: 13.0402, Train: 87.26%, Valid: 87.06%, Test: 87.29%
Run 01:
Highest Train: 88.10
Highest Valid: 87.98
  Final Train: 88.10
   Final Test: 88.09
All runs:
Highest Train: 88.10, nan
Highest Valid: 87.98, nan
  Final Train: 88.10, nan
   Final Test: 88.09, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 4.3055, Train: 88.10%, Valid: 88.17%, Test: 88.15%
Epoch: 25, Loss: 1470.0510, Train: 17.08%, Valid: 17.02%, Test: 17.01%
Epoch: 50, Loss: 6.7518, Train: 85.06%, Valid: 85.04%, Test: 85.16%
Epoch: 75, Loss: 7.1697, Train: 88.38%, Valid: 88.41%, Test: 88.40%
Epoch: 100, Loss: 13.9955, Train: 83.92%, Valid: 83.79%, Test: 84.04%
Epoch: 125, Loss: 6.7732, Train: 87.46%, Valid: 87.32%, Test: 87.47%
Epoch: 150, Loss: 11.6652, Train: 87.46%, Valid: 87.31%, Test: 87.48%
Epoch: 175, Loss: 26.5918, Train: 83.77%, Valid: 83.61%, Test: 83.91%
Epoch: 200, Loss: 26.8252, Train: 83.75%, Valid: 83.58%, Test: 83.90%
Epoch: 225, Loss: 14.6811, Train: 83.88%, Valid: 83.75%, Test: 84.00%
Epoch: 250, Loss: 18.5958, Train: 84.00%, Valid: 83.86%, Test: 84.12%
Epoch: 275, Loss: 11.3438, Train: 87.27%, Valid: 87.15%, Test: 87.29%
Epoch: 300, Loss: 7.1584, Train: 86.37%, Valid: 86.36%, Test: 86.44%
Epoch: 325, Loss: 10.0172, Train: 86.64%, Valid: 86.61%, Test: 86.72%
Epoch: 350, Loss: 8.7555, Train: 85.90%, Valid: 85.75%, Test: 85.94%
Epoch: 375, Loss: 6.5010, Train: 85.93%, Valid: 85.77%, Test: 85.98%
Epoch: 400, Loss: 7.9942, Train: 85.94%, Valid: 85.80%, Test: 85.99%
Epoch: 425, Loss: 7.7736, Train: 85.96%, Valid: 85.83%, Test: 86.01%
Epoch: 450, Loss: 4.9752, Train: 86.08%, Valid: 85.96%, Test: 86.13%
Epoch: 475, Loss: 1.4811, Train: 85.73%, Valid: 85.66%, Test: 85.81%
Run 01:
Highest Train: 88.45
Highest Valid: 88.48
  Final Train: 88.45
   Final Test: 88.46
All runs:
Highest Train: 88.45, nan
Highest Valid: 88.48, nan
  Final Train: 88.45, nan
   Final Test: 88.46, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 10.2107, Train: 86.10%, Valid: 86.00%, Test: 86.21%
Epoch: 25, Loss: 11.4892, Train: 86.61%, Valid: 86.65%, Test: 86.68%
Epoch: 50, Loss: 175.4658, Train: 84.97%, Valid: 84.91%, Test: 85.04%
Epoch: 75, Loss: 281.9001, Train: 84.75%, Valid: 84.71%, Test: 84.84%
Epoch: 100, Loss: 372.2317, Train: 84.16%, Valid: 84.18%, Test: 84.30%
Epoch: 125, Loss: 555.7150, Train: 86.65%, Valid: 86.67%, Test: 86.73%
Epoch: 150, Loss: 637.9166, Train: 86.59%, Valid: 86.56%, Test: 86.67%
Epoch: 175, Loss: 661.7688, Train: 86.51%, Valid: 86.50%, Test: 86.63%
Epoch: 200, Loss: 612.0663, Train: 86.48%, Valid: 86.48%, Test: 86.60%
Epoch: 225, Loss: 572.7941, Train: 86.48%, Valid: 86.51%, Test: 86.57%
Epoch: 250, Loss: 323.5727, Train: 85.32%, Valid: 85.16%, Test: 85.43%
Epoch: 275, Loss: 12.3607, Train: 86.54%, Valid: 86.56%, Test: 86.61%
Epoch: 300, Loss: 11.3633, Train: 86.42%, Valid: 86.45%, Test: 86.49%
Epoch: 325, Loss: 10.4816, Train: 86.40%, Valid: 86.43%, Test: 86.47%
Epoch: 350, Loss: 5.0882, Train: 85.18%, Valid: 85.27%, Test: 85.26%
Epoch: 375, Loss: 5.9828, Train: 85.50%, Valid: 85.60%, Test: 85.57%
Epoch: 400, Loss: 3.5301, Train: 85.31%, Valid: 85.42%, Test: 85.39%
Epoch: 425, Loss: 1.4961, Train: 85.95%, Valid: 85.99%, Test: 85.98%
Epoch: 450, Loss: 3.4560, Train: 85.93%, Valid: 86.00%, Test: 86.02%
Epoch: 475, Loss: 3.0595, Train: 84.97%, Valid: 84.87%, Test: 85.04%
Run 01:
Highest Train: 86.90
Highest Valid: 86.92
  Final Train: 86.90
   Final Test: 86.98
All runs:
Highest Train: 86.90, nan
Highest Valid: 86.92, nan
  Final Train: 86.90, nan
   Final Test: 86.98, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 52.1527, Train: 84.31%, Valid: 84.29%, Test: 84.44%
Epoch: 25, Loss: 3.4725, Train: 86.30%, Valid: 86.26%, Test: 86.42%
Epoch: 50, Loss: 5.7584, Train: 86.30%, Valid: 86.25%, Test: 86.41%
Epoch: 75, Loss: 22.9060, Train: 84.82%, Valid: 84.59%, Test: 84.90%
Epoch: 100, Loss: 31755.2070, Train: 84.76%, Valid: 84.58%, Test: 84.94%
Epoch: 125, Loss: 831.3622, Train: 85.05%, Valid: 84.92%, Test: 85.18%
Epoch: 150, Loss: 1111.6010, Train: 85.04%, Valid: 84.91%, Test: 85.17%
Epoch: 175, Loss: 1135.7487, Train: 85.03%, Valid: 84.88%, Test: 85.15%
Epoch: 200, Loss: 1048.1772, Train: 85.04%, Valid: 84.90%, Test: 85.16%
Epoch: 225, Loss: 1152.3691, Train: 85.02%, Valid: 84.88%, Test: 85.15%
Epoch: 250, Loss: 1305.6608, Train: 85.03%, Valid: 84.88%, Test: 85.15%
Epoch: 275, Loss: 1196.2021, Train: 85.04%, Valid: 84.90%, Test: 85.16%
Epoch: 300, Loss: 1056.1031, Train: 85.04%, Valid: 84.90%, Test: 85.17%
Epoch: 325, Loss: 1221.4022, Train: 85.03%, Valid: 84.89%, Test: 85.15%
Epoch: 350, Loss: 1339.6078, Train: 85.03%, Valid: 84.90%, Test: 85.16%
Epoch: 375, Loss: 1021.1760, Train: 85.03%, Valid: 84.89%, Test: 85.16%
Epoch: 400, Loss: 1179.1320, Train: 85.03%, Valid: 84.89%, Test: 85.15%
Epoch: 425, Loss: 1261.6448, Train: 85.00%, Valid: 84.86%, Test: 85.14%
Epoch: 450, Loss: 1092.1929, Train: 84.91%, Valid: 84.76%, Test: 85.04%
Epoch: 475, Loss: 1254.6970, Train: 84.94%, Valid: 84.78%, Test: 85.06%
Run 01:
Highest Train: 87.25
Highest Valid: 87.20
  Final Train: 87.25
   Final Test: 87.32
All runs:
Highest Train: 87.25, nan
Highest Valid: 87.20, nan
  Final Train: 87.25, nan
   Final Test: 87.32, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.7714, Train: 85.25%, Valid: 85.07%, Test: 85.30%
Epoch: 25, Loss: 8.0636, Train: 88.05%, Valid: 88.13%, Test: 88.15%
Epoch: 50, Loss: 21.2501, Train: 84.66%, Valid: 84.66%, Test: 84.77%
Epoch: 75, Loss: 30.4758, Train: 84.47%, Valid: 84.47%, Test: 84.62%
Epoch: 100, Loss: 37.9663, Train: 84.44%, Valid: 84.44%, Test: 84.58%
Epoch: 125, Loss: 30.1996, Train: 84.44%, Valid: 84.45%, Test: 84.58%
Epoch: 150, Loss: 31.9083, Train: 84.44%, Valid: 84.45%, Test: 84.59%
Epoch: 175, Loss: 30.9331, Train: 84.46%, Valid: 84.47%, Test: 84.61%
Epoch: 200, Loss: 30.2915, Train: 84.45%, Valid: 84.46%, Test: 84.60%
Epoch: 225, Loss: 30.3872, Train: 84.45%, Valid: 84.46%, Test: 84.60%
Epoch: 250, Loss: 28.9939, Train: 84.45%, Valid: 84.46%, Test: 84.60%
Epoch: 275, Loss: 31.7994, Train: 84.46%, Valid: 84.47%, Test: 84.61%
Epoch: 300, Loss: 31.4889, Train: 84.46%, Valid: 84.46%, Test: 84.61%
Epoch: 325, Loss: 30.5629, Train: 84.46%, Valid: 84.47%, Test: 84.61%
Epoch: 350, Loss: 31.1280, Train: 84.46%, Valid: 84.47%, Test: 84.61%
Epoch: 375, Loss: 31.6491, Train: 84.46%, Valid: 84.47%, Test: 84.61%
Epoch: 400, Loss: 31.0616, Train: 84.46%, Valid: 84.47%, Test: 84.61%
Epoch: 425, Loss: 30.7668, Train: 84.47%, Valid: 84.47%, Test: 84.62%
Epoch: 450, Loss: 31.5075, Train: 84.47%, Valid: 84.47%, Test: 84.61%
Epoch: 475, Loss: 31.5879, Train: 84.47%, Valid: 84.48%, Test: 84.62%
Run 01:
Highest Train: 88.20
Highest Valid: 88.27
  Final Train: 88.20
   Final Test: 88.30
All runs:
Highest Train: 88.20, nan
Highest Valid: 88.27, nan
  Final Train: 88.20, nan
   Final Test: 88.30, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.9751, Train: 87.61%, Valid: 87.56%, Test: 87.58%
Epoch: 25, Loss: 1234.1121, Train: 17.58%, Valid: 17.68%, Test: 17.48%
Epoch: 50, Loss: 774.1841, Train: 18.12%, Valid: 18.04%, Test: 18.05%
Epoch: 75, Loss: 8.9973, Train: 86.72%, Valid: 86.71%, Test: 86.76%
Epoch: 100, Loss: 2.2293, Train: 86.85%, Valid: 86.86%, Test: 86.91%
Epoch: 125, Loss: 5.1540, Train: 86.81%, Valid: 86.82%, Test: 86.86%
Epoch: 150, Loss: 4.7127, Train: 86.79%, Valid: 86.79%, Test: 86.84%
Epoch: 175, Loss: 3.1576, Train: 86.81%, Valid: 86.82%, Test: 86.87%
Epoch: 200, Loss: 6.1378, Train: 86.79%, Valid: 86.80%, Test: 86.85%
Epoch: 225, Loss: 6.6223, Train: 86.80%, Valid: 86.80%, Test: 86.86%
Epoch: 250, Loss: 5.2593, Train: 86.79%, Valid: 86.79%, Test: 86.86%
Epoch: 275, Loss: 3.6338, Train: 86.72%, Valid: 86.68%, Test: 86.89%
Epoch: 300, Loss: 1.8210, Train: 85.89%, Valid: 85.76%, Test: 85.92%
Epoch: 325, Loss: 0.7704, Train: 86.10%, Valid: 85.97%, Test: 86.10%
Epoch: 350, Loss: 0.4170, Train: 85.91%, Valid: 85.71%, Test: 86.02%
Epoch: 375, Loss: 0.3872, Train: 85.76%, Valid: 85.59%, Test: 85.94%
Epoch: 400, Loss: 0.3822, Train: 85.83%, Valid: 85.64%, Test: 85.98%
Epoch: 425, Loss: 0.3812, Train: 85.80%, Valid: 85.62%, Test: 85.97%
Epoch: 450, Loss: 0.3789, Train: 85.84%, Valid: 85.66%, Test: 85.99%
Epoch: 475, Loss: 0.3781, Train: 85.86%, Valid: 85.68%, Test: 86.02%
Run 01:
Highest Train: 87.61
Highest Valid: 87.56
  Final Train: 87.61
   Final Test: 87.58
All runs:
Highest Train: 87.61, nan
Highest Valid: 87.56, nan
  Final Train: 87.61, nan
   Final Test: 87.58, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 43.0066, Train: 17.63%, Valid: 17.78%, Test: 17.54%
Epoch: 25, Loss: 11.3630, Train: 85.43%, Valid: 85.23%, Test: 85.45%
Epoch: 50, Loss: 130.3780, Train: 85.13%, Valid: 84.98%, Test: 85.28%
Epoch: 75, Loss: 1716.7146, Train: 15.13%, Valid: 15.36%, Test: 15.02%
Epoch: 100, Loss: 1992.9854, Train: 14.91%, Valid: 15.10%, Test: 14.80%
Epoch: 125, Loss: 1978.4404, Train: 14.90%, Valid: 15.10%, Test: 14.79%
Epoch: 150, Loss: 1482.7009, Train: 17.18%, Valid: 17.18%, Test: 17.06%
Epoch: 175, Loss: 1293.3165, Train: 17.17%, Valid: 17.16%, Test: 17.05%
Epoch: 200, Loss: 1106.4863, Train: 17.15%, Valid: 17.14%, Test: 17.04%
Epoch: 225, Loss: 866.6902, Train: 18.21%, Valid: 18.13%, Test: 18.13%
Epoch: 250, Loss: 564.7549, Train: 19.66%, Valid: 19.47%, Test: 19.52%
Epoch: 275, Loss: 188.6972, Train: 85.45%, Valid: 85.24%, Test: 85.51%
Epoch: 300, Loss: 3.5621, Train: 85.48%, Valid: 85.22%, Test: 85.49%
Epoch: 325, Loss: 3.7861, Train: 85.46%, Valid: 85.20%, Test: 85.46%
Epoch: 350, Loss: 3.7375, Train: 85.47%, Valid: 85.21%, Test: 85.47%
Epoch: 375, Loss: 3.6725, Train: 85.48%, Valid: 85.22%, Test: 85.48%
Epoch: 400, Loss: 3.6113, Train: 85.50%, Valid: 85.25%, Test: 85.51%
Epoch: 425, Loss: 3.5553, Train: 85.51%, Valid: 85.26%, Test: 85.53%
Epoch: 450, Loss: 3.5038, Train: 85.52%, Valid: 85.27%, Test: 85.53%
Epoch: 475, Loss: 3.4554, Train: 85.53%, Valid: 85.28%, Test: 85.54%
Run 01:
Highest Train: 86.87
Highest Valid: 86.90
  Final Train: 86.87
   Final Test: 86.89
All runs:
Highest Train: 86.87, nan
Highest Valid: 86.90, nan
  Final Train: 86.87, nan
   Final Test: 86.89, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 5.5771, Train: 87.85%, Valid: 87.91%, Test: 87.86%
Epoch: 25, Loss: 9.2939, Train: 85.07%, Valid: 84.97%, Test: 85.12%
Epoch: 50, Loss: 3.0654, Train: 85.84%, Valid: 85.90%, Test: 85.96%
Epoch: 75, Loss: 0.5092, Train: 85.35%, Valid: 85.23%, Test: 85.39%
Epoch: 100, Loss: 0.3813, Train: 85.24%, Valid: 85.23%, Test: 85.43%
Epoch: 125, Loss: 0.3551, Train: 84.96%, Valid: 84.94%, Test: 85.12%
Epoch: 150, Loss: 4.5364, Train: 87.22%, Valid: 87.01%, Test: 87.23%
Epoch: 175, Loss: 5.2917, Train: 85.51%, Valid: 85.55%, Test: 85.60%
Epoch: 200, Loss: 6.2429, Train: 85.32%, Valid: 85.30%, Test: 85.40%
Epoch: 225, Loss: 3.2931, Train: 85.61%, Valid: 85.68%, Test: 85.71%
Epoch: 250, Loss: 0.4189, Train: 85.36%, Valid: 85.34%, Test: 85.47%
Epoch: 275, Loss: 0.3568, Train: 87.21%, Valid: 86.98%, Test: 87.27%
Epoch: 300, Loss: 0.3479, Train: 85.95%, Valid: 85.93%, Test: 86.03%
Epoch: 325, Loss: 0.3387, Train: 86.33%, Valid: 86.13%, Test: 86.38%
Epoch: 350, Loss: 0.3297, Train: 85.70%, Valid: 85.64%, Test: 85.77%
Epoch: 375, Loss: 12.5565, Train: 84.54%, Valid: 84.38%, Test: 84.70%
Epoch: 400, Loss: 4.3123, Train: 85.27%, Valid: 85.26%, Test: 85.39%
Epoch: 425, Loss: 0.5766, Train: 85.53%, Valid: 85.50%, Test: 85.67%
Epoch: 450, Loss: 0.3575, Train: 85.41%, Valid: 85.21%, Test: 85.48%
Epoch: 475, Loss: 0.3412, Train: 86.77%, Valid: 86.63%, Test: 86.86%
Run 01:
Highest Train: 88.08
Highest Valid: 88.14
  Final Train: 88.08
   Final Test: 88.14
All runs:
Highest Train: 88.08, nan
Highest Valid: 88.14, nan
  Final Train: 88.08, nan
   Final Test: 88.14, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 4.9202, Train: 86.49%, Valid: 86.46%, Test: 86.51%
Epoch: 25, Loss: 907.0137, Train: 17.81%, Valid: 17.90%, Test: 17.73%
Epoch: 50, Loss: 2650.1819, Train: 16.96%, Valid: 17.04%, Test: 16.81%
Epoch: 75, Loss: 2968.5850, Train: 16.11%, Valid: 16.18%, Test: 15.92%
Epoch: 100, Loss: 9.7621, Train: 86.33%, Valid: 86.27%, Test: 86.37%
Epoch: 125, Loss: 18.8429, Train: 85.11%, Valid: 85.07%, Test: 85.22%
Epoch: 150, Loss: 18.6650, Train: 84.84%, Valid: 84.77%, Test: 84.87%
Epoch: 175, Loss: 17.1540, Train: 84.57%, Valid: 84.46%, Test: 84.65%
Epoch: 200, Loss: 15.5617, Train: 84.60%, Valid: 84.49%, Test: 84.67%
Epoch: 225, Loss: 13.8927, Train: 84.67%, Valid: 84.58%, Test: 84.73%
Epoch: 250, Loss: 12.3066, Train: 86.02%, Valid: 85.93%, Test: 85.96%
Epoch: 275, Loss: 10.9338, Train: 86.02%, Valid: 85.91%, Test: 85.94%
Epoch: 300, Loss: 9.5049, Train: 86.08%, Valid: 85.99%, Test: 86.05%
Epoch: 325, Loss: 7.9642, Train: 86.12%, Valid: 86.04%, Test: 86.10%
Epoch: 350, Loss: 6.2444, Train: 86.11%, Valid: 86.06%, Test: 86.14%
Epoch: 375, Loss: 4.3125, Train: 86.10%, Valid: 86.07%, Test: 86.12%
Epoch: 400, Loss: 1.8959, Train: 86.10%, Valid: 86.07%, Test: 86.12%
Epoch: 425, Loss: 9.7571, Train: 85.50%, Valid: 85.43%, Test: 85.57%
Epoch: 450, Loss: 10.8600, Train: 85.46%, Valid: 85.35%, Test: 85.53%
Epoch: 475, Loss: 9.1757, Train: 85.63%, Valid: 85.52%, Test: 85.70%
Run 01:
Highest Train: 86.83
Highest Valid: 86.67
  Final Train: 86.83
   Final Test: 86.90
All runs:
Highest Train: 86.83, nan
Highest Valid: 86.67, nan
  Final Train: 86.83, nan
   Final Test: 86.90, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 9.4528, Train: 85.41%, Valid: 85.24%, Test: 85.48%
Epoch: 25, Loss: 20.4440, Train: 85.48%, Valid: 85.34%, Test: 85.54%
Epoch: 50, Loss: 8.9615, Train: 86.80%, Valid: 86.81%, Test: 86.92%
Epoch: 75, Loss: 16.0905, Train: 84.87%, Valid: 84.71%, Test: 84.97%
Epoch: 100, Loss: 42.6227, Train: 84.87%, Valid: 84.67%, Test: 84.96%
Epoch: 125, Loss: 308.3190, Train: 83.68%, Valid: 83.75%, Test: 83.88%
Epoch: 150, Loss: 723.5156, Train: 83.69%, Valid: 83.75%, Test: 83.89%
Epoch: 175, Loss: 713.2377, Train: 83.69%, Valid: 83.75%, Test: 83.89%
Epoch: 200, Loss: 711.7408, Train: 83.69%, Valid: 83.76%, Test: 83.89%
Epoch: 225, Loss: 696.6796, Train: 83.70%, Valid: 83.77%, Test: 83.89%
Epoch: 250, Loss: 695.8293, Train: 83.70%, Valid: 83.77%, Test: 83.89%
Epoch: 275, Loss: 694.8904, Train: 83.70%, Valid: 83.77%, Test: 83.89%
Epoch: 300, Loss: 694.5297, Train: 83.70%, Valid: 83.77%, Test: 83.89%
Epoch: 325, Loss: 693.9342, Train: 83.70%, Valid: 83.77%, Test: 83.90%
Epoch: 350, Loss: 693.2855, Train: 83.70%, Valid: 83.77%, Test: 83.89%
Epoch: 375, Loss: 693.6779, Train: 83.70%, Valid: 83.77%, Test: 83.89%
Epoch: 400, Loss: 693.2596, Train: 83.70%, Valid: 83.77%, Test: 83.89%
Epoch: 425, Loss: 693.4741, Train: 83.70%, Valid: 83.77%, Test: 83.89%
Epoch: 450, Loss: 692.7980, Train: 83.70%, Valid: 83.77%, Test: 83.89%
Epoch: 475, Loss: 692.9604, Train: 83.70%, Valid: 83.77%, Test: 83.89%
Run 01:
Highest Train: 86.84
Highest Valid: 86.85
  Final Train: 86.84
   Final Test: 86.95
All runs:
Highest Train: 86.84, nan
Highest Valid: 86.85, nan
  Final Train: 86.84, nan
   Final Test: 86.95, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.2968, Train: 84.51%, Valid: 84.30%, Test: 84.56%
Epoch: 25, Loss: 1647.9645, Train: 88.16%, Valid: 88.18%, Test: 88.17%
Epoch: 50, Loss: 96604.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 75797.1641, Train: 16.20%, Valid: 16.40%, Test: 16.11%
Epoch: 100, Loss: 687.4850, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 5.9427, Train: 16.25%, Valid: 16.45%, Test: 16.15%
Epoch: 150, Loss: 1.2513, Train: 88.42%, Valid: 88.48%, Test: 88.44%
Epoch: 175, Loss: 1.5159, Train: 88.42%, Valid: 88.48%, Test: 88.44%
Epoch: 200, Loss: 1.8034, Train: 88.42%, Valid: 88.48%, Test: 88.44%
Epoch: 225, Loss: 1.1796, Train: 88.42%, Valid: 88.48%, Test: 88.44%
Epoch: 250, Loss: 0.7628, Train: 88.42%, Valid: 88.48%, Test: 88.44%
Epoch: 275, Loss: 1.2165, Train: 88.42%, Valid: 88.48%, Test: 88.44%
Epoch: 300, Loss: 1.4273, Train: 88.42%, Valid: 88.48%, Test: 88.44%
Epoch: 325, Loss: 1.7210, Train: 88.42%, Valid: 88.48%, Test: 88.44%
Epoch: 350, Loss: 0.9630, Train: 88.42%, Valid: 88.48%, Test: 88.44%
Epoch: 375, Loss: 0.9174, Train: 88.42%, Valid: 88.48%, Test: 88.44%
Epoch: 400, Loss: 1.5522, Train: 88.42%, Valid: 88.48%, Test: 88.44%
Epoch: 425, Loss: 1.2359, Train: 88.42%, Valid: 88.48%, Test: 88.44%
Epoch: 450, Loss: 0.7888, Train: 88.42%, Valid: 88.48%, Test: 88.44%
Epoch: 475, Loss: 1.5907, Train: 88.42%, Valid: 88.48%, Test: 88.44%
Run 01:
Highest Train: 88.42
Highest Valid: 88.48
  Final Train: 88.42
   Final Test: 88.44
All runs:
Highest Train: 88.42, nan
Highest Valid: 88.48, nan
  Final Train: 88.42, nan
   Final Test: 88.44, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.9853, Train: 85.90%, Valid: 85.88%, Test: 85.92%
Epoch: 25, Loss: 279.0025, Train: 15.38%, Valid: 15.46%, Test: 15.32%
Epoch: 50, Loss: 142.7236, Train: 86.11%, Valid: 86.11%, Test: 86.29%
Epoch: 75, Loss: 1.3986, Train: 13.70%, Valid: 13.76%, Test: 13.65%
Epoch: 100, Loss: 124.4567, Train: 85.86%, Valid: 85.75%, Test: 85.97%
Epoch: 125, Loss: 216.2183, Train: 13.68%, Valid: 13.74%, Test: 13.66%
Epoch: 150, Loss: 43.3745, Train: 13.28%, Valid: 13.34%, Test: 13.29%
Epoch: 175, Loss: 55.8025, Train: 13.91%, Valid: 13.96%, Test: 13.76%
Epoch: 200, Loss: 66.0674, Train: 13.86%, Valid: 13.96%, Test: 13.73%
Epoch: 225, Loss: 58.4495, Train: 13.86%, Valid: 13.94%, Test: 13.72%
Epoch: 250, Loss: 55.4037, Train: 13.82%, Valid: 13.90%, Test: 13.68%
Epoch: 275, Loss: 54.1789, Train: 13.84%, Valid: 13.92%, Test: 13.71%
Epoch: 300, Loss: 62.8750, Train: 13.83%, Valid: 13.91%, Test: 13.70%
Epoch: 325, Loss: 56.2501, Train: 13.83%, Valid: 13.91%, Test: 13.70%
Epoch: 350, Loss: 50.8283, Train: 13.90%, Valid: 14.00%, Test: 13.78%
Epoch: 375, Loss: 52.6050, Train: 13.84%, Valid: 13.92%, Test: 13.71%
Epoch: 400, Loss: 58.9207, Train: 13.83%, Valid: 13.91%, Test: 13.70%
Epoch: 425, Loss: 56.2591, Train: 13.82%, Valid: 13.90%, Test: 13.68%
Epoch: 450, Loss: 63.4610, Train: 13.87%, Valid: 13.96%, Test: 13.74%
Epoch: 475, Loss: 56.7176, Train: 13.80%, Valid: 13.88%, Test: 13.67%
Run 01:
Highest Train: 86.52
Highest Valid: 86.53
  Final Train: 86.52
   Final Test: 86.66
All runs:
Highest Train: 86.52, nan
Highest Valid: 86.53, nan
  Final Train: 86.52, nan
   Final Test: 86.66, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.5410, Train: 86.67%, Valid: 86.51%, Test: 86.65%
Epoch: 25, Loss: 4.4556, Train: 85.20%, Valid: 84.98%, Test: 85.30%
Epoch: 50, Loss: 0.7549, Train: 86.34%, Valid: 86.28%, Test: 86.37%
Epoch: 75, Loss: 3.0251, Train: 86.54%, Valid: 86.48%, Test: 86.62%
Epoch: 100, Loss: 3.1352, Train: 86.51%, Valid: 86.46%, Test: 86.60%
Epoch: 125, Loss: 1.8244, Train: 86.54%, Valid: 86.49%, Test: 86.64%
Epoch: 150, Loss: 2.6506, Train: 86.57%, Valid: 86.51%, Test: 86.67%
Epoch: 175, Loss: 1.3877, Train: 86.54%, Valid: 86.48%, Test: 86.64%
Epoch: 200, Loss: 1.8005, Train: 86.60%, Valid: 86.54%, Test: 86.70%
Epoch: 225, Loss: 2.0287, Train: 86.59%, Valid: 86.53%, Test: 86.70%
Epoch: 250, Loss: 1.3721, Train: 86.62%, Valid: 86.57%, Test: 86.73%
Epoch: 275, Loss: 0.9598, Train: 86.15%, Valid: 86.06%, Test: 86.18%
Epoch: 300, Loss: 1.0381, Train: 85.92%, Valid: 85.84%, Test: 85.95%
Epoch: 325, Loss: 1.0016, Train: 86.04%, Valid: 85.95%, Test: 86.07%
Epoch: 350, Loss: 0.9734, Train: 86.07%, Valid: 85.99%, Test: 86.11%
Epoch: 375, Loss: 0.8954, Train: 86.13%, Valid: 86.05%, Test: 86.17%
Epoch: 400, Loss: 0.9274, Train: 86.20%, Valid: 86.15%, Test: 86.27%
Epoch: 425, Loss: 0.9250, Train: 86.30%, Valid: 86.27%, Test: 86.37%
Epoch: 450, Loss: 0.8994, Train: 86.34%, Valid: 86.30%, Test: 86.42%
Epoch: 475, Loss: 0.9842, Train: 86.33%, Valid: 86.30%, Test: 86.43%
Run 01:
Highest Train: 87.57
Highest Valid: 87.50
  Final Train: 87.57
   Final Test: 87.55
All runs:
Highest Train: 87.57, nan
Highest Valid: 87.50, nan
  Final Train: 87.57, nan
   Final Test: 87.55, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.4979, Train: 85.41%, Valid: 85.40%, Test: 85.57%
Epoch: 25, Loss: 271.7641, Train: 85.58%, Valid: 85.43%, Test: 85.69%
Epoch: 50, Loss: 1066277525432051957760.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 30675249133518848.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 32065188597334016.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 114513122689024.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 5157982768734404608.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 303314198216048640.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 496687499968512.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 28678658424045568.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 193333411840.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 2420881133404160.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 32766926258176.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 4797174382592.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 461376618496.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 48867793960960.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 984247456235520.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 77210326432481280.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 338491217320869888.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 5337842263785472.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 87.85
Highest Valid: 87.88
  Final Train: 87.85
   Final Test: 87.85
All runs:
Highest Train: 87.85, nan
Highest Valid: 87.88, nan
  Final Train: 87.85, nan
   Final Test: 87.85, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.5992, Train: 85.70%, Valid: 85.63%, Test: 85.82%
Epoch: 25, Loss: 2328129536.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 50477702053888.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 1802596700389376.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 861021820658735120384.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 176182762930176.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 7.9524, Train: 14.59%, Valid: 14.74%, Test: 14.46%
Epoch: 175, Loss: 744.7891, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 16883362.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 18559850.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 6415.2642, Train: 84.90%, Valid: 84.87%, Test: 85.00%
Epoch: 275, Loss: 4799.9922, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 1556331.8750, Train: 16.10%, Valid: 16.25%, Test: 15.92%
Epoch: 325, Loss: 11683708.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 125567.5781, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 477.1943, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 17963.7285, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 32.8073, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 5.3001, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 1261883392.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 87.06
Highest Valid: 87.08
  Final Train: 87.06
   Final Test: 87.07
All runs:
Highest Train: 87.06, nan
Highest Valid: 87.08, nan
  Final Train: 87.06, nan
   Final Test: 87.07, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.8492, Train: 84.29%, Valid: 84.25%, Test: 84.34%
Epoch: 25, Loss: 0.4224, Train: 85.57%, Valid: 85.41%, Test: 85.62%
Epoch: 50, Loss: 0.3892, Train: 85.59%, Valid: 85.43%, Test: 85.63%
Epoch: 75, Loss: 0.3681, Train: 85.62%, Valid: 85.46%, Test: 85.66%
Epoch: 100, Loss: 0.3647, Train: 85.62%, Valid: 85.45%, Test: 85.64%
Epoch: 125, Loss: 0.3612, Train: 85.63%, Valid: 85.44%, Test: 85.65%
Epoch: 150, Loss: 0.3550, Train: 85.69%, Valid: 85.49%, Test: 85.71%
Epoch: 175, Loss: 0.3472, Train: 85.56%, Valid: 85.35%, Test: 85.59%
Epoch: 200, Loss: 0.3371, Train: 85.30%, Valid: 85.11%, Test: 85.37%
Epoch: 225, Loss: 0.3338, Train: 85.67%, Valid: 85.50%, Test: 85.75%
Epoch: 250, Loss: 0.3270, Train: 85.90%, Valid: 85.68%, Test: 85.98%
Epoch: 275, Loss: 0.3248, Train: 86.07%, Valid: 85.88%, Test: 86.09%
Epoch: 300, Loss: 0.3254, Train: 86.09%, Valid: 85.95%, Test: 86.18%
Epoch: 325, Loss: 0.8587, Train: 85.43%, Valid: 85.21%, Test: 85.49%
Epoch: 350, Loss: 0.7468, Train: 85.48%, Valid: 85.26%, Test: 85.53%
Epoch: 375, Loss: 0.5869, Train: 85.48%, Valid: 85.26%, Test: 85.53%
Epoch: 400, Loss: 0.4783, Train: 85.47%, Valid: 85.25%, Test: 85.52%
Epoch: 425, Loss: 0.3408, Train: 85.89%, Valid: 85.66%, Test: 85.94%
Epoch: 450, Loss: 0.3342, Train: 85.88%, Valid: 85.66%, Test: 85.94%
Epoch: 475, Loss: 0.3312, Train: 85.85%, Valid: 85.65%, Test: 85.92%
Run 01:
Highest Train: 86.62
Highest Valid: 86.45
  Final Train: 86.62
   Final Test: 86.67
All runs:
Highest Train: 86.62, nan
Highest Valid: 86.45, nan
  Final Train: 86.62, nan
   Final Test: 86.67, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.5390, Train: 85.01%, Valid: 84.77%, Test: 85.10%
Epoch: 25, Loss: 1038053341029481446375424.0000, Train: 88.27%, Valid: 88.34%, Test: 88.28%
Epoch: 50, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 88.34
Highest Valid: 88.38
  Final Train: 88.30
   Final Test: 88.32
All runs:
Highest Train: 88.34, nan
Highest Valid: 88.38, nan
  Final Train: 88.30, nan
   Final Test: 88.32, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.6909, Train: 85.77%, Valid: 85.73%, Test: 85.73%
Epoch: 25, Loss: 6.7085, Train: 15.29%, Valid: 15.45%, Test: 15.17%
Epoch: 50, Loss: 128012330113681063936.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 36.5844, Train: 14.03%, Valid: 14.03%, Test: 13.92%
Epoch: 100, Loss: 55.0728, Train: 13.52%, Valid: 13.55%, Test: 13.38%
Epoch: 125, Loss: 54.3153, Train: 13.68%, Valid: 13.73%, Test: 13.55%
Epoch: 150, Loss: 24380.1602, Train: 13.71%, Valid: 13.76%, Test: 13.58%
Epoch: 175, Loss: 62.4236, Train: 13.51%, Valid: 13.55%, Test: 13.37%
Epoch: 200, Loss: 52.3795, Train: 13.70%, Valid: 13.75%, Test: 13.57%
Epoch: 225, Loss: 49.4846, Train: 13.51%, Valid: 13.55%, Test: 13.36%
Epoch: 250, Loss: 62.1572, Train: 13.53%, Valid: 13.56%, Test: 13.40%
Epoch: 275, Loss: 55.8336, Train: 13.53%, Valid: 13.56%, Test: 13.40%
Epoch: 300, Loss: 67.3504, Train: 86.00%, Valid: 85.86%, Test: 85.98%
Epoch: 325, Loss: 1515.9374, Train: 13.52%, Valid: 13.55%, Test: 13.37%
Epoch: 350, Loss: 54.1683, Train: 13.70%, Valid: 13.75%, Test: 13.57%
Epoch: 375, Loss: 60.2219, Train: 13.53%, Valid: 13.56%, Test: 13.40%
Epoch: 400, Loss: 50.9972, Train: 13.54%, Valid: 13.58%, Test: 13.39%
Epoch: 425, Loss: 48.3479, Train: 13.51%, Valid: 13.55%, Test: 13.36%
Epoch: 450, Loss: 48.3286, Train: 13.52%, Valid: 13.55%, Test: 13.36%
Epoch: 475, Loss: 48.3286, Train: 13.52%, Valid: 13.55%, Test: 13.36%
Run 01:
Highest Train: 86.70
Highest Valid: 86.57
  Final Train: 86.70
   Final Test: 86.89
All runs:
Highest Train: 86.70, nan
Highest Valid: 86.57, nan
  Final Train: 86.70, nan
   Final Test: 86.89, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.2456, Train: 84.36%, Valid: 84.43%, Test: 84.51%
Epoch: 25, Loss: 23.1700, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 342802023157695811893478817792.0000, Train: 49.94%, Valid: 49.93%, Test: 49.94%
Epoch: 75, Loss: 586254445969408.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 608733035912473169199366144.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 84.85
Highest Valid: 84.77
  Final Train: 84.85
   Final Test: 84.92
All runs:
Highest Train: 84.85, nan
Highest Valid: 84.77, nan
  Final Train: 84.85, nan
   Final Test: 84.92, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.7891, Train: 85.04%, Valid: 85.03%, Test: 85.18%
Epoch: 25, Loss: 1.1280, Train: 85.06%, Valid: 85.03%, Test: 85.20%
Epoch: 50, Loss: 6.1455, Train: 87.97%, Valid: 88.00%, Test: 87.97%
Epoch: 75, Loss: 1.9590, Train: 86.87%, Valid: 86.92%, Test: 86.95%
Epoch: 100, Loss: 2.6870, Train: 86.82%, Valid: 86.88%, Test: 86.89%
Epoch: 125, Loss: 2.0588, Train: 86.80%, Valid: 86.87%, Test: 86.86%
Epoch: 150, Loss: 2.0327, Train: 86.80%, Valid: 86.87%, Test: 86.87%
Epoch: 175, Loss: 2.0106, Train: 86.80%, Valid: 86.87%, Test: 86.87%
Epoch: 200, Loss: 1.9921, Train: 86.83%, Valid: 86.89%, Test: 86.90%
Epoch: 225, Loss: 1.9744, Train: 86.88%, Valid: 86.94%, Test: 86.95%
Epoch: 250, Loss: 1.9576, Train: 86.63%, Valid: 86.66%, Test: 86.70%
Epoch: 275, Loss: 1.8282, Train: 86.61%, Valid: 86.65%, Test: 86.67%
Epoch: 300, Loss: 1.9119, Train: 86.56%, Valid: 86.58%, Test: 86.62%
Epoch: 325, Loss: 1.9151, Train: 86.55%, Valid: 86.58%, Test: 86.61%
Epoch: 350, Loss: 1.8973, Train: 86.54%, Valid: 86.58%, Test: 86.61%
Epoch: 375, Loss: 1.8824, Train: 86.54%, Valid: 86.57%, Test: 86.60%
Epoch: 400, Loss: 1.8386, Train: 86.52%, Valid: 86.55%, Test: 86.58%
Epoch: 425, Loss: 1.7809, Train: 86.51%, Valid: 86.54%, Test: 86.57%
Epoch: 450, Loss: 1.7929, Train: 86.50%, Valid: 86.53%, Test: 86.57%
Epoch: 475, Loss: 1.7530, Train: 86.49%, Valid: 86.52%, Test: 86.56%
Run 01:
Highest Train: 88.19
Highest Valid: 88.23
  Final Train: 88.19
   Final Test: 88.21
All runs:
Highest Train: 88.19, nan
Highest Valid: 88.23, nan
  Final Train: 88.19, nan
   Final Test: 88.21, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 16.9950, Train: 86.03%, Valid: 85.97%, Test: 86.01%
Epoch: 25, Loss: 36.1586, Train: 15.64%, Valid: 15.73%, Test: 15.50%
Epoch: 50, Loss: 1176.1410, Train: 16.69%, Valid: 16.86%, Test: 16.57%
Epoch: 75, Loss: 7540.0737, Train: 85.52%, Valid: 85.51%, Test: 85.67%
Epoch: 100, Loss: 4.2529, Train: 86.40%, Valid: 86.36%, Test: 86.52%
Epoch: 125, Loss: 1263.4940, Train: 86.41%, Valid: 86.36%, Test: 86.52%
Epoch: 150, Loss: 41.1618, Train: 86.41%, Valid: 86.38%, Test: 86.54%
Epoch: 175, Loss: 29.8296, Train: 86.40%, Valid: 86.35%, Test: 86.52%
Epoch: 200, Loss: 57.7150, Train: 86.42%, Valid: 86.38%, Test: 86.53%
Epoch: 225, Loss: 40.4659, Train: 86.41%, Valid: 86.37%, Test: 86.53%
Epoch: 250, Loss: 18.6399, Train: 14.98%, Valid: 15.07%, Test: 14.93%
Epoch: 275, Loss: 25.1209, Train: 86.42%, Valid: 86.38%, Test: 86.54%
Epoch: 300, Loss: 0.8363, Train: 86.42%, Valid: 86.37%, Test: 86.52%
Epoch: 325, Loss: 23.0835, Train: 86.47%, Valid: 86.43%, Test: 86.57%
Epoch: 350, Loss: 0.9839, Train: 16.15%, Valid: 16.22%, Test: 16.18%
Epoch: 375, Loss: 4.7024, Train: 86.47%, Valid: 86.43%, Test: 86.57%
Epoch: 400, Loss: 10.0392, Train: 86.44%, Valid: 86.41%, Test: 86.55%
Epoch: 425, Loss: 18.3480, Train: 86.43%, Valid: 86.40%, Test: 86.54%
Epoch: 450, Loss: 20.7936, Train: 86.42%, Valid: 86.37%, Test: 86.52%
Epoch: 475, Loss: 25.5074, Train: 86.44%, Valid: 86.40%, Test: 86.55%
Run 01:
Highest Train: 86.48
Highest Valid: 86.47
  Final Train: 86.46
   Final Test: 86.58
All runs:
Highest Train: 86.48, nan
Highest Valid: 86.47, nan
  Final Train: 86.46, nan
   Final Test: 86.58, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 4.7174, Train: 85.04%, Valid: 84.89%, Test: 85.08%
Epoch: 25, Loss: 1.1393, Train: 86.04%, Valid: 85.90%, Test: 86.04%
Epoch: 50, Loss: 99.3500, Train: 85.28%, Valid: 85.07%, Test: 85.35%
Epoch: 75, Loss: 1.8963, Train: 85.50%, Valid: 85.32%, Test: 85.54%
Epoch: 100, Loss: 2.0228, Train: 85.54%, Valid: 85.35%, Test: 85.56%
Epoch: 125, Loss: 1.9378, Train: 85.89%, Valid: 85.71%, Test: 85.91%
Epoch: 150, Loss: 1.8824, Train: 85.60%, Valid: 85.42%, Test: 85.63%
Epoch: 175, Loss: 1.9264, Train: 85.29%, Valid: 85.10%, Test: 85.34%
Epoch: 200, Loss: 2.5561, Train: 86.59%, Valid: 86.54%, Test: 86.64%
Epoch: 225, Loss: 2.3509, Train: 86.23%, Valid: 86.07%, Test: 86.26%
Epoch: 250, Loss: 67.6184, Train: 87.23%, Valid: 87.25%, Test: 87.26%
Epoch: 275, Loss: 566.2899, Train: 85.60%, Valid: 85.56%, Test: 85.72%
Epoch: 300, Loss: 15.6125, Train: 86.57%, Valid: 86.60%, Test: 86.67%
Epoch: 325, Loss: 12.6422, Train: 86.55%, Valid: 86.58%, Test: 86.65%
Epoch: 350, Loss: 12.8534, Train: 86.55%, Valid: 86.58%, Test: 86.65%
Epoch: 375, Loss: 12.2679, Train: 86.55%, Valid: 86.58%, Test: 86.65%
Epoch: 400, Loss: 12.6306, Train: 86.54%, Valid: 86.57%, Test: 86.64%
Epoch: 425, Loss: 12.2888, Train: 86.55%, Valid: 86.57%, Test: 86.64%
Epoch: 450, Loss: 12.4198, Train: 86.54%, Valid: 86.57%, Test: 86.64%
Epoch: 475, Loss: 12.1650, Train: 86.54%, Valid: 86.57%, Test: 86.64%
Run 01:
Highest Train: 88.61
Highest Valid: 88.60
  Final Train: 88.60
   Final Test: 88.54
All runs:
Highest Train: 88.61, nan
Highest Valid: 88.60, nan
  Final Train: 88.60, nan
   Final Test: 88.54, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 12.2553, Train: 15.39%, Valid: 15.46%, Test: 15.40%
Epoch: 25, Loss: 34.7789, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 3348476.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 156.2895, Train: 84.70%, Valid: 84.70%, Test: 84.83%
Epoch: 100, Loss: 2207.7842, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 23825.0352, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 4055.8794, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 150.7905, Train: 16.17%, Valid: 16.14%, Test: 16.08%
Epoch: 200, Loss: 23.2267, Train: 15.41%, Valid: 15.38%, Test: 15.28%
Epoch: 225, Loss: 3.7266, Train: 85.55%, Valid: 85.41%, Test: 85.64%
Epoch: 250, Loss: 5.8416, Train: 15.60%, Valid: 15.69%, Test: 15.47%
Epoch: 275, Loss: 1064.4866, Train: 15.47%, Valid: 15.45%, Test: 15.34%
Epoch: 300, Loss: 201.6922, Train: 16.07%, Valid: 16.04%, Test: 15.94%
Epoch: 325, Loss: 150.0143, Train: 15.47%, Valid: 15.44%, Test: 15.35%
Epoch: 350, Loss: 203.7468, Train: 15.64%, Valid: 15.72%, Test: 15.52%
Epoch: 375, Loss: 705.1242, Train: 84.44%, Valid: 84.41%, Test: 84.56%
Epoch: 400, Loss: 160.1698, Train: 84.29%, Valid: 84.12%, Test: 84.40%
Epoch: 425, Loss: 227.0991, Train: 16.51%, Valid: 16.50%, Test: 16.40%
Epoch: 450, Loss: 5621.0479, Train: 14.84%, Valid: 14.81%, Test: 14.82%
Epoch: 475, Loss: 990.7135, Train: 15.60%, Valid: 15.70%, Test: 15.47%
Run 01:
Highest Train: 88.08
Highest Valid: 88.13
  Final Train: 88.08
   Final Test: 88.08
All runs:
Highest Train: 88.08, nan
Highest Valid: 88.13, nan
  Final Train: 88.08, nan
   Final Test: 88.08, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 4.1547, Train: 86.95%, Valid: 86.99%, Test: 86.98%
Epoch: 25, Loss: 15.8317, Train: 86.08%, Valid: 85.95%, Test: 86.15%
Epoch: 50, Loss: 1.3761, Train: 86.08%, Valid: 85.96%, Test: 86.16%
Epoch: 75, Loss: 6859.2798, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 8748331008.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 3434908928.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 58928324608.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 305893703680.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 18363277312.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 1898308352.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 753376768.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 1389826539520.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 506253606912.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 15090638848.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 36711784448.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 44572924.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 37762728.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 21117773824.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 387965288448.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 402871220699136.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.95
Highest Valid: 86.99
  Final Train: 86.95
   Final Test: 86.98
All runs:
Highest Train: 86.95, nan
Highest Valid: 86.99, nan
  Final Train: 86.95, nan
   Final Test: 86.98, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 3.5167, Train: 15.98%, Valid: 16.05%, Test: 15.89%
Epoch: 25, Loss: 0.7932, Train: 86.18%, Valid: 86.16%, Test: 86.29%
Epoch: 50, Loss: 380.2815, Train: 86.43%, Valid: 86.45%, Test: 86.51%
Epoch: 75, Loss: 30337290.0000, Train: 49.99%, Valid: 49.99%, Test: 50.00%
Epoch: 100, Loss: 246.2279, Train: 16.20%, Valid: 16.37%, Test: 16.05%
Epoch: 125, Loss: 272.7224, Train: 16.23%, Valid: 16.40%, Test: 16.10%
Epoch: 150, Loss: 264.8331, Train: 16.23%, Valid: 16.41%, Test: 16.11%
Epoch: 175, Loss: 259.2563, Train: 16.23%, Valid: 16.41%, Test: 16.11%
Epoch: 200, Loss: 244.2863, Train: 16.23%, Valid: 16.41%, Test: 16.11%
Epoch: 225, Loss: 245.0632, Train: 16.23%, Valid: 16.41%, Test: 16.11%
Epoch: 250, Loss: 257.9098, Train: 16.23%, Valid: 16.40%, Test: 16.09%
Epoch: 275, Loss: 255.1676, Train: 16.23%, Valid: 16.40%, Test: 16.10%
Epoch: 300, Loss: 255.7812, Train: 16.23%, Valid: 16.41%, Test: 16.10%
Epoch: 325, Loss: 265.3910, Train: 16.12%, Valid: 16.29%, Test: 15.99%
Epoch: 350, Loss: 257.3134, Train: 16.23%, Valid: 16.41%, Test: 16.10%
Epoch: 375, Loss: 262.7365, Train: 16.23%, Valid: 16.41%, Test: 16.11%
Epoch: 400, Loss: 253.5261, Train: 16.23%, Valid: 16.41%, Test: 16.11%
Epoch: 425, Loss: 259.7161, Train: 16.23%, Valid: 16.41%, Test: 16.11%
Epoch: 450, Loss: 257.8623, Train: 16.23%, Valid: 16.41%, Test: 16.10%
Epoch: 475, Loss: 264.7436, Train: 16.23%, Valid: 16.40%, Test: 16.09%
Run 01:
Highest Train: 86.79
Highest Valid: 86.82
  Final Train: 86.79
   Final Test: 86.86
All runs:
Highest Train: 86.79, nan
Highest Valid: 86.82, nan
  Final Train: 86.79, nan
   Final Test: 86.86, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.4666, Train: 86.36%, Valid: 86.37%, Test: 86.45%
Epoch: 25, Loss: 66.6857, Train: 14.65%, Valid: 14.65%, Test: 14.58%
Epoch: 50, Loss: 84.6827, Train: 14.82%, Valid: 14.79%, Test: 14.73%
Epoch: 75, Loss: 82.6867, Train: 14.83%, Valid: 14.81%, Test: 14.75%
Epoch: 100, Loss: 72.6630, Train: 14.67%, Valid: 14.64%, Test: 14.56%
Epoch: 125, Loss: 39.7003, Train: 14.78%, Valid: 14.74%, Test: 14.70%
Epoch: 150, Loss: 0.5522, Train: 85.83%, Valid: 85.70%, Test: 85.88%
Epoch: 175, Loss: 0.5810, Train: 85.84%, Valid: 85.71%, Test: 85.89%
Epoch: 200, Loss: 0.5620, Train: 85.87%, Valid: 85.75%, Test: 85.93%
Epoch: 225, Loss: 0.5463, Train: 85.91%, Valid: 85.80%, Test: 85.96%
Epoch: 250, Loss: 0.5336, Train: 85.98%, Valid: 85.85%, Test: 86.03%
Epoch: 275, Loss: 0.5221, Train: 86.09%, Valid: 86.00%, Test: 86.13%
Epoch: 300, Loss: 0.5127, Train: 86.16%, Valid: 86.08%, Test: 86.19%
Epoch: 325, Loss: 0.5039, Train: 86.24%, Valid: 86.17%, Test: 86.26%
Epoch: 350, Loss: 0.4926, Train: 86.32%, Valid: 86.28%, Test: 86.35%
Epoch: 375, Loss: 0.4821, Train: 86.34%, Valid: 86.21%, Test: 86.35%
Epoch: 400, Loss: 0.4707, Train: 86.52%, Valid: 86.40%, Test: 86.53%
Epoch: 425, Loss: 0.4616, Train: 86.62%, Valid: 86.49%, Test: 86.63%
Epoch: 450, Loss: 0.4519, Train: 86.71%, Valid: 86.55%, Test: 86.72%
Epoch: 475, Loss: 0.4413, Train: 86.78%, Valid: 86.63%, Test: 86.78%
Run 01:
Highest Train: 87.96
Highest Valid: 87.96
  Final Train: 87.96
   Final Test: 87.92
All runs:
Highest Train: 87.96, nan
Highest Valid: 87.96, nan
  Final Train: 87.96, nan
   Final Test: 87.92, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 4.0881, Train: 85.66%, Valid: 85.53%, Test: 85.78%
Epoch: 25, Loss: 3.1556, Train: 85.88%, Valid: 85.82%, Test: 85.93%
Epoch: 50, Loss: 155.4173, Train: 85.88%, Valid: 85.82%, Test: 85.93%
Epoch: 75, Loss: 5835.2231, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 27.1367, Train: 85.81%, Valid: 85.75%, Test: 85.86%
Epoch: 125, Loss: 20.8646, Train: 14.87%, Valid: 15.03%, Test: 14.74%
Epoch: 150, Loss: 6901515.5000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 2453588943014140575744.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 76086444556288.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 573093183488.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 102884129099481088.0000, Train: 85.43%, Valid: 85.40%, Test: 85.49%
Epoch: 275, Loss: 67544354816.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 111309.8359, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 30.7199, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 17247089197056.0000, Train: 85.57%, Valid: 85.55%, Test: 85.65%
Epoch: 375, Loss: 3112.9851, Train: 85.55%, Valid: 85.52%, Test: 85.62%
Epoch: 400, Loss: 28.9832, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 41865151774720.0000, Train: 85.57%, Valid: 85.56%, Test: 85.66%
Epoch: 450, Loss: 151371860934656.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 22175980.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.66
Highest Valid: 86.59
  Final Train: 86.60
   Final Test: 86.63
All runs:
Highest Train: 86.66, nan
Highest Valid: 86.59, nan
  Final Train: 86.60, nan
   Final Test: 86.63, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.8505, Train: 85.24%, Valid: 85.14%, Test: 85.32%
Epoch: 25, Loss: 14.5711, Train: 86.40%, Valid: 86.27%, Test: 86.40%
Epoch: 50, Loss: 1.5938, Train: 86.40%, Valid: 86.28%, Test: 86.39%
Epoch: 75, Loss: 1.6361, Train: 86.42%, Valid: 86.30%, Test: 86.41%
Epoch: 100, Loss: 1.6404, Train: 86.42%, Valid: 86.30%, Test: 86.42%
Epoch: 125, Loss: 1.6460, Train: 86.42%, Valid: 86.30%, Test: 86.42%
Epoch: 150, Loss: 1.6343, Train: 86.42%, Valid: 86.30%, Test: 86.42%
Epoch: 175, Loss: 1.6362, Train: 86.43%, Valid: 86.30%, Test: 86.42%
Epoch: 200, Loss: 1.6444, Train: 86.43%, Valid: 86.30%, Test: 86.42%
Epoch: 225, Loss: 1.6400, Train: 86.43%, Valid: 86.30%, Test: 86.42%
Epoch: 250, Loss: 1.6333, Train: 86.43%, Valid: 86.30%, Test: 86.42%
Epoch: 275, Loss: 1.6341, Train: 86.43%, Valid: 86.30%, Test: 86.42%
Epoch: 300, Loss: 1.6262, Train: 86.43%, Valid: 86.31%, Test: 86.43%
Epoch: 325, Loss: 1.6289, Train: 86.44%, Valid: 86.31%, Test: 86.43%
Epoch: 350, Loss: 1.6305, Train: 86.44%, Valid: 86.31%, Test: 86.43%
Epoch: 375, Loss: 1.6259, Train: 86.44%, Valid: 86.31%, Test: 86.43%
Epoch: 400, Loss: 1.6286, Train: 86.44%, Valid: 86.31%, Test: 86.43%
Epoch: 425, Loss: 1.6257, Train: 86.44%, Valid: 86.31%, Test: 86.43%
Epoch: 450, Loss: 1.6291, Train: 86.43%, Valid: 86.31%, Test: 86.42%
Epoch: 475, Loss: 1.6268, Train: 86.43%, Valid: 86.30%, Test: 86.42%
Run 01:
Highest Train: 86.44
Highest Valid: 86.32
  Final Train: 86.44
   Final Test: 86.43
All runs:
Highest Train: 86.44, nan
Highest Valid: 86.32, nan
  Final Train: 86.44, nan
   Final Test: 86.43, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 19.3315, Train: 85.36%, Valid: 85.38%, Test: 85.46%
Epoch: 25, Loss: 1.2800, Train: 85.72%, Valid: 85.77%, Test: 85.85%
Epoch: 50, Loss: 1.3513, Train: 85.62%, Valid: 85.66%, Test: 85.74%
Epoch: 75, Loss: 0.9909, Train: 85.40%, Valid: 85.44%, Test: 85.51%
Epoch: 100, Loss: 0.3657, Train: 85.49%, Valid: 85.49%, Test: 85.60%
Epoch: 125, Loss: 0.3499, Train: 85.39%, Valid: 85.42%, Test: 85.51%
Epoch: 150, Loss: 0.3420, Train: 86.02%, Valid: 85.86%, Test: 86.05%
Epoch: 175, Loss: 0.3451, Train: 85.84%, Valid: 85.64%, Test: 85.92%
Epoch: 200, Loss: 0.3361, Train: 85.93%, Valid: 85.72%, Test: 85.96%
Epoch: 225, Loss: 0.3312, Train: 86.28%, Valid: 86.08%, Test: 86.35%
Epoch: 250, Loss: 0.7955, Train: 85.46%, Valid: 85.47%, Test: 85.54%
Epoch: 275, Loss: 0.7575, Train: 85.16%, Valid: 85.20%, Test: 85.28%
Epoch: 300, Loss: 0.3539, Train: 85.38%, Valid: 85.32%, Test: 85.48%
Epoch: 325, Loss: 0.3363, Train: 84.73%, Valid: 84.50%, Test: 84.81%
Epoch: 350, Loss: 0.3288, Train: 85.85%, Valid: 85.71%, Test: 85.90%
Epoch: 375, Loss: 1.0111, Train: 85.45%, Valid: 85.45%, Test: 85.53%
Epoch: 400, Loss: 0.7857, Train: 85.16%, Valid: 85.19%, Test: 85.25%
Epoch: 425, Loss: 0.6505, Train: 85.01%, Valid: 84.99%, Test: 85.11%
Epoch: 450, Loss: 0.4931, Train: 84.95%, Valid: 84.91%, Test: 85.03%
Epoch: 475, Loss: 0.3662, Train: 84.67%, Valid: 84.44%, Test: 84.76%
Run 01:
Highest Train: 87.62
Highest Valid: 87.44
  Final Train: 87.62
   Final Test: 87.67
All runs:
Highest Train: 87.62, nan
Highest Valid: 87.44, nan
  Final Train: 87.62, nan
   Final Test: 87.67, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 7.0660, Train: 86.45%, Valid: 86.31%, Test: 86.52%
Epoch: 25, Loss: 4.5501, Train: 86.06%, Valid: 86.00%, Test: 86.14%
Epoch: 50, Loss: 2.8279, Train: 85.64%, Valid: 85.49%, Test: 85.76%
Epoch: 75, Loss: 5901.2305, Train: 15.98%, Valid: 16.17%, Test: 15.86%
Epoch: 100, Loss: 1490.7389, Train: 15.70%, Valid: 15.82%, Test: 15.70%
Epoch: 125, Loss: 218.8684, Train: 85.97%, Valid: 85.90%, Test: 85.99%
Epoch: 150, Loss: 212.7186, Train: 85.53%, Valid: 85.48%, Test: 85.59%
Epoch: 175, Loss: 216.7411, Train: 85.32%, Valid: 85.27%, Test: 85.38%
Epoch: 200, Loss: 190.3255, Train: 85.20%, Valid: 85.14%, Test: 85.24%
Epoch: 225, Loss: 220.0542, Train: 86.58%, Valid: 86.54%, Test: 86.58%
Epoch: 250, Loss: 235.3456, Train: 85.23%, Valid: 85.17%, Test: 85.28%
Epoch: 275, Loss: 262.9985, Train: 84.69%, Valid: 84.55%, Test: 84.74%
Epoch: 300, Loss: 162.5559, Train: 85.44%, Valid: 85.39%, Test: 85.50%
Epoch: 325, Loss: 216.4509, Train: 85.35%, Valid: 85.31%, Test: 85.41%
Epoch: 350, Loss: 218.7917, Train: 85.40%, Valid: 85.36%, Test: 85.46%
Epoch: 375, Loss: 220.0267, Train: 85.49%, Valid: 85.46%, Test: 85.55%
Epoch: 400, Loss: 274.0030, Train: 85.59%, Valid: 85.56%, Test: 85.65%
Epoch: 425, Loss: 230.1482, Train: 86.23%, Valid: 86.17%, Test: 86.24%
Epoch: 450, Loss: 216.0431, Train: 85.79%, Valid: 85.73%, Test: 85.84%
Epoch: 475, Loss: 223.2075, Train: 86.34%, Valid: 86.28%, Test: 86.36%
Run 01:
Highest Train: 87.19
Highest Valid: 87.23
  Final Train: 87.19
   Final Test: 87.29
All runs:
Highest Train: 87.19, nan
Highest Valid: 87.23, nan
  Final Train: 87.19, nan
   Final Test: 87.29, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 5.0498, Train: 15.92%, Valid: 15.98%, Test: 15.84%
Epoch: 25, Loss: 0.9513, Train: 86.24%, Valid: 86.10%, Test: 86.26%
Epoch: 50, Loss: 0.6389, Train: 86.01%, Valid: 85.86%, Test: 86.03%
Epoch: 75, Loss: 0.3638, Train: 85.58%, Valid: 85.40%, Test: 85.63%
Epoch: 100, Loss: 0.3592, Train: 85.59%, Valid: 85.43%, Test: 85.65%
Epoch: 125, Loss: 0.3525, Train: 85.67%, Valid: 85.50%, Test: 85.73%
Epoch: 150, Loss: 0.3448, Train: 85.35%, Valid: 85.16%, Test: 85.42%
Epoch: 175, Loss: 0.3390, Train: 85.34%, Valid: 85.16%, Test: 85.41%
Epoch: 200, Loss: 0.3357, Train: 85.48%, Valid: 85.31%, Test: 85.55%
Epoch: 225, Loss: 0.4077, Train: 85.56%, Valid: 85.40%, Test: 85.61%
Epoch: 250, Loss: 0.3441, Train: 85.74%, Valid: 85.55%, Test: 85.81%
Epoch: 275, Loss: 0.3342, Train: 85.64%, Valid: 85.44%, Test: 85.70%
Epoch: 300, Loss: 0.3383, Train: 85.42%, Valid: 85.18%, Test: 85.51%
Epoch: 325, Loss: 0.3310, Train: 85.60%, Valid: 85.40%, Test: 85.68%
Epoch: 350, Loss: 0.3281, Train: 85.56%, Valid: 85.35%, Test: 85.63%
Epoch: 375, Loss: 0.3272, Train: 85.63%, Valid: 85.41%, Test: 85.69%
Epoch: 400, Loss: 0.9032, Train: 85.41%, Valid: 85.22%, Test: 85.47%
Epoch: 425, Loss: 1.7536, Train: 85.37%, Valid: 85.29%, Test: 85.50%
Epoch: 450, Loss: 1.2479, Train: 85.99%, Valid: 85.98%, Test: 86.15%
Epoch: 475, Loss: 0.5748, Train: 85.38%, Valid: 85.17%, Test: 85.44%
Run 01:
Highest Train: 87.42
Highest Valid: 87.33
  Final Train: 87.42
   Final Test: 87.46
All runs:
Highest Train: 87.42, nan
Highest Valid: 87.33, nan
  Final Train: 87.42, nan
   Final Test: 87.46, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 7.2235, Train: 85.41%, Valid: 85.40%, Test: 85.58%
Epoch: 25, Loss: 1.6134, Train: 86.01%, Valid: 86.09%, Test: 86.10%
Epoch: 50, Loss: 1.6375, Train: 85.92%, Valid: 85.99%, Test: 86.00%
Epoch: 75, Loss: 0.9736, Train: 85.70%, Valid: 85.76%, Test: 85.80%
Epoch: 100, Loss: 0.3640, Train: 85.96%, Valid: 85.98%, Test: 86.05%
Epoch: 125, Loss: 0.3457, Train: 85.87%, Valid: 85.88%, Test: 85.95%
Epoch: 150, Loss: 0.3340, Train: 85.89%, Valid: 85.93%, Test: 85.99%
Epoch: 175, Loss: 0.3498, Train: 86.06%, Valid: 86.06%, Test: 86.14%
Epoch: 200, Loss: 0.3284, Train: 86.09%, Valid: 86.09%, Test: 86.19%
Epoch: 225, Loss: 0.3241, Train: 86.08%, Valid: 86.06%, Test: 86.15%
Epoch: 250, Loss: 0.6036, Train: 85.67%, Valid: 85.70%, Test: 85.78%
Epoch: 275, Loss: 0.3587, Train: 85.77%, Valid: 85.78%, Test: 85.90%
Epoch: 300, Loss: 0.3438, Train: 85.99%, Valid: 85.98%, Test: 86.09%
Epoch: 325, Loss: 0.3356, Train: 86.03%, Valid: 86.02%, Test: 86.13%
Epoch: 350, Loss: 0.3291, Train: 86.56%, Valid: 86.42%, Test: 86.64%
Epoch: 375, Loss: 0.3337, Train: 86.04%, Valid: 86.07%, Test: 86.13%
Epoch: 400, Loss: 0.3398, Train: 85.73%, Valid: 85.74%, Test: 85.83%
Epoch: 425, Loss: 0.3370, Train: 85.73%, Valid: 85.72%, Test: 85.82%
Epoch: 450, Loss: 0.3324, Train: 85.99%, Valid: 85.98%, Test: 86.08%
Epoch: 475, Loss: 0.3304, Train: 86.05%, Valid: 86.05%, Test: 86.14%
Run 01:
Highest Train: 87.77
Highest Valid: 87.63
  Final Train: 87.77
   Final Test: 87.84
All runs:
Highest Train: 87.77, nan
Highest Valid: 87.63, nan
  Final Train: 87.77, nan
   Final Test: 87.84, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.5746, Train: 85.42%, Valid: 85.29%, Test: 85.56%
Epoch: 25, Loss: 1.8436, Train: 85.62%, Valid: 85.49%, Test: 85.73%
Epoch: 50, Loss: 3.5420, Train: 85.68%, Valid: 85.54%, Test: 85.78%
Epoch: 75, Loss: 3.9522, Train: 85.60%, Valid: 85.48%, Test: 85.70%
Epoch: 100, Loss: 2.2080, Train: 85.30%, Valid: 85.18%, Test: 85.41%
Epoch: 125, Loss: 0.7117, Train: 86.54%, Valid: 86.55%, Test: 86.55%
Epoch: 150, Loss: 0.4079, Train: 85.57%, Valid: 85.49%, Test: 85.74%
Epoch: 175, Loss: 0.3548, Train: 85.64%, Valid: 85.61%, Test: 85.76%
Epoch: 200, Loss: 0.3419, Train: 86.01%, Valid: 85.82%, Test: 86.13%
Epoch: 225, Loss: 0.3598, Train: 85.92%, Valid: 85.95%, Test: 86.04%
Epoch: 250, Loss: 4.9546, Train: 85.94%, Valid: 85.84%, Test: 86.05%
Epoch: 275, Loss: 5.5705, Train: 85.85%, Valid: 85.70%, Test: 85.95%
Epoch: 300, Loss: 3.9885, Train: 85.80%, Valid: 85.66%, Test: 85.90%
Epoch: 325, Loss: 2.6092, Train: 85.82%, Valid: 85.67%, Test: 85.92%
Epoch: 350, Loss: 0.7526, Train: 86.49%, Valid: 86.42%, Test: 86.51%
Epoch: 375, Loss: 0.5543, Train: 86.06%, Valid: 86.02%, Test: 86.07%
Epoch: 400, Loss: 0.3846, Train: 86.26%, Valid: 86.10%, Test: 86.37%
Epoch: 425, Loss: 0.3645, Train: 86.25%, Valid: 86.08%, Test: 86.37%
Epoch: 450, Loss: 0.3605, Train: 86.28%, Valid: 86.10%, Test: 86.40%
Epoch: 475, Loss: 0.3947, Train: 86.79%, Valid: 86.70%, Test: 86.82%
Run 01:
Highest Train: 87.55
Highest Valid: 87.44
  Final Train: 87.55
   Final Test: 87.59
All runs:
Highest Train: 87.55, nan
Highest Valid: 87.44, nan
  Final Train: 87.55, nan
   Final Test: 87.59, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 6.5835, Train: 85.45%, Valid: 85.38%, Test: 85.50%
Epoch: 25, Loss: 28.7431, Train: 86.29%, Valid: 86.27%, Test: 86.36%
Epoch: 50, Loss: 3.9572, Train: 86.69%, Valid: 86.72%, Test: 86.80%
Epoch: 75, Loss: 22876004.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 21.6289, Train: 85.61%, Valid: 85.53%, Test: 85.69%
Epoch: 125, Loss: 5.9349, Train: 85.66%, Valid: 85.56%, Test: 85.72%
Epoch: 150, Loss: 5.9019, Train: 85.66%, Valid: 85.56%, Test: 85.73%
Epoch: 175, Loss: 5.8943, Train: 85.66%, Valid: 85.57%, Test: 85.73%
Epoch: 200, Loss: 5.8824, Train: 85.66%, Valid: 85.57%, Test: 85.73%
Epoch: 225, Loss: 5.9043, Train: 85.66%, Valid: 85.57%, Test: 85.73%
Epoch: 250, Loss: 5.8977, Train: 85.66%, Valid: 85.57%, Test: 85.73%
Epoch: 275, Loss: 5.9280, Train: 85.66%, Valid: 85.57%, Test: 85.73%
Epoch: 300, Loss: 5.9255, Train: 85.66%, Valid: 85.57%, Test: 85.73%
Epoch: 325, Loss: 5.9078, Train: 85.66%, Valid: 85.57%, Test: 85.73%
Epoch: 350, Loss: 5.8808, Train: 85.66%, Valid: 85.57%, Test: 85.73%
Epoch: 375, Loss: 5.9075, Train: 85.66%, Valid: 85.57%, Test: 85.73%
Epoch: 400, Loss: 5.9078, Train: 85.66%, Valid: 85.57%, Test: 85.73%
Epoch: 425, Loss: 5.8650, Train: 85.66%, Valid: 85.57%, Test: 85.73%
Epoch: 450, Loss: 5.8942, Train: 85.66%, Valid: 85.57%, Test: 85.73%
Epoch: 475, Loss: 5.9496, Train: 85.66%, Valid: 85.57%, Test: 85.73%
Run 01:
Highest Train: 86.91
Highest Valid: 86.93
  Final Train: 86.91
   Final Test: 86.95
All runs:
Highest Train: 86.91, nan
Highest Valid: 86.93, nan
  Final Train: 86.91, nan
   Final Test: 86.95, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 10.8541, Train: 87.40%, Valid: 87.44%, Test: 87.49%
Epoch: 25, Loss: 0.5162, Train: 86.43%, Valid: 86.44%, Test: 86.53%
Epoch: 50, Loss: 0.8994, Train: 85.80%, Valid: 85.85%, Test: 85.88%
Epoch: 75, Loss: 2.0184, Train: 85.73%, Valid: 85.76%, Test: 85.83%
Epoch: 100, Loss: 3.8399, Train: 85.88%, Valid: 85.93%, Test: 85.97%
Epoch: 125, Loss: 4.3902, Train: 85.65%, Valid: 85.71%, Test: 85.71%
Epoch: 150, Loss: 3.2437, Train: 85.66%, Valid: 85.70%, Test: 85.71%
Epoch: 175, Loss: 1.0361, Train: 84.47%, Valid: 84.53%, Test: 84.59%
Epoch: 200, Loss: 0.3914, Train: 85.37%, Valid: 85.36%, Test: 85.48%
Epoch: 225, Loss: 0.3770, Train: 85.76%, Valid: 85.77%, Test: 85.89%
Epoch: 250, Loss: 0.3538, Train: 85.91%, Valid: 85.90%, Test: 86.01%
Epoch: 275, Loss: 0.3408, Train: 86.74%, Valid: 86.74%, Test: 86.80%
Epoch: 300, Loss: 2.7628, Train: 85.63%, Valid: 85.67%, Test: 85.71%
Epoch: 325, Loss: 2.3840, Train: 85.60%, Valid: 85.65%, Test: 85.69%
Epoch: 350, Loss: 1.2154, Train: 86.95%, Valid: 86.74%, Test: 86.96%
Epoch: 375, Loss: 1.1311, Train: 86.82%, Valid: 86.59%, Test: 86.81%
Epoch: 400, Loss: 1.4496, Train: 87.36%, Valid: 87.19%, Test: 87.34%
Epoch: 425, Loss: 1.1965, Train: 87.37%, Valid: 87.20%, Test: 87.35%
Epoch: 450, Loss: 0.8039, Train: 87.36%, Valid: 87.21%, Test: 87.34%
Epoch: 475, Loss: 0.3576, Train: 87.38%, Valid: 87.39%, Test: 87.47%
Run 01:
Highest Train: 88.56
Highest Valid: 88.61
  Final Train: 88.56
   Final Test: 88.63
All runs:
Highest Train: 88.56, nan
Highest Valid: 88.61, nan
  Final Train: 88.56, nan
   Final Test: 88.63, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 13.1530, Train: 85.83%, Valid: 85.75%, Test: 85.85%
Epoch: 25, Loss: 1.1457, Train: 86.16%, Valid: 86.14%, Test: 86.16%
Epoch: 50, Loss: 1.0426, Train: 86.27%, Valid: 86.15%, Test: 86.27%
Epoch: 75, Loss: 0.5171, Train: 85.91%, Valid: 85.77%, Test: 85.99%
Epoch: 100, Loss: 0.3557, Train: 87.13%, Valid: 87.09%, Test: 87.12%
Epoch: 125, Loss: 0.3384, Train: 86.48%, Valid: 86.39%, Test: 86.54%
Epoch: 150, Loss: 0.3422, Train: 86.04%, Valid: 85.89%, Test: 86.16%
Epoch: 175, Loss: 0.3342, Train: 86.03%, Valid: 85.88%, Test: 86.17%
Epoch: 200, Loss: 0.3277, Train: 85.97%, Valid: 85.83%, Test: 86.14%
Epoch: 225, Loss: 1.8657, Train: 85.64%, Valid: 85.52%, Test: 85.79%
Epoch: 250, Loss: 1.5178, Train: 85.87%, Valid: 85.74%, Test: 85.98%
Epoch: 275, Loss: 1.6518, Train: 85.89%, Valid: 85.77%, Test: 86.00%
Epoch: 300, Loss: 0.9257, Train: 86.77%, Valid: 86.77%, Test: 86.78%
Epoch: 325, Loss: 0.3571, Train: 86.03%, Valid: 85.83%, Test: 86.18%
Epoch: 350, Loss: 0.3381, Train: 85.90%, Valid: 85.75%, Test: 86.05%
Epoch: 375, Loss: 1.7487, Train: 86.02%, Valid: 85.86%, Test: 86.13%
Epoch: 400, Loss: 6.5565, Train: 87.13%, Valid: 87.10%, Test: 87.14%
Epoch: 425, Loss: 28.2560, Train: 87.00%, Valid: 87.00%, Test: 87.02%
Epoch: 450, Loss: 8.8434, Train: 85.98%, Valid: 85.85%, Test: 86.07%
Epoch: 475, Loss: 8.1160, Train: 86.33%, Valid: 86.21%, Test: 86.42%
Run 01:
Highest Train: 87.62
Highest Valid: 87.59
  Final Train: 87.61
   Final Test: 87.64
All runs:
Highest Train: 87.62, nan
Highest Valid: 87.59, nan
  Final Train: 87.61, nan
   Final Test: 87.64, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 12.3302, Train: 85.07%, Valid: 85.09%, Test: 85.14%
Epoch: 25, Loss: 0.5738, Train: 85.98%, Valid: 85.83%, Test: 86.06%
Epoch: 50, Loss: 0.8787, Train: 86.08%, Valid: 86.07%, Test: 86.13%
Epoch: 75, Loss: 0.6986, Train: 85.55%, Valid: 85.39%, Test: 85.61%
Epoch: 100, Loss: 0.3594, Train: 86.15%, Valid: 86.00%, Test: 86.23%
Epoch: 125, Loss: 0.8475, Train: 85.70%, Valid: 85.52%, Test: 85.69%
Epoch: 150, Loss: 0.4569, Train: 85.61%, Valid: 85.40%, Test: 85.65%
Epoch: 175, Loss: 0.3479, Train: 85.76%, Valid: 85.57%, Test: 85.83%
Epoch: 200, Loss: 0.3357, Train: 85.83%, Valid: 85.63%, Test: 85.90%
Epoch: 225, Loss: 0.3303, Train: 85.67%, Valid: 85.46%, Test: 85.75%
Epoch: 250, Loss: 0.3453, Train: 85.88%, Valid: 85.68%, Test: 85.95%
Epoch: 275, Loss: 0.3330, Train: 85.75%, Valid: 85.56%, Test: 85.83%
Epoch: 300, Loss: 0.7395, Train: 85.53%, Valid: 85.37%, Test: 85.59%
Epoch: 325, Loss: 1.0293, Train: 85.70%, Valid: 85.49%, Test: 85.72%
Epoch: 350, Loss: 0.4927, Train: 86.24%, Valid: 86.06%, Test: 86.21%
Epoch: 375, Loss: 0.3519, Train: 85.88%, Valid: 85.70%, Test: 85.95%
Epoch: 400, Loss: 0.3488, Train: 85.90%, Valid: 85.72%, Test: 85.97%
Epoch: 425, Loss: 0.3463, Train: 85.90%, Valid: 85.72%, Test: 85.97%
Epoch: 450, Loss: 0.3442, Train: 85.85%, Valid: 85.68%, Test: 85.93%
Epoch: 475, Loss: 0.3421, Train: 85.80%, Valid: 85.63%, Test: 85.88%
Run 01:
Highest Train: 87.38
Highest Valid: 87.28
  Final Train: 87.38
   Final Test: 87.41
All runs:
Highest Train: 87.38, nan
Highest Valid: 87.28, nan
  Final Train: 87.38, nan
   Final Test: 87.41, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.2085, Train: 85.62%, Valid: 85.52%, Test: 85.67%
Epoch: 25, Loss: 10.9788, Train: 85.18%, Valid: 85.02%, Test: 85.31%
Epoch: 50, Loss: 29.4787, Train: 17.21%, Valid: 17.29%, Test: 17.18%
Epoch: 75, Loss: 20.9102, Train: 86.64%, Valid: 86.70%, Test: 86.70%
Epoch: 100, Loss: 19.6240, Train: 86.64%, Valid: 86.71%, Test: 86.72%
Epoch: 125, Loss: 19.2200, Train: 86.65%, Valid: 86.72%, Test: 86.71%
Epoch: 150, Loss: 19.2211, Train: 86.64%, Valid: 86.72%, Test: 86.71%
Epoch: 175, Loss: 19.2055, Train: 86.63%, Valid: 86.69%, Test: 86.70%
Epoch: 200, Loss: 19.2127, Train: 86.64%, Valid: 86.70%, Test: 86.70%
Epoch: 225, Loss: 19.1860, Train: 86.63%, Valid: 86.69%, Test: 86.70%
Epoch: 250, Loss: 18.6313, Train: 86.58%, Valid: 86.63%, Test: 86.66%
Epoch: 275, Loss: 14.0437, Train: 86.55%, Valid: 86.61%, Test: 86.62%
Epoch: 300, Loss: 0.4206, Train: 88.05%, Valid: 88.09%, Test: 88.11%
Epoch: 325, Loss: 0.4075, Train: 88.14%, Valid: 88.22%, Test: 88.18%
Epoch: 350, Loss: 0.3983, Train: 88.09%, Valid: 88.18%, Test: 88.13%
Epoch: 375, Loss: 0.3846, Train: 87.77%, Valid: 87.81%, Test: 87.85%
Epoch: 400, Loss: 0.3723, Train: 87.64%, Valid: 87.67%, Test: 87.73%
Epoch: 425, Loss: 0.3633, Train: 87.45%, Valid: 87.46%, Test: 87.53%
Epoch: 450, Loss: 0.3565, Train: 86.84%, Valid: 86.83%, Test: 86.89%
Epoch: 475, Loss: 0.3521, Train: 86.79%, Valid: 86.77%, Test: 86.86%
Run 01:
Highest Train: 88.28
Highest Valid: 88.34
  Final Train: 88.28
   Final Test: 88.34
All runs:
Highest Train: 88.28, nan
Highest Valid: 88.34, nan
  Final Train: 88.28, nan
   Final Test: 88.34, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 5.4053, Train: 14.27%, Valid: 14.20%, Test: 14.28%
Epoch: 25, Loss: 290.9470, Train: 85.87%, Valid: 85.88%, Test: 85.93%
Epoch: 50, Loss: 23432.6309, Train: 84.48%, Valid: 84.38%, Test: 84.67%
Epoch: 75, Loss: 37.0387, Train: 16.32%, Valid: 16.49%, Test: 16.17%
Epoch: 100, Loss: 2997.2263, Train: 85.55%, Valid: 85.54%, Test: 85.59%
Epoch: 125, Loss: 3123.4053, Train: 85.54%, Valid: 85.53%, Test: 85.58%
Epoch: 150, Loss: 1014.1854, Train: 85.55%, Valid: 85.54%, Test: 85.59%
Epoch: 175, Loss: 314.3192, Train: 16.42%, Valid: 16.57%, Test: 16.26%
Epoch: 200, Loss: 34.0091, Train: 16.42%, Valid: 16.58%, Test: 16.27%
Epoch: 225, Loss: 27.1829, Train: 16.42%, Valid: 16.58%, Test: 16.26%
Epoch: 250, Loss: 15.7987, Train: 13.09%, Valid: 13.22%, Test: 12.99%
Epoch: 275, Loss: 32.1935, Train: 15.82%, Valid: 15.84%, Test: 15.80%
Epoch: 300, Loss: 32.0656, Train: 13.08%, Valid: 13.21%, Test: 12.99%
Epoch: 325, Loss: 33.5544, Train: 13.09%, Valid: 13.22%, Test: 12.99%
Epoch: 350, Loss: 19.8320, Train: 83.02%, Valid: 83.15%, Test: 83.01%
Epoch: 375, Loss: 32.0121, Train: 13.09%, Valid: 13.22%, Test: 12.99%
Epoch: 400, Loss: 30.6126, Train: 14.77%, Valid: 14.85%, Test: 14.73%
Epoch: 425, Loss: 31.7921, Train: 13.08%, Valid: 13.22%, Test: 12.99%
Epoch: 450, Loss: 32.7582, Train: 13.09%, Valid: 13.22%, Test: 12.99%
Epoch: 475, Loss: 32.8758, Train: 15.30%, Valid: 15.31%, Test: 15.25%
Run 01:
Highest Train: 86.97
Highest Valid: 86.96
  Final Train: 86.97
   Final Test: 87.01
All runs:
Highest Train: 86.97, nan
Highest Valid: 86.96, nan
  Final Train: 86.97, nan
   Final Test: 87.01, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.3578, Train: 85.74%, Valid: 85.62%, Test: 85.80%
Epoch: 25, Loss: 1270.4622, Train: 86.46%, Valid: 86.37%, Test: 86.48%
Epoch: 50, Loss: 967.1329, Train: 87.86%, Valid: 87.83%, Test: 87.85%
Epoch: 75, Loss: 679.7498, Train: 15.93%, Valid: 16.05%, Test: 15.78%
Epoch: 100, Loss: 56.1774, Train: 15.31%, Valid: 15.49%, Test: 15.16%
Epoch: 125, Loss: 220.9413, Train: 87.37%, Valid: 87.43%, Test: 87.34%
Epoch: 150, Loss: 182.0774, Train: 15.04%, Valid: 15.21%, Test: 14.90%
Epoch: 175, Loss: 73.4669, Train: 15.06%, Valid: 15.22%, Test: 14.92%
Epoch: 200, Loss: 60.2782, Train: 15.07%, Valid: 15.22%, Test: 14.92%
Epoch: 225, Loss: 55.6063, Train: 15.03%, Valid: 15.19%, Test: 14.89%
Epoch: 250, Loss: 70.0531, Train: 14.91%, Valid: 15.08%, Test: 14.78%
Epoch: 275, Loss: 75.0961, Train: 15.03%, Valid: 15.20%, Test: 14.89%
Epoch: 300, Loss: 76.8832, Train: 15.04%, Valid: 15.21%, Test: 14.90%
Epoch: 325, Loss: 68.3649, Train: 15.05%, Valid: 15.22%, Test: 14.91%
Epoch: 350, Loss: 69.3056, Train: 15.03%, Valid: 15.20%, Test: 14.90%
Epoch: 375, Loss: 70.0843, Train: 15.03%, Valid: 15.20%, Test: 14.89%
Epoch: 400, Loss: 69.8933, Train: 15.04%, Valid: 15.21%, Test: 14.90%
Epoch: 425, Loss: 69.5333, Train: 15.04%, Valid: 15.21%, Test: 14.91%
Epoch: 450, Loss: 69.4301, Train: 15.03%, Valid: 15.20%, Test: 14.90%
Epoch: 475, Loss: 70.3378, Train: 15.03%, Valid: 15.20%, Test: 14.89%
Run 01:
Highest Train: 88.19
Highest Valid: 88.13
  Final Train: 88.12
   Final Test: 88.15
All runs:
Highest Train: 88.19, nan
Highest Valid: 88.13, nan
  Final Train: 88.12, nan
   Final Test: 88.15, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.0485, Train: 86.27%, Valid: 86.34%, Test: 86.31%
Epoch: 25, Loss: 0.5645, Train: 86.77%, Valid: 86.77%, Test: 86.82%
Epoch: 50, Loss: 400224224.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 19914264576.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 2573319.7500, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 16.7106, Train: 86.54%, Valid: 86.57%, Test: 86.56%
Epoch: 150, Loss: 1158.4474, Train: 86.51%, Valid: 86.54%, Test: 86.54%
Epoch: 175, Loss: 3190.6545, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 42635012.0000, Train: 86.55%, Valid: 86.57%, Test: 86.57%
Epoch: 225, Loss: 2544.3721, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 1169.0046, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 0.4015, Train: 85.23%, Valid: 85.22%, Test: 85.32%
Epoch: 300, Loss: 3722.7502, Train: 86.45%, Valid: 86.48%, Test: 86.48%
Epoch: 325, Loss: 8.4246, Train: 85.33%, Valid: 85.30%, Test: 85.42%
Epoch: 350, Loss: 56.4525, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 480.4019, Train: 85.26%, Valid: 85.24%, Test: 85.34%
Epoch: 400, Loss: 556.9385, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 101.0536, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 222.7076, Train: 86.55%, Valid: 86.58%, Test: 86.57%
Epoch: 475, Loss: 965.2303, Train: 14.83%, Valid: 14.96%, Test: 14.71%
Run 01:
Highest Train: 86.93
Highest Valid: 87.00
  Final Train: 86.93
   Final Test: 87.00
All runs:
Highest Train: 86.93, nan
Highest Valid: 87.00, nan
  Final Train: 86.93, nan
   Final Test: 87.00, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.3577, Train: 85.04%, Valid: 84.93%, Test: 85.06%
Epoch: 25, Loss: 0.5182, Train: 85.62%, Valid: 85.49%, Test: 85.72%
Epoch: 50, Loss: 0.4703, Train: 86.03%, Valid: 85.91%, Test: 86.14%
Epoch: 75, Loss: 0.6123, Train: 86.29%, Valid: 86.22%, Test: 86.41%
Epoch: 100, Loss: 0.5552, Train: 86.32%, Valid: 86.26%, Test: 86.45%
Epoch: 125, Loss: 0.5487, Train: 86.30%, Valid: 86.23%, Test: 86.44%
Epoch: 150, Loss: 0.5451, Train: 86.32%, Valid: 86.25%, Test: 86.45%
Epoch: 175, Loss: 0.5378, Train: 86.42%, Valid: 86.36%, Test: 86.55%
Epoch: 200, Loss: 0.5127, Train: 86.52%, Valid: 86.46%, Test: 86.66%
Epoch: 225, Loss: 0.5162, Train: 86.50%, Valid: 86.44%, Test: 86.64%
Epoch: 250, Loss: 0.5178, Train: 86.50%, Valid: 86.43%, Test: 86.63%
Epoch: 275, Loss: 0.5078, Train: 86.53%, Valid: 86.47%, Test: 86.66%
Epoch: 300, Loss: 0.5060, Train: 86.53%, Valid: 86.47%, Test: 86.66%
Epoch: 325, Loss: 0.5087, Train: 86.52%, Valid: 86.46%, Test: 86.65%
Epoch: 350, Loss: 0.5032, Train: 86.53%, Valid: 86.47%, Test: 86.66%
Epoch: 375, Loss: 0.5100, Train: 86.51%, Valid: 86.45%, Test: 86.64%
Epoch: 400, Loss: 0.5164, Train: 86.49%, Valid: 86.44%, Test: 86.63%
Epoch: 425, Loss: 0.5153, Train: 86.49%, Valid: 86.43%, Test: 86.63%
Epoch: 450, Loss: 0.5152, Train: 86.49%, Valid: 86.43%, Test: 86.63%
Epoch: 475, Loss: 0.5165, Train: 86.48%, Valid: 86.43%, Test: 86.62%
Run 01:
Highest Train: 86.83
Highest Valid: 86.74
  Final Train: 86.83
   Final Test: 86.95
All runs:
Highest Train: 86.83, nan
Highest Valid: 86.74, nan
  Final Train: 86.83, nan
   Final Test: 86.95, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.9739, Train: 85.77%, Valid: 85.66%, Test: 85.83%
Epoch: 25, Loss: 1.5915, Train: 84.20%, Valid: 84.19%, Test: 84.31%
Epoch: 50, Loss: 519856992.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 109846734942765056.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 2891590221692928.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 12639957755625472.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 7829551855683533769727925026816.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 386036088315028543805722059603968.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 228625571163125535108055580016640.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 7325840771736871066228752384.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 68079951518919187030140273033216.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 36180108577050883147438227456.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 340423065684398105201189700239360.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 11215889080578816733862078447616.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 4442697701811143915339776.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 521190172626733918984079409152.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 65588395563698006624533741568.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 893304334356467960250368.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 32574660952021093784992274710528.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.50
Highest Valid: 86.42
  Final Train: 86.50
   Final Test: 86.66
All runs:
Highest Train: 86.50, nan
Highest Valid: 86.42, nan
  Final Train: 86.50, nan
   Final Test: 86.66, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.5817, Train: 87.38%, Valid: 87.47%, Test: 87.39%
Epoch: 25, Loss: 4.7487, Train: 84.88%, Valid: 84.73%, Test: 84.95%
Epoch: 50, Loss: 4.7677, Train: 84.71%, Valid: 84.54%, Test: 84.81%
Epoch: 75, Loss: 4.1817, Train: 84.67%, Valid: 84.49%, Test: 84.78%
Epoch: 100, Loss: 4.0376, Train: 84.66%, Valid: 84.49%, Test: 84.79%
Epoch: 125, Loss: 3.9427, Train: 84.65%, Valid: 84.49%, Test: 84.79%
Epoch: 150, Loss: 3.9539, Train: 84.66%, Valid: 84.49%, Test: 84.79%
Epoch: 175, Loss: 4.0233, Train: 84.65%, Valid: 84.49%, Test: 84.79%
Epoch: 200, Loss: 3.9808, Train: 84.66%, Valid: 84.50%, Test: 84.79%
Epoch: 225, Loss: 3.9059, Train: 84.66%, Valid: 84.50%, Test: 84.79%
Epoch: 250, Loss: 3.9241, Train: 84.66%, Valid: 84.50%, Test: 84.79%
Epoch: 275, Loss: 3.9346, Train: 84.66%, Valid: 84.50%, Test: 84.79%
Epoch: 300, Loss: 3.9267, Train: 84.67%, Valid: 84.50%, Test: 84.79%
Epoch: 325, Loss: 3.9297, Train: 84.67%, Valid: 84.51%, Test: 84.79%
Epoch: 350, Loss: 3.9304, Train: 84.66%, Valid: 84.50%, Test: 84.79%
Epoch: 375, Loss: 3.9240, Train: 84.67%, Valid: 84.50%, Test: 84.79%
Epoch: 400, Loss: 3.9118, Train: 84.66%, Valid: 84.50%, Test: 84.78%
Epoch: 425, Loss: 3.9155, Train: 84.67%, Valid: 84.51%, Test: 84.79%
Epoch: 450, Loss: 3.8522, Train: 84.68%, Valid: 84.52%, Test: 84.80%
Epoch: 475, Loss: 3.8395, Train: 84.68%, Valid: 84.53%, Test: 84.81%
Run 01:
Highest Train: 87.38
Highest Valid: 87.47
  Final Train: 87.38
   Final Test: 87.39
All runs:
Highest Train: 87.38, nan
Highest Valid: 87.47, nan
  Final Train: 87.38, nan
   Final Test: 87.39, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.5309, Train: 86.38%, Valid: 86.43%, Test: 86.44%
Epoch: 25, Loss: 916620023479326616125586800640.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 45.7057, Train: 13.78%, Valid: 13.95%, Test: 13.72%
Epoch: 75, Loss: 3986.7974, Train: 49.98%, Valid: 49.97%, Test: 49.97%
Epoch: 100, Loss: 65.7718, Train: 13.68%, Valid: 13.84%, Test: 13.62%
Epoch: 125, Loss: 107173664.0000, Train: 16.50%, Valid: 16.67%, Test: 16.36%
Epoch: 150, Loss: 49.5928, Train: 50.02%, Valid: 50.03%, Test: 50.03%
Epoch: 175, Loss: 1746522996736.0000, Train: 85.85%, Valid: 85.82%, Test: 85.88%
Epoch: 200, Loss: 51.1576, Train: 13.68%, Valid: 13.84%, Test: 13.63%
Epoch: 225, Loss: 38663098466304.0000, Train: 50.02%, Valid: 50.03%, Test: 50.03%
Epoch: 250, Loss: 35.5038, Train: 50.02%, Valid: 50.03%, Test: 50.03%
Epoch: 275, Loss: 60181.3711, Train: 49.97%, Valid: 49.96%, Test: 49.96%
Epoch: 300, Loss: 66.3840, Train: 50.02%, Valid: 50.03%, Test: 50.03%
Epoch: 325, Loss: 31.5051, Train: 13.64%, Valid: 13.80%, Test: 13.59%
Epoch: 350, Loss: 40.2406, Train: 50.02%, Valid: 50.03%, Test: 50.03%
Epoch: 375, Loss: 23652340858880.0000, Train: 85.84%, Valid: 85.82%, Test: 85.88%
Epoch: 400, Loss: 34.3362, Train: 50.02%, Valid: 50.03%, Test: 50.03%
Epoch: 425, Loss: 35891254919168.0000, Train: 16.58%, Valid: 16.74%, Test: 16.44%
Epoch: 450, Loss: 5169290.0000, Train: 49.98%, Valid: 49.97%, Test: 49.97%
Epoch: 475, Loss: 4308289536.0000, Train: 13.65%, Valid: 13.82%, Test: 13.60%
Run 01:
Highest Train: 86.54
Highest Valid: 86.50
  Final Train: 86.54
   Final Test: 86.58
All runs:
Highest Train: 86.54, nan
Highest Valid: 86.50, nan
  Final Train: 86.54, nan
   Final Test: 86.58, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 3.1484, Train: 85.17%, Valid: 85.02%, Test: 85.24%
Epoch: 25, Loss: 0.4849, Train: 85.29%, Valid: 85.12%, Test: 85.35%
Epoch: 50, Loss: 0.5141, Train: 85.30%, Valid: 85.13%, Test: 85.36%
Epoch: 75, Loss: 0.5138, Train: 85.57%, Valid: 85.43%, Test: 85.61%
Epoch: 100, Loss: 0.8051, Train: 85.42%, Valid: 85.24%, Test: 85.48%
Epoch: 125, Loss: 2.9223, Train: 85.40%, Valid: 85.22%, Test: 85.46%
Epoch: 150, Loss: 3.0736, Train: 85.40%, Valid: 85.23%, Test: 85.46%
Epoch: 175, Loss: 3.0753, Train: 85.40%, Valid: 85.23%, Test: 85.46%
Epoch: 200, Loss: 3.0903, Train: 85.40%, Valid: 85.23%, Test: 85.46%
Epoch: 225, Loss: 3.0814, Train: 85.40%, Valid: 85.23%, Test: 85.46%
Epoch: 250, Loss: 3.0530, Train: 85.41%, Valid: 85.23%, Test: 85.46%
Epoch: 275, Loss: 3.0287, Train: 85.40%, Valid: 85.23%, Test: 85.46%
Epoch: 300, Loss: 3.0663, Train: 85.41%, Valid: 85.23%, Test: 85.46%
Epoch: 325, Loss: 2.9964, Train: 85.41%, Valid: 85.23%, Test: 85.46%
Epoch: 350, Loss: 3.0439, Train: 85.41%, Valid: 85.23%, Test: 85.46%
Epoch: 375, Loss: 3.0186, Train: 85.41%, Valid: 85.23%, Test: 85.46%
Epoch: 400, Loss: 3.0095, Train: 85.41%, Valid: 85.23%, Test: 85.46%
Epoch: 425, Loss: 2.9947, Train: 85.41%, Valid: 85.23%, Test: 85.46%
Epoch: 450, Loss: 3.0181, Train: 85.41%, Valid: 85.23%, Test: 85.46%
Epoch: 475, Loss: 2.9945, Train: 85.41%, Valid: 85.23%, Test: 85.46%
Run 01:
Highest Train: 85.58
Highest Valid: 85.44
  Final Train: 85.58
   Final Test: 85.63
All runs:
Highest Train: 85.58, nan
Highest Valid: 85.44, nan
  Final Train: 85.58, nan
   Final Test: 85.63, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 10.7578, Train: 85.97%, Valid: 85.94%, Test: 86.06%
Epoch: 25, Loss: 1.2989, Train: 86.94%, Valid: 86.78%, Test: 86.97%
Epoch: 50, Loss: 210.0544, Train: 86.66%, Valid: 86.65%, Test: 86.66%
Epoch: 75, Loss: 127.9995, Train: 17.12%, Valid: 17.12%, Test: 17.08%
Epoch: 100, Loss: 42.6396, Train: 16.12%, Valid: 16.28%, Test: 15.99%
Epoch: 125, Loss: 2.0574, Train: 85.17%, Valid: 85.13%, Test: 85.28%
Epoch: 150, Loss: 1.9620, Train: 84.23%, Valid: 84.00%, Test: 84.34%
Epoch: 175, Loss: 2.0054, Train: 84.22%, Valid: 84.00%, Test: 84.34%
Epoch: 200, Loss: 1.9818, Train: 84.22%, Valid: 84.00%, Test: 84.34%
Epoch: 225, Loss: 1.9798, Train: 84.22%, Valid: 84.00%, Test: 84.34%
Epoch: 250, Loss: 1.9900, Train: 84.22%, Valid: 84.00%, Test: 84.34%
Epoch: 275, Loss: 1.9915, Train: 84.22%, Valid: 84.00%, Test: 84.34%
Epoch: 300, Loss: 1.9646, Train: 84.22%, Valid: 84.00%, Test: 84.34%
Epoch: 325, Loss: 2.0055, Train: 84.22%, Valid: 84.00%, Test: 84.34%
Epoch: 350, Loss: 1.9989, Train: 84.22%, Valid: 84.00%, Test: 84.34%
Epoch: 375, Loss: 1.9957, Train: 84.22%, Valid: 84.00%, Test: 84.34%
Epoch: 400, Loss: 1.9544, Train: 84.22%, Valid: 84.00%, Test: 84.34%
Epoch: 425, Loss: 1.9726, Train: 84.22%, Valid: 84.00%, Test: 84.34%
Epoch: 450, Loss: 1.9780, Train: 84.22%, Valid: 84.00%, Test: 84.34%
Epoch: 475, Loss: 1.9753, Train: 84.22%, Valid: 84.00%, Test: 84.34%
Run 01:
Highest Train: 88.23
Highest Valid: 88.28
  Final Train: 88.23
   Final Test: 88.22
All runs:
Highest Train: 88.23, nan
Highest Valid: 88.28, nan
  Final Train: 88.23, nan
   Final Test: 88.22, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 15.7474, Train: 85.73%, Valid: 85.53%, Test: 85.82%
Epoch: 25, Loss: 207.4760, Train: 15.85%, Valid: 15.86%, Test: 15.76%
Epoch: 50, Loss: 209.6492, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 24559.3184, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 80741.0234, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 32.3836, Train: 84.27%, Valid: 84.29%, Test: 84.35%
Epoch: 150, Loss: 7.4237, Train: 87.05%, Valid: 87.08%, Test: 87.06%
Epoch: 175, Loss: 4.4687, Train: 87.03%, Valid: 87.06%, Test: 87.04%
Epoch: 200, Loss: 4.2888, Train: 87.05%, Valid: 87.08%, Test: 87.06%
Epoch: 225, Loss: 4.3793, Train: 87.05%, Valid: 87.07%, Test: 87.06%
Epoch: 250, Loss: 4.3220, Train: 85.89%, Valid: 85.86%, Test: 85.95%
Epoch: 275, Loss: 4.3515, Train: 87.03%, Valid: 87.05%, Test: 87.03%
Epoch: 300, Loss: 4.2568, Train: 86.35%, Valid: 86.36%, Test: 86.40%
Epoch: 325, Loss: 4.3590, Train: 85.87%, Valid: 85.85%, Test: 85.93%
Epoch: 350, Loss: 4.3290, Train: 87.05%, Valid: 87.08%, Test: 87.05%
Epoch: 375, Loss: 4.4484, Train: 87.02%, Valid: 87.05%, Test: 87.03%
Epoch: 400, Loss: 4.3737, Train: 86.27%, Valid: 86.20%, Test: 86.33%
Epoch: 425, Loss: 4.3752, Train: 87.02%, Valid: 87.05%, Test: 87.02%
Epoch: 450, Loss: 4.0919, Train: 86.05%, Valid: 86.01%, Test: 86.11%
Epoch: 475, Loss: 4.3244, Train: 85.80%, Valid: 85.80%, Test: 85.84%
Run 01:
Highest Train: 87.09
Highest Valid: 87.09
  Final Train: 87.07
   Final Test: 87.07
All runs:
Highest Train: 87.09, nan
Highest Valid: 87.09, nan
  Final Train: 87.07, nan
   Final Test: 87.07, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 8.1482, Train: 31.86%, Valid: 31.69%, Test: 31.59%
Epoch: 25, Loss: 0.6912, Train: 85.05%, Valid: 85.01%, Test: 85.21%
Epoch: 50, Loss: 0.7264, Train: 86.09%, Valid: 86.07%, Test: 86.26%
Epoch: 75, Loss: 0.4389, Train: 84.96%, Valid: 84.92%, Test: 85.09%
Epoch: 100, Loss: 0.3586, Train: 85.63%, Valid: 85.42%, Test: 85.68%
Epoch: 125, Loss: 0.3488, Train: 85.60%, Valid: 85.42%, Test: 85.66%
Epoch: 150, Loss: 0.3475, Train: 85.60%, Valid: 85.43%, Test: 85.68%
Epoch: 175, Loss: 0.3323, Train: 85.56%, Valid: 85.36%, Test: 85.63%
Epoch: 200, Loss: 1.4849, Train: 85.47%, Valid: 85.37%, Test: 85.58%
Epoch: 225, Loss: 0.5746, Train: 84.90%, Valid: 84.81%, Test: 85.07%
Epoch: 250, Loss: 2.1398, Train: 85.65%, Valid: 85.50%, Test: 85.74%
Epoch: 275, Loss: 3.7161, Train: 85.45%, Valid: 85.23%, Test: 85.49%
Epoch: 300, Loss: 3.0112, Train: 85.62%, Valid: 85.41%, Test: 85.63%
Epoch: 325, Loss: 2.1220, Train: 85.61%, Valid: 85.41%, Test: 85.64%
Epoch: 350, Loss: 1.4769, Train: 85.52%, Valid: 85.32%, Test: 85.56%
Epoch: 375, Loss: 0.9357, Train: 85.44%, Valid: 85.24%, Test: 85.50%
Epoch: 400, Loss: 0.3807, Train: 85.46%, Valid: 85.26%, Test: 85.53%
Epoch: 425, Loss: 0.3525, Train: 85.69%, Valid: 85.48%, Test: 85.78%
Epoch: 450, Loss: 0.5877, Train: 84.66%, Valid: 84.61%, Test: 84.87%
Epoch: 475, Loss: 0.3684, Train: 85.19%, Valid: 85.13%, Test: 85.41%
Run 01:
Highest Train: 86.23
Highest Valid: 86.21
  Final Train: 86.23
   Final Test: 86.38
All runs:
Highest Train: 86.23, nan
Highest Valid: 86.21, nan
  Final Train: 86.23, nan
   Final Test: 86.38, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.5148, Train: 85.86%, Valid: 85.73%, Test: 85.89%
Epoch: 25, Loss: 0.8416, Train: 85.25%, Valid: 85.08%, Test: 85.38%
Epoch: 50, Loss: 0.9322, Train: 88.18%, Valid: 88.24%, Test: 88.19%
Epoch: 75, Loss: 0.8558, Train: 87.82%, Valid: 87.82%, Test: 87.83%
Epoch: 100, Loss: 0.8091, Train: 87.18%, Valid: 87.09%, Test: 87.17%
Epoch: 125, Loss: 0.9946, Train: 87.48%, Valid: 87.41%, Test: 87.43%
Epoch: 150, Loss: 0.8786, Train: 86.23%, Valid: 86.26%, Test: 86.31%
Epoch: 175, Loss: 0.8335, Train: 85.52%, Valid: 85.57%, Test: 85.61%
Epoch: 200, Loss: 0.9232, Train: 86.01%, Valid: 86.06%, Test: 86.10%
Epoch: 225, Loss: 1.0527, Train: 86.56%, Valid: 86.57%, Test: 86.63%
Epoch: 250, Loss: 1.1911, Train: 86.03%, Valid: 86.02%, Test: 86.10%
Epoch: 275, Loss: 1.1058, Train: 85.83%, Valid: 85.83%, Test: 85.90%
Epoch: 300, Loss: 1.0879, Train: 85.68%, Valid: 85.69%, Test: 85.77%
Epoch: 325, Loss: 1.0757, Train: 85.60%, Valid: 85.62%, Test: 85.69%
Epoch: 350, Loss: 1.0649, Train: 85.62%, Valid: 85.66%, Test: 85.73%
Epoch: 375, Loss: 1.0171, Train: 85.56%, Valid: 85.59%, Test: 85.67%
Epoch: 400, Loss: 0.9199, Train: 85.50%, Valid: 85.52%, Test: 85.62%
Epoch: 425, Loss: 0.6010, Train: 85.33%, Valid: 85.35%, Test: 85.42%
Epoch: 450, Loss: 0.3750, Train: 85.36%, Valid: 85.44%, Test: 85.50%
Epoch: 475, Loss: 0.3612, Train: 85.26%, Valid: 85.30%, Test: 85.40%
Run 01:
Highest Train: 88.18
Highest Valid: 88.24
  Final Train: 88.18
   Final Test: 88.19
All runs:
Highest Train: 88.18, nan
Highest Valid: 88.24, nan
  Final Train: 88.18, nan
   Final Test: 88.19, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.5270, Train: 85.74%, Valid: 85.54%, Test: 85.78%
Epoch: 25, Loss: 1.0947, Train: 86.40%, Valid: 86.27%, Test: 86.46%
Epoch: 50, Loss: 18.4293, Train: 85.29%, Valid: 85.15%, Test: 85.39%
Epoch: 75, Loss: 19.6957, Train: 85.28%, Valid: 85.14%, Test: 85.38%
Epoch: 100, Loss: 21.2340, Train: 85.28%, Valid: 85.15%, Test: 85.39%
Epoch: 125, Loss: 20.7952, Train: 85.28%, Valid: 85.15%, Test: 85.38%
Epoch: 150, Loss: 20.7384, Train: 85.28%, Valid: 85.15%, Test: 85.39%
Epoch: 175, Loss: 21.2932, Train: 85.28%, Valid: 85.15%, Test: 85.38%
Epoch: 200, Loss: 20.9617, Train: 85.28%, Valid: 85.15%, Test: 85.38%
Epoch: 225, Loss: 21.0300, Train: 85.28%, Valid: 85.15%, Test: 85.39%
Epoch: 250, Loss: 20.9542, Train: 85.28%, Valid: 85.15%, Test: 85.39%
Epoch: 275, Loss: 20.7606, Train: 85.28%, Valid: 85.14%, Test: 85.38%
Epoch: 300, Loss: 21.3918, Train: 85.28%, Valid: 85.14%, Test: 85.39%
Epoch: 325, Loss: 20.7984, Train: 85.28%, Valid: 85.14%, Test: 85.38%
Epoch: 350, Loss: 21.0767, Train: 85.28%, Valid: 85.14%, Test: 85.38%
Epoch: 375, Loss: 20.6352, Train: 85.28%, Valid: 85.15%, Test: 85.38%
Epoch: 400, Loss: 21.1303, Train: 85.28%, Valid: 85.15%, Test: 85.39%
Epoch: 425, Loss: 21.1863, Train: 85.28%, Valid: 85.15%, Test: 85.38%
Epoch: 450, Loss: 20.7351, Train: 85.28%, Valid: 85.15%, Test: 85.39%
Epoch: 475, Loss: 20.8685, Train: 85.28%, Valid: 85.15%, Test: 85.39%
Run 01:
Highest Train: 86.40
Highest Valid: 86.29
  Final Train: 86.38
   Final Test: 86.45
All runs:
Highest Train: 86.40, nan
Highest Valid: 86.29, nan
  Final Train: 86.38, nan
   Final Test: 86.45, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 5.4325, Train: 83.96%, Valid: 83.93%, Test: 84.03%
Epoch: 25, Loss: 1.2394, Train: 86.82%, Valid: 86.81%, Test: 86.85%
Epoch: 50, Loss: 1.0198, Train: 85.38%, Valid: 85.19%, Test: 85.44%
Epoch: 75, Loss: 1.3313, Train: 86.35%, Valid: 86.39%, Test: 86.38%
Epoch: 100, Loss: 1.8244, Train: 86.72%, Valid: 86.76%, Test: 86.77%
Epoch: 125, Loss: 1.4803, Train: 85.18%, Valid: 84.94%, Test: 85.17%
Epoch: 150, Loss: 1.5046, Train: 84.76%, Valid: 84.65%, Test: 84.85%
Epoch: 175, Loss: 1.4956, Train: 84.78%, Valid: 84.69%, Test: 84.88%
Epoch: 200, Loss: 1.4816, Train: 84.80%, Valid: 84.71%, Test: 84.90%
Epoch: 225, Loss: 1.4695, Train: 84.81%, Valid: 84.73%, Test: 84.92%
Epoch: 250, Loss: 1.4581, Train: 84.80%, Valid: 84.73%, Test: 84.92%
Epoch: 275, Loss: 1.4480, Train: 84.80%, Valid: 84.74%, Test: 84.91%
Epoch: 300, Loss: 1.4383, Train: 84.81%, Valid: 84.75%, Test: 84.92%
Epoch: 325, Loss: 1.4302, Train: 84.81%, Valid: 84.76%, Test: 84.93%
Epoch: 350, Loss: 1.4207, Train: 84.81%, Valid: 84.76%, Test: 84.93%
Epoch: 375, Loss: 1.4125, Train: 84.82%, Valid: 84.77%, Test: 84.93%
Epoch: 400, Loss: 1.4046, Train: 84.81%, Valid: 84.76%, Test: 84.93%
Epoch: 425, Loss: 1.3947, Train: 84.80%, Valid: 84.75%, Test: 84.92%
Epoch: 450, Loss: 1.3856, Train: 84.79%, Valid: 84.74%, Test: 84.91%
Epoch: 475, Loss: 1.3758, Train: 84.77%, Valid: 84.72%, Test: 84.89%
Run 01:
Highest Train: 87.29
Highest Valid: 87.33
  Final Train: 87.29
   Final Test: 87.32
All runs:
Highest Train: 87.29, nan
Highest Valid: 87.33, nan
  Final Train: 87.29, nan
   Final Test: 87.32, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.4160, Train: 85.29%, Valid: 85.08%, Test: 85.32%
Epoch: 25, Loss: 1.3205, Train: 86.41%, Valid: 86.47%, Test: 86.44%
Epoch: 50, Loss: 1.1348, Train: 86.19%, Valid: 86.22%, Test: 86.26%
Epoch: 75, Loss: 0.4027, Train: 85.76%, Valid: 85.81%, Test: 85.83%
Epoch: 100, Loss: 0.3493, Train: 85.88%, Valid: 85.91%, Test: 85.95%
Epoch: 125, Loss: 0.3388, Train: 85.64%, Valid: 85.63%, Test: 85.73%
Epoch: 150, Loss: 0.3317, Train: 85.62%, Valid: 85.60%, Test: 85.72%
Epoch: 175, Loss: 0.6264, Train: 85.63%, Valid: 85.68%, Test: 85.76%
Epoch: 200, Loss: 0.6443, Train: 85.73%, Valid: 85.77%, Test: 85.81%
Epoch: 225, Loss: 0.3450, Train: 86.07%, Valid: 86.07%, Test: 86.17%
Epoch: 250, Loss: 0.3383, Train: 86.10%, Valid: 86.09%, Test: 86.20%
Epoch: 275, Loss: 0.3347, Train: 86.09%, Valid: 86.08%, Test: 86.20%
Epoch: 300, Loss: 0.3297, Train: 86.08%, Valid: 86.08%, Test: 86.18%
Epoch: 325, Loss: 2.4351, Train: 85.35%, Valid: 85.35%, Test: 85.48%
Epoch: 350, Loss: nan, Train: 84.44%, Valid: 84.32%, Test: 84.57%
Epoch: 375, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 24142654816779535134285035995136.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 432781323168464145676050439864320.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 88.43
Highest Valid: 88.46
  Final Train: 88.43
   Final Test: 88.53
All runs:
Highest Train: 88.43, nan
Highest Valid: 88.46, nan
  Final Train: 88.43, nan
   Final Test: 88.53, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.6077, Train: 86.34%, Valid: 86.27%, Test: 86.38%
Epoch: 25, Loss: 0.3721, Train: 86.10%, Valid: 85.99%, Test: 86.15%
Epoch: 50, Loss: 0.3542, Train: 85.79%, Valid: 85.63%, Test: 85.89%
Epoch: 75, Loss: 0.3385, Train: 85.35%, Valid: 85.19%, Test: 85.47%
Epoch: 100, Loss: 0.3364, Train: 86.17%, Valid: 86.01%, Test: 86.29%
Epoch: 125, Loss: 0.3270, Train: 86.03%, Valid: 85.87%, Test: 86.14%
Epoch: 150, Loss: 0.5276, Train: 86.31%, Valid: 86.17%, Test: 86.41%
Epoch: 175, Loss: 1.1794, Train: 85.97%, Valid: 85.86%, Test: 86.07%
Epoch: 200, Loss: 1.1295, Train: 85.97%, Valid: 85.86%, Test: 86.06%
Epoch: 225, Loss: 0.9814, Train: 86.96%, Valid: 86.91%, Test: 87.00%
Epoch: 250, Loss: 0.6226, Train: 87.59%, Valid: 87.68%, Test: 87.66%
Epoch: 275, Loss: 0.6559, Train: 87.77%, Valid: 87.80%, Test: 87.77%
Epoch: 300, Loss: 0.6512, Train: 87.79%, Valid: 87.80%, Test: 87.77%
Epoch: 325, Loss: 0.5725, Train: 87.55%, Valid: 87.58%, Test: 87.55%
Epoch: 350, Loss: 0.4750, Train: 87.39%, Valid: 87.41%, Test: 87.45%
Epoch: 375, Loss: 0.3636, Train: 86.78%, Valid: 86.69%, Test: 86.86%
Epoch: 400, Loss: 0.3478, Train: 86.04%, Valid: 85.85%, Test: 86.16%
Epoch: 425, Loss: 0.3464, Train: 86.15%, Valid: 85.98%, Test: 86.27%
Epoch: 450, Loss: 0.3458, Train: 86.17%, Valid: 85.99%, Test: 86.28%
Epoch: 475, Loss: 0.3454, Train: 86.17%, Valid: 85.99%, Test: 86.29%
Run 01:
Highest Train: 87.88
Highest Valid: 87.92
  Final Train: 87.88
   Final Test: 87.88
All runs:
Highest Train: 87.88, nan
Highest Valid: 87.92, nan
  Final Train: 87.88, nan
   Final Test: 87.88, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 3.9178, Train: 83.64%, Valid: 83.67%, Test: 83.83%
Epoch: 25, Loss: 1.0069, Train: 85.94%, Valid: 85.80%, Test: 86.02%
Epoch: 50, Loss: 0.5948, Train: 85.51%, Valid: 85.34%, Test: 85.56%
Epoch: 75, Loss: 0.7723, Train: 86.60%, Valid: 86.69%, Test: 86.74%
Epoch: 100, Loss: 0.7262, Train: 86.28%, Valid: 86.30%, Test: 86.40%
Epoch: 125, Loss: 0.3906, Train: 86.18%, Valid: 85.99%, Test: 86.24%
Epoch: 150, Loss: 0.3527, Train: 85.55%, Valid: 85.37%, Test: 85.62%
Epoch: 175, Loss: 0.3422, Train: 85.33%, Valid: 85.15%, Test: 85.42%
Epoch: 200, Loss: 0.3287, Train: 84.98%, Valid: 84.76%, Test: 85.07%
Epoch: 225, Loss: 0.4024, Train: 85.58%, Valid: 85.36%, Test: 85.62%
Epoch: 250, Loss: 0.3438, Train: 85.48%, Valid: 85.30%, Test: 85.57%
Epoch: 275, Loss: 0.5024, Train: 85.92%, Valid: 85.89%, Test: 86.02%
Epoch: 300, Loss: 0.5585, Train: 85.45%, Valid: 85.23%, Test: 85.51%
Epoch: 325, Loss: 0.3551, Train: 85.92%, Valid: 85.75%, Test: 85.98%
Epoch: 350, Loss: 0.3475, Train: 85.60%, Valid: 85.39%, Test: 85.67%
Epoch: 375, Loss: 0.3450, Train: 85.60%, Valid: 85.44%, Test: 85.67%
Epoch: 400, Loss: 0.6427, Train: 85.48%, Valid: 85.32%, Test: 85.55%
Epoch: 425, Loss: 0.3573, Train: 85.67%, Valid: 85.49%, Test: 85.72%
Epoch: 450, Loss: 0.3419, Train: 85.87%, Valid: 85.71%, Test: 85.93%
Epoch: 475, Loss: 0.3343, Train: 85.82%, Valid: 85.64%, Test: 85.87%
Run 01:
Highest Train: 86.62
Highest Valid: 86.72
  Final Train: 86.62
   Final Test: 86.76
All runs:
Highest Train: 86.62, nan
Highest Valid: 86.72, nan
  Final Train: 86.62, nan
   Final Test: 86.76, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.7708, Train: 87.67%, Valid: 87.70%, Test: 87.68%
Epoch: 25, Loss: 1.4945, Train: 87.49%, Valid: 87.61%, Test: 87.55%
Epoch: 50, Loss: 1.5595, Train: 85.73%, Valid: 85.75%, Test: 85.85%
Epoch: 75, Loss: 1.3318, Train: 85.70%, Valid: 85.75%, Test: 85.74%
Epoch: 100, Loss: 0.4694, Train: 85.35%, Valid: 85.39%, Test: 85.45%
Epoch: 125, Loss: 0.3563, Train: 85.36%, Valid: 85.39%, Test: 85.49%
Epoch: 150, Loss: 0.3450, Train: 85.71%, Valid: 85.71%, Test: 85.81%
Epoch: 175, Loss: 0.3811, Train: 85.69%, Valid: 85.70%, Test: 85.78%
Epoch: 200, Loss: 0.3374, Train: 85.43%, Valid: 85.45%, Test: 85.53%
Epoch: 225, Loss: 0.5118, Train: 85.12%, Valid: 85.13%, Test: 85.21%
Epoch: 250, Loss: 0.3570, Train: 86.18%, Valid: 86.15%, Test: 86.25%
Epoch: 275, Loss: 0.3404, Train: 85.57%, Valid: 85.57%, Test: 85.64%
Epoch: 300, Loss: 0.3331, Train: 85.46%, Valid: 85.45%, Test: 85.52%
Epoch: 325, Loss: 0.3457, Train: 85.96%, Valid: 86.01%, Test: 86.05%
Epoch: 350, Loss: 0.3307, Train: 85.60%, Valid: 85.61%, Test: 85.67%
Epoch: 375, Loss: 1.3369, Train: 85.34%, Valid: 85.37%, Test: 85.41%
Epoch: 400, Loss: 2.3308, Train: 85.33%, Valid: 85.36%, Test: 85.43%
Epoch: 425, Loss: 1.6048, Train: 87.16%, Valid: 86.96%, Test: 87.18%
Epoch: 450, Loss: 0.4072, Train: 87.64%, Valid: 87.43%, Test: 87.68%
Epoch: 475, Loss: 0.3443, Train: 87.57%, Valid: 87.38%, Test: 87.60%
Run 01:
Highest Train: 88.08
Highest Valid: 88.14
  Final Train: 88.08
   Final Test: 88.11
All runs:
Highest Train: 88.08, nan
Highest Valid: 88.14, nan
  Final Train: 88.08, nan
   Final Test: 88.11, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.2925, Train: 86.21%, Valid: 85.97%, Test: 86.28%
Epoch: 25, Loss: 0.3904, Train: 85.82%, Valid: 85.70%, Test: 85.91%
Epoch: 50, Loss: 1.0787, Train: 85.78%, Valid: 85.66%, Test: 85.88%
Epoch: 75, Loss: 0.7782, Train: 85.73%, Valid: 85.59%, Test: 85.83%
Epoch: 100, Loss: 0.3792, Train: 85.81%, Valid: 85.66%, Test: 85.94%
Epoch: 125, Loss: 0.8321, Train: 86.11%, Valid: 86.03%, Test: 86.23%
Epoch: 150, Loss: 0.5904, Train: 86.26%, Valid: 86.17%, Test: 86.36%
Epoch: 175, Loss: 0.6877, Train: 86.49%, Valid: 86.39%, Test: 86.61%
Epoch: 200, Loss: 0.3713, Train: 86.33%, Valid: 86.17%, Test: 86.45%
Epoch: 225, Loss: 0.3334, Train: 86.65%, Valid: 86.47%, Test: 86.76%
Epoch: 250, Loss: 0.5034, Train: 86.92%, Valid: 86.83%, Test: 86.96%
Epoch: 275, Loss: 0.3394, Train: 87.40%, Valid: 87.24%, Test: 87.53%
Epoch: 300, Loss: 0.3331, Train: 87.77%, Valid: 87.64%, Test: 87.75%
Epoch: 325, Loss: 0.3276, Train: 87.38%, Valid: 87.17%, Test: 87.48%
Epoch: 350, Loss: 0.4551, Train: 85.98%, Valid: 85.88%, Test: 86.01%
Epoch: 375, Loss: 0.3337, Train: 87.07%, Valid: 87.00%, Test: 87.10%
Epoch: 400, Loss: 0.3304, Train: 87.22%, Valid: 87.14%, Test: 87.21%
Epoch: 425, Loss: 0.5919, Train: 86.08%, Valid: 85.94%, Test: 86.19%
Epoch: 450, Loss: 0.3874, Train: 86.33%, Valid: 86.17%, Test: 86.44%
Epoch: 475, Loss: 0.3458, Train: 87.31%, Valid: 87.14%, Test: 87.42%
Run 01:
Highest Train: 88.09
Highest Valid: 87.95
  Final Train: 88.09
   Final Test: 88.09
All runs:
Highest Train: 88.09, nan
Highest Valid: 87.95, nan
  Final Train: 88.09, nan
   Final Test: 88.09, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.5944, Train: 27.26%, Valid: 27.12%, Test: 26.91%
Epoch: 25, Loss: 0.8946, Train: 85.42%, Valid: 85.40%, Test: 85.60%
Epoch: 50, Loss: 0.4480, Train: 85.78%, Valid: 85.58%, Test: 85.84%
Epoch: 75, Loss: 0.3670, Train: 85.37%, Valid: 85.22%, Test: 85.46%
Epoch: 100, Loss: 0.3618, Train: 85.38%, Valid: 85.24%, Test: 85.47%
Epoch: 125, Loss: 0.3565, Train: 85.41%, Valid: 85.26%, Test: 85.48%
Epoch: 150, Loss: 0.3480, Train: 85.45%, Valid: 85.30%, Test: 85.53%
Epoch: 175, Loss: 0.3456, Train: 85.50%, Valid: 85.33%, Test: 85.58%
Epoch: 200, Loss: 1.1815, Train: 86.04%, Valid: 85.95%, Test: 86.14%
Epoch: 225, Loss: 2.3411, Train: 86.14%, Valid: 86.02%, Test: 86.19%
Epoch: 250, Loss: 1.1977, Train: 85.71%, Valid: 85.56%, Test: 85.77%
Epoch: 275, Loss: 0.5677, Train: 86.01%, Valid: 85.86%, Test: 86.08%
Epoch: 300, Loss: 0.3763, Train: 86.10%, Valid: 85.92%, Test: 86.18%
Epoch: 325, Loss: 0.3558, Train: 85.84%, Valid: 85.66%, Test: 85.91%
Epoch: 350, Loss: 0.3479, Train: 85.82%, Valid: 85.64%, Test: 85.89%
Epoch: 375, Loss: 0.3386, Train: 85.48%, Valid: 85.28%, Test: 85.55%
Epoch: 400, Loss: 0.3346, Train: 85.40%, Valid: 85.19%, Test: 85.48%
Epoch: 425, Loss: 0.3322, Train: 85.47%, Valid: 85.26%, Test: 85.55%
Epoch: 450, Loss: 0.3301, Train: 85.23%, Valid: 85.00%, Test: 85.33%
Epoch: 475, Loss: 0.7433, Train: 86.22%, Valid: 86.05%, Test: 86.27%
Run 01:
Highest Train: 87.93
Highest Valid: 87.76
  Final Train: 87.93
   Final Test: 87.90
All runs:
Highest Train: 87.93, nan
Highest Valid: 87.76, nan
  Final Train: 87.93, nan
   Final Test: 87.90, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 16.4057, Train: 86.34%, Valid: 86.25%, Test: 86.40%
Epoch: 25, Loss: 1.4620, Train: 87.31%, Valid: 87.26%, Test: 87.35%
Epoch: 50, Loss: 1.3799, Train: 85.69%, Valid: 85.75%, Test: 85.77%
Epoch: 75, Loss: 0.3702, Train: 87.10%, Valid: 86.95%, Test: 87.12%
Epoch: 100, Loss: 0.3492, Train: 85.71%, Valid: 85.69%, Test: 85.83%
Epoch: 125, Loss: 0.3715, Train: 85.92%, Valid: 85.94%, Test: 86.00%
Epoch: 150, Loss: 0.3391, Train: 85.53%, Valid: 85.55%, Test: 85.62%
Epoch: 175, Loss: 0.4118, Train: 85.80%, Valid: 85.80%, Test: 85.92%
Epoch: 200, Loss: 0.4614, Train: 86.25%, Valid: 86.22%, Test: 86.30%
Epoch: 225, Loss: 1.3784, Train: 85.65%, Valid: 85.71%, Test: 85.75%
Epoch: 250, Loss: 0.9706, Train: 85.89%, Valid: 85.93%, Test: 85.97%
Epoch: 275, Loss: 0.6456, Train: 85.93%, Valid: 85.96%, Test: 85.99%
Epoch: 300, Loss: 0.4448, Train: 86.44%, Valid: 86.46%, Test: 86.48%
Epoch: 325, Loss: 0.3428, Train: 86.12%, Valid: 86.11%, Test: 86.23%
Epoch: 350, Loss: 0.3360, Train: 85.92%, Valid: 85.93%, Test: 86.02%
Epoch: 375, Loss: 0.3625, Train: 85.60%, Valid: 85.57%, Test: 85.69%
Epoch: 400, Loss: 0.3399, Train: 86.01%, Valid: 86.02%, Test: 86.13%
Epoch: 425, Loss: 0.3321, Train: 85.90%, Valid: 85.91%, Test: 86.00%
Epoch: 450, Loss: 0.3314, Train: 85.98%, Valid: 85.98%, Test: 86.07%
Epoch: 475, Loss: 0.3275, Train: 85.92%, Valid: 85.92%, Test: 86.02%
Run 01:
Highest Train: 88.00
Highest Valid: 88.01
  Final Train: 88.00
   Final Test: 88.08
All runs:
Highest Train: 88.00, nan
Highest Valid: 88.01, nan
  Final Train: 88.00, nan
   Final Test: 88.08, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 12.5355, Train: 85.90%, Valid: 85.79%, Test: 85.95%
Epoch: 25, Loss: 0.3751, Train: 84.76%, Valid: 84.81%, Test: 84.82%
Epoch: 50, Loss: 1.2702, Train: 86.97%, Valid: 86.97%, Test: 87.05%
Epoch: 75, Loss: 0.6627, Train: 86.61%, Valid: 86.53%, Test: 86.71%
Epoch: 100, Loss: 0.3553, Train: 86.74%, Valid: 86.57%, Test: 86.84%
Epoch: 125, Loss: 0.3393, Train: 86.14%, Valid: 85.95%, Test: 86.26%
Epoch: 150, Loss: 0.7655, Train: 87.11%, Valid: 87.01%, Test: 87.20%
Epoch: 175, Loss: 1.0942, Train: 86.54%, Valid: 86.54%, Test: 86.64%
Epoch: 200, Loss: 0.7428, Train: 86.55%, Valid: 86.48%, Test: 86.61%
Epoch: 225, Loss: 0.4838, Train: 85.91%, Valid: 85.78%, Test: 86.04%
Epoch: 250, Loss: 0.3467, Train: 86.21%, Valid: 86.01%, Test: 86.34%
Epoch: 275, Loss: 0.3309, Train: 86.36%, Valid: 86.15%, Test: 86.46%
Epoch: 300, Loss: 0.3614, Train: 86.36%, Valid: 86.25%, Test: 86.35%
Epoch: 325, Loss: 0.3331, Train: 86.19%, Valid: 86.02%, Test: 86.31%
Epoch: 350, Loss: 0.8406, Train: 85.89%, Valid: 85.74%, Test: 86.01%
Epoch: 375, Loss: 0.3885, Train: 86.10%, Valid: 85.96%, Test: 86.23%
Epoch: 400, Loss: 0.3451, Train: 86.24%, Valid: 86.05%, Test: 86.38%
Epoch: 425, Loss: 0.3334, Train: 86.57%, Valid: 86.40%, Test: 86.67%
Epoch: 450, Loss: 0.3294, Train: 85.99%, Valid: 85.77%, Test: 86.09%
Epoch: 475, Loss: 0.6292, Train: 86.32%, Valid: 86.20%, Test: 86.43%
Run 01:
Highest Train: 87.51
Highest Valid: 87.57
  Final Train: 87.51
   Final Test: 87.63
All runs:
Highest Train: 87.51, nan
Highest Valid: 87.57, nan
  Final Train: 87.51, nan
   Final Test: 87.63, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 7.5543, Train: 83.18%, Valid: 83.12%, Test: 83.32%
Epoch: 25, Loss: 1.0328, Train: 85.53%, Valid: 85.38%, Test: 85.57%
Epoch: 50, Loss: 0.5819, Train: 85.27%, Valid: 85.11%, Test: 85.31%
Epoch: 75, Loss: 1.1534, Train: 85.38%, Valid: 85.20%, Test: 85.43%
Epoch: 100, Loss: 0.7268, Train: 85.57%, Valid: 85.39%, Test: 85.61%
Epoch: 125, Loss: 0.4022, Train: 85.61%, Valid: 85.43%, Test: 85.67%
Epoch: 150, Loss: 0.3572, Train: 86.89%, Valid: 86.91%, Test: 87.01%
Epoch: 175, Loss: 0.3400, Train: 86.96%, Valid: 86.84%, Test: 87.10%
Epoch: 200, Loss: 0.3525, Train: 87.27%, Valid: 87.14%, Test: 87.40%
Epoch: 225, Loss: 0.3402, Train: 86.29%, Valid: 86.09%, Test: 86.41%
Epoch: 250, Loss: 0.3301, Train: 86.99%, Valid: 86.79%, Test: 87.12%
Epoch: 275, Loss: 0.3266, Train: 87.55%, Valid: 87.35%, Test: 87.68%
Epoch: 300, Loss: 0.3327, Train: 86.21%, Valid: 86.04%, Test: 86.34%
Epoch: 325, Loss: 0.3370, Train: 87.13%, Valid: 87.07%, Test: 87.15%
Epoch: 350, Loss: 0.6122, Train: 85.43%, Valid: 85.23%, Test: 85.50%
Epoch: 375, Loss: 0.4809, Train: 86.25%, Valid: 86.13%, Test: 86.26%
Epoch: 400, Loss: 0.3532, Train: 85.78%, Valid: 85.66%, Test: 85.89%
Epoch: 425, Loss: 0.3457, Train: 86.26%, Valid: 86.16%, Test: 86.39%
Epoch: 450, Loss: 0.3359, Train: 86.99%, Valid: 86.93%, Test: 87.01%
Epoch: 475, Loss: 0.6093, Train: 85.57%, Valid: 85.42%, Test: 85.69%
Run 01:
Highest Train: 87.86
Highest Valid: 87.68
  Final Train: 87.86
   Final Test: 87.98
All runs:
Highest Train: 87.86, nan
Highest Valid: 87.68, nan
  Final Train: 87.86, nan
   Final Test: 87.98, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 12.5925, Train: 84.54%, Valid: 84.39%, Test: 84.69%
Epoch: 25, Loss: 2.0236, Train: 84.46%, Valid: 84.24%, Test: 84.58%
Epoch: 50, Loss: 2.1307, Train: 86.94%, Valid: 87.00%, Test: 87.01%
Epoch: 75, Loss: 1.9614, Train: 86.63%, Valid: 86.71%, Test: 86.70%
Epoch: 100, Loss: 1.3974, Train: 87.29%, Valid: 87.19%, Test: 87.32%
Epoch: 125, Loss: 0.8387, Train: 87.34%, Valid: 87.20%, Test: 87.35%
Epoch: 150, Loss: 0.6098, Train: 86.79%, Valid: 86.83%, Test: 86.83%
Epoch: 175, Loss: 0.7867, Train: 86.32%, Valid: 86.31%, Test: 86.36%
Epoch: 200, Loss: 0.5630, Train: 86.08%, Valid: 86.09%, Test: 86.14%
Epoch: 225, Loss: 0.3548, Train: 85.68%, Valid: 85.67%, Test: 85.80%
Epoch: 250, Loss: 0.3467, Train: 86.13%, Valid: 86.08%, Test: 86.25%
Epoch: 275, Loss: 0.3427, Train: 86.04%, Valid: 86.01%, Test: 86.12%
Epoch: 300, Loss: 0.7635, Train: 87.01%, Valid: 86.86%, Test: 87.00%
Epoch: 325, Loss: 0.5613, Train: 87.07%, Valid: 86.97%, Test: 87.13%
Epoch: 350, Loss: 0.3514, Train: 87.23%, Valid: 87.05%, Test: 87.29%
Epoch: 375, Loss: 0.3459, Train: 86.08%, Valid: 85.87%, Test: 86.17%
Epoch: 400, Loss: 0.3436, Train: 85.80%, Valid: 85.80%, Test: 85.91%
Epoch: 425, Loss: 0.3406, Train: 85.94%, Valid: 85.93%, Test: 86.04%
Epoch: 450, Loss: 0.3358, Train: 86.00%, Valid: 85.98%, Test: 86.10%
Epoch: 475, Loss: 0.3323, Train: 86.05%, Valid: 86.04%, Test: 86.16%
Run 01:
Highest Train: 88.47
Highest Valid: 88.50
  Final Train: 88.47
   Final Test: 88.51
All runs:
Highest Train: 88.47, nan
Highest Valid: 88.50, nan
  Final Train: 88.47, nan
   Final Test: 88.51, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.6445, Train: 85.97%, Valid: 85.80%, Test: 86.01%
Epoch: 25, Loss: 0.6173, Train: 86.68%, Valid: 86.66%, Test: 86.76%
Epoch: 50, Loss: 0.9216, Train: 87.20%, Valid: 87.17%, Test: 87.25%
Epoch: 75, Loss: 0.5019, Train: 86.37%, Valid: 86.25%, Test: 86.42%
Epoch: 100, Loss: 0.5553, Train: 87.14%, Valid: 87.05%, Test: 87.19%
Epoch: 125, Loss: 0.3509, Train: 86.13%, Valid: 85.96%, Test: 86.26%
Epoch: 150, Loss: 0.3321, Train: 86.28%, Valid: 86.11%, Test: 86.38%
Epoch: 175, Loss: 1.3441, Train: 87.05%, Valid: 87.00%, Test: 87.09%
Epoch: 200, Loss: 0.3745, Train: 86.15%, Valid: 86.00%, Test: 86.26%
Epoch: 225, Loss: 0.3410, Train: 86.14%, Valid: 85.98%, Test: 86.26%
Epoch: 250, Loss: 0.3451, Train: 86.06%, Valid: 85.91%, Test: 86.21%
Epoch: 275, Loss: 0.3371, Train: 85.96%, Valid: 85.80%, Test: 86.10%
Epoch: 300, Loss: 0.3272, Train: 86.11%, Valid: 85.94%, Test: 86.25%
Epoch: 325, Loss: 0.9621, Train: 85.60%, Valid: 85.48%, Test: 85.72%
Epoch: 350, Loss: 0.4296, Train: 85.42%, Valid: 85.38%, Test: 85.56%
Epoch: 375, Loss: 0.3465, Train: 86.28%, Valid: 86.09%, Test: 86.31%
Epoch: 400, Loss: 0.3398, Train: 86.16%, Valid: 85.96%, Test: 86.28%
Epoch: 425, Loss: 0.3331, Train: 86.18%, Valid: 85.97%, Test: 86.30%
Epoch: 450, Loss: 0.8009, Train: 85.71%, Valid: 85.60%, Test: 85.83%
Epoch: 475, Loss: 0.8074, Train: 87.20%, Valid: 87.13%, Test: 87.21%
Run 01:
Highest Train: 87.39
Highest Valid: 87.33
  Final Train: 87.39
   Final Test: 87.42
All runs:
Highest Train: 87.39, nan
Highest Valid: 87.33, nan
  Final Train: 87.39, nan
   Final Test: 87.42, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 16.2817, Train: 80.37%, Valid: 80.44%, Test: 80.67%
Epoch: 25, Loss: 9.2324, Train: 85.67%, Valid: 85.66%, Test: 85.70%
Epoch: 50, Loss: 3.6096, Train: 86.99%, Valid: 87.02%, Test: 87.01%
Epoch: 75, Loss: 3.6478, Train: 86.85%, Valid: 86.90%, Test: 86.90%
Epoch: 100, Loss: 3.1214, Train: 86.79%, Valid: 86.84%, Test: 86.85%
Epoch: 125, Loss: 3.1459, Train: 85.89%, Valid: 85.71%, Test: 85.93%
Epoch: 150, Loss: 3.1637, Train: 85.81%, Valid: 85.60%, Test: 85.86%
Epoch: 175, Loss: 3.0823, Train: 85.77%, Valid: 85.61%, Test: 85.76%
Epoch: 200, Loss: 3.0810, Train: 86.79%, Valid: 86.71%, Test: 86.78%
Epoch: 225, Loss: 3.0809, Train: 87.21%, Valid: 87.07%, Test: 87.17%
Epoch: 250, Loss: 3.0803, Train: 87.18%, Valid: 87.03%, Test: 87.13%
Epoch: 275, Loss: 3.0798, Train: 87.13%, Valid: 86.98%, Test: 87.08%
Epoch: 300, Loss: 3.0792, Train: 87.20%, Valid: 87.06%, Test: 87.16%
Epoch: 325, Loss: 3.0789, Train: 87.32%, Valid: 87.16%, Test: 87.27%
Epoch: 350, Loss: 3.0783, Train: 87.28%, Valid: 87.14%, Test: 87.22%
Epoch: 375, Loss: 3.0776, Train: 87.26%, Valid: 87.11%, Test: 87.19%
Epoch: 400, Loss: 3.0769, Train: 87.30%, Valid: 87.15%, Test: 87.24%
Epoch: 425, Loss: 3.0763, Train: 87.35%, Valid: 87.20%, Test: 87.28%
Epoch: 450, Loss: 3.0756, Train: 87.33%, Valid: 87.17%, Test: 87.24%
Epoch: 475, Loss: 3.0749, Train: 87.37%, Valid: 87.22%, Test: 87.30%
Run 01:
Highest Train: 87.46
Highest Valid: 87.43
  Final Train: 87.46
   Final Test: 87.43
All runs:
Highest Train: 87.46, nan
Highest Valid: 87.43, nan
  Final Train: 87.46, nan
   Final Test: 87.43, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.8904, Train: 85.01%, Valid: 85.00%, Test: 85.12%
Epoch: 25, Loss: 1.2049, Train: 16.24%, Valid: 16.41%, Test: 16.11%
Epoch: 50, Loss: 1055.2881, Train: 16.09%, Valid: 16.28%, Test: 15.96%
Epoch: 75, Loss: 1.5436, Train: 86.44%, Valid: 86.47%, Test: 86.50%
Epoch: 100, Loss: 6707.1641, Train: 16.21%, Valid: 16.37%, Test: 16.09%
Epoch: 125, Loss: 9226.6816, Train: 15.67%, Valid: 15.83%, Test: 15.56%
Epoch: 150, Loss: 478.9199, Train: 86.62%, Valid: 86.61%, Test: 86.70%
Epoch: 175, Loss: 193.3346, Train: 86.62%, Valid: 86.62%, Test: 86.71%
Epoch: 200, Loss: 2.0422, Train: 86.62%, Valid: 86.62%, Test: 86.71%
Epoch: 225, Loss: 2.1165, Train: 86.62%, Valid: 86.62%, Test: 86.71%
Epoch: 250, Loss: 2.0248, Train: 86.62%, Valid: 86.62%, Test: 86.71%
Epoch: 275, Loss: 2.0664, Train: 86.62%, Valid: 86.62%, Test: 86.71%
Epoch: 300, Loss: 2.0963, Train: 86.62%, Valid: 86.62%, Test: 86.71%
Epoch: 325, Loss: 2.0593, Train: 86.62%, Valid: 86.62%, Test: 86.71%
Epoch: 350, Loss: 2.0593, Train: 86.62%, Valid: 86.62%, Test: 86.71%
Epoch: 375, Loss: 2.0704, Train: 86.62%, Valid: 86.62%, Test: 86.71%
Epoch: 400, Loss: 2.0440, Train: 86.62%, Valid: 86.62%, Test: 86.71%
Epoch: 425, Loss: 2.0421, Train: 86.62%, Valid: 86.62%, Test: 86.71%
Epoch: 450, Loss: 2.0572, Train: 86.62%, Valid: 86.62%, Test: 86.71%
Epoch: 475, Loss: 2.0500, Train: 86.62%, Valid: 86.62%, Test: 86.71%
Run 01:
Highest Train: 88.36
Highest Valid: 88.43
  Final Train: 88.36
   Final Test: 88.41
All runs:
Highest Train: 88.36, nan
Highest Valid: 88.43, nan
  Final Train: 88.36, nan
   Final Test: 88.41, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.3885, Train: 85.64%, Valid: 85.61%, Test: 85.65%
Epoch: 25, Loss: 0.6433, Train: 85.13%, Valid: 84.98%, Test: 85.27%
Epoch: 50, Loss: 25.8867, Train: 84.93%, Valid: 84.83%, Test: 85.05%
Epoch: 75, Loss: 30082.9043, Train: 15.48%, Valid: 15.47%, Test: 15.36%
Epoch: 100, Loss: 205.3663, Train: 15.32%, Valid: 15.39%, Test: 15.27%
Epoch: 125, Loss: 48.8764, Train: 15.33%, Valid: 15.39%, Test: 15.28%
Epoch: 150, Loss: 55.2428, Train: 13.83%, Valid: 13.91%, Test: 13.83%
Epoch: 175, Loss: 92.3595, Train: 13.41%, Valid: 13.46%, Test: 13.42%
Epoch: 200, Loss: 49.3313, Train: 13.70%, Valid: 13.73%, Test: 13.68%
Epoch: 225, Loss: 56.4617, Train: 13.67%, Valid: 13.70%, Test: 13.67%
Epoch: 250, Loss: 76.0090, Train: 13.43%, Valid: 13.49%, Test: 13.45%
Epoch: 275, Loss: 82.1755, Train: 13.39%, Valid: 13.44%, Test: 13.40%
Epoch: 300, Loss: 97.9907, Train: 13.64%, Valid: 13.68%, Test: 13.62%
Epoch: 325, Loss: 63.5614, Train: 13.67%, Valid: 13.70%, Test: 13.65%
Epoch: 350, Loss: 99.9671, Train: 13.68%, Valid: 13.72%, Test: 13.67%
Epoch: 375, Loss: 75.4389, Train: 13.69%, Valid: 13.72%, Test: 13.68%
Epoch: 400, Loss: 76.7722, Train: 13.68%, Valid: 13.71%, Test: 13.67%
Epoch: 425, Loss: 81.7415, Train: 13.70%, Valid: 13.73%, Test: 13.69%
Epoch: 450, Loss: 82.4201, Train: 13.50%, Valid: 13.54%, Test: 13.50%
Epoch: 475, Loss: 83.7138, Train: 13.54%, Valid: 13.60%, Test: 13.56%
Run 01:
Highest Train: 86.67
Highest Valid: 86.62
  Final Train: 86.67
   Final Test: 86.68
All runs:
Highest Train: 86.67, nan
Highest Valid: 86.62, nan
  Final Train: 86.67, nan
   Final Test: 86.68, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 12.6187, Train: 76.09%, Valid: 76.26%, Test: 76.30%
Epoch: 25, Loss: 0.5954, Train: 85.31%, Valid: 85.15%, Test: 85.35%
Epoch: 50, Loss: 0.5326, Train: 85.33%, Valid: 85.17%, Test: 85.38%
Epoch: 75, Loss: 0.4525, Train: 85.36%, Valid: 85.21%, Test: 85.39%
Epoch: 100, Loss: 0.4094, Train: 85.74%, Valid: 85.60%, Test: 85.75%
Epoch: 125, Loss: 0.3871, Train: 85.64%, Valid: 85.51%, Test: 85.65%
Epoch: 150, Loss: 0.3758, Train: 85.39%, Valid: 85.24%, Test: 85.42%
Epoch: 175, Loss: 0.3679, Train: 85.43%, Valid: 85.26%, Test: 85.47%
Epoch: 200, Loss: 0.3605, Train: 85.57%, Valid: 85.37%, Test: 85.59%
Epoch: 225, Loss: 0.3496, Train: 85.81%, Valid: 85.60%, Test: 85.80%
Epoch: 250, Loss: 0.3396, Train: 85.94%, Valid: 85.80%, Test: 85.96%
Epoch: 275, Loss: 0.3342, Train: 85.79%, Valid: 85.66%, Test: 85.81%
Epoch: 300, Loss: 0.3314, Train: 85.91%, Valid: 85.81%, Test: 85.97%
Epoch: 325, Loss: 0.3545, Train: 85.70%, Valid: 85.53%, Test: 85.71%
Epoch: 350, Loss: 0.3326, Train: 85.84%, Valid: 85.70%, Test: 85.86%
Epoch: 375, Loss: 0.3298, Train: 85.87%, Valid: 85.77%, Test: 85.95%
Epoch: 400, Loss: 0.3284, Train: 85.80%, Valid: 85.67%, Test: 85.86%
Epoch: 425, Loss: 0.3273, Train: 85.75%, Valid: 85.60%, Test: 85.78%
Epoch: 450, Loss: 0.3265, Train: 85.93%, Valid: 85.77%, Test: 85.94%
Epoch: 475, Loss: 0.3275, Train: 86.50%, Valid: 86.33%, Test: 86.51%
Run 01:
Highest Train: 86.61
Highest Valid: 86.44
  Final Train: 86.61
   Final Test: 86.64
All runs:
Highest Train: 86.61, nan
Highest Valid: 86.44, nan
  Final Train: 86.61, nan
   Final Test: 86.64, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.0874, Train: 85.50%, Valid: 85.37%, Test: 85.66%
Epoch: 25, Loss: 8734.6641, Train: 86.79%, Valid: 86.79%, Test: 86.73%
Epoch: 50, Loss: 26.4167, Train: 16.02%, Valid: 16.17%, Test: 15.95%
Epoch: 75, Loss: 26.9259, Train: 16.06%, Valid: 16.20%, Test: 15.99%
Epoch: 100, Loss: 27.3665, Train: 16.04%, Valid: 16.18%, Test: 15.98%
Epoch: 125, Loss: 29.3485, Train: 16.04%, Valid: 16.18%, Test: 15.97%
Epoch: 150, Loss: 28.1099, Train: 16.03%, Valid: 16.17%, Test: 15.97%
Epoch: 175, Loss: 27.8116, Train: 16.03%, Valid: 16.17%, Test: 15.96%
Epoch: 200, Loss: 29.4160, Train: 16.02%, Valid: 16.16%, Test: 15.95%
Epoch: 225, Loss: 29.3049, Train: 16.03%, Valid: 16.17%, Test: 15.96%
Epoch: 250, Loss: 28.7029, Train: 16.03%, Valid: 16.17%, Test: 15.96%
Epoch: 275, Loss: 28.1427, Train: 16.03%, Valid: 16.17%, Test: 15.96%
Epoch: 300, Loss: 28.6066, Train: 16.03%, Valid: 16.17%, Test: 15.96%
Epoch: 325, Loss: 28.2928, Train: 16.02%, Valid: 16.16%, Test: 15.96%
Epoch: 350, Loss: 29.6123, Train: 16.01%, Valid: 16.15%, Test: 15.94%
Epoch: 375, Loss: 29.1574, Train: 16.01%, Valid: 16.15%, Test: 15.95%
Epoch: 400, Loss: 29.5554, Train: 16.01%, Valid: 16.15%, Test: 15.95%
Epoch: 425, Loss: 28.9636, Train: 16.01%, Valid: 16.15%, Test: 15.94%
Epoch: 450, Loss: 29.2758, Train: 16.01%, Valid: 16.15%, Test: 15.94%
Epoch: 475, Loss: 28.6763, Train: 15.98%, Valid: 16.13%, Test: 15.92%
Run 01:
Highest Train: 87.64
Highest Valid: 87.66
  Final Train: 87.64
   Final Test: 87.69
All runs:
Highest Train: 87.64, nan
Highest Valid: 87.66, nan
  Final Train: 87.64, nan
   Final Test: 87.69, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.8140, Train: 80.26%, Valid: 80.27%, Test: 80.59%
Epoch: 25, Loss: 0.5327, Train: 86.21%, Valid: 86.20%, Test: 86.29%
Epoch: 50, Loss: 0.4961, Train: 86.18%, Valid: 86.11%, Test: 86.27%
Epoch: 75, Loss: 0.4539, Train: 85.89%, Valid: 85.80%, Test: 86.01%
Epoch: 100, Loss: 0.4864, Train: 85.91%, Valid: 85.79%, Test: 85.98%
Epoch: 125, Loss: 0.4260, Train: 85.74%, Valid: 85.61%, Test: 85.84%
Epoch: 150, Loss: 0.3527, Train: 85.95%, Valid: 85.79%, Test: 86.05%
Epoch: 175, Loss: 0.3388, Train: 85.92%, Valid: 85.76%, Test: 86.05%
Epoch: 200, Loss: 0.3377, Train: 86.21%, Valid: 86.03%, Test: 86.33%
Epoch: 225, Loss: 0.3288, Train: 86.19%, Valid: 86.00%, Test: 86.30%
Epoch: 250, Loss: 0.3256, Train: 86.10%, Valid: 85.92%, Test: 86.22%
Epoch: 275, Loss: 0.3247, Train: 86.07%, Valid: 85.89%, Test: 86.19%
Epoch: 300, Loss: 0.3241, Train: 85.96%, Valid: 85.77%, Test: 86.10%
Epoch: 325, Loss: 0.3239, Train: 85.94%, Valid: 85.74%, Test: 86.07%
Epoch: 350, Loss: 0.3427, Train: 86.26%, Valid: 86.09%, Test: 86.40%
Epoch: 375, Loss: 0.3355, Train: 86.35%, Valid: 86.18%, Test: 86.48%
Epoch: 400, Loss: 0.3292, Train: 86.02%, Valid: 85.84%, Test: 86.14%
Epoch: 425, Loss: 0.3250, Train: 86.07%, Valid: 85.86%, Test: 86.19%
Epoch: 450, Loss: 0.3357, Train: 86.30%, Valid: 86.10%, Test: 86.41%
Epoch: 475, Loss: 1.0197, Train: 85.83%, Valid: 85.66%, Test: 85.97%
Run 01:
Highest Train: 87.36
Highest Valid: 87.32
  Final Train: 87.36
   Final Test: 87.51
All runs:
Highest Train: 87.36, nan
Highest Valid: 87.32, nan
  Final Train: 87.36, nan
   Final Test: 87.51, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.8934, Train: 74.21%, Valid: 74.38%, Test: 74.51%
Epoch: 25, Loss: 3.2490, Train: 82.80%, Valid: 82.93%, Test: 83.05%
Epoch: 50, Loss: 4.1974, Train: 84.60%, Valid: 84.69%, Test: 84.75%
Epoch: 75, Loss: 230655.9375, Train: 86.85%, Valid: 86.91%, Test: 86.90%
Epoch: 100, Loss: 81.6465, Train: 14.95%, Valid: 15.16%, Test: 14.92%
Epoch: 125, Loss: 78.2274, Train: 14.79%, Valid: 14.99%, Test: 14.75%
Epoch: 150, Loss: 98.1246, Train: 14.74%, Valid: 14.95%, Test: 14.69%
Epoch: 175, Loss: 102.6150, Train: 14.81%, Valid: 15.01%, Test: 14.77%
Epoch: 200, Loss: 103.3951, Train: 14.80%, Valid: 15.00%, Test: 14.76%
Epoch: 225, Loss: 97.0095, Train: 14.66%, Valid: 14.82%, Test: 14.61%
Epoch: 250, Loss: 140.4149, Train: 14.97%, Valid: 15.15%, Test: 14.91%
Epoch: 275, Loss: 74.6118, Train: 15.15%, Valid: 15.37%, Test: 15.07%
Epoch: 300, Loss: 236.6018, Train: 15.38%, Valid: 15.54%, Test: 15.34%
Epoch: 325, Loss: 101.0248, Train: 15.15%, Valid: 15.36%, Test: 15.06%
Epoch: 350, Loss: 158.8159, Train: 14.85%, Valid: 14.98%, Test: 14.78%
Epoch: 375, Loss: 79.3236, Train: 15.15%, Valid: 15.37%, Test: 15.06%
Epoch: 400, Loss: 90.0198, Train: 14.78%, Valid: 14.98%, Test: 14.74%
Epoch: 425, Loss: 118.1597, Train: 14.90%, Valid: 15.06%, Test: 14.84%
Epoch: 450, Loss: 188.9069, Train: 14.79%, Valid: 14.99%, Test: 14.75%
Epoch: 475, Loss: 89.3781, Train: 14.88%, Valid: 15.07%, Test: 14.83%
Run 01:
Highest Train: 86.85
Highest Valid: 86.91
  Final Train: 86.85
   Final Test: 86.90
All runs:
Highest Train: 86.85, nan
Highest Valid: 86.91, nan
  Final Train: 86.85, nan
   Final Test: 86.90, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.7631, Train: 87.65%, Valid: 87.72%, Test: 87.67%
Epoch: 25, Loss: 9.3405, Train: 85.76%, Valid: 85.81%, Test: 85.84%
Epoch: 50, Loss: 10.6973, Train: 86.49%, Valid: 86.52%, Test: 86.55%
Epoch: 75, Loss: 10.4291, Train: 86.68%, Valid: 86.70%, Test: 86.72%
Epoch: 100, Loss: 9.9635, Train: 86.96%, Valid: 86.99%, Test: 86.99%
Epoch: 125, Loss: 9.9892, Train: 87.05%, Valid: 87.07%, Test: 87.07%
Epoch: 150, Loss: 10.0991, Train: 87.07%, Valid: 87.07%, Test: 87.08%
Epoch: 175, Loss: 9.5755, Train: 87.33%, Valid: 87.33%, Test: 87.36%
Epoch: 200, Loss: 10.6336, Train: 87.46%, Valid: 87.41%, Test: 87.42%
Epoch: 225, Loss: 11.0335, Train: 87.27%, Valid: 87.20%, Test: 87.24%
Epoch: 250, Loss: 10.5126, Train: 87.95%, Valid: 87.99%, Test: 87.97%
Epoch: 275, Loss: 10.9970, Train: 87.44%, Valid: 87.41%, Test: 87.42%
Epoch: 300, Loss: 11.0833, Train: 87.15%, Valid: 87.14%, Test: 87.17%
Epoch: 325, Loss: 11.0141, Train: 87.15%, Valid: 87.14%, Test: 87.17%
Epoch: 350, Loss: 9.9664, Train: 87.46%, Valid: 87.43%, Test: 87.45%
Epoch: 375, Loss: 7.7851, Train: 87.93%, Valid: 87.95%, Test: 87.96%
Epoch: 400, Loss: 7.6580, Train: 87.92%, Valid: 87.95%, Test: 87.96%
Epoch: 425, Loss: 7.4149, Train: 87.95%, Valid: 87.96%, Test: 87.99%
Epoch: 450, Loss: 7.0997, Train: 87.98%, Valid: 87.98%, Test: 88.02%
Epoch: 475, Loss: 6.9781, Train: 87.98%, Valid: 87.99%, Test: 88.03%
Run 01:
Highest Train: 87.98
Highest Valid: 87.99
  Final Train: 87.98
   Final Test: 88.04
All runs:
Highest Train: 87.98, nan
Highest Valid: 87.99, nan
  Final Train: 87.98, nan
   Final Test: 88.04, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.8721, Train: 83.95%, Valid: 83.93%, Test: 84.13%
Epoch: 25, Loss: 0.5128, Train: 85.30%, Valid: 85.17%, Test: 85.42%
Epoch: 50, Loss: 0.7636, Train: 85.33%, Valid: 85.20%, Test: 85.45%
Epoch: 75, Loss: 0.5762, Train: 85.47%, Valid: 85.35%, Test: 85.60%
Epoch: 100, Loss: 0.6328, Train: 86.03%, Valid: 86.00%, Test: 86.13%
Epoch: 125, Loss: 0.6428, Train: 86.10%, Valid: 86.08%, Test: 86.20%
Epoch: 150, Loss: 0.6415, Train: 86.13%, Valid: 86.11%, Test: 86.23%
Epoch: 175, Loss: 0.6396, Train: 86.17%, Valid: 86.15%, Test: 86.27%
Epoch: 200, Loss: 0.6372, Train: 86.16%, Valid: 86.15%, Test: 86.27%
Epoch: 225, Loss: 0.6342, Train: 86.21%, Valid: 86.17%, Test: 86.30%
Epoch: 250, Loss: 0.6076, Train: 85.94%, Valid: 85.81%, Test: 86.06%
Epoch: 275, Loss: 0.5716, Train: 85.72%, Valid: 85.60%, Test: 85.85%
Epoch: 300, Loss: 0.4918, Train: 85.62%, Valid: 85.47%, Test: 85.73%
Epoch: 325, Loss: 0.3691, Train: 86.51%, Valid: 86.53%, Test: 86.53%
Epoch: 350, Loss: 0.3611, Train: 85.42%, Valid: 85.35%, Test: 85.53%
Epoch: 375, Loss: 0.3541, Train: 85.69%, Valid: 85.49%, Test: 85.79%
Epoch: 400, Loss: 0.3475, Train: 86.66%, Valid: 86.69%, Test: 86.85%
Epoch: 425, Loss: 0.3423, Train: 86.56%, Valid: 86.54%, Test: 86.74%
Epoch: 450, Loss: 0.3376, Train: 86.66%, Valid: 86.64%, Test: 86.80%
Epoch: 475, Loss: 0.3348, Train: 86.75%, Valid: 86.66%, Test: 86.85%
Run 01:
Highest Train: 87.39
Highest Valid: 87.28
  Final Train: 87.39
   Final Test: 87.51
All runs:
Highest Train: 87.39, nan
Highest Valid: 87.28, nan
  Final Train: 87.39, nan
   Final Test: 87.51, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 6.3547, Train: 77.60%, Valid: 77.68%, Test: 77.90%
Epoch: 25, Loss: 0.6914, Train: 85.12%, Valid: 84.96%, Test: 85.18%
Epoch: 50, Loss: 0.5809, Train: 85.19%, Valid: 85.04%, Test: 85.25%
Epoch: 75, Loss: 0.5424, Train: 85.16%, Valid: 85.00%, Test: 85.23%
Epoch: 100, Loss: 0.5022, Train: 85.12%, Valid: 84.96%, Test: 85.19%
Epoch: 125, Loss: 0.6324, Train: 85.69%, Valid: 85.53%, Test: 85.74%
Epoch: 150, Loss: 0.4681, Train: 85.87%, Valid: 85.74%, Test: 85.94%
Epoch: 175, Loss: 0.4176, Train: 85.82%, Valid: 85.69%, Test: 85.89%
Epoch: 200, Loss: 0.3976, Train: 85.34%, Valid: 85.17%, Test: 85.40%
Epoch: 225, Loss: 0.3756, Train: 85.28%, Valid: 85.08%, Test: 85.34%
Epoch: 250, Loss: 0.3902, Train: 85.42%, Valid: 85.19%, Test: 85.46%
Epoch: 275, Loss: 0.4227, Train: 86.64%, Valid: 86.64%, Test: 86.69%
Epoch: 300, Loss: 0.3741, Train: 85.48%, Valid: 85.36%, Test: 85.53%
Epoch: 325, Loss: 0.3524, Train: 87.16%, Valid: 87.15%, Test: 87.20%
Epoch: 350, Loss: 0.3474, Train: 87.30%, Valid: 87.29%, Test: 87.30%
Epoch: 375, Loss: 0.3425, Train: 85.71%, Valid: 85.60%, Test: 85.73%
Epoch: 400, Loss: 0.3377, Train: 87.36%, Valid: 87.33%, Test: 87.35%
Epoch: 425, Loss: 0.3329, Train: 87.39%, Valid: 87.39%, Test: 87.40%
Epoch: 450, Loss: 0.3289, Train: 87.81%, Valid: 87.79%, Test: 87.79%
Epoch: 475, Loss: 0.3267, Train: 87.76%, Valid: 87.72%, Test: 87.74%
Run 01:
Highest Train: 88.11
Highest Valid: 88.04
  Final Train: 88.08
   Final Test: 88.07
All runs:
Highest Train: 88.11, nan
Highest Valid: 88.04, nan
  Final Train: 88.08, nan
   Final Test: 88.07, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.9367, Train: 84.78%, Valid: 84.77%, Test: 84.83%
Epoch: 25, Loss: 1.7322, Train: 86.94%, Valid: 86.91%, Test: 87.02%
Epoch: 50, Loss: 36.2739, Train: 86.50%, Valid: 86.47%, Test: 86.59%
Epoch: 75, Loss: 134.8339, Train: 16.20%, Valid: 16.40%, Test: 16.08%
Epoch: 100, Loss: 121.6241, Train: 88.12%, Valid: 88.17%, Test: 88.12%
Epoch: 125, Loss: 43.7239, Train: 88.16%, Valid: 88.21%, Test: 88.15%
Epoch: 150, Loss: 38.1152, Train: 88.16%, Valid: 88.21%, Test: 88.15%
Epoch: 175, Loss: 37.7756, Train: 88.17%, Valid: 88.22%, Test: 88.15%
Epoch: 200, Loss: 39.0803, Train: 88.16%, Valid: 88.22%, Test: 88.15%
Epoch: 225, Loss: 40.3139, Train: 88.16%, Valid: 88.21%, Test: 88.15%
Epoch: 250, Loss: 39.0765, Train: 88.17%, Valid: 88.22%, Test: 88.15%
Epoch: 275, Loss: 37.0991, Train: 88.17%, Valid: 88.22%, Test: 88.15%
Epoch: 300, Loss: 38.5391, Train: 88.16%, Valid: 88.21%, Test: 88.15%
Epoch: 325, Loss: 37.3135, Train: 88.16%, Valid: 88.21%, Test: 88.15%
Epoch: 350, Loss: 39.1341, Train: 88.16%, Valid: 88.21%, Test: 88.15%
Epoch: 375, Loss: 39.1550, Train: 88.16%, Valid: 88.21%, Test: 88.15%
Epoch: 400, Loss: 37.6772, Train: 88.16%, Valid: 88.21%, Test: 88.15%
Epoch: 425, Loss: 37.2361, Train: 88.16%, Valid: 88.21%, Test: 88.15%
Epoch: 450, Loss: 35.4644, Train: 84.26%, Valid: 84.05%, Test: 84.40%
Epoch: 475, Loss: 35.8268, Train: 84.27%, Valid: 84.05%, Test: 84.40%
Run 01:
Highest Train: 88.28
Highest Valid: 88.34
  Final Train: 88.28
   Final Test: 88.27
All runs:
Highest Train: 88.28, nan
Highest Valid: 88.34, nan
  Final Train: 88.28, nan
   Final Test: 88.27, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.1326, Train: 85.92%, Valid: 85.89%, Test: 85.96%
Epoch: 25, Loss: 0.6281, Train: 85.95%, Valid: 85.89%, Test: 85.99%
Epoch: 50, Loss: 0.3832, Train: 85.56%, Valid: 85.44%, Test: 85.71%
Epoch: 75, Loss: 0.3577, Train: 85.69%, Valid: 85.51%, Test: 85.80%
Epoch: 100, Loss: 0.3470, Train: 85.72%, Valid: 85.50%, Test: 85.85%
Epoch: 125, Loss: 0.3368, Train: 85.79%, Valid: 85.59%, Test: 85.91%
Epoch: 150, Loss: 0.3296, Train: 86.14%, Valid: 85.97%, Test: 86.26%
Epoch: 175, Loss: 0.5199, Train: 86.24%, Valid: 86.12%, Test: 86.36%
Epoch: 200, Loss: 0.3442, Train: 85.97%, Valid: 85.81%, Test: 86.10%
Epoch: 225, Loss: 0.3382, Train: 86.05%, Valid: 85.90%, Test: 86.18%
Epoch: 250, Loss: 0.3307, Train: 86.06%, Valid: 85.90%, Test: 86.19%
Epoch: 275, Loss: 0.7421, Train: 86.63%, Valid: 86.56%, Test: 86.72%
Epoch: 300, Loss: 0.3733, Train: 86.45%, Valid: 86.29%, Test: 86.50%
Epoch: 325, Loss: 0.3416, Train: 86.16%, Valid: 86.00%, Test: 86.28%
Epoch: 350, Loss: 0.3318, Train: 86.09%, Valid: 85.95%, Test: 86.22%
Epoch: 375, Loss: 0.3322, Train: 86.32%, Valid: 86.21%, Test: 86.39%
Epoch: 400, Loss: 0.3417, Train: 86.16%, Valid: 86.04%, Test: 86.18%
Epoch: 425, Loss: 0.3273, Train: 86.18%, Valid: 85.99%, Test: 86.22%
Epoch: 450, Loss: 0.3254, Train: 86.36%, Valid: 86.24%, Test: 86.42%
Epoch: 475, Loss: 0.3252, Train: 86.19%, Valid: 86.00%, Test: 86.31%
Run 01:
Highest Train: 87.40
Highest Valid: 87.26
  Final Train: 87.40
   Final Test: 87.50
All runs:
Highest Train: 87.40, nan
Highest Valid: 87.26, nan
  Final Train: 87.40, nan
   Final Test: 87.50, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 10.4138, Train: 82.57%, Valid: 82.64%, Test: 82.67%
Epoch: 25, Loss: 1.2873, Train: 85.42%, Valid: 85.26%, Test: 85.46%
Epoch: 50, Loss: 0.9068, Train: 85.80%, Valid: 85.64%, Test: 85.86%
Epoch: 75, Loss: 0.4486, Train: 85.50%, Valid: 85.35%, Test: 85.59%
Epoch: 100, Loss: 0.3955, Train: 85.39%, Valid: 85.22%, Test: 85.47%
Epoch: 125, Loss: 0.3747, Train: 85.43%, Valid: 85.26%, Test: 85.52%
Epoch: 150, Loss: 0.3643, Train: 85.37%, Valid: 85.21%, Test: 85.46%
Epoch: 175, Loss: 0.3582, Train: 85.28%, Valid: 85.13%, Test: 85.37%
Epoch: 200, Loss: 0.3534, Train: 85.21%, Valid: 85.06%, Test: 85.30%
Epoch: 225, Loss: 0.3485, Train: 85.22%, Valid: 85.06%, Test: 85.30%
Epoch: 250, Loss: 0.3764, Train: 85.32%, Valid: 85.16%, Test: 85.38%
Epoch: 275, Loss: 0.3569, Train: 85.38%, Valid: 85.21%, Test: 85.46%
Epoch: 300, Loss: 0.3497, Train: 85.39%, Valid: 85.22%, Test: 85.47%
Epoch: 325, Loss: 0.3462, Train: 85.42%, Valid: 85.24%, Test: 85.51%
Epoch: 350, Loss: 0.3425, Train: 85.36%, Valid: 85.16%, Test: 85.45%
Epoch: 375, Loss: 0.3401, Train: 85.33%, Valid: 85.13%, Test: 85.42%
Epoch: 400, Loss: 0.6107, Train: 85.42%, Valid: 85.27%, Test: 85.50%
Epoch: 425, Loss: 0.8315, Train: 85.40%, Valid: 85.32%, Test: 85.49%
Epoch: 450, Loss: 0.3562, Train: 85.64%, Valid: 85.43%, Test: 85.71%
Epoch: 475, Loss: 0.3479, Train: 85.62%, Valid: 85.41%, Test: 85.69%
Run 01:
Highest Train: 86.58
Highest Valid: 86.48
  Final Train: 86.58
   Final Test: 86.63
All runs:
Highest Train: 86.58, nan
Highest Valid: 86.48, nan
  Final Train: 86.58, nan
   Final Test: 86.63, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 5.7246, Train: 87.28%, Valid: 87.21%, Test: 87.24%
Epoch: 25, Loss: 1.0849, Train: 87.30%, Valid: 87.33%, Test: 87.32%
Epoch: 50, Loss: 1.2270, Train: 88.53%, Valid: 88.58%, Test: 88.56%
Epoch: 75, Loss: 1.2496, Train: 88.54%, Valid: 88.58%, Test: 88.59%
Epoch: 100, Loss: 1.2372, Train: 88.26%, Valid: 88.26%, Test: 88.27%
Epoch: 125, Loss: 1.1789, Train: 88.04%, Valid: 88.00%, Test: 88.04%
Epoch: 150, Loss: 0.9889, Train: 87.32%, Valid: 87.33%, Test: 87.34%
Epoch: 175, Loss: 0.4432, Train: 85.64%, Valid: 85.68%, Test: 85.74%
Epoch: 200, Loss: 0.3605, Train: 85.83%, Valid: 85.82%, Test: 85.94%
Epoch: 225, Loss: 0.3395, Train: 85.90%, Valid: 85.91%, Test: 86.01%
Epoch: 250, Loss: 0.3515, Train: 85.79%, Valid: 85.82%, Test: 85.94%
Epoch: 275, Loss: 0.6498, Train: 85.74%, Valid: 85.80%, Test: 85.84%
Epoch: 300, Loss: 0.3500, Train: 85.59%, Valid: 85.55%, Test: 85.73%
Epoch: 325, Loss: 0.3396, Train: 86.03%, Valid: 86.03%, Test: 86.14%
Epoch: 350, Loss: 0.3389, Train: 86.07%, Valid: 86.08%, Test: 86.19%
Epoch: 375, Loss: 1.4292, Train: 85.19%, Valid: 85.18%, Test: 85.32%
Epoch: 400, Loss: 1.9998, Train: 85.72%, Valid: 85.77%, Test: 85.81%
Epoch: 425, Loss: 1.1357, Train: 85.56%, Valid: 85.59%, Test: 85.66%
Epoch: 450, Loss: 0.4225, Train: 86.29%, Valid: 86.34%, Test: 86.31%
Epoch: 475, Loss: 0.3549, Train: 87.35%, Valid: 87.19%, Test: 87.44%
Run 01:
Highest Train: 88.54
Highest Valid: 88.59
  Final Train: 88.54
   Final Test: 88.56
All runs:
Highest Train: 88.54, nan
Highest Valid: 88.59, nan
  Final Train: 88.54, nan
   Final Test: 88.56, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 4.8490, Train: 85.56%, Valid: 85.41%, Test: 85.63%
Epoch: 25, Loss: 1.4538, Train: 85.39%, Valid: 85.25%, Test: 85.51%
Epoch: 50, Loss: 2.0137, Train: 85.65%, Valid: 85.51%, Test: 85.76%
Epoch: 75, Loss: 2.1417, Train: 85.84%, Valid: 85.70%, Test: 85.95%
Epoch: 100, Loss: 2.8987, Train: 85.63%, Valid: 85.50%, Test: 85.75%
Epoch: 125, Loss: 2.1616, Train: 85.67%, Valid: 85.53%, Test: 85.80%
Epoch: 150, Loss: 2.0212, Train: 85.63%, Valid: 85.48%, Test: 85.75%
Epoch: 175, Loss: 0.4355, Train: 85.52%, Valid: 85.36%, Test: 85.64%
Epoch: 200, Loss: 0.3614, Train: 85.88%, Valid: 85.69%, Test: 86.00%
Epoch: 225, Loss: 0.3508, Train: 86.14%, Valid: 85.94%, Test: 86.24%
Epoch: 250, Loss: 0.3380, Train: 86.19%, Valid: 85.99%, Test: 86.29%
Epoch: 275, Loss: 1.1202, Train: 85.80%, Valid: 85.66%, Test: 85.92%
Epoch: 300, Loss: 1.4850, Train: 85.83%, Valid: 85.67%, Test: 85.93%
Epoch: 325, Loss: 0.8415, Train: 85.81%, Valid: 85.66%, Test: 85.92%
Epoch: 350, Loss: 0.3988, Train: 85.84%, Valid: 85.68%, Test: 85.96%
Epoch: 375, Loss: 0.3588, Train: 86.19%, Valid: 86.01%, Test: 86.30%
Epoch: 400, Loss: 0.3463, Train: 86.15%, Valid: 85.99%, Test: 86.28%
Epoch: 425, Loss: 0.8764, Train: 85.55%, Valid: 85.63%, Test: 85.62%
Epoch: 450, Loss: 0.3726, Train: 86.42%, Valid: 86.17%, Test: 86.48%
Epoch: 475, Loss: 0.3596, Train: 86.36%, Valid: 86.12%, Test: 86.46%
Run 01:
Highest Train: 87.30
Highest Valid: 87.08
  Final Train: 87.30
   Final Test: 87.25
All runs:
Highest Train: 87.30, nan
Highest Valid: 87.08, nan
  Final Train: 87.30, nan
   Final Test: 87.25, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 4.4539, Train: 80.62%, Valid: 80.65%, Test: 81.02%
Epoch: 25, Loss: 1.5524, Train: 85.34%, Valid: 85.17%, Test: 85.40%
Epoch: 50, Loss: 7.9103, Train: 85.34%, Valid: 85.15%, Test: 85.39%
Epoch: 75, Loss: 129.0771, Train: 85.47%, Valid: 85.34%, Test: 85.49%
Epoch: 100, Loss: 44.3903, Train: 85.37%, Valid: 85.18%, Test: 85.42%
Epoch: 125, Loss: 4.9626, Train: 85.40%, Valid: 85.20%, Test: 85.46%
Epoch: 150, Loss: 98.2493, Train: 85.11%, Valid: 84.93%, Test: 85.17%
Epoch: 175, Loss: 94.7450, Train: 85.09%, Valid: 84.88%, Test: 85.15%
Epoch: 200, Loss: 7.1657, Train: 85.22%, Valid: 85.03%, Test: 85.27%
Epoch: 225, Loss: 7.2780, Train: 85.23%, Valid: 85.03%, Test: 85.27%
Epoch: 250, Loss: 7.2916, Train: 85.23%, Valid: 85.03%, Test: 85.27%
Epoch: 275, Loss: 7.2951, Train: 85.23%, Valid: 85.03%, Test: 85.27%
Epoch: 300, Loss: 7.2936, Train: 85.23%, Valid: 85.03%, Test: 85.27%
Epoch: 325, Loss: 7.2897, Train: 85.23%, Valid: 85.03%, Test: 85.27%
Epoch: 350, Loss: 7.2870, Train: 85.23%, Valid: 85.03%, Test: 85.27%
Epoch: 375, Loss: 7.2919, Train: 85.23%, Valid: 85.03%, Test: 85.27%
Epoch: 400, Loss: 7.2933, Train: 85.23%, Valid: 85.03%, Test: 85.27%
Epoch: 425, Loss: 7.2936, Train: 85.23%, Valid: 85.03%, Test: 85.27%
Epoch: 450, Loss: 7.2885, Train: 85.23%, Valid: 85.03%, Test: 85.27%
Epoch: 475, Loss: 7.2917, Train: 85.23%, Valid: 85.03%, Test: 85.27%
Run 01:
Highest Train: 86.12
Highest Valid: 86.02
  Final Train: 86.12
   Final Test: 86.17
All runs:
Highest Train: 86.12, nan
Highest Valid: 86.02, nan
  Final Train: 86.12, nan
   Final Test: 86.17, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.5363, Train: 87.52%, Valid: 87.47%, Test: 87.49%
Epoch: 25, Loss: 22.4554, Train: 87.34%, Valid: 87.30%, Test: 87.39%
Epoch: 50, Loss: 28.0777, Train: 85.97%, Valid: 86.05%, Test: 86.07%
Epoch: 75, Loss: 1.2318, Train: 85.66%, Valid: 85.71%, Test: 85.74%
Epoch: 100, Loss: 1.1188, Train: 85.62%, Valid: 85.66%, Test: 85.70%
Epoch: 125, Loss: 0.8392, Train: 87.05%, Valid: 86.90%, Test: 87.07%
Epoch: 150, Loss: 0.5636, Train: 87.80%, Valid: 87.83%, Test: 87.87%
Epoch: 175, Loss: 0.3748, Train: 85.16%, Valid: 85.21%, Test: 85.34%
Epoch: 200, Loss: 0.3518, Train: 86.69%, Valid: 86.66%, Test: 86.79%
Epoch: 225, Loss: 0.3476, Train: 85.82%, Valid: 85.78%, Test: 85.95%
Epoch: 250, Loss: 0.3433, Train: 87.01%, Valid: 87.03%, Test: 87.09%
Epoch: 275, Loss: 0.3345, Train: 86.66%, Valid: 86.72%, Test: 86.78%
Epoch: 300, Loss: 1.4033, Train: 85.58%, Valid: 85.65%, Test: 85.66%
Epoch: 325, Loss: 1.8864, Train: 85.69%, Valid: 85.75%, Test: 85.78%
Epoch: 350, Loss: 1.7303, Train: 85.66%, Valid: 85.71%, Test: 85.75%
Epoch: 375, Loss: 1.4092, Train: 85.69%, Valid: 85.73%, Test: 85.77%
Epoch: 400, Loss: 1.0157, Train: 85.42%, Valid: 85.46%, Test: 85.53%
Epoch: 425, Loss: 0.4545, Train: 87.02%, Valid: 86.89%, Test: 87.01%
Epoch: 450, Loss: 0.3543, Train: 85.38%, Valid: 85.40%, Test: 85.54%
Epoch: 475, Loss: 0.3357, Train: 85.58%, Valid: 85.54%, Test: 85.69%
Run 01:
Highest Train: 88.34
Highest Valid: 88.38
  Final Train: 88.34
   Final Test: 88.34
All runs:
Highest Train: 88.34, nan
Highest Valid: 88.38, nan
  Final Train: 88.34, nan
   Final Test: 88.34, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 4.9107, Train: 85.60%, Valid: 85.44%, Test: 85.69%
Epoch: 25, Loss: 1.2539, Train: 85.52%, Valid: 85.37%, Test: 85.60%
Epoch: 50, Loss: 11.2567, Train: 86.06%, Valid: 86.02%, Test: 86.12%
Epoch: 75, Loss: 1.8617, Train: 85.82%, Valid: 85.75%, Test: 85.92%
Epoch: 100, Loss: 6.0624, Train: 85.79%, Valid: 85.69%, Test: 85.89%
Epoch: 125, Loss: 1.9065, Train: 86.26%, Valid: 86.19%, Test: 86.35%
Epoch: 150, Loss: 1.9076, Train: 86.28%, Valid: 86.21%, Test: 86.37%
Epoch: 175, Loss: 1.8921, Train: 86.26%, Valid: 86.19%, Test: 86.36%
Epoch: 200, Loss: 1.8746, Train: 86.24%, Valid: 86.15%, Test: 86.32%
Epoch: 225, Loss: 1.8524, Train: 86.18%, Valid: 86.09%, Test: 86.27%
Epoch: 250, Loss: 1.8370, Train: 86.16%, Valid: 86.06%, Test: 86.23%
Epoch: 275, Loss: 1.8237, Train: 86.13%, Valid: 86.03%, Test: 86.21%
Epoch: 300, Loss: 1.8099, Train: 86.10%, Valid: 85.99%, Test: 86.18%
Epoch: 325, Loss: 1.7981, Train: 86.08%, Valid: 85.96%, Test: 86.16%
Epoch: 350, Loss: 1.7869, Train: 86.06%, Valid: 85.92%, Test: 86.14%
Epoch: 375, Loss: 1.7770, Train: 86.04%, Valid: 85.91%, Test: 86.12%
Epoch: 400, Loss: 1.7674, Train: 86.02%, Valid: 85.88%, Test: 86.10%
Epoch: 425, Loss: 1.7581, Train: 86.01%, Valid: 85.86%, Test: 86.09%
Epoch: 450, Loss: 1.7487, Train: 85.99%, Valid: 85.85%, Test: 86.09%
Epoch: 475, Loss: 1.7391, Train: 85.98%, Valid: 85.85%, Test: 86.08%
Run 01:
Highest Train: 86.32
Highest Valid: 86.29
  Final Train: 86.32
   Final Test: 86.42
All runs:
Highest Train: 86.32, nan
Highest Valid: 86.29, nan
  Final Train: 86.32, nan
   Final Test: 86.42, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.7438, Train: 28.24%, Valid: 28.08%, Test: 27.92%
Epoch: 25, Loss: 1.0775, Train: 85.07%, Valid: 84.90%, Test: 85.17%
Epoch: 50, Loss: 0.9192, Train: 85.64%, Valid: 85.54%, Test: 85.66%
Epoch: 75, Loss: 0.9503, Train: 85.30%, Valid: 85.16%, Test: 85.39%
Epoch: 100, Loss: 0.9006, Train: 85.31%, Valid: 85.18%, Test: 85.39%
Epoch: 125, Loss: 0.8560, Train: 85.70%, Valid: 85.66%, Test: 85.76%
Epoch: 150, Loss: 0.8941, Train: 85.13%, Valid: 84.95%, Test: 85.20%
Epoch: 175, Loss: 0.7949, Train: 85.25%, Valid: 85.08%, Test: 85.31%
Epoch: 200, Loss: 0.5000, Train: 85.16%, Valid: 84.99%, Test: 85.23%
Epoch: 225, Loss: 0.3664, Train: 86.54%, Valid: 86.53%, Test: 86.62%
Epoch: 250, Loss: 0.3528, Train: 86.94%, Valid: 86.95%, Test: 86.98%
Epoch: 275, Loss: 0.3466, Train: 87.11%, Valid: 87.11%, Test: 87.14%
Epoch: 300, Loss: 0.3408, Train: 86.93%, Valid: 86.91%, Test: 86.97%
Epoch: 325, Loss: 0.3343, Train: 86.79%, Valid: 86.80%, Test: 86.84%
Epoch: 350, Loss: 0.4339, Train: 86.88%, Valid: 86.89%, Test: 86.93%
Epoch: 375, Loss: 0.3494, Train: 87.04%, Valid: 87.04%, Test: 87.10%
Epoch: 400, Loss: 0.3390, Train: 86.10%, Valid: 86.10%, Test: 86.12%
Epoch: 425, Loss: 0.3328, Train: 86.00%, Valid: 85.95%, Test: 86.03%
Epoch: 450, Loss: 0.3286, Train: 87.50%, Valid: 87.52%, Test: 87.56%
Epoch: 475, Loss: 0.3326, Train: 87.53%, Valid: 87.54%, Test: 87.61%
Run 01:
Highest Train: 87.56
Highest Valid: 87.59
  Final Train: 87.56
   Final Test: 87.64
All runs:
Highest Train: 87.56, nan
Highest Valid: 87.59, nan
  Final Train: 87.56, nan
   Final Test: 87.64, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 4.1364, Train: 85.30%, Valid: 85.26%, Test: 85.41%
Epoch: 25, Loss: 1.7987, Train: 86.01%, Valid: 86.05%, Test: 86.03%
Epoch: 50, Loss: 1.7646, Train: 85.13%, Valid: 85.19%, Test: 85.24%
Epoch: 75, Loss: 0.9029, Train: 85.51%, Valid: 85.56%, Test: 85.59%
Epoch: 100, Loss: 0.3810, Train: 85.98%, Valid: 85.84%, Test: 86.08%
Epoch: 125, Loss: 0.3464, Train: 85.62%, Valid: 85.64%, Test: 85.73%
Epoch: 150, Loss: 0.3335, Train: 85.65%, Valid: 85.68%, Test: 85.75%
Epoch: 175, Loss: 0.3586, Train: 85.60%, Valid: 85.62%, Test: 85.68%
Epoch: 200, Loss: 0.9652, Train: 85.71%, Valid: 85.76%, Test: 85.85%
Epoch: 225, Loss: 1.4861, Train: 87.88%, Valid: 87.95%, Test: 87.94%
Epoch: 250, Loss: 0.4381, Train: 86.17%, Valid: 86.18%, Test: 86.25%
Epoch: 275, Loss: 0.3441, Train: 85.84%, Valid: 85.84%, Test: 85.95%
Epoch: 300, Loss: 0.3481, Train: 86.44%, Valid: 86.28%, Test: 86.49%
Epoch: 325, Loss: 0.3361, Train: 85.83%, Valid: 85.80%, Test: 85.92%
Epoch: 350, Loss: 0.3331, Train: 85.87%, Valid: 85.88%, Test: 85.98%
Epoch: 375, Loss: 0.7221, Train: 86.80%, Valid: 86.85%, Test: 86.86%
Epoch: 400, Loss: 0.3625, Train: 86.11%, Valid: 86.06%, Test: 86.16%
Epoch: 425, Loss: 0.3492, Train: 86.68%, Valid: 86.64%, Test: 86.77%
Epoch: 450, Loss: 0.3421, Train: 86.73%, Valid: 86.75%, Test: 86.76%
Epoch: 475, Loss: 0.3287, Train: 85.92%, Valid: 85.90%, Test: 85.98%
Run 01:
Highest Train: 87.92
Highest Valid: 88.00
  Final Train: 87.92
   Final Test: 87.98
All runs:
Highest Train: 87.92, nan
Highest Valid: 88.00, nan
  Final Train: 87.92, nan
   Final Test: 87.98, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 12.5965, Train: 87.32%, Valid: 87.20%, Test: 87.27%
Epoch: 25, Loss: 0.4782, Train: 86.10%, Valid: 85.97%, Test: 86.16%
Epoch: 50, Loss: 0.4149, Train: 86.27%, Valid: 86.26%, Test: 86.32%
Epoch: 75, Loss: 0.3582, Train: 85.84%, Valid: 85.66%, Test: 85.93%
Epoch: 100, Loss: 0.3494, Train: 85.73%, Valid: 85.53%, Test: 85.84%
Epoch: 125, Loss: 0.4984, Train: 85.67%, Valid: 85.54%, Test: 85.77%
Epoch: 150, Loss: 2.5318, Train: 85.57%, Valid: 85.39%, Test: 85.67%
Epoch: 175, Loss: 1.1723, Train: 85.03%, Valid: 84.91%, Test: 85.18%
Epoch: 200, Loss: 0.3535, Train: 86.05%, Valid: 85.88%, Test: 86.11%
Epoch: 225, Loss: 0.3345, Train: 86.25%, Valid: 86.10%, Test: 86.25%
Epoch: 250, Loss: 0.3304, Train: 85.74%, Valid: 85.57%, Test: 85.85%
Epoch: 275, Loss: 0.3286, Train: 85.95%, Valid: 85.77%, Test: 86.07%
Epoch: 300, Loss: 0.3385, Train: 86.29%, Valid: 86.15%, Test: 86.40%
Epoch: 325, Loss: 0.3272, Train: 86.01%, Valid: 85.83%, Test: 86.13%
Epoch: 350, Loss: 0.7252, Train: 86.52%, Valid: 86.41%, Test: 86.64%
Epoch: 375, Loss: 0.8590, Train: 85.99%, Valid: 85.89%, Test: 86.10%
Epoch: 400, Loss: 0.5185, Train: 86.57%, Valid: 86.47%, Test: 86.65%
Epoch: 425, Loss: 0.3900, Train: 86.40%, Valid: 86.24%, Test: 86.48%
Epoch: 450, Loss: 0.3445, Train: 86.85%, Valid: 86.62%, Test: 86.92%
Epoch: 475, Loss: 0.3336, Train: 85.65%, Valid: 85.46%, Test: 85.78%
Run 01:
Highest Train: 87.52
Highest Valid: 87.35
  Final Train: 87.52
   Final Test: 87.55
All runs:
Highest Train: 87.52, nan
Highest Valid: 87.35, nan
  Final Train: 87.52, nan
   Final Test: 87.55, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 95.8362, Train: 41.44%, Valid: 41.18%, Test: 40.95%
Epoch: 25, Loss: 0.8160, Train: 85.13%, Valid: 84.98%, Test: 85.17%
Epoch: 50, Loss: 0.6113, Train: 85.07%, Valid: 84.93%, Test: 85.12%
Epoch: 75, Loss: 0.4504, Train: 85.13%, Valid: 85.00%, Test: 85.19%
Epoch: 100, Loss: 0.4076, Train: 85.12%, Valid: 84.98%, Test: 85.18%
Epoch: 125, Loss: 0.4007, Train: 85.13%, Valid: 84.99%, Test: 85.20%
Epoch: 150, Loss: 0.3928, Train: 85.16%, Valid: 85.01%, Test: 85.22%
Epoch: 175, Loss: 0.3855, Train: 85.13%, Valid: 84.98%, Test: 85.21%
Epoch: 200, Loss: 0.3770, Train: 85.11%, Valid: 84.96%, Test: 85.19%
Epoch: 225, Loss: 0.3699, Train: 85.13%, Valid: 84.97%, Test: 85.21%
Epoch: 250, Loss: 0.3601, Train: 85.17%, Valid: 85.01%, Test: 85.25%
Epoch: 275, Loss: 0.3493, Train: 85.26%, Valid: 85.06%, Test: 85.33%
Epoch: 300, Loss: 0.3514, Train: 86.51%, Valid: 86.31%, Test: 86.46%
Epoch: 325, Loss: 0.4931, Train: 85.27%, Valid: 85.10%, Test: 85.33%
Epoch: 350, Loss: 0.3463, Train: 87.15%, Valid: 86.93%, Test: 87.03%
Epoch: 375, Loss: 0.3458, Train: 87.07%, Valid: 86.91%, Test: 87.02%
Epoch: 400, Loss: 0.3419, Train: 87.10%, Valid: 86.94%, Test: 87.02%
Epoch: 425, Loss: 0.3387, Train: 87.01%, Valid: 86.83%, Test: 86.91%
Epoch: 450, Loss: 0.3355, Train: 87.13%, Valid: 86.97%, Test: 87.06%
Epoch: 475, Loss: 0.3320, Train: 87.23%, Valid: 87.08%, Test: 87.18%
Run 01:
Highest Train: 87.62
Highest Valid: 87.48
  Final Train: 87.62
   Final Test: 87.55
All runs:
Highest Train: 87.62, nan
Highest Valid: 87.48, nan
  Final Train: 87.62, nan
   Final Test: 87.55, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 9.5282, Train: 87.58%, Valid: 87.60%, Test: 87.61%
Epoch: 25, Loss: 0.8343, Train: 87.42%, Valid: 87.43%, Test: 87.45%
Epoch: 50, Loss: 0.3631, Train: 87.69%, Valid: 87.72%, Test: 87.80%
Epoch: 75, Loss: 0.3379, Train: 85.45%, Valid: 85.45%, Test: 85.55%
Epoch: 100, Loss: 0.4277, Train: 85.58%, Valid: 85.63%, Test: 85.69%
Epoch: 125, Loss: 1.7804, Train: 85.41%, Valid: 85.42%, Test: 85.53%
Epoch: 150, Loss: 0.9495, Train: 85.54%, Valid: 85.44%, Test: 85.63%
Epoch: 175, Loss: 0.7881, Train: 85.46%, Valid: 85.34%, Test: 85.51%
Epoch: 200, Loss: 0.3913, Train: 85.68%, Valid: 85.70%, Test: 85.79%
Epoch: 225, Loss: 0.3453, Train: 87.70%, Valid: 87.60%, Test: 87.73%
Epoch: 250, Loss: 0.3319, Train: 87.10%, Valid: 86.90%, Test: 87.16%
Epoch: 275, Loss: 0.3416, Train: 87.37%, Valid: 87.15%, Test: 87.41%
Epoch: 300, Loss: 0.3321, Train: 87.12%, Valid: 87.10%, Test: 87.25%
Epoch: 325, Loss: 0.4832, Train: 85.83%, Valid: 85.83%, Test: 85.94%
Epoch: 350, Loss: 0.5611, Train: 85.69%, Valid: 85.76%, Test: 85.79%
Epoch: 375, Loss: 0.3555, Train: 85.81%, Valid: 85.84%, Test: 85.94%
Epoch: 400, Loss: 0.3379, Train: 86.07%, Valid: 86.05%, Test: 86.17%
Epoch: 425, Loss: 0.4266, Train: 86.84%, Valid: 86.84%, Test: 86.91%
Epoch: 450, Loss: 0.3389, Train: 86.26%, Valid: 86.12%, Test: 86.38%
Epoch: 475, Loss: 0.3288, Train: 86.88%, Valid: 86.80%, Test: 86.88%
Run 01:
Highest Train: 88.27
Highest Valid: 88.31
  Final Train: 88.27
   Final Test: 88.31
All runs:
Highest Train: 88.27, nan
Highest Valid: 88.31, nan
  Final Train: 88.27, nan
   Final Test: 88.31, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.0662, Train: 84.69%, Valid: 84.66%, Test: 84.86%
Epoch: 25, Loss: 0.9658, Train: 87.12%, Valid: 87.06%, Test: 87.12%
Epoch: 50, Loss: 0.9576, Train: 87.18%, Valid: 87.07%, Test: 87.11%
Epoch: 75, Loss: 1.4636, Train: 86.71%, Valid: 86.56%, Test: 86.76%
Epoch: 100, Loss: 1.1864, Train: 86.88%, Valid: 86.72%, Test: 86.92%
Epoch: 125, Loss: 0.4806, Train: 86.98%, Valid: 86.81%, Test: 87.06%
Epoch: 150, Loss: 1.0428, Train: 86.54%, Valid: 86.68%, Test: 86.66%
Epoch: 175, Loss: 0.9017, Train: 86.43%, Valid: 86.40%, Test: 86.49%
Epoch: 200, Loss: 0.8196, Train: 86.47%, Valid: 86.45%, Test: 86.50%
Epoch: 225, Loss: 0.5757, Train: 86.18%, Valid: 86.10%, Test: 86.20%
Epoch: 250, Loss: 0.3505, Train: 86.04%, Valid: 85.86%, Test: 86.15%
Epoch: 275, Loss: 0.3401, Train: 85.51%, Valid: 85.32%, Test: 85.65%
Epoch: 300, Loss: 0.3322, Train: 85.35%, Valid: 85.18%, Test: 85.52%
Epoch: 325, Loss: 0.9152, Train: 85.82%, Valid: 85.73%, Test: 85.98%
Epoch: 350, Loss: 1.7946, Train: 86.08%, Valid: 86.02%, Test: 86.12%
Epoch: 375, Loss: 0.4599, Train: 86.50%, Valid: 86.38%, Test: 86.47%
Epoch: 400, Loss: 0.3565, Train: 85.84%, Valid: 85.71%, Test: 85.98%
Epoch: 425, Loss: 0.3413, Train: 85.89%, Valid: 85.70%, Test: 86.02%
Epoch: 450, Loss: 0.3292, Train: 85.80%, Valid: 85.62%, Test: 85.92%
Epoch: 475, Loss: 1.2092, Train: 87.23%, Valid: 87.07%, Test: 87.32%
Run 01:
Highest Train: 87.68
Highest Valid: 87.65
  Final Train: 87.68
   Final Test: 87.73
All runs:
Highest Train: 87.68, nan
Highest Valid: 87.65, nan
  Final Train: 87.68, nan
   Final Test: 87.73, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 10.4782, Train: 80.45%, Valid: 80.57%, Test: 80.82%
Epoch: 25, Loss: 1.2691, Train: 85.20%, Valid: 85.06%, Test: 85.26%
Epoch: 50, Loss: 1.0002, Train: 85.15%, Valid: 84.99%, Test: 85.20%
Epoch: 75, Loss: 0.9175, Train: 85.09%, Valid: 84.93%, Test: 85.15%
Epoch: 100, Loss: 1.1259, Train: 85.05%, Valid: 84.87%, Test: 85.14%
Epoch: 125, Loss: 1.5610, Train: 85.35%, Valid: 85.19%, Test: 85.40%
Epoch: 150, Loss: 0.9373, Train: 85.28%, Valid: 85.11%, Test: 85.33%
Epoch: 175, Loss: 0.6581, Train: 85.09%, Valid: 84.92%, Test: 85.15%
Epoch: 200, Loss: 0.5189, Train: 84.99%, Valid: 84.82%, Test: 85.05%
Epoch: 225, Loss: 0.4782, Train: 85.84%, Valid: 85.70%, Test: 85.97%
Epoch: 250, Loss: 0.3815, Train: 86.32%, Valid: 86.18%, Test: 86.42%
Epoch: 275, Loss: 0.3645, Train: 87.81%, Valid: 87.68%, Test: 87.91%
Epoch: 300, Loss: 0.3573, Train: 87.63%, Valid: 87.51%, Test: 87.70%
Epoch: 325, Loss: 0.3646, Train: 85.65%, Valid: 85.48%, Test: 85.72%
Epoch: 350, Loss: 0.5337, Train: 87.11%, Valid: 87.14%, Test: 87.14%
Epoch: 375, Loss: 0.3669, Train: 87.21%, Valid: 87.18%, Test: 87.22%
Epoch: 400, Loss: 0.3562, Train: 86.71%, Valid: 86.68%, Test: 86.79%
Epoch: 425, Loss: 0.3482, Train: 86.75%, Valid: 86.74%, Test: 86.83%
Epoch: 450, Loss: 0.3418, Train: 86.17%, Valid: 86.11%, Test: 86.16%
Epoch: 475, Loss: 0.3339, Train: 86.40%, Valid: 86.38%, Test: 86.45%
Run 01:
Highest Train: 87.84
Highest Valid: 87.71
  Final Train: 87.84
   Final Test: 87.93
All runs:
Highest Train: 87.84, nan
Highest Valid: 87.71, nan
  Final Train: 87.84, nan
   Final Test: 87.93, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 3.7497, Train: 87.42%, Valid: 87.41%, Test: 87.43%
Epoch: 25, Loss: 0.4327, Train: 85.52%, Valid: 85.57%, Test: 85.57%
Epoch: 50, Loss: 1.1681, Train: 85.72%, Valid: 85.79%, Test: 85.79%
Epoch: 75, Loss: 0.9631, Train: 85.57%, Valid: 85.64%, Test: 85.66%
Epoch: 100, Loss: 1.3104, Train: 85.55%, Valid: 85.59%, Test: 85.65%
Epoch: 125, Loss: 0.5271, Train: 86.73%, Valid: 86.69%, Test: 86.83%
Epoch: 150, Loss: 0.3865, Train: 86.30%, Valid: 86.36%, Test: 86.36%
Epoch: 175, Loss: 0.3519, Train: 85.91%, Valid: 85.90%, Test: 86.00%
Epoch: 200, Loss: 0.3331, Train: 85.99%, Valid: 85.98%, Test: 86.06%
Epoch: 225, Loss: 3.1566, Train: 86.85%, Valid: 86.87%, Test: 87.00%
Epoch: 250, Loss: 2.0671, Train: 86.03%, Valid: 85.91%, Test: 86.07%
Epoch: 275, Loss: 2.4961, Train: 87.25%, Valid: 87.25%, Test: 87.32%
Epoch: 300, Loss: 0.8970, Train: 87.56%, Valid: 87.40%, Test: 87.58%
Epoch: 325, Loss: 0.4621, Train: 87.45%, Valid: 87.31%, Test: 87.49%
Epoch: 350, Loss: 0.3702, Train: 87.50%, Valid: 87.47%, Test: 87.57%
Epoch: 375, Loss: 0.3531, Train: 86.85%, Valid: 86.76%, Test: 86.90%
Epoch: 400, Loss: 0.3302, Train: 85.94%, Valid: 85.96%, Test: 86.04%
Epoch: 425, Loss: 0.3359, Train: 86.60%, Valid: 86.56%, Test: 86.61%
Epoch: 450, Loss: 0.4301, Train: 86.93%, Valid: 86.87%, Test: 86.96%
Epoch: 475, Loss: 0.3506, Train: 86.31%, Valid: 86.30%, Test: 86.33%
Run 01:
Highest Train: 88.44
Highest Valid: 88.50
  Final Train: 88.44
   Final Test: 88.52
All runs:
Highest Train: 88.44, nan
Highest Valid: 88.50, nan
  Final Train: 88.44, nan
   Final Test: 88.52, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.4672, Train: 85.55%, Valid: 85.49%, Test: 85.61%
Epoch: 25, Loss: 1.7096, Train: 86.41%, Valid: 86.25%, Test: 86.42%
Epoch: 50, Loss: 1.3682, Train: 87.64%, Valid: 87.58%, Test: 87.62%
Epoch: 75, Loss: 0.8870, Train: 87.32%, Valid: 87.22%, Test: 87.35%
Epoch: 100, Loss: 0.3624, Train: 86.48%, Valid: 86.26%, Test: 86.59%
Epoch: 125, Loss: 0.3497, Train: 86.16%, Valid: 85.95%, Test: 86.27%
Epoch: 150, Loss: 0.3368, Train: 86.01%, Valid: 85.82%, Test: 86.13%
Epoch: 175, Loss: 0.5379, Train: 85.63%, Valid: 85.50%, Test: 85.77%
Epoch: 200, Loss: 0.5087, Train: 85.30%, Valid: 85.30%, Test: 85.46%
Epoch: 225, Loss: 0.3442, Train: 85.93%, Valid: 85.77%, Test: 86.05%
Epoch: 250, Loss: 0.3389, Train: 85.90%, Valid: 85.74%, Test: 86.01%
Epoch: 275, Loss: 0.3293, Train: 86.02%, Valid: 85.85%, Test: 86.15%
Epoch: 300, Loss: 1.4507, Train: 86.48%, Valid: 86.36%, Test: 86.63%
Epoch: 325, Loss: 1.8084, Train: 85.86%, Valid: 85.80%, Test: 86.04%
Epoch: 350, Loss: 1.3581, Train: 86.67%, Valid: 86.66%, Test: 86.70%
Epoch: 375, Loss: 1.7362, Train: 85.89%, Valid: 85.88%, Test: 85.93%
Epoch: 400, Loss: 0.7636, Train: 86.19%, Valid: 86.17%, Test: 86.20%
Epoch: 425, Loss: 0.3754, Train: 86.29%, Valid: 86.19%, Test: 86.38%
Epoch: 450, Loss: 0.3578, Train: 86.11%, Valid: 85.90%, Test: 86.23%
Epoch: 475, Loss: 0.3352, Train: 85.84%, Valid: 85.65%, Test: 85.97%
Run 01:
Highest Train: 87.72
Highest Valid: 87.69
  Final Train: 87.72
   Final Test: 87.72
All runs:
Highest Train: 87.72, nan
Highest Valid: 87.69, nan
  Final Train: 87.72, nan
   Final Test: 87.72, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.2367, Train: 24.62%, Valid: 24.48%, Test: 24.17%
Epoch: 25, Loss: 1.4324, Train: 85.35%, Valid: 85.21%, Test: 85.40%
Epoch: 50, Loss: 1.0169, Train: 85.19%, Valid: 85.04%, Test: 85.25%
Epoch: 75, Loss: 0.8167, Train: 85.58%, Valid: 85.42%, Test: 85.62%
Epoch: 100, Loss: 0.4004, Train: 85.73%, Valid: 85.54%, Test: 85.81%
Epoch: 125, Loss: 0.3663, Train: 85.78%, Valid: 85.62%, Test: 85.83%
Epoch: 150, Loss: 0.3618, Train: 85.97%, Valid: 85.79%, Test: 86.04%
Epoch: 175, Loss: 0.3583, Train: 86.65%, Valid: 86.48%, Test: 86.75%
Epoch: 200, Loss: 0.3536, Train: 86.51%, Valid: 86.33%, Test: 86.60%
Epoch: 225, Loss: 0.3474, Train: 86.46%, Valid: 86.27%, Test: 86.56%
Epoch: 250, Loss: 0.3439, Train: 86.02%, Valid: 85.85%, Test: 86.10%
Epoch: 275, Loss: 0.3390, Train: 86.37%, Valid: 86.14%, Test: 86.45%
Epoch: 300, Loss: 0.3512, Train: 86.70%, Valid: 86.62%, Test: 86.84%
Epoch: 325, Loss: 0.3408, Train: 86.41%, Valid: 86.24%, Test: 86.51%
Epoch: 350, Loss: 0.3360, Train: 86.73%, Valid: 86.50%, Test: 86.83%
Epoch: 375, Loss: 0.3327, Train: 86.77%, Valid: 86.53%, Test: 86.86%
Epoch: 400, Loss: 0.3699, Train: 86.16%, Valid: 86.02%, Test: 86.28%
Epoch: 425, Loss: 1.6492, Train: 86.57%, Valid: 86.52%, Test: 86.60%
Epoch: 450, Loss: 4.8135, Train: 85.73%, Valid: 85.61%, Test: 85.80%
Epoch: 475, Loss: 2.6068, Train: 86.69%, Valid: 86.52%, Test: 86.80%
Run 01:
Highest Train: 87.31
Highest Valid: 87.22
  Final Train: 87.31
   Final Test: 87.42
All runs:
Highest Train: 87.31, nan
Highest Valid: 87.22, nan
  Final Train: 87.31, nan
   Final Test: 87.42, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.5713, Train: 13.35%, Valid: 13.23%, Test: 13.36%
Epoch: 25, Loss: 2409.0996, Train: 16.12%, Valid: 16.29%, Test: 15.97%
Epoch: 50, Loss: 1365905.8750, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 24919.6152, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 869461.8750, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 2897536.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 819909.8750, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 48645.4453, Train: 83.58%, Valid: 83.41%, Test: 83.70%
Epoch: 200, Loss: 95843.5547, Train: 49.78%, Valid: 49.78%, Test: 49.76%
Epoch: 225, Loss: 22737.2578, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 7209.5347, Train: 80.65%, Valid: 80.63%, Test: 80.75%
Epoch: 275, Loss: 196.7245, Train: 77.80%, Valid: 77.85%, Test: 78.02%
Epoch: 300, Loss: 72.4936, Train: 84.99%, Valid: 84.87%, Test: 85.03%
Epoch: 325, Loss: 18922.8828, Train: 16.45%, Valid: 16.38%, Test: 16.36%
Epoch: 350, Loss: 10057.2422, Train: 16.39%, Valid: 16.33%, Test: 16.31%
Epoch: 375, Loss: 3854.9971, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 164.3002, Train: 83.17%, Valid: 83.01%, Test: 83.30%
Epoch: 425, Loss: 263.4938, Train: 83.72%, Valid: 83.66%, Test: 83.83%
Epoch: 450, Loss: 97.7182, Train: 16.78%, Valid: 16.69%, Test: 16.68%
Epoch: 475, Loss: 5812.7593, Train: 48.95%, Valid: 48.97%, Test: 48.94%
Run 01:
Highest Train: 86.36
Highest Valid: 86.39
  Final Train: 86.36
   Final Test: 86.40
All runs:
Highest Train: 86.36, nan
Highest Valid: 86.39, nan
  Final Train: 86.36, nan
   Final Test: 86.40, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.8601, Train: 13.77%, Valid: 13.78%, Test: 13.76%
Epoch: 25, Loss: 28305.9551, Train: 15.56%, Valid: 15.66%, Test: 15.43%
Epoch: 50, Loss: 475320.6562, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 75340624.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 29831470.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 1571.9889, Train: 84.14%, Valid: 83.96%, Test: 84.32%
Epoch: 150, Loss: 7661.6685, Train: 15.22%, Valid: 15.26%, Test: 15.15%
Epoch: 175, Loss: 6223.8496, Train: 85.56%, Valid: 85.41%, Test: 85.67%
Epoch: 200, Loss: 3185.1355, Train: 15.67%, Valid: 15.74%, Test: 15.57%
Epoch: 225, Loss: 2166.5454, Train: 85.56%, Valid: 85.41%, Test: 85.67%
Epoch: 250, Loss: 1902.1271, Train: 15.27%, Valid: 15.29%, Test: 15.18%
Epoch: 275, Loss: 3153.6492, Train: 83.72%, Valid: 83.59%, Test: 83.87%
Epoch: 300, Loss: 1146.3911, Train: 15.32%, Valid: 15.35%, Test: 15.24%
Epoch: 325, Loss: 13.7116, Train: 15.31%, Valid: 15.33%, Test: 15.24%
Epoch: 350, Loss: 15.5516, Train: 85.64%, Valid: 85.49%, Test: 85.74%
Epoch: 375, Loss: 335.9958, Train: 15.53%, Valid: 15.53%, Test: 15.45%
Epoch: 400, Loss: 1123.0835, Train: 85.46%, Valid: 85.30%, Test: 85.54%
Epoch: 425, Loss: 6.7233, Train: 15.58%, Valid: 15.57%, Test: 15.50%
Epoch: 450, Loss: 6.7056, Train: 13.46%, Valid: 13.47%, Test: 13.44%
Epoch: 475, Loss: 2.4164, Train: 85.51%, Valid: 85.35%, Test: 85.60%
Run 01:
Highest Train: 86.63
Highest Valid: 86.50
  Final Train: 86.63
   Final Test: 86.70
All runs:
Highest Train: 86.63, nan
Highest Valid: 86.50, nan
  Final Train: 86.63, nan
   Final Test: 86.70, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.0731, Train: 86.41%, Valid: 86.27%, Test: 86.39%
Epoch: 25, Loss: 1188.3149, Train: 84.69%, Valid: 84.46%, Test: 84.78%
Epoch: 50, Loss: 4926.1333, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 34166.3672, Train: 84.73%, Valid: 84.51%, Test: 84.84%
Epoch: 100, Loss: 2079.4656, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 213381.5938, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 20002.9277, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 231583.5469, Train: 84.63%, Valid: 84.42%, Test: 84.75%
Epoch: 200, Loss: 67571.4688, Train: 15.51%, Valid: 15.62%, Test: 15.40%
Epoch: 225, Loss: 1040.7688, Train: 86.19%, Valid: 86.01%, Test: 86.19%
Epoch: 250, Loss: 64.2840, Train: 15.81%, Valid: 15.80%, Test: 15.69%
Epoch: 275, Loss: 20.2003, Train: 86.79%, Valid: 86.55%, Test: 86.80%
Epoch: 300, Loss: 19.2443, Train: 85.91%, Valid: 85.80%, Test: 85.94%
Epoch: 325, Loss: 37.3101, Train: 85.91%, Valid: 85.78%, Test: 85.92%
Epoch: 350, Loss: 1.8582, Train: 86.01%, Valid: 85.84%, Test: 86.01%
Epoch: 375, Loss: 1.9027, Train: 86.73%, Valid: 86.51%, Test: 86.74%
Epoch: 400, Loss: 258.2568, Train: 86.79%, Valid: 86.55%, Test: 86.79%
Epoch: 425, Loss: 215.9490, Train: 86.01%, Valid: 85.86%, Test: 86.01%
Epoch: 450, Loss: 2.2659, Train: 86.68%, Valid: 86.46%, Test: 86.67%
Epoch: 475, Loss: 2.4132, Train: 15.15%, Valid: 15.04%, Test: 15.03%
Run 01:
Highest Train: 86.82
Highest Valid: 86.60
  Final Train: 86.82
   Final Test: 86.81
All runs:
Highest Train: 86.82, nan
Highest Valid: 86.60, nan
  Final Train: 86.82, nan
   Final Test: 86.81, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.6364, Train: 84.78%, Valid: 84.81%, Test: 84.93%
Epoch: 25, Loss: 390670.8750, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 107561836049512529920.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 147709371327971328.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 16092249456640.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 1298408608316260352.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 1869215669682176.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 1548664832000.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 37636173824.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 2827893603982901248.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 6439387021705216.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 200252997632.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 42484629504000.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 66202475576688640.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 68243972096.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 37995355169619968.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 316698708148224.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 2702075595063296.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 927961186304.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 477863664943104.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 88.14
Highest Valid: 88.19
  Final Train: 88.14
   Final Test: 88.11
All runs:
Highest Train: 88.14, nan
Highest Valid: 88.19, nan
  Final Train: 88.14, nan
   Final Test: 88.11, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.7664, Train: 14.71%, Valid: 14.72%, Test: 14.72%
Epoch: 25, Loss: 32992.3516, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 59352379555840.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 197048139776.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 1137002.8750, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 841518.6875, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 25029046.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 345.4341, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 893440.0000, Train: 85.78%, Valid: 85.64%, Test: 85.88%
Epoch: 225, Loss: 3472466.2500, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 7.7729, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 72686304.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 312337.4062, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 24714560.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 1397.6139, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 908675712.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 5967743488.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 31348.1406, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 7875948.5000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 213.7345, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.73
Highest Valid: 86.71
  Final Train: 86.73
   Final Test: 86.75
All runs:
Highest Train: 86.73, nan
Highest Valid: 86.71, nan
  Final Train: 86.73, nan
   Final Test: 86.75, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.2426, Train: 84.93%, Valid: 84.79%, Test: 84.98%
Epoch: 25, Loss: 12491625070592.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 1409333657075712.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 80934716047360.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 276426249278712780792266752.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 2502913151184404480.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 1387493039310811168768.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 3908958725179703296.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 5560717183004197257216.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 523154915660843188224.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 398674190507299569664.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 6250928165845884469248.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 403049252334755330916352.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 39367328933532598796288.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 221696482241125810176.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 1573025722097268162560.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 29740637243154890752.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 1365039076731160363008.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 21181449112839042105344.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 261048918615115624873984.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 85.40
Highest Valid: 85.21
  Final Train: 85.40
   Final Test: 85.45
All runs:
Highest Train: 85.40, nan
Highest Valid: 85.21, nan
  Final Train: 85.40, nan
   Final Test: 85.45, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.8241, Train: 49.77%, Valid: 49.78%, Test: 49.75%
Epoch: 25, Loss: 6846227914507559196515780001792.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 101.5060, Train: 14.56%, Valid: 14.73%, Test: 14.54%
Epoch: 75, Loss: 119.0838, Train: 14.52%, Valid: 14.69%, Test: 14.51%
Epoch: 100, Loss: 64.9057, Train: 14.63%, Valid: 14.84%, Test: 14.65%
Epoch: 125, Loss: 2.0682, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 48.0813, Train: 15.30%, Valid: 15.48%, Test: 15.36%
Epoch: 175, Loss: 215.0335, Train: 14.69%, Valid: 14.90%, Test: 14.71%
Epoch: 200, Loss: 60.3848, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 1452375424.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 98.3387, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 189.8732, Train: 14.58%, Valid: 14.75%, Test: 14.57%
Epoch: 300, Loss: 38.7080, Train: 86.87%, Valid: 86.95%, Test: 87.05%
Epoch: 325, Loss: 327.0078, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 74.9663, Train: 14.57%, Valid: 14.74%, Test: 14.55%
Epoch: 375, Loss: 133.5549, Train: 16.43%, Valid: 16.61%, Test: 16.36%
Epoch: 400, Loss: 125.0029, Train: 14.59%, Valid: 14.75%, Test: 14.57%
Epoch: 425, Loss: 100.5532, Train: 14.52%, Valid: 14.69%, Test: 14.51%
Epoch: 450, Loss: 120.4699, Train: 16.43%, Valid: 16.62%, Test: 16.36%
Epoch: 475, Loss: 100.6950, Train: 14.52%, Valid: 14.69%, Test: 14.50%
Run 01:
Highest Train: 87.70
Highest Valid: 87.73
  Final Train: 87.70
   Final Test: 87.70
All runs:
Highest Train: 87.70, nan
Highest Valid: 87.73, nan
  Final Train: 87.70, nan
   Final Test: 87.70, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.5457, Train: 86.57%, Valid: 86.35%, Test: 86.54%
Epoch: 25, Loss: inf, Train: 16.01%, Valid: 16.11%, Test: 15.89%
Epoch: 50, Loss: 84.6059, Train: 14.41%, Valid: 14.39%, Test: 14.36%
Epoch: 75, Loss: 842.6370, Train: 14.50%, Valid: 14.47%, Test: 14.45%
Epoch: 100, Loss: 90.6347, Train: 14.46%, Valid: 14.41%, Test: 14.41%
Epoch: 125, Loss: 95.9246, Train: 15.22%, Valid: 15.33%, Test: 15.09%
Epoch: 150, Loss: 81.8382, Train: 14.63%, Valid: 14.59%, Test: 14.58%
Epoch: 175, Loss: 149.2581, Train: 14.49%, Valid: 14.46%, Test: 14.45%
Epoch: 200, Loss: 36.0662, Train: 16.34%, Valid: 16.32%, Test: 16.25%
Epoch: 225, Loss: 82.4010, Train: 14.57%, Valid: 14.53%, Test: 14.53%
Epoch: 250, Loss: 28.4532, Train: 14.83%, Valid: 14.76%, Test: 14.80%
Epoch: 275, Loss: 122.5246, Train: 50.07%, Valid: 50.09%, Test: 50.08%
Epoch: 300, Loss: 94.0204, Train: 14.57%, Valid: 14.54%, Test: 14.53%
Epoch: 325, Loss: 85.6843, Train: 14.59%, Valid: 14.55%, Test: 14.55%
Epoch: 350, Loss: 79.7592, Train: 14.48%, Valid: 14.44%, Test: 14.43%
Epoch: 375, Loss: 87.0051, Train: 15.41%, Valid: 15.35%, Test: 15.39%
Epoch: 400, Loss: 97.7115, Train: 14.46%, Valid: 14.42%, Test: 14.42%
Epoch: 425, Loss: 56.2202, Train: 14.48%, Valid: 14.45%, Test: 14.44%
Epoch: 450, Loss: 84.8458, Train: 15.46%, Valid: 15.39%, Test: 15.43%
Epoch: 475, Loss: 106.6745, Train: 14.48%, Valid: 14.45%, Test: 14.44%
Run 01:
Highest Train: 86.57
Highest Valid: 86.35
  Final Train: 86.57
   Final Test: 86.54
All runs:
Highest Train: 86.57, nan
Highest Valid: 86.35, nan
  Final Train: 86.57, nan
   Final Test: 86.54, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.0402, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 25, Loss: 2.6172, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.45
Highest Valid: 86.46
  Final Train: 86.45
   Final Test: 86.62
All runs:
Highest Train: 86.45, nan
Highest Valid: 86.46, nan
  Final Train: 86.45, nan
   Final Test: 86.62, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.7483, Train: 15.12%, Valid: 15.11%, Test: 14.97%
Epoch: 25, Loss: 274643.6562, Train: 84.45%, Valid: 84.42%, Test: 84.60%
Epoch: 50, Loss: 761.7181, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 38895.1445, Train: 84.83%, Valid: 84.75%, Test: 84.97%
Epoch: 100, Loss: 4737.0913, Train: 15.27%, Valid: 15.25%, Test: 15.12%
Epoch: 125, Loss: 23795.1699, Train: 84.27%, Valid: 84.06%, Test: 84.37%
Epoch: 150, Loss: 16970.2363, Train: 85.03%, Valid: 85.05%, Test: 85.16%
Epoch: 175, Loss: 18.9571, Train: 16.36%, Valid: 16.57%, Test: 16.25%
Epoch: 200, Loss: 6375.7549, Train: 86.48%, Valid: 86.51%, Test: 86.51%
Epoch: 225, Loss: 3557.3701, Train: 15.56%, Valid: 15.62%, Test: 15.41%
Epoch: 250, Loss: 5.7897, Train: 15.16%, Valid: 15.31%, Test: 15.06%
Epoch: 275, Loss: 618.0026, Train: 86.36%, Valid: 86.39%, Test: 86.41%
Epoch: 300, Loss: 755.7929, Train: 15.66%, Valid: 15.70%, Test: 15.52%
Epoch: 325, Loss: 555.2202, Train: 16.53%, Valid: 16.72%, Test: 16.42%
Epoch: 350, Loss: 449.5562, Train: 16.52%, Valid: 16.71%, Test: 16.40%
Epoch: 375, Loss: 545.4463, Train: 16.53%, Valid: 16.71%, Test: 16.42%
Epoch: 400, Loss: 521.6096, Train: 15.64%, Valid: 15.68%, Test: 15.49%
Epoch: 425, Loss: 580.2326, Train: 16.54%, Valid: 16.72%, Test: 16.42%
Epoch: 450, Loss: 577.8052, Train: 16.48%, Valid: 16.67%, Test: 16.36%
Epoch: 475, Loss: 571.1102, Train: 15.63%, Valid: 15.68%, Test: 15.49%
Run 01:
Highest Train: 87.98
Highest Valid: 88.00
  Final Train: 87.98
   Final Test: 87.96
All runs:
Highest Train: 87.98, nan
Highest Valid: 88.00, nan
  Final Train: 87.98, nan
   Final Test: 87.96, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.4243, Train: 85.48%, Valid: 85.50%, Test: 85.57%
Epoch: 25, Loss: 23321.0879, Train: 85.67%, Valid: 85.56%, Test: 85.81%
Epoch: 50, Loss: 1803960.2500, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 33232002.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 139652.0625, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 6362.1611, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 840854.3750, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 607696.5000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 46643.3789, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 114917.8203, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 1031.3517, Train: 85.05%, Valid: 84.98%, Test: 85.23%
Epoch: 275, Loss: 225.1300, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 39639.4492, Train: 84.66%, Valid: 84.63%, Test: 84.82%
Epoch: 325, Loss: 28.2686, Train: 15.50%, Valid: 15.68%, Test: 15.35%
Epoch: 350, Loss: 234.6014, Train: 86.98%, Valid: 87.00%, Test: 87.01%
Epoch: 375, Loss: 10.5611, Train: 87.01%, Valid: 87.03%, Test: 87.04%
Epoch: 400, Loss: 10.9504, Train: 85.32%, Valid: 85.26%, Test: 85.47%
Epoch: 425, Loss: 10.8250, Train: 86.96%, Valid: 86.99%, Test: 86.99%
Epoch: 450, Loss: 22.1404, Train: 84.84%, Valid: 84.77%, Test: 85.00%
Epoch: 475, Loss: 558.6750, Train: 16.59%, Valid: 16.72%, Test: 16.48%
Run 01:
Highest Train: 87.03
Highest Valid: 87.05
  Final Train: 87.03
   Final Test: 87.06
All runs:
Highest Train: 87.03, nan
Highest Valid: 87.05, nan
  Final Train: 87.03, nan
   Final Test: 87.06, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.6585, Train: 86.13%, Valid: 86.09%, Test: 86.17%
Epoch: 25, Loss: 19.4113, Train: 86.22%, Valid: 86.11%, Test: 86.27%
Epoch: 50, Loss: 16.4135, Train: 84.72%, Valid: 84.51%, Test: 84.84%
Epoch: 75, Loss: 3.9407, Train: 85.21%, Valid: 84.96%, Test: 85.29%
Epoch: 100, Loss: 4.1977, Train: 85.22%, Valid: 84.97%, Test: 85.30%
Epoch: 125, Loss: 3.8782, Train: 85.23%, Valid: 84.99%, Test: 85.31%
Epoch: 150, Loss: 3.9038, Train: 85.25%, Valid: 85.01%, Test: 85.32%
Epoch: 175, Loss: 3.7860, Train: 85.26%, Valid: 85.02%, Test: 85.33%
Epoch: 200, Loss: 3.9010, Train: 85.27%, Valid: 85.03%, Test: 85.34%
Epoch: 225, Loss: 3.8718, Train: 85.28%, Valid: 85.05%, Test: 85.35%
Epoch: 250, Loss: 4.1469, Train: 85.29%, Valid: 85.06%, Test: 85.36%
Epoch: 275, Loss: 3.9403, Train: 85.30%, Valid: 85.07%, Test: 85.37%
Epoch: 300, Loss: 3.9493, Train: 85.32%, Valid: 85.09%, Test: 85.38%
Epoch: 325, Loss: 4.1384, Train: 85.34%, Valid: 85.11%, Test: 85.40%
Epoch: 350, Loss: 3.8113, Train: 85.36%, Valid: 85.13%, Test: 85.41%
Epoch: 375, Loss: 3.8200, Train: 85.38%, Valid: 85.15%, Test: 85.44%
Epoch: 400, Loss: 3.7110, Train: 85.39%, Valid: 85.16%, Test: 85.45%
Epoch: 425, Loss: 3.9166, Train: 85.40%, Valid: 85.18%, Test: 85.46%
Epoch: 450, Loss: 3.5980, Train: 85.42%, Valid: 85.19%, Test: 85.48%
Epoch: 475, Loss: 3.6904, Train: 85.44%, Valid: 85.22%, Test: 85.50%
Run 01:
Highest Train: 86.30
Highest Valid: 86.29
  Final Train: 86.30
   Final Test: 86.35
All runs:
Highest Train: 86.30, nan
Highest Valid: 86.29, nan
  Final Train: 86.30, nan
   Final Test: 86.35, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.5617, Train: 85.67%, Valid: 85.48%, Test: 85.75%
Epoch: 25, Loss: 804.7633, Train: 16.43%, Valid: 16.60%, Test: 16.33%
Epoch: 50, Loss: 6780684288.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 1659818.1250, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 46.0696, Train: 87.62%, Valid: 87.64%, Test: 87.61%
Epoch: 125, Loss: 7.2571, Train: 87.78%, Valid: 87.82%, Test: 87.77%
Epoch: 150, Loss: 7.3398, Train: 87.75%, Valid: 87.81%, Test: 87.76%
Epoch: 175, Loss: 7.1234, Train: 87.86%, Valid: 87.87%, Test: 87.84%
Epoch: 200, Loss: 7.3389, Train: 87.80%, Valid: 87.84%, Test: 87.82%
Epoch: 225, Loss: 7.4224, Train: 87.79%, Valid: 87.80%, Test: 87.77%
Epoch: 250, Loss: 7.3130, Train: 87.75%, Valid: 87.82%, Test: 87.76%
Epoch: 275, Loss: 7.5073, Train: 87.79%, Valid: 87.84%, Test: 87.81%
Epoch: 300, Loss: 7.3777, Train: 87.83%, Valid: 87.89%, Test: 87.82%
Epoch: 325, Loss: 7.4388, Train: 87.84%, Valid: 87.88%, Test: 87.83%
Epoch: 350, Loss: 7.4231, Train: 87.80%, Valid: 87.85%, Test: 87.80%
Epoch: 375, Loss: 7.3218, Train: 87.77%, Valid: 87.80%, Test: 87.77%
Epoch: 400, Loss: 7.2530, Train: 87.73%, Valid: 87.76%, Test: 87.77%
Epoch: 425, Loss: 7.3823, Train: 87.82%, Valid: 87.89%, Test: 87.83%
Epoch: 450, Loss: 7.3697, Train: 87.77%, Valid: 87.84%, Test: 87.77%
Epoch: 475, Loss: 7.3300, Train: 87.76%, Valid: 87.80%, Test: 87.77%
Run 01:
Highest Train: 88.21
Highest Valid: 88.28
  Final Train: 88.21
   Final Test: 88.23
All runs:
Highest Train: 88.21, nan
Highest Valid: 88.28, nan
  Final Train: 88.21, nan
   Final Test: 88.23, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 3.2965, Train: 85.44%, Valid: 85.30%, Test: 85.51%
Epoch: 25, Loss: 1586133598208.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 1343967854592.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 1144760316198912.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 3115666636800.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 3542738534400.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 7297145765888.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 7079101440.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 1001639926300672.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 1407621660672.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 11249622450176.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 68634751795200.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 163714405761024.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 5556323840.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 584667693056.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 8380310528.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 1124614012928.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 140087052468224.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 54984930492416.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 901179834368.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.91
Highest Valid: 86.89
  Final Train: 86.91
   Final Test: 86.97
All runs:
Highest Train: 86.91, nan
Highest Valid: 86.89, nan
  Final Train: 86.91, nan
   Final Test: 86.97, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 5.4477, Train: 86.19%, Valid: 86.08%, Test: 86.30%
Epoch: 25, Loss: 25732.3105, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 239334.8281, Train: 86.10%, Valid: 86.09%, Test: 86.21%
Epoch: 75, Loss: 2636.8503, Train: 83.20%, Valid: 83.13%, Test: 83.34%
Epoch: 100, Loss: 1869.5065, Train: 85.99%, Valid: 85.95%, Test: 86.10%
Epoch: 125, Loss: 71.4149, Train: 49.97%, Valid: 49.96%, Test: 49.96%
Epoch: 150, Loss: 2.7125, Train: 49.98%, Valid: 49.96%, Test: 49.97%
Epoch: 175, Loss: 2333.6401, Train: 85.98%, Valid: 85.95%, Test: 86.10%
Epoch: 200, Loss: 9709.1143, Train: 83.79%, Valid: 83.66%, Test: 83.97%
Epoch: 225, Loss: 4822.7114, Train: 85.99%, Valid: 85.95%, Test: 86.09%
Epoch: 250, Loss: 33.4508, Train: 50.03%, Valid: 50.04%, Test: 50.03%
Epoch: 275, Loss: 25643.8535, Train: 49.99%, Valid: 49.98%, Test: 49.98%
Epoch: 300, Loss: 836.4488, Train: 83.32%, Valid: 83.22%, Test: 83.45%
Epoch: 325, Loss: 21812876.0000, Train: 50.01%, Valid: 50.02%, Test: 50.02%
Epoch: 350, Loss: 68524.0078, Train: 49.99%, Valid: 49.98%, Test: 49.98%
Epoch: 375, Loss: 30.8694, Train: 50.04%, Valid: 50.05%, Test: 50.05%
Epoch: 400, Loss: 7603.4072, Train: 16.63%, Valid: 16.75%, Test: 16.53%
Epoch: 425, Loss: 2.0434, Train: 14.82%, Valid: 15.00%, Test: 14.79%
Epoch: 450, Loss: 43.6069, Train: 85.99%, Valid: 85.96%, Test: 86.11%
Epoch: 475, Loss: 37.7951, Train: 85.92%, Valid: 85.88%, Test: 86.03%
Run 01:
Highest Train: 86.33
Highest Valid: 86.36
  Final Train: 86.33
   Final Test: 86.45
All runs:
Highest Train: 86.33, nan
Highest Valid: 86.36, nan
  Final Train: 86.33, nan
   Final Test: 86.45, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 5.5453, Train: 85.40%, Valid: 85.49%, Test: 85.46%
Epoch: 25, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.74
Highest Valid: 86.77
  Final Train: 86.74
   Final Test: 86.72
All runs:
Highest Train: 86.74, nan
Highest Valid: 86.77, nan
  Final Train: 86.74, nan
   Final Test: 86.72, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 3.8027, Train: 85.94%, Valid: 85.91%, Test: 85.97%
Epoch: 25, Loss: 3.1865, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 6158281614045478912.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 81700656.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 780775129088.0000, Train: 84.24%, Valid: 84.05%, Test: 84.40%
Epoch: 125, Loss: 5185203200.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 72.0385, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 40.6315, Train: 85.72%, Valid: 85.59%, Test: 85.81%
Epoch: 200, Loss: 720470.5000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 5955679744.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 1512.1815, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 63.0155, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 323083.0312, Train: 84.99%, Valid: 84.93%, Test: 85.10%
Epoch: 325, Loss: 112693.1562, Train: 84.15%, Valid: 83.98%, Test: 84.31%
Epoch: 350, Loss: 79.2327, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 62.3964, Train: 85.72%, Valid: 85.60%, Test: 85.82%
Epoch: 400, Loss: 8593972133888.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 91578960.0000, Train: 85.71%, Valid: 85.58%, Test: 85.80%
Epoch: 450, Loss: 31.6634, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 9996437.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.41
Highest Valid: 86.38
  Final Train: 86.41
   Final Test: 86.46
All runs:
Highest Train: 86.41, nan
Highest Valid: 86.38, nan
  Final Train: 86.41, nan
   Final Test: 86.46, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.4978, Train: 85.24%, Valid: 85.17%, Test: 85.29%
Epoch: 25, Loss: 28956637495042229174035021824.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 406995959808.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 24562.0938, Train: 83.55%, Valid: 83.32%, Test: 83.58%
Epoch: 100, Loss: 5013.5205, Train: 17.29%, Valid: 17.13%, Test: 17.16%
Epoch: 125, Loss: 149.8588, Train: 83.57%, Valid: 83.36%, Test: 83.61%
Epoch: 150, Loss: 89825220906450944.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 144.9316, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 198.4951, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 4169847808.0000, Train: 83.26%, Valid: 83.05%, Test: 83.31%
Epoch: 250, Loss: 3676927944556544.0000, Train: 17.16%, Valid: 17.30%, Test: 17.05%
Epoch: 275, Loss: 74443107860480.0000, Train: 49.56%, Valid: 49.57%, Test: 49.55%
Epoch: 300, Loss: 52.5605, Train: 14.84%, Valid: 14.80%, Test: 14.84%
Epoch: 325, Loss: 650526457856.0000, Train: 16.35%, Valid: 16.35%, Test: 16.25%
Epoch: 350, Loss: 432.8564, Train: 15.11%, Valid: 15.07%, Test: 15.08%
Epoch: 375, Loss: 185.5272, Train: 49.57%, Valid: 49.58%, Test: 49.56%
Epoch: 400, Loss: 23061.4336, Train: 14.48%, Valid: 14.47%, Test: 14.44%
Epoch: 425, Loss: 213.3635, Train: 15.81%, Valid: 15.88%, Test: 15.71%
Epoch: 450, Loss: 2397465739264.0000, Train: 14.65%, Valid: 14.64%, Test: 14.61%
Epoch: 475, Loss: 437.9048, Train: 16.03%, Valid: 16.18%, Test: 15.95%
Run 01:
Highest Train: 86.55
Highest Valid: 86.57
  Final Train: 86.55
   Final Test: 86.65
All runs:
Highest Train: 86.55, nan
Highest Valid: 86.57, nan
  Final Train: 86.55, nan
   Final Test: 86.65, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 7.8455, Train: 86.47%, Valid: 86.54%, Test: 86.61%
Epoch: 25, Loss: 12.2483, Train: 86.32%, Valid: 86.33%, Test: 86.36%
Epoch: 50, Loss: 57.4852, Train: 84.05%, Valid: 83.82%, Test: 84.13%
Epoch: 75, Loss: 1395.5149, Train: 83.89%, Valid: 83.66%, Test: 84.00%
Epoch: 100, Loss: 156.9536, Train: 83.65%, Valid: 83.45%, Test: 83.77%
Epoch: 125, Loss: 295.9143, Train: 14.86%, Valid: 14.87%, Test: 14.73%
Epoch: 150, Loss: 239.3934, Train: 14.80%, Valid: 14.81%, Test: 14.66%
Epoch: 175, Loss: 194.5671, Train: 85.67%, Valid: 85.48%, Test: 85.77%
Epoch: 200, Loss: 606.5377, Train: 83.49%, Valid: 83.32%, Test: 83.61%
Epoch: 225, Loss: 6.3462, Train: 83.52%, Valid: 83.34%, Test: 83.64%
Epoch: 250, Loss: 349.5143, Train: 83.50%, Valid: 83.32%, Test: 83.63%
Epoch: 275, Loss: 174.5241, Train: 83.60%, Valid: 83.39%, Test: 83.71%
Epoch: 300, Loss: 505.4604, Train: 83.56%, Valid: 83.36%, Test: 83.67%
Epoch: 325, Loss: 324.8958, Train: 83.50%, Valid: 83.32%, Test: 83.63%
Epoch: 350, Loss: 268.3871, Train: 83.52%, Valid: 83.33%, Test: 83.64%
Epoch: 375, Loss: 613.3088, Train: 83.59%, Valid: 83.39%, Test: 83.71%
Epoch: 400, Loss: 402.0867, Train: 83.53%, Valid: 83.34%, Test: 83.65%
Epoch: 425, Loss: 238.8156, Train: 83.62%, Valid: 83.41%, Test: 83.74%
Epoch: 450, Loss: 221.1737, Train: 83.56%, Valid: 83.36%, Test: 83.67%
Epoch: 475, Loss: 247.2738, Train: 83.48%, Valid: 83.30%, Test: 83.60%
Run 01:
Highest Train: 86.99
Highest Valid: 87.03
  Final Train: 86.99
   Final Test: 87.05
All runs:
Highest Train: 86.99, nan
Highest Valid: 87.03, nan
  Final Train: 86.99, nan
   Final Test: 87.05, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 4.7715, Train: 87.02%, Valid: 87.08%, Test: 87.04%
Epoch: 25, Loss: 2.5090, Train: 87.04%, Valid: 87.07%, Test: 87.06%
Epoch: 50, Loss: 32.1057, Train: 86.65%, Valid: 86.64%, Test: 86.69%
Epoch: 75, Loss: 324.5418, Train: 85.67%, Valid: 85.62%, Test: 85.72%
Epoch: 100, Loss: 214.1419, Train: 85.66%, Valid: 85.60%, Test: 85.71%
Epoch: 125, Loss: 186.7179, Train: 85.66%, Valid: 85.60%, Test: 85.71%
Epoch: 150, Loss: 147.6645, Train: 85.73%, Valid: 85.67%, Test: 85.79%
Epoch: 175, Loss: 171.0615, Train: 85.71%, Valid: 85.66%, Test: 85.77%
Epoch: 200, Loss: 174.5886, Train: 85.75%, Valid: 85.69%, Test: 85.79%
Epoch: 225, Loss: 174.0599, Train: 85.80%, Valid: 85.74%, Test: 85.83%
Epoch: 250, Loss: 166.0958, Train: 85.92%, Valid: 85.85%, Test: 85.95%
Epoch: 275, Loss: 181.6070, Train: 85.96%, Valid: 85.89%, Test: 85.99%
Epoch: 300, Loss: 179.8269, Train: 85.87%, Valid: 85.81%, Test: 85.91%
Epoch: 325, Loss: 194.9721, Train: 85.89%, Valid: 85.82%, Test: 85.92%
Epoch: 350, Loss: 181.8996, Train: 85.85%, Valid: 85.79%, Test: 85.88%
Epoch: 375, Loss: 184.6895, Train: 85.87%, Valid: 85.80%, Test: 85.90%
Epoch: 400, Loss: 188.1283, Train: 85.82%, Valid: 85.76%, Test: 85.85%
Epoch: 425, Loss: 170.3744, Train: 85.78%, Valid: 85.72%, Test: 85.81%
Epoch: 450, Loss: 190.6170, Train: 85.80%, Valid: 85.74%, Test: 85.83%
Epoch: 475, Loss: 171.9110, Train: 85.77%, Valid: 85.72%, Test: 85.81%
Run 01:
Highest Train: 87.11
Highest Valid: 87.14
  Final Train: 87.11
   Final Test: 87.14
All runs:
Highest Train: 87.11, nan
Highest Valid: 87.14, nan
  Final Train: 87.11, nan
   Final Test: 87.14, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.8477, Train: 85.48%, Valid: 85.37%, Test: 85.54%
Epoch: 25, Loss: 2.5645, Train: 86.20%, Valid: 86.10%, Test: 86.24%
Epoch: 50, Loss: 14.6959, Train: 85.42%, Valid: 85.37%, Test: 85.54%
Epoch: 75, Loss: 824.1263, Train: 84.55%, Valid: 84.31%, Test: 84.69%
Epoch: 100, Loss: 137.0252, Train: 84.91%, Valid: 84.75%, Test: 85.03%
Epoch: 125, Loss: 23114.0352, Train: 15.32%, Valid: 15.46%, Test: 15.14%
Epoch: 150, Loss: 1811.4727, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 26.8259, Train: 84.45%, Valid: 84.51%, Test: 84.56%
Epoch: 200, Loss: 16.7758, Train: 88.19%, Valid: 88.27%, Test: 88.20%
Epoch: 225, Loss: 14.8270, Train: 84.35%, Valid: 84.43%, Test: 84.47%
Epoch: 250, Loss: 14.7357, Train: 83.47%, Valid: 83.56%, Test: 83.55%
Epoch: 275, Loss: 17.2577, Train: 84.49%, Valid: 84.54%, Test: 84.60%
Epoch: 300, Loss: 14.4231, Train: 83.99%, Valid: 84.07%, Test: 84.07%
Epoch: 325, Loss: 14.0360, Train: 82.28%, Valid: 82.41%, Test: 82.36%
Epoch: 350, Loss: 15.2297, Train: 84.23%, Valid: 84.30%, Test: 84.34%
Epoch: 375, Loss: 14.7740, Train: 84.33%, Valid: 84.40%, Test: 84.44%
Epoch: 400, Loss: 15.6425, Train: 84.35%, Valid: 84.48%, Test: 84.33%
Epoch: 425, Loss: 14.7467, Train: 84.38%, Valid: 84.45%, Test: 84.49%
Epoch: 450, Loss: 15.3552, Train: 84.38%, Valid: 84.46%, Test: 84.50%
Epoch: 475, Loss: 14.2931, Train: 84.47%, Valid: 84.53%, Test: 84.58%
Run 01:
Highest Train: 88.23
Highest Valid: 88.30
  Final Train: 88.23
   Final Test: 88.23
All runs:
Highest Train: 88.23, nan
Highest Valid: 88.30, nan
  Final Train: 88.23, nan
   Final Test: 88.23, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 6.2022, Train: 87.49%, Valid: 87.58%, Test: 87.60%
Epoch: 25, Loss: 3.1141, Train: 84.51%, Valid: 84.35%, Test: 84.66%
Epoch: 50, Loss: 3.1203, Train: 86.95%, Valid: 87.00%, Test: 87.02%
Epoch: 75, Loss: 2.4693, Train: 86.92%, Valid: 86.98%, Test: 86.99%
Epoch: 100, Loss: 0.4966, Train: 85.74%, Valid: 85.73%, Test: 85.78%
Epoch: 125, Loss: 1.3650, Train: 87.90%, Valid: 87.95%, Test: 87.94%
Epoch: 150, Loss: 0.4090, Train: 85.51%, Valid: 85.54%, Test: 85.53%
Epoch: 175, Loss: 0.3685, Train: 85.72%, Valid: 85.59%, Test: 85.77%
Epoch: 200, Loss: 0.3613, Train: 85.38%, Valid: 85.40%, Test: 85.45%
Epoch: 225, Loss: 0.3493, Train: 85.91%, Valid: 85.90%, Test: 86.02%
Epoch: 250, Loss: 0.3426, Train: 85.77%, Valid: 85.78%, Test: 85.85%
Epoch: 275, Loss: 0.4106, Train: 85.92%, Valid: 85.94%, Test: 85.99%
Epoch: 300, Loss: 0.3427, Train: 85.80%, Valid: 85.79%, Test: 85.90%
Epoch: 325, Loss: 0.3390, Train: 85.26%, Valid: 85.23%, Test: 85.33%
Epoch: 350, Loss: 0.3691, Train: 85.62%, Valid: 85.63%, Test: 85.70%
Epoch: 375, Loss: 1.6581, Train: 85.63%, Valid: 85.67%, Test: 85.73%
Epoch: 400, Loss: 1.1259, Train: 87.38%, Valid: 87.25%, Test: 87.36%
Epoch: 425, Loss: 1.0459, Train: 87.15%, Valid: 87.03%, Test: 87.12%
Epoch: 450, Loss: 0.5471, Train: 87.28%, Valid: 87.16%, Test: 87.27%
Epoch: 475, Loss: 0.3521, Train: 86.03%, Valid: 86.00%, Test: 86.06%
Run 01:
Highest Train: 88.07
Highest Valid: 88.08
  Final Train: 88.07
   Final Test: 88.13
All runs:
Highest Train: 88.07, nan
Highest Valid: 88.08, nan
  Final Train: 88.07, nan
   Final Test: 88.13, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.9152, Train: 85.80%, Valid: 85.68%, Test: 85.86%
Epoch: 25, Loss: 283.8158, Train: 18.52%, Valid: 18.56%, Test: 18.41%
Epoch: 50, Loss: 401.6033, Train: 18.08%, Valid: 18.12%, Test: 17.95%
Epoch: 75, Loss: 2.3886, Train: 85.54%, Valid: 85.38%, Test: 85.64%
Epoch: 100, Loss: 7.8804, Train: 86.27%, Valid: 86.22%, Test: 86.32%
Epoch: 125, Loss: 14.2938, Train: 86.43%, Valid: 86.39%, Test: 86.49%
Epoch: 150, Loss: 9.1129, Train: 86.48%, Valid: 86.43%, Test: 86.52%
Epoch: 175, Loss: 8.5485, Train: 85.97%, Valid: 85.87%, Test: 86.04%
Epoch: 200, Loss: 8.5104, Train: 86.14%, Valid: 86.04%, Test: 86.19%
Epoch: 225, Loss: 8.4557, Train: 86.13%, Valid: 86.03%, Test: 86.17%
Epoch: 250, Loss: 8.3808, Train: 86.11%, Valid: 85.98%, Test: 86.08%
Epoch: 275, Loss: 8.2893, Train: 86.34%, Valid: 86.21%, Test: 86.30%
Epoch: 300, Loss: 7.2966, Train: 86.13%, Valid: 86.02%, Test: 86.14%
Epoch: 325, Loss: 3.8788, Train: 85.81%, Valid: 85.68%, Test: 85.89%
Epoch: 350, Loss: 3.5545, Train: 86.34%, Valid: 86.20%, Test: 86.30%
Epoch: 375, Loss: 3.9047, Train: 86.35%, Valid: 86.22%, Test: 86.33%
Epoch: 400, Loss: 3.5309, Train: 86.38%, Valid: 86.26%, Test: 86.39%
Epoch: 425, Loss: 3.0718, Train: 86.38%, Valid: 86.27%, Test: 86.40%
Epoch: 450, Loss: 2.5713, Train: 86.40%, Valid: 86.29%, Test: 86.43%
Epoch: 475, Loss: 2.0287, Train: 86.43%, Valid: 86.33%, Test: 86.46%
Run 01:
Highest Train: 87.36
Highest Valid: 87.32
  Final Train: 87.36
   Final Test: 87.40
All runs:
Highest Train: 87.36, nan
Highest Valid: 87.32, nan
  Final Train: 87.36, nan
   Final Test: 87.40, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.7384, Train: 85.65%, Valid: 85.46%, Test: 85.70%
Epoch: 25, Loss: 4.0335, Train: 86.73%, Valid: 86.67%, Test: 86.80%
Epoch: 50, Loss: 6.0038, Train: 85.84%, Valid: 85.95%, Test: 85.95%
Epoch: 75, Loss: 5.7619, Train: 86.19%, Valid: 86.22%, Test: 86.28%
Epoch: 100, Loss: 5.7169, Train: 86.22%, Valid: 86.25%, Test: 86.32%
Epoch: 125, Loss: 5.6995, Train: 86.22%, Valid: 86.25%, Test: 86.30%
Epoch: 150, Loss: 5.6940, Train: 86.24%, Valid: 86.26%, Test: 86.31%
Epoch: 175, Loss: 5.6805, Train: 86.24%, Valid: 86.26%, Test: 86.32%
Epoch: 200, Loss: 5.6647, Train: 86.25%, Valid: 86.26%, Test: 86.32%
Epoch: 225, Loss: 5.6380, Train: 86.25%, Valid: 86.26%, Test: 86.32%
Epoch: 250, Loss: 5.6139, Train: 86.26%, Valid: 86.26%, Test: 86.32%
Epoch: 275, Loss: 5.5853, Train: 86.26%, Valid: 86.26%, Test: 86.33%
Epoch: 300, Loss: 5.5686, Train: 86.28%, Valid: 86.28%, Test: 86.34%
Epoch: 325, Loss: 5.5468, Train: 86.28%, Valid: 86.28%, Test: 86.34%
Epoch: 350, Loss: 5.5217, Train: 86.29%, Valid: 86.28%, Test: 86.34%
Epoch: 375, Loss: 5.4976, Train: 86.29%, Valid: 86.28%, Test: 86.34%
Epoch: 400, Loss: 5.4662, Train: 86.29%, Valid: 86.28%, Test: 86.35%
Epoch: 425, Loss: 5.4357, Train: 86.29%, Valid: 86.28%, Test: 86.35%
Epoch: 450, Loss: 5.4044, Train: 86.30%, Valid: 86.28%, Test: 86.35%
Epoch: 475, Loss: 5.3727, Train: 86.30%, Valid: 86.29%, Test: 86.36%
Run 01:
Highest Train: 86.73
Highest Valid: 86.68
  Final Train: 86.73
   Final Test: 86.80
All runs:
Highest Train: 86.73, nan
Highest Valid: 86.68, nan
  Final Train: 86.73, nan
   Final Test: 86.80, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 4.4626, Train: 86.65%, Valid: 86.64%, Test: 86.77%
Epoch: 25, Loss: 5.4091, Train: 84.46%, Valid: 84.26%, Test: 84.57%
Epoch: 50, Loss: 6.5494, Train: 88.34%, Valid: 88.36%, Test: 88.32%
Epoch: 75, Loss: 6.7025, Train: 88.33%, Valid: 88.36%, Test: 88.32%
Epoch: 100, Loss: 6.7131, Train: 88.33%, Valid: 88.36%, Test: 88.32%
Epoch: 125, Loss: 6.7116, Train: 88.33%, Valid: 88.36%, Test: 88.32%
Epoch: 150, Loss: 6.7091, Train: 88.33%, Valid: 88.36%, Test: 88.32%
Epoch: 175, Loss: 6.7090, Train: 88.33%, Valid: 88.36%, Test: 88.32%
Epoch: 200, Loss: 6.7078, Train: 88.33%, Valid: 88.36%, Test: 88.32%
Epoch: 225, Loss: 6.7064, Train: 88.33%, Valid: 88.36%, Test: 88.32%
Epoch: 250, Loss: 6.7049, Train: 88.33%, Valid: 88.36%, Test: 88.32%
Epoch: 275, Loss: 6.7012, Train: 88.33%, Valid: 88.36%, Test: 88.32%
Epoch: 300, Loss: 6.7012, Train: 88.33%, Valid: 88.36%, Test: 88.32%
Epoch: 325, Loss: 6.6985, Train: 88.33%, Valid: 88.36%, Test: 88.31%
Epoch: 350, Loss: 6.6965, Train: 88.33%, Valid: 88.36%, Test: 88.31%
Epoch: 375, Loss: 6.6948, Train: 88.33%, Valid: 88.36%, Test: 88.31%
Epoch: 400, Loss: 6.6924, Train: 88.33%, Valid: 88.36%, Test: 88.31%
Epoch: 425, Loss: 6.6883, Train: 88.33%, Valid: 88.36%, Test: 88.31%
Epoch: 450, Loss: 6.6892, Train: 88.33%, Valid: 88.36%, Test: 88.31%
Epoch: 475, Loss: 6.6855, Train: 88.33%, Valid: 88.35%, Test: 88.31%
Run 01:
Highest Train: 88.35
Highest Valid: 88.37
  Final Train: 88.35
   Final Test: 88.34
All runs:
Highest Train: 88.35, nan
Highest Valid: 88.37, nan
  Final Train: 88.35, nan
   Final Test: 88.34, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.9729, Train: 86.85%, Valid: 86.83%, Test: 86.86%
Epoch: 25, Loss: 3.3697, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 97.0912, Train: 85.45%, Valid: 85.34%, Test: 85.59%
Epoch: 75, Loss: 345.1714, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 724238784.0000, Train: 84.00%, Valid: 83.86%, Test: 84.15%
Epoch: 125, Loss: 265778112.0000, Train: 84.90%, Valid: 84.83%, Test: 84.96%
Epoch: 150, Loss: 553.0385, Train: 85.66%, Valid: 85.54%, Test: 85.78%
Epoch: 175, Loss: 409.9658, Train: 85.52%, Valid: 85.39%, Test: 85.66%
Epoch: 200, Loss: 467.5048, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 568.5778, Train: 16.73%, Valid: 16.69%, Test: 16.67%
Epoch: 250, Loss: 289.0729, Train: 85.51%, Valid: 85.37%, Test: 85.64%
Epoch: 275, Loss: 396.8051, Train: 84.13%, Valid: 83.99%, Test: 84.29%
Epoch: 300, Loss: 94899.6094, Train: 15.06%, Valid: 14.96%, Test: 15.02%
Epoch: 325, Loss: 564.3613, Train: 85.55%, Valid: 85.41%, Test: 85.68%
Epoch: 350, Loss: 58.3731, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 15.2774, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.85
Highest Valid: 86.83
  Final Train: 86.85
   Final Test: 86.86
All runs:
Highest Train: 86.85, nan
Highest Valid: 86.83, nan
  Final Train: 86.85, nan
   Final Test: 86.86, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.5737, Train: 15.15%, Valid: 15.24%, Test: 15.11%
Epoch: 25, Loss: 471.0071, Train: 15.05%, Valid: 15.19%, Test: 14.96%
Epoch: 50, Loss: 251.4391, Train: 82.95%, Valid: 82.96%, Test: 83.09%
Epoch: 75, Loss: 183.5328, Train: 85.31%, Valid: 85.12%, Test: 85.38%
Epoch: 100, Loss: 192.0556, Train: 85.33%, Valid: 85.14%, Test: 85.40%
Epoch: 125, Loss: 136.9829, Train: 85.34%, Valid: 85.15%, Test: 85.40%
Epoch: 150, Loss: 198.4094, Train: 85.30%, Valid: 85.11%, Test: 85.37%
Epoch: 175, Loss: 191.4036, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 201.0150, Train: 85.26%, Valid: 85.06%, Test: 85.33%
Epoch: 225, Loss: 190.7708, Train: 85.31%, Valid: 85.12%, Test: 85.38%
Epoch: 250, Loss: 3.3669, Train: 85.29%, Valid: 85.10%, Test: 85.36%
Epoch: 275, Loss: 145.0518, Train: 85.21%, Valid: 85.01%, Test: 85.28%
Epoch: 300, Loss: 209.2705, Train: 85.27%, Valid: 85.07%, Test: 85.33%
Epoch: 325, Loss: 196.8425, Train: 85.31%, Valid: 85.11%, Test: 85.37%
Epoch: 350, Loss: 182.6727, Train: 85.31%, Valid: 85.12%, Test: 85.38%
Epoch: 375, Loss: 192.2885, Train: 85.32%, Valid: 85.13%, Test: 85.38%
Epoch: 400, Loss: 193.8198, Train: 85.30%, Valid: 85.10%, Test: 85.36%
Epoch: 425, Loss: 203.1246, Train: 86.63%, Valid: 86.63%, Test: 86.76%
Epoch: 450, Loss: 185.2911, Train: 85.26%, Valid: 85.07%, Test: 85.32%
Epoch: 475, Loss: 180.7576, Train: 85.29%, Valid: 85.10%, Test: 85.35%
Run 01:
Highest Train: 86.63
Highest Valid: 86.63
  Final Train: 86.63
   Final Test: 86.76
All runs:
Highest Train: 86.63, nan
Highest Valid: 86.63, nan
  Final Train: 86.63, nan
   Final Test: 86.76, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.4548, Train: 14.52%, Valid: 14.67%, Test: 14.48%
Epoch: 25, Loss: 19.1628, Train: 16.23%, Valid: 16.43%, Test: 16.13%
Epoch: 50, Loss: 44.7704, Train: 16.06%, Valid: 16.24%, Test: 15.95%
Epoch: 75, Loss: 4.4172, Train: 88.17%, Valid: 88.22%, Test: 88.22%
Epoch: 100, Loss: 1.8286, Train: 88.15%, Valid: 88.21%, Test: 88.21%
Epoch: 125, Loss: 1.6034, Train: 87.53%, Valid: 87.63%, Test: 87.54%
Epoch: 150, Loss: 2.2709, Train: 88.16%, Valid: 88.23%, Test: 88.22%
Epoch: 175, Loss: 1.1956, Train: 88.16%, Valid: 88.23%, Test: 88.22%
Epoch: 200, Loss: 2.9569, Train: 88.16%, Valid: 88.22%, Test: 88.21%
Epoch: 225, Loss: 0.8444, Train: 87.93%, Valid: 87.97%, Test: 87.97%
Epoch: 250, Loss: 1.2939, Train: 88.16%, Valid: 88.22%, Test: 88.22%
Epoch: 275, Loss: 0.7844, Train: 88.17%, Valid: 88.23%, Test: 88.23%
Epoch: 300, Loss: 7.7543, Train: 88.17%, Valid: 88.23%, Test: 88.24%
Epoch: 325, Loss: 68.3076, Train: 88.18%, Valid: 88.25%, Test: 88.25%
Epoch: 350, Loss: 2.5412, Train: 88.16%, Valid: 88.22%, Test: 88.23%
Epoch: 375, Loss: 3.0412, Train: 88.17%, Valid: 88.23%, Test: 88.24%
Epoch: 400, Loss: 3.4997, Train: 14.42%, Valid: 14.60%, Test: 14.39%
Epoch: 425, Loss: 1.3970, Train: 16.94%, Valid: 17.09%, Test: 16.92%
Epoch: 450, Loss: 2.2754, Train: 16.38%, Valid: 16.58%, Test: 16.29%
Epoch: 475, Loss: 1.3324, Train: 88.15%, Valid: 88.20%, Test: 88.23%
Run 01:
Highest Train: 88.26
Highest Valid: 88.34
  Final Train: 88.26
   Final Test: 88.32
All runs:
Highest Train: 88.26, nan
Highest Valid: 88.34, nan
  Final Train: 88.26, nan
   Final Test: 88.32, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.9210, Train: 86.68%, Valid: 86.68%, Test: 86.75%
Epoch: 25, Loss: 2649.5066, Train: 16.08%, Valid: 16.21%, Test: 15.90%
Epoch: 50, Loss: 851.5544, Train: 15.71%, Valid: 15.83%, Test: 15.54%
Epoch: 75, Loss: 484.6709, Train: 15.77%, Valid: 15.89%, Test: 15.60%
Epoch: 100, Loss: 8.7271, Train: 86.57%, Valid: 86.58%, Test: 86.62%
Epoch: 125, Loss: 35.4882, Train: 15.63%, Valid: 15.75%, Test: 15.46%
Epoch: 150, Loss: 473.2176, Train: 15.62%, Valid: 15.74%, Test: 15.45%
Epoch: 175, Loss: 2.6162, Train: 86.47%, Valid: 86.51%, Test: 86.53%
Epoch: 200, Loss: 60.0307, Train: 86.45%, Valid: 86.50%, Test: 86.51%
Epoch: 225, Loss: 380.5662, Train: 15.64%, Valid: 15.77%, Test: 15.47%
Epoch: 250, Loss: 409.5844, Train: 15.64%, Valid: 15.76%, Test: 15.47%
Epoch: 275, Loss: 261.4174, Train: 15.74%, Valid: 15.86%, Test: 15.58%
Epoch: 300, Loss: 261.4700, Train: 15.76%, Valid: 15.89%, Test: 15.59%
Epoch: 325, Loss: 109.5018, Train: 14.78%, Valid: 14.92%, Test: 14.73%
Epoch: 350, Loss: 17.4328, Train: 15.56%, Valid: 15.65%, Test: 15.50%
Epoch: 375, Loss: 79.2480, Train: 13.96%, Valid: 14.09%, Test: 13.86%
Epoch: 400, Loss: 51.6706, Train: 14.51%, Valid: 14.61%, Test: 14.42%
Epoch: 425, Loss: 81.2955, Train: 15.36%, Valid: 15.50%, Test: 15.31%
Epoch: 450, Loss: 32.6677, Train: 14.50%, Valid: 14.64%, Test: 14.40%
Epoch: 475, Loss: 50.7897, Train: 14.63%, Valid: 14.78%, Test: 14.57%
Run 01:
Highest Train: 86.91
Highest Valid: 86.92
  Final Train: 86.91
   Final Test: 86.93
All runs:
Highest Train: 86.91, nan
Highest Valid: 86.92, nan
  Final Train: 86.91, nan
   Final Test: 86.93, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.5288, Train: 85.27%, Valid: 85.30%, Test: 85.38%
Epoch: 25, Loss: 2.3331, Train: 85.93%, Valid: 85.79%, Test: 85.97%
Epoch: 50, Loss: 1.8233, Train: 86.90%, Valid: 86.99%, Test: 87.00%
Epoch: 75, Loss: 1969.1809, Train: 15.21%, Valid: 15.45%, Test: 15.12%
Epoch: 100, Loss: 2.3983, Train: 86.98%, Valid: 87.05%, Test: 87.10%
Epoch: 125, Loss: 103.0076, Train: 15.13%, Valid: 15.40%, Test: 15.05%
Epoch: 150, Loss: 3.2808, Train: 13.25%, Valid: 13.47%, Test: 13.23%
Epoch: 175, Loss: 2.9521, Train: 86.89%, Valid: 86.91%, Test: 87.00%
Epoch: 200, Loss: 3.1620, Train: 86.99%, Valid: 87.07%, Test: 87.12%
Epoch: 225, Loss: 12.9079, Train: 86.92%, Valid: 86.95%, Test: 87.04%
Epoch: 250, Loss: 2.9581, Train: 87.00%, Valid: 87.08%, Test: 87.12%
Epoch: 275, Loss: 3.1165, Train: 87.01%, Valid: 87.09%, Test: 87.12%
Epoch: 300, Loss: 3.1093, Train: 86.97%, Valid: 87.03%, Test: 87.10%
Epoch: 325, Loss: 2.9675, Train: 86.97%, Valid: 87.04%, Test: 87.10%
Epoch: 350, Loss: 2.9502, Train: 87.05%, Valid: 87.14%, Test: 87.16%
Epoch: 375, Loss: 2.9744, Train: 87.06%, Valid: 87.15%, Test: 87.16%
Epoch: 400, Loss: 2.9195, Train: 86.97%, Valid: 87.04%, Test: 87.10%
Epoch: 425, Loss: 3.1825, Train: 86.99%, Valid: 87.07%, Test: 87.12%
Epoch: 450, Loss: 3.0625, Train: 87.04%, Valid: 87.13%, Test: 87.15%
Epoch: 475, Loss: 3.2262, Train: 86.91%, Valid: 86.94%, Test: 87.02%
Run 01:
Highest Train: 87.28
Highest Valid: 87.22
  Final Train: 87.13
   Final Test: 87.25
All runs:
Highest Train: 87.28, nan
Highest Valid: 87.22, nan
  Final Train: 87.13, nan
   Final Test: 87.25, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.5390, Train: 85.66%, Valid: 85.48%, Test: 85.82%
Epoch: 25, Loss: 0.6273, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 4790500221517824.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 1580999245824.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 16972238848.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 216475629518848.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 97399196352512.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 3313353984.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 220111252226048.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 92369240064.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 2344579328.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 373547566366720.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 236751.8594, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 8756947968.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 76084903936.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 281247973376.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 25946266.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 177090789376.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 4598045184.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 99070718771200.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 87.17
Highest Valid: 87.25
  Final Train: 87.17
   Final Test: 87.27
All runs:
Highest Train: 87.17, nan
Highest Valid: 87.25, nan
  Final Train: 87.17, nan
   Final Test: 87.27, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.7045, Train: 84.83%, Valid: 84.76%, Test: 84.97%
Epoch: 25, Loss: 2402.8469, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 87.7965, Train: 15.28%, Valid: 15.31%, Test: 15.20%
Epoch: 75, Loss: 110.9294, Train: 15.55%, Valid: 15.58%, Test: 15.53%
Epoch: 100, Loss: 91.6491, Train: 15.33%, Valid: 15.37%, Test: 15.32%
Epoch: 125, Loss: 2036.5947, Train: 15.67%, Valid: 15.68%, Test: 15.61%
Epoch: 150, Loss: 78.0422, Train: 86.22%, Valid: 86.19%, Test: 86.34%
Epoch: 175, Loss: 58.8535, Train: 16.54%, Valid: 16.52%, Test: 16.50%
Epoch: 200, Loss: 145.9046, Train: 16.51%, Valid: 16.59%, Test: 16.49%
Epoch: 225, Loss: 8921.1748, Train: 84.93%, Valid: 84.96%, Test: 85.04%
Epoch: 250, Loss: 84.3730, Train: 85.66%, Valid: 85.65%, Test: 85.75%
Epoch: 275, Loss: 26.7407, Train: 15.02%, Valid: 15.03%, Test: 15.02%
Epoch: 300, Loss: 79.7853, Train: 14.92%, Valid: 14.88%, Test: 14.91%
Epoch: 325, Loss: 59.8801, Train: 86.19%, Valid: 86.16%, Test: 86.30%
Epoch: 350, Loss: 34.8392, Train: 85.48%, Valid: 85.50%, Test: 85.57%
Epoch: 375, Loss: 37.2865, Train: 15.70%, Valid: 15.74%, Test: 15.68%
Epoch: 400, Loss: 198.2668, Train: 86.14%, Valid: 86.08%, Test: 86.23%
Epoch: 425, Loss: 65.3202, Train: 15.46%, Valid: 15.50%, Test: 15.45%
Epoch: 450, Loss: 150.4351, Train: 15.29%, Valid: 15.34%, Test: 15.28%
Epoch: 475, Loss: 222.6377, Train: 15.46%, Valid: 15.51%, Test: 15.46%
Run 01:
Highest Train: 86.38
Highest Valid: 86.33
  Final Train: 86.38
   Final Test: 86.49
All runs:
Highest Train: 86.38, nan
Highest Valid: 86.33, nan
  Final Train: 86.38, nan
   Final Test: 86.49, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 3.7066, Train: 82.16%, Valid: 82.36%, Test: 82.44%
Epoch: 25, Loss: 862.2932, Train: 14.63%, Valid: 14.81%, Test: 14.60%
Epoch: 50, Loss: 83.0476, Train: 14.42%, Valid: 14.60%, Test: 14.38%
Epoch: 75, Loss: 86.3271, Train: 14.48%, Valid: 14.66%, Test: 14.43%
Epoch: 100, Loss: 85.9086, Train: 14.48%, Valid: 14.65%, Test: 14.42%
Epoch: 125, Loss: 87.7843, Train: 14.44%, Valid: 14.61%, Test: 14.39%
Epoch: 150, Loss: 88.0564, Train: 14.48%, Valid: 14.67%, Test: 14.43%
Epoch: 175, Loss: 86.9350, Train: 14.44%, Valid: 14.61%, Test: 14.40%
Epoch: 200, Loss: 86.2720, Train: 14.50%, Valid: 14.67%, Test: 14.45%
Epoch: 225, Loss: 86.3255, Train: 14.45%, Valid: 14.62%, Test: 14.40%
Epoch: 250, Loss: 86.5285, Train: 14.50%, Valid: 14.67%, Test: 14.45%
Epoch: 275, Loss: 86.3602, Train: 14.54%, Valid: 14.71%, Test: 14.50%
Epoch: 300, Loss: 85.8627, Train: 14.47%, Valid: 14.65%, Test: 14.42%
Epoch: 325, Loss: 87.4840, Train: 14.50%, Valid: 14.66%, Test: 14.44%
Epoch: 350, Loss: 87.3634, Train: 14.46%, Valid: 14.63%, Test: 14.42%
Epoch: 375, Loss: 86.2988, Train: 14.44%, Valid: 14.61%, Test: 14.40%
Epoch: 400, Loss: 87.7241, Train: 14.46%, Valid: 14.61%, Test: 14.40%
Epoch: 425, Loss: 86.9255, Train: 14.45%, Valid: 14.61%, Test: 14.40%
Epoch: 450, Loss: 86.8421, Train: 14.52%, Valid: 14.70%, Test: 14.48%
Epoch: 475, Loss: 87.7861, Train: 14.51%, Valid: 14.68%, Test: 14.46%
Run 01:
Highest Train: 84.91
Highest Valid: 84.95
  Final Train: 84.91
   Final Test: 84.97
All runs:
Highest Train: 84.91, nan
Highest Valid: 84.95, nan
  Final Train: 84.91, nan
   Final Test: 84.97, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.5840, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 25, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 88.16
Highest Valid: 88.23
  Final Train: 88.16
   Final Test: 88.20
All runs:
Highest Train: 88.16, nan
Highest Valid: 88.23, nan
  Final Train: 88.16, nan
   Final Test: 88.20, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.0222, Train: 86.59%, Valid: 86.57%, Test: 86.61%
Epoch: 25, Loss: 35536781312.0000, Train: 85.51%, Valid: 85.39%, Test: 85.62%
Epoch: 50, Loss: 208291840.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 118358573056.0000, Train: 85.70%, Valid: 85.59%, Test: 85.82%
Epoch: 100, Loss: 684476156140847104.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 19.1297, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 133701713920.0000, Train: 85.70%, Valid: 85.59%, Test: 85.82%
Epoch: 175, Loss: 0.9992, Train: 15.68%, Valid: 15.73%, Test: 15.60%
Epoch: 200, Loss: 0.8385, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 3262262269605576704.0000, Train: 12.91%, Valid: 12.93%, Test: 12.90%
Epoch: 250, Loss: 138.3591, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 0.7810, Train: 85.72%, Valid: 85.60%, Test: 85.84%
Epoch: 300, Loss: 3968848101376.0000, Train: 85.69%, Valid: 85.58%, Test: 85.82%
Epoch: 325, Loss: 1.4914, Train: 85.70%, Valid: 85.59%, Test: 85.83%
Epoch: 350, Loss: 0.7163, Train: 84.21%, Valid: 84.08%, Test: 84.38%
Epoch: 375, Loss: 0.9099, Train: 85.67%, Valid: 85.56%, Test: 85.79%
Epoch: 400, Loss: 84114.2422, Train: 85.72%, Valid: 85.59%, Test: 85.82%
Epoch: 425, Loss: 1.1290, Train: 85.69%, Valid: 85.57%, Test: 85.80%
Epoch: 450, Loss: 0.8276, Train: 85.68%, Valid: 85.56%, Test: 85.80%
Epoch: 475, Loss: 14227.2344, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.86
Highest Valid: 86.82
  Final Train: 86.86
   Final Test: 86.86
All runs:
Highest Train: 86.86, nan
Highest Valid: 86.82, nan
  Final Train: 86.86, nan
   Final Test: 86.86, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.2819, Train: 84.95%, Valid: 84.81%, Test: 85.02%
Epoch: 25, Loss: 28.9870, Train: 84.11%, Valid: 84.12%, Test: 84.22%
Epoch: 50, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 1.3363, Train: 86.94%, Valid: 87.00%, Test: 87.00%
Epoch: 125, Loss: 2.0781, Train: 86.90%, Valid: 86.97%, Test: 86.97%
Epoch: 150, Loss: 2.1672, Train: 86.90%, Valid: 86.97%, Test: 86.97%
Epoch: 175, Loss: 2.1783, Train: 86.90%, Valid: 86.97%, Test: 86.97%
Epoch: 200, Loss: 2.1808, Train: 86.90%, Valid: 86.97%, Test: 86.97%
Epoch: 225, Loss: 2.1792, Train: 86.90%, Valid: 86.97%, Test: 86.97%
Epoch: 250, Loss: 2.1812, Train: 86.90%, Valid: 86.97%, Test: 86.97%
Epoch: 275, Loss: 2.1799, Train: 86.90%, Valid: 86.97%, Test: 86.97%
Epoch: 300, Loss: 2.1800, Train: 86.90%, Valid: 86.97%, Test: 86.97%
Epoch: 325, Loss: 2.1796, Train: 86.90%, Valid: 86.97%, Test: 86.97%
Epoch: 350, Loss: 2.1792, Train: 86.90%, Valid: 86.97%, Test: 86.97%
Epoch: 375, Loss: 2.1753, Train: 86.90%, Valid: 86.97%, Test: 86.97%
Epoch: 400, Loss: 2.1808, Train: 86.90%, Valid: 86.97%, Test: 86.97%
Epoch: 425, Loss: 2.1781, Train: 86.90%, Valid: 86.97%, Test: 86.97%
Epoch: 450, Loss: 2.1778, Train: 86.90%, Valid: 86.97%, Test: 86.97%
Epoch: 475, Loss: 2.1778, Train: 86.90%, Valid: 86.97%, Test: 86.97%
Run 01:
Highest Train: 86.94
Highest Valid: 87.00
  Final Train: 86.94
   Final Test: 87.00
All runs:
Highest Train: 86.94, nan
Highest Valid: 87.00, nan
  Final Train: 86.94, nan
   Final Test: 87.00, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 5.1732, Train: 85.34%, Valid: 85.19%, Test: 85.46%
Epoch: 25, Loss: 2626.9438, Train: 16.04%, Valid: 16.23%, Test: 15.90%
Epoch: 50, Loss: 20.8122, Train: 86.95%, Valid: 87.01%, Test: 86.99%
Epoch: 75, Loss: 2.6410, Train: 87.41%, Valid: 87.45%, Test: 87.44%
Epoch: 100, Loss: 2.7704, Train: 87.46%, Valid: 87.51%, Test: 87.49%
Epoch: 125, Loss: 2.6947, Train: 87.47%, Valid: 87.51%, Test: 87.49%
Epoch: 150, Loss: 2.7580, Train: 87.47%, Valid: 87.51%, Test: 87.49%
Epoch: 175, Loss: 2.7756, Train: 87.47%, Valid: 87.51%, Test: 87.49%
Epoch: 200, Loss: 2.7144, Train: 87.46%, Valid: 87.51%, Test: 87.49%
Epoch: 225, Loss: 2.7768, Train: 87.46%, Valid: 87.51%, Test: 87.49%
Epoch: 250, Loss: 2.8216, Train: 87.47%, Valid: 87.51%, Test: 87.49%
Epoch: 275, Loss: 2.7792, Train: 87.47%, Valid: 87.52%, Test: 87.50%
Epoch: 300, Loss: 2.7389, Train: 87.47%, Valid: 87.52%, Test: 87.50%
Epoch: 325, Loss: 2.7873, Train: 87.47%, Valid: 87.52%, Test: 87.50%
Epoch: 350, Loss: 2.7629, Train: 87.47%, Valid: 87.52%, Test: 87.50%
Epoch: 375, Loss: 2.7644, Train: 87.47%, Valid: 87.52%, Test: 87.50%
Epoch: 400, Loss: 2.7926, Train: 87.47%, Valid: 87.52%, Test: 87.50%
Epoch: 425, Loss: 2.7475, Train: 87.47%, Valid: 87.52%, Test: 87.50%
Epoch: 450, Loss: 2.7695, Train: 87.47%, Valid: 87.52%, Test: 87.50%
Epoch: 475, Loss: 2.8035, Train: 87.47%, Valid: 87.52%, Test: 87.51%
Run 01:
Highest Train: 87.91
Highest Valid: 87.93
  Final Train: 87.91
   Final Test: 87.91
All runs:
Highest Train: 87.91, nan
Highest Valid: 87.93, nan
  Final Train: 87.91, nan
   Final Test: 87.91, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.6460, Train: 85.67%, Valid: 85.55%, Test: 85.77%
Epoch: 25, Loss: 2.6216, Train: 86.84%, Valid: 86.83%, Test: 86.89%
Epoch: 50, Loss: 936.0781, Train: 85.88%, Valid: 85.85%, Test: 85.94%
Epoch: 75, Loss: 3.3804, Train: 85.58%, Valid: 85.45%, Test: 85.71%
Epoch: 100, Loss: 2.4590, Train: 85.64%, Valid: 85.50%, Test: 85.74%
Epoch: 125, Loss: 6.4166, Train: 85.68%, Valid: 85.54%, Test: 85.78%
Epoch: 150, Loss: 8.3788, Train: 86.80%, Valid: 86.78%, Test: 86.81%
Epoch: 175, Loss: 65.5834, Train: 86.81%, Valid: 86.81%, Test: 86.81%
Epoch: 200, Loss: 20.7908, Train: 86.82%, Valid: 86.82%, Test: 86.83%
Epoch: 225, Loss: 15.1543, Train: 86.82%, Valid: 86.83%, Test: 86.83%
Epoch: 250, Loss: 7.9346, Train: 86.83%, Valid: 86.84%, Test: 86.84%
Epoch: 275, Loss: 7.4962, Train: 86.84%, Valid: 86.84%, Test: 86.85%
Epoch: 300, Loss: 7.2917, Train: 86.86%, Valid: 86.86%, Test: 86.87%
Epoch: 325, Loss: 7.0195, Train: 86.86%, Valid: 86.86%, Test: 86.87%
Epoch: 350, Loss: 6.7989, Train: 86.85%, Valid: 86.86%, Test: 86.87%
Epoch: 375, Loss: 5.3390, Train: 86.75%, Valid: 86.75%, Test: 86.78%
Epoch: 400, Loss: 1.8301, Train: 86.70%, Valid: 86.73%, Test: 86.76%
Epoch: 425, Loss: 1.9264, Train: 86.78%, Valid: 86.81%, Test: 86.84%
Epoch: 450, Loss: 1.3589, Train: 86.83%, Valid: 86.84%, Test: 86.90%
Epoch: 475, Loss: 2.0466, Train: 86.79%, Valid: 86.83%, Test: 86.84%
Run 01:
Highest Train: 87.31
Highest Valid: 87.32
  Final Train: 87.31
   Final Test: 87.42
All runs:
Highest Train: 87.31, nan
Highest Valid: 87.32, nan
  Final Train: 87.31, nan
   Final Test: 87.42, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.8624, Train: 17.64%, Valid: 17.44%, Test: 17.44%
Epoch: 25, Loss: 2.9099, Train: 85.32%, Valid: 85.09%, Test: 85.37%
Epoch: 50, Loss: 4.2980, Train: 85.33%, Valid: 85.11%, Test: 85.38%
Epoch: 75, Loss: 37.3647, Train: 85.30%, Valid: 85.09%, Test: 85.35%
Epoch: 100, Loss: 21.7490, Train: 85.31%, Valid: 85.09%, Test: 85.36%
Epoch: 125, Loss: 18.0508, Train: 85.31%, Valid: 85.10%, Test: 85.37%
Epoch: 150, Loss: 15.9864, Train: 85.32%, Valid: 85.11%, Test: 85.38%
Epoch: 175, Loss: 17.8763, Train: 85.33%, Valid: 85.12%, Test: 85.38%
Epoch: 200, Loss: 18.1794, Train: 85.33%, Valid: 85.13%, Test: 85.39%
Epoch: 225, Loss: 18.4476, Train: 85.34%, Valid: 85.13%, Test: 85.39%
Epoch: 250, Loss: 22.9883, Train: 85.34%, Valid: 85.14%, Test: 85.39%
Epoch: 275, Loss: 17.8387, Train: 85.34%, Valid: 85.13%, Test: 85.39%
Epoch: 300, Loss: 21.0199, Train: 85.33%, Valid: 85.13%, Test: 85.38%
Epoch: 325, Loss: 29.9177, Train: 85.33%, Valid: 85.12%, Test: 85.38%
Epoch: 350, Loss: 19.0274, Train: 85.32%, Valid: 85.11%, Test: 85.37%
Epoch: 375, Loss: 19.9742, Train: 85.32%, Valid: 85.11%, Test: 85.37%
Epoch: 400, Loss: 18.1845, Train: 85.30%, Valid: 85.09%, Test: 85.35%
Epoch: 425, Loss: 22.0597, Train: 85.30%, Valid: 85.09%, Test: 85.35%
Epoch: 450, Loss: 18.3692, Train: 85.28%, Valid: 85.07%, Test: 85.34%
Epoch: 475, Loss: 18.5491, Train: 85.27%, Valid: 85.06%, Test: 85.33%
Run 01:
Highest Train: 85.40
Highest Valid: 85.16
  Final Train: 85.40
   Final Test: 85.44
All runs:
Highest Train: 85.40, nan
Highest Valid: 85.16, nan
  Final Train: 85.40, nan
   Final Test: 85.44, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.1539, Train: 84.97%, Valid: 84.81%, Test: 85.02%
Epoch: 25, Loss: 63.8317, Train: 83.89%, Valid: 83.73%, Test: 84.01%
Epoch: 50, Loss: 22.3881, Train: 84.33%, Valid: 84.16%, Test: 84.47%
Epoch: 75, Loss: 2.4219, Train: 84.34%, Valid: 84.17%, Test: 84.48%
Epoch: 100, Loss: 2.4414, Train: 84.35%, Valid: 84.18%, Test: 84.50%
Epoch: 125, Loss: 2.4310, Train: 84.36%, Valid: 84.20%, Test: 84.51%
Epoch: 150, Loss: 2.4151, Train: 84.37%, Valid: 84.21%, Test: 84.52%
Epoch: 175, Loss: 2.3996, Train: 84.38%, Valid: 84.21%, Test: 84.53%
Epoch: 200, Loss: 2.3866, Train: 84.39%, Valid: 84.21%, Test: 84.53%
Epoch: 225, Loss: 2.3684, Train: 84.40%, Valid: 84.23%, Test: 84.54%
Epoch: 250, Loss: 2.3562, Train: 84.41%, Valid: 84.24%, Test: 84.55%
Epoch: 275, Loss: 2.3426, Train: 84.42%, Valid: 84.25%, Test: 84.56%
Epoch: 300, Loss: 2.3267, Train: 84.43%, Valid: 84.27%, Test: 84.58%
Epoch: 325, Loss: 2.3128, Train: 84.46%, Valid: 84.29%, Test: 84.60%
Epoch: 350, Loss: 2.3014, Train: 84.50%, Valid: 84.32%, Test: 84.63%
Epoch: 375, Loss: 2.2879, Train: 84.55%, Valid: 84.36%, Test: 84.66%
Epoch: 400, Loss: 2.2735, Train: 84.63%, Valid: 84.45%, Test: 84.74%
Epoch: 425, Loss: 2.2603, Train: 84.73%, Valid: 84.56%, Test: 84.83%
Epoch: 450, Loss: 2.2483, Train: 84.83%, Valid: 84.67%, Test: 84.93%
Epoch: 475, Loss: 2.2349, Train: 84.93%, Valid: 84.75%, Test: 85.02%
Run 01:
Highest Train: 85.85
Highest Valid: 85.89
  Final Train: 85.85
   Final Test: 85.88
All runs:
Highest Train: 85.85, nan
Highest Valid: 85.89, nan
  Final Train: 85.85, nan
   Final Test: 85.88, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 4.3198, Train: 85.33%, Valid: 85.18%, Test: 85.33%
Epoch: 25, Loss: 2.8393, Train: 86.69%, Valid: 86.61%, Test: 86.77%
Epoch: 50, Loss: 3.0906, Train: 86.28%, Valid: 86.21%, Test: 86.31%
Epoch: 75, Loss: 2.2552, Train: 86.34%, Valid: 86.27%, Test: 86.37%
Epoch: 100, Loss: 0.6067, Train: 86.68%, Valid: 86.60%, Test: 86.73%
Epoch: 125, Loss: 0.8851, Train: 86.74%, Valid: 86.66%, Test: 86.80%
Epoch: 150, Loss: 0.7863, Train: 86.51%, Valid: 86.44%, Test: 86.53%
Epoch: 175, Loss: 0.6420, Train: 86.58%, Valid: 86.49%, Test: 86.60%
Epoch: 200, Loss: 0.4774, Train: 86.67%, Valid: 86.56%, Test: 86.72%
Epoch: 225, Loss: 0.3760, Train: 86.08%, Valid: 86.06%, Test: 86.13%
Epoch: 250, Loss: 0.3641, Train: 86.02%, Valid: 85.88%, Test: 86.07%
Epoch: 275, Loss: 0.3609, Train: 85.70%, Valid: 85.54%, Test: 85.78%
Epoch: 300, Loss: 0.3576, Train: 85.73%, Valid: 85.57%, Test: 85.83%
Epoch: 325, Loss: 0.3536, Train: 85.72%, Valid: 85.56%, Test: 85.82%
Epoch: 350, Loss: 0.3495, Train: 85.67%, Valid: 85.53%, Test: 85.78%
Epoch: 375, Loss: 0.3463, Train: 85.70%, Valid: 85.57%, Test: 85.83%
Epoch: 400, Loss: 0.3436, Train: 85.78%, Valid: 85.66%, Test: 85.93%
Epoch: 425, Loss: 0.3419, Train: 85.88%, Valid: 85.73%, Test: 86.00%
Epoch: 450, Loss: 0.3404, Train: 85.87%, Valid: 85.73%, Test: 86.00%
Epoch: 475, Loss: 0.3386, Train: 86.28%, Valid: 86.14%, Test: 86.36%
Run 01:
Highest Train: 86.91
Highest Valid: 86.82
  Final Train: 86.91
   Final Test: 86.96
All runs:
Highest Train: 86.91, nan
Highest Valid: 86.82, nan
  Final Train: 86.91, nan
   Final Test: 86.96, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 8.0419, Train: 84.57%, Valid: 84.66%, Test: 84.65%
Epoch: 25, Loss: 1.5155, Train: 86.09%, Valid: 86.08%, Test: 86.18%
Epoch: 50, Loss: 1.5147, Train: 86.14%, Valid: 86.12%, Test: 86.25%
Epoch: 75, Loss: 1.4295, Train: 86.29%, Valid: 86.25%, Test: 86.37%
Epoch: 100, Loss: 0.6486, Train: 86.00%, Valid: 85.92%, Test: 86.07%
Epoch: 125, Loss: 0.4715, Train: 85.50%, Valid: 85.27%, Test: 85.52%
Epoch: 150, Loss: 0.3592, Train: 85.49%, Valid: 85.27%, Test: 85.56%
Epoch: 175, Loss: 0.3505, Train: 85.61%, Valid: 85.39%, Test: 85.67%
Epoch: 200, Loss: 0.3368, Train: 85.80%, Valid: 85.59%, Test: 85.86%
Epoch: 225, Loss: 0.3459, Train: 85.82%, Valid: 85.60%, Test: 85.88%
Epoch: 250, Loss: 0.3363, Train: 85.86%, Valid: 85.65%, Test: 85.91%
Epoch: 275, Loss: 0.3280, Train: 85.72%, Valid: 85.51%, Test: 85.78%
Epoch: 300, Loss: 0.9698, Train: 85.48%, Valid: 85.29%, Test: 85.57%
Epoch: 325, Loss: 3.1666, Train: 85.48%, Valid: 85.24%, Test: 85.53%
Epoch: 350, Loss: 5.0015, Train: 87.10%, Valid: 87.16%, Test: 87.19%
Epoch: 375, Loss: 5.1077, Train: 86.77%, Valid: 86.82%, Test: 86.90%
Epoch: 400, Loss: 5.0520, Train: 86.81%, Valid: 86.84%, Test: 86.94%
Epoch: 425, Loss: 5.0608, Train: 86.74%, Valid: 86.78%, Test: 86.87%
Epoch: 450, Loss: 5.0445, Train: 86.73%, Valid: 86.77%, Test: 86.87%
Epoch: 475, Loss: 5.0309, Train: 86.78%, Valid: 86.81%, Test: 86.90%
Run 01:
Highest Train: 88.10
Highest Valid: 88.11
  Final Train: 88.10
   Final Test: 88.16
All runs:
Highest Train: 88.10, nan
Highest Valid: 88.11, nan
  Final Train: 88.10, nan
   Final Test: 88.16, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 6.5315, Train: 85.71%, Valid: 85.53%, Test: 85.79%
Epoch: 25, Loss: 10835130640136271587513320079360.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 5.9137, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 1881328128.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 349485261712170187220844544.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 3288158648333333533032448.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 13142.7158, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 553058515041867706829862076416.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 29756969388873875456.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 9461373423314796544.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 18361152962560.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 6856001533771776.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 299877770780197268066295349248.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 11823730154139118053397364736.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 19828862684083425028918476800.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 1830774442749549196049360355328.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 95428684348915712.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 3162212621352960.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 223052624443282301242048512.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 88.33
Highest Valid: 88.38
  Final Train: 88.33
   Final Test: 88.37
All runs:
Highest Train: 88.33, nan
Highest Valid: 88.38, nan
  Final Train: 88.33, nan
   Final Test: 88.37, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.4292, Train: 86.41%, Valid: 86.30%, Test: 86.48%
Epoch: 25, Loss: 8366841286809247838896128.0000, Train: 85.92%, Valid: 85.79%, Test: 85.91%
Epoch: 50, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 387.3099, Train: 15.05%, Valid: 15.12%, Test: 14.89%
Epoch: 175, Loss: 484153281547476400987547115716608.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 5667042878981269514136550637568.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 15987842083236328158156292096.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 2634831458722421415632378527744.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 96294662757093715045775310848.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.41
Highest Valid: 86.30
  Final Train: 86.41
   Final Test: 86.48
All runs:
Highest Train: 86.41, nan
Highest Valid: 86.30, nan
  Final Train: 86.41, nan
   Final Test: 86.48, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 8.3084, Train: 81.91%, Valid: 82.12%, Test: 82.13%
Epoch: 25, Loss: 188.8553, Train: 50.04%, Valid: 50.04%, Test: 50.07%
Epoch: 50, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 2591321344.0000, Train: 86.62%, Valid: 86.63%, Test: 86.68%
Epoch: 125, Loss: 13006921290404069376.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 28978981864734720.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 571096247700941000786128817618944.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 14.5934, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.66
Highest Valid: 86.68
  Final Train: 86.66
   Final Test: 86.73
All runs:
Highest Train: 86.66, nan
Highest Valid: 86.68, nan
  Final Train: 86.66, nan
   Final Test: 86.73, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 3.5999, Train: 85.78%, Valid: 85.63%, Test: 85.87%
Epoch: 25, Loss: 4.3011, Train: 85.57%, Valid: 85.62%, Test: 85.68%
Epoch: 50, Loss: 2.8581, Train: 84.94%, Valid: 84.99%, Test: 85.05%
Epoch: 75, Loss: 0.3552, Train: 87.27%, Valid: 87.24%, Test: 87.33%
Epoch: 100, Loss: 0.3483, Train: 87.59%, Valid: 87.63%, Test: 87.71%
Epoch: 125, Loss: 0.3399, Train: 87.56%, Valid: 87.34%, Test: 87.60%
Epoch: 150, Loss: 0.3334, Train: 87.72%, Valid: 87.56%, Test: 87.73%
Epoch: 175, Loss: 0.3282, Train: 87.40%, Valid: 87.19%, Test: 87.43%
Epoch: 200, Loss: 0.3262, Train: 87.60%, Valid: 87.39%, Test: 87.63%
Epoch: 225, Loss: 0.3440, Train: 87.33%, Valid: 87.27%, Test: 87.37%
Epoch: 250, Loss: 0.3355, Train: 87.30%, Valid: 87.27%, Test: 87.35%
Epoch: 275, Loss: 0.3288, Train: 87.45%, Valid: 87.24%, Test: 87.48%
Epoch: 300, Loss: 0.3291, Train: 87.98%, Valid: 87.83%, Test: 88.01%
Epoch: 325, Loss: 0.3285, Train: 85.69%, Valid: 85.62%, Test: 85.72%
Epoch: 350, Loss: 0.3244, Train: 87.51%, Valid: 87.32%, Test: 87.54%
Epoch: 375, Loss: 0.3290, Train: 85.89%, Valid: 85.81%, Test: 85.87%
Epoch: 400, Loss: 0.3249, Train: 87.50%, Valid: 87.26%, Test: 87.52%
Epoch: 425, Loss: 0.3461, Train: 86.29%, Valid: 86.14%, Test: 86.35%
Epoch: 450, Loss: 0.3354, Train: 87.71%, Valid: 87.71%, Test: 87.79%
Epoch: 475, Loss: 0.3313, Train: 87.47%, Valid: 87.25%, Test: 87.51%
Run 01:
Highest Train: 88.02
Highest Valid: 88.04
  Final Train: 88.02
   Final Test: 88.10
All runs:
Highest Train: 88.02, nan
Highest Valid: 88.04, nan
  Final Train: 88.02, nan
   Final Test: 88.10, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 9.9520, Train: 86.38%, Valid: 86.30%, Test: 86.48%
Epoch: 25, Loss: 3.8891, Train: 86.90%, Valid: 86.94%, Test: 86.92%
Epoch: 50, Loss: 4.9280, Train: 86.78%, Valid: 86.74%, Test: 86.93%
Epoch: 75, Loss: 5.6937, Train: 86.89%, Valid: 86.88%, Test: 87.08%
Epoch: 100, Loss: 5.2728, Train: 87.21%, Valid: 87.27%, Test: 87.31%
Epoch: 125, Loss: 6.4166, Train: 87.10%, Valid: 87.12%, Test: 87.28%
Epoch: 150, Loss: 7.5277, Train: 86.89%, Valid: 86.88%, Test: 87.05%
Epoch: 175, Loss: 5.0113, Train: 85.83%, Valid: 85.69%, Test: 85.90%
Epoch: 200, Loss: 3.7953, Train: 85.55%, Valid: 85.43%, Test: 85.63%
Epoch: 225, Loss: 0.8469, Train: 85.39%, Valid: 85.31%, Test: 85.51%
Epoch: 250, Loss: 0.3526, Train: 86.11%, Valid: 85.98%, Test: 86.19%
Epoch: 275, Loss: 0.3381, Train: 86.19%, Valid: 86.03%, Test: 86.30%
Epoch: 300, Loss: 0.3296, Train: 86.35%, Valid: 86.21%, Test: 86.49%
Epoch: 325, Loss: 0.3261, Train: 86.92%, Valid: 86.79%, Test: 87.02%
Epoch: 350, Loss: 0.3246, Train: 87.26%, Valid: 87.10%, Test: 87.34%
Epoch: 375, Loss: 0.3275, Train: 87.13%, Valid: 87.10%, Test: 87.16%
Epoch: 400, Loss: 0.3246, Train: 87.18%, Valid: 87.01%, Test: 87.27%
Epoch: 425, Loss: 0.3239, Train: 87.23%, Valid: 87.07%, Test: 87.35%
Epoch: 450, Loss: 0.3241, Train: 85.53%, Valid: 85.48%, Test: 85.61%
Epoch: 475, Loss: 0.3236, Train: 87.00%, Valid: 86.85%, Test: 87.09%
Run 01:
Highest Train: 87.51
Highest Valid: 87.37
  Final Train: 87.29
   Final Test: 87.42
All runs:
Highest Train: 87.51, nan
Highest Valid: 87.37, nan
  Final Train: 87.29, nan
   Final Test: 87.42, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.8052, Train: 16.47%, Valid: 16.38%, Test: 16.29%
Epoch: 25, Loss: 2.7093, Train: 86.09%, Valid: 85.97%, Test: 86.17%
Epoch: 50, Loss: 2.0388, Train: 86.86%, Valid: 86.77%, Test: 86.90%
Epoch: 75, Loss: 1.1420, Train: 87.11%, Valid: 87.00%, Test: 87.07%
Epoch: 100, Loss: 0.3807, Train: 85.46%, Valid: 85.29%, Test: 85.53%
Epoch: 125, Loss: 0.3549, Train: 85.47%, Valid: 85.32%, Test: 85.57%
Epoch: 150, Loss: 0.3488, Train: 85.40%, Valid: 85.24%, Test: 85.50%
Epoch: 175, Loss: 2.6739, Train: 87.06%, Valid: 87.00%, Test: 87.05%
Epoch: 200, Loss: 1.0902, Train: 86.21%, Valid: 86.06%, Test: 86.24%
Epoch: 225, Loss: 4.3704, Train: 85.56%, Valid: 85.33%, Test: 85.58%
Epoch: 250, Loss: 4.3716, Train: 85.61%, Valid: 85.42%, Test: 85.62%
Epoch: 275, Loss: 3.1950, Train: 86.08%, Valid: 85.89%, Test: 86.10%
Epoch: 300, Loss: 1.1316, Train: 86.16%, Valid: 86.03%, Test: 86.19%
Epoch: 325, Loss: 0.3669, Train: 85.69%, Valid: 85.48%, Test: 85.77%
Epoch: 350, Loss: 0.3556, Train: 85.79%, Valid: 85.60%, Test: 85.86%
Epoch: 375, Loss: 0.3405, Train: 85.76%, Valid: 85.59%, Test: 85.84%
Epoch: 400, Loss: 0.3336, Train: 85.55%, Valid: 85.35%, Test: 85.62%
Epoch: 425, Loss: 0.3329, Train: 85.57%, Valid: 85.38%, Test: 85.65%
Epoch: 450, Loss: 0.6911, Train: 86.94%, Valid: 86.96%, Test: 87.01%
Epoch: 475, Loss: 0.3730, Train: 85.61%, Valid: 85.38%, Test: 85.70%
Run 01:
Highest Train: 87.81
Highest Valid: 87.68
  Final Train: 87.81
   Final Test: 87.79
All runs:
Highest Train: 87.81, nan
Highest Valid: 87.68, nan
  Final Train: 87.81, nan
   Final Test: 87.79, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 12.1164, Train: 85.55%, Valid: 85.43%, Test: 85.67%
Epoch: 25, Loss: 4.2573, Train: 86.72%, Valid: 86.74%, Test: 86.74%
Epoch: 50, Loss: 5.5144, Train: 88.16%, Valid: 88.19%, Test: 88.19%
Epoch: 75, Loss: 6.0408, Train: 88.24%, Valid: 88.27%, Test: 88.28%
Epoch: 100, Loss: 6.1104, Train: 88.18%, Valid: 88.21%, Test: 88.23%
Epoch: 125, Loss: 6.0807, Train: 88.20%, Valid: 88.24%, Test: 88.24%
Epoch: 150, Loss: 6.0500, Train: 88.18%, Valid: 88.21%, Test: 88.22%
Epoch: 175, Loss: 6.0475, Train: 88.21%, Valid: 88.24%, Test: 88.25%
Epoch: 200, Loss: 6.0540, Train: 88.20%, Valid: 88.24%, Test: 88.25%
Epoch: 225, Loss: 6.0523, Train: 88.19%, Valid: 88.22%, Test: 88.24%
Epoch: 250, Loss: 6.0603, Train: 88.17%, Valid: 88.20%, Test: 88.21%
Epoch: 275, Loss: 6.0239, Train: 88.18%, Valid: 88.21%, Test: 88.21%
Epoch: 300, Loss: 6.0590, Train: 88.22%, Valid: 88.25%, Test: 88.27%
Epoch: 325, Loss: 6.0927, Train: 88.19%, Valid: 88.21%, Test: 88.23%
Epoch: 350, Loss: 6.1125, Train: 88.14%, Valid: 88.18%, Test: 88.17%
Epoch: 375, Loss: 6.1512, Train: 87.75%, Valid: 87.76%, Test: 87.80%
Epoch: 400, Loss: 6.0960, Train: 88.03%, Valid: 88.07%, Test: 88.05%
Epoch: 425, Loss: 6.1109, Train: 87.94%, Valid: 87.96%, Test: 87.98%
Epoch: 450, Loss: 6.1426, Train: 87.91%, Valid: 87.94%, Test: 87.96%
Epoch: 475, Loss: 6.1124, Train: 87.86%, Valid: 87.87%, Test: 87.90%
Run 01:
Highest Train: 88.40
Highest Valid: 88.44
  Final Train: 88.40
   Final Test: 88.41
All runs:
Highest Train: 88.40, nan
Highest Valid: 88.44, nan
  Final Train: 88.40, nan
   Final Test: 88.41, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.4983, Train: 86.43%, Valid: 86.30%, Test: 86.51%
Epoch: 25, Loss: 2.9398, Train: 85.96%, Valid: 85.92%, Test: 85.99%
Epoch: 50, Loss: 3.6738, Train: 86.03%, Valid: 85.94%, Test: 86.07%
Epoch: 75, Loss: 3.1363, Train: 85.72%, Valid: 85.61%, Test: 85.83%
Epoch: 100, Loss: 0.7934, Train: 86.36%, Valid: 86.36%, Test: 86.39%
Epoch: 125, Loss: 3.5845, Train: 87.66%, Valid: 87.72%, Test: 87.75%
Epoch: 150, Loss: 2.1888, Train: 86.41%, Valid: 86.43%, Test: 86.46%
Epoch: 175, Loss: 2.3231, Train: 87.15%, Valid: 87.14%, Test: 87.14%
Epoch: 200, Loss: 1.4753, Train: 86.90%, Valid: 86.92%, Test: 86.89%
Epoch: 225, Loss: 2.9027, Train: 86.87%, Valid: 86.89%, Test: 86.88%
Epoch: 250, Loss: 2.4592, Train: 86.87%, Valid: 86.89%, Test: 86.88%
Epoch: 275, Loss: 0.5690, Train: 86.83%, Valid: 86.79%, Test: 86.94%
Epoch: 300, Loss: 0.4156, Train: 86.03%, Valid: 85.90%, Test: 86.18%
Epoch: 325, Loss: 0.3684, Train: 87.36%, Valid: 87.20%, Test: 87.46%
Epoch: 350, Loss: 1.7166, Train: 85.97%, Valid: 85.96%, Test: 86.00%
Epoch: 375, Loss: 0.4477, Train: 87.04%, Valid: 86.99%, Test: 87.17%
Epoch: 400, Loss: 0.3914, Train: 86.12%, Valid: 86.02%, Test: 86.23%
Epoch: 425, Loss: 0.3633, Train: 86.05%, Valid: 85.91%, Test: 86.18%
Epoch: 450, Loss: 0.3503, Train: 86.16%, Valid: 86.01%, Test: 86.30%
Epoch: 475, Loss: 0.3393, Train: 86.07%, Valid: 85.92%, Test: 86.21%
Run 01:
Highest Train: 87.73
Highest Valid: 87.73
  Final Train: 87.73
   Final Test: 87.81
All runs:
Highest Train: 87.73, nan
Highest Valid: 87.73, nan
  Final Train: 87.73, nan
   Final Test: 87.81, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 8.6695, Train: 84.05%, Valid: 84.20%, Test: 84.24%
Epoch: 25, Loss: 3.9503, Train: 86.31%, Valid: 86.19%, Test: 86.32%
Epoch: 50, Loss: 176.8333, Train: 86.30%, Valid: 86.21%, Test: 86.36%
Epoch: 75, Loss: 9.1555, Train: 85.71%, Valid: 85.74%, Test: 85.80%
Epoch: 100, Loss: 9.9141, Train: 85.78%, Valid: 85.80%, Test: 85.86%
Epoch: 125, Loss: 9.1665, Train: 85.91%, Valid: 85.92%, Test: 85.96%
Epoch: 150, Loss: 9.2199, Train: 85.91%, Valid: 85.92%, Test: 85.96%
Epoch: 175, Loss: 8.4914, Train: 85.86%, Valid: 85.88%, Test: 85.93%
Epoch: 200, Loss: 7.8398, Train: 85.56%, Valid: 85.59%, Test: 85.60%
Epoch: 225, Loss: 6.8088, Train: 85.48%, Valid: 85.49%, Test: 85.52%
Epoch: 250, Loss: 5.3472, Train: 85.03%, Valid: 85.04%, Test: 85.00%
Epoch: 275, Loss: 3.6324, Train: 84.59%, Valid: 84.63%, Test: 84.59%
Epoch: 300, Loss: 0.8159, Train: 47.64%, Valid: 47.64%, Test: 47.20%
Epoch: 325, Loss: 0.6621, Train: 81.90%, Valid: 81.91%, Test: 82.00%
Epoch: 350, Loss: 0.6177, Train: 81.97%, Valid: 81.97%, Test: 82.06%
Epoch: 375, Loss: 0.5948, Train: 82.04%, Valid: 82.05%, Test: 82.14%
Epoch: 400, Loss: 0.5693, Train: 82.09%, Valid: 82.09%, Test: 82.17%
Epoch: 425, Loss: 0.5389, Train: 82.19%, Valid: 82.17%, Test: 82.26%
Epoch: 450, Loss: 0.5014, Train: 82.93%, Valid: 82.85%, Test: 83.01%
Epoch: 475, Loss: 0.5044, Train: 83.35%, Valid: 83.23%, Test: 83.45%
Run 01:
Highest Train: 86.81
Highest Valid: 86.87
  Final Train: 86.81
   Final Test: 86.82
All runs:
Highest Train: 86.81, nan
Highest Valid: 86.87, nan
  Final Train: 86.81, nan
   Final Test: 86.82, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.4005, Train: 86.54%, Valid: 86.53%, Test: 86.51%
Epoch: 25, Loss: 368.9554, Train: 15.67%, Valid: 15.85%, Test: 15.53%
Epoch: 50, Loss: 354.9635, Train: 16.66%, Valid: 16.82%, Test: 16.51%
Epoch: 75, Loss: 343.4746, Train: 16.66%, Valid: 16.81%, Test: 16.52%
Epoch: 100, Loss: 344.6046, Train: 16.66%, Valid: 16.82%, Test: 16.51%
Epoch: 125, Loss: 344.7113, Train: 16.66%, Valid: 16.82%, Test: 16.51%
Epoch: 150, Loss: 344.7196, Train: 16.66%, Valid: 16.82%, Test: 16.51%
Epoch: 175, Loss: 344.7220, Train: 16.66%, Valid: 16.82%, Test: 16.51%
Epoch: 200, Loss: 344.7216, Train: 16.66%, Valid: 16.82%, Test: 16.51%
Epoch: 225, Loss: 344.7218, Train: 16.66%, Valid: 16.82%, Test: 16.51%
Epoch: 250, Loss: 344.7207, Train: 16.66%, Valid: 16.82%, Test: 16.51%
Epoch: 275, Loss: 344.7207, Train: 16.66%, Valid: 16.82%, Test: 16.51%
Epoch: 300, Loss: 344.7198, Train: 16.66%, Valid: 16.82%, Test: 16.51%
Epoch: 325, Loss: 344.7207, Train: 16.66%, Valid: 16.82%, Test: 16.51%
Epoch: 350, Loss: 344.7204, Train: 16.66%, Valid: 16.82%, Test: 16.51%
Epoch: 375, Loss: 344.7208, Train: 16.66%, Valid: 16.82%, Test: 16.51%
Epoch: 400, Loss: 344.7202, Train: 16.66%, Valid: 16.82%, Test: 16.51%
Epoch: 425, Loss: 344.7210, Train: 16.66%, Valid: 16.82%, Test: 16.51%
Epoch: 450, Loss: 344.7204, Train: 16.66%, Valid: 16.82%, Test: 16.51%
Epoch: 475, Loss: 344.7207, Train: 16.66%, Valid: 16.82%, Test: 16.51%
Run 01:
Highest Train: 86.54
Highest Valid: 86.53
  Final Train: 86.54
   Final Test: 86.51
All runs:
Highest Train: 86.54, nan
Highest Valid: 86.53, nan
  Final Train: 86.54, nan
   Final Test: 86.51, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.9088, Train: 86.27%, Valid: 86.20%, Test: 86.33%
Epoch: 25, Loss: 5.1048, Train: 86.62%, Valid: 86.62%, Test: 86.65%
Epoch: 50, Loss: 49.8569, Train: 86.72%, Valid: 86.72%, Test: 86.73%
Epoch: 75, Loss: 123963039894814588928.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 252.9246, Train: 85.81%, Valid: 85.72%, Test: 85.87%
Epoch: 125, Loss: 278.4314, Train: 85.85%, Valid: 85.76%, Test: 85.90%
Epoch: 150, Loss: 274.4021, Train: 85.85%, Valid: 85.76%, Test: 85.91%
Epoch: 175, Loss: 277.0957, Train: 15.63%, Valid: 15.77%, Test: 15.49%
Epoch: 200, Loss: 265.5278, Train: 85.86%, Valid: 85.76%, Test: 85.92%
Epoch: 225, Loss: 275.3448, Train: 85.87%, Valid: 85.77%, Test: 85.92%
Epoch: 250, Loss: 257.2106, Train: 84.13%, Valid: 84.00%, Test: 84.23%
Epoch: 275, Loss: 587969.1875, Train: 85.89%, Valid: 85.78%, Test: 85.93%
Epoch: 300, Loss: 12616.4121, Train: 15.48%, Valid: 15.54%, Test: 15.32%
Epoch: 325, Loss: 7.6613, Train: 85.82%, Valid: 85.74%, Test: 85.89%
Epoch: 350, Loss: 273.3234, Train: 84.13%, Valid: 84.00%, Test: 84.23%
Epoch: 375, Loss: 304.1024, Train: 85.83%, Valid: 85.74%, Test: 85.89%
Epoch: 400, Loss: 279.5805, Train: 84.14%, Valid: 84.01%, Test: 84.24%
Epoch: 425, Loss: 283.3191, Train: 84.13%, Valid: 84.00%, Test: 84.23%
Epoch: 450, Loss: 275.2279, Train: 85.83%, Valid: 85.74%, Test: 85.90%
Epoch: 475, Loss: 43714.4531, Train: 85.83%, Valid: 85.74%, Test: 85.89%
Run 01:
Highest Train: 87.12
Highest Valid: 87.17
  Final Train: 87.12
   Final Test: 87.12
All runs:
Highest Train: 87.12, nan
Highest Valid: 87.17, nan
  Final Train: 87.12, nan
   Final Test: 87.12, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.5862, Train: 84.86%, Valid: 84.71%, Test: 84.92%
Epoch: 25, Loss: 3.3919, Train: 86.11%, Valid: 86.04%, Test: 86.11%
Epoch: 50, Loss: 3.4711, Train: 85.61%, Valid: 85.44%, Test: 85.66%
Epoch: 75, Loss: 3.3214, Train: 85.66%, Valid: 85.50%, Test: 85.71%
Epoch: 100, Loss: 2.4348, Train: 86.00%, Valid: 85.83%, Test: 86.05%
Epoch: 125, Loss: 1.0084, Train: 85.44%, Valid: 85.25%, Test: 85.49%
Epoch: 150, Loss: 0.4721, Train: 85.65%, Valid: 85.44%, Test: 85.70%
Epoch: 175, Loss: 0.3834, Train: 85.76%, Valid: 85.54%, Test: 85.82%
Epoch: 200, Loss: 0.3627, Train: 85.82%, Valid: 85.61%, Test: 85.89%
Epoch: 225, Loss: 1.1158, Train: 87.10%, Valid: 87.13%, Test: 87.11%
Epoch: 250, Loss: 3.3145, Train: 85.51%, Valid: 85.32%, Test: 85.59%
Epoch: 275, Loss: 2.7727, Train: 85.62%, Valid: 85.43%, Test: 85.64%
Epoch: 300, Loss: 1.5098, Train: 87.10%, Valid: 87.13%, Test: 87.16%
Epoch: 325, Loss: 1.2982, Train: 86.53%, Valid: 86.44%, Test: 86.60%
Epoch: 350, Loss: 0.9003, Train: 87.27%, Valid: 87.23%, Test: 87.30%
Epoch: 375, Loss: 0.5796, Train: 86.60%, Valid: 86.49%, Test: 86.60%
Epoch: 400, Loss: 0.3615, Train: 85.84%, Valid: 85.63%, Test: 85.88%
Epoch: 425, Loss: 0.3502, Train: 86.80%, Valid: 86.67%, Test: 86.96%
Epoch: 450, Loss: 0.3734, Train: 86.38%, Valid: 86.24%, Test: 86.42%
Epoch: 475, Loss: 2.8445, Train: 87.85%, Valid: 87.73%, Test: 87.80%
Run 01:
Highest Train: 88.28
Highest Valid: 88.13
  Final Train: 88.28
   Final Test: 88.23
All runs:
Highest Train: 88.28, nan
Highest Valid: 88.13, nan
  Final Train: 88.28, nan
   Final Test: 88.23, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.4562, Train: 86.58%, Valid: 86.65%, Test: 86.62%
Epoch: 25, Loss: 3229.5000, Train: 16.38%, Valid: 16.56%, Test: 16.27%
Epoch: 50, Loss: 2.9427, Train: 87.39%, Valid: 87.44%, Test: 87.46%
Epoch: 75, Loss: 1.9485, Train: 87.47%, Valid: 87.55%, Test: 87.55%
Epoch: 100, Loss: 2.0774, Train: 87.46%, Valid: 87.53%, Test: 87.52%
Epoch: 125, Loss: 2.1454, Train: 87.45%, Valid: 87.53%, Test: 87.52%
Epoch: 150, Loss: 2.1584, Train: 87.45%, Valid: 87.53%, Test: 87.52%
Epoch: 175, Loss: 2.1691, Train: 87.45%, Valid: 87.53%, Test: 87.52%
Epoch: 200, Loss: 2.1671, Train: 87.45%, Valid: 87.53%, Test: 87.52%
Epoch: 225, Loss: 2.1378, Train: 87.45%, Valid: 87.53%, Test: 87.52%
Epoch: 250, Loss: 2.1439, Train: 87.45%, Valid: 87.53%, Test: 87.52%
Epoch: 275, Loss: 2.1338, Train: 87.45%, Valid: 87.53%, Test: 87.52%
Epoch: 300, Loss: 2.1677, Train: 87.45%, Valid: 87.53%, Test: 87.52%
Epoch: 325, Loss: 2.1457, Train: 87.45%, Valid: 87.53%, Test: 87.52%
Epoch: 350, Loss: 2.1365, Train: 87.45%, Valid: 87.53%, Test: 87.52%
Epoch: 375, Loss: 2.1357, Train: 87.45%, Valid: 87.53%, Test: 87.52%
Epoch: 400, Loss: 2.1482, Train: 87.45%, Valid: 87.53%, Test: 87.52%
Epoch: 425, Loss: 2.1383, Train: 87.45%, Valid: 87.53%, Test: 87.52%
Epoch: 450, Loss: 2.1272, Train: 87.45%, Valid: 87.53%, Test: 87.52%
Epoch: 475, Loss: 2.1392, Train: 87.45%, Valid: 87.53%, Test: 87.52%
Run 01:
Highest Train: 88.12
Highest Valid: 88.16
  Final Train: 88.09
   Final Test: 88.15
All runs:
Highest Train: 88.12, nan
Highest Valid: 88.16, nan
  Final Train: 88.09, nan
   Final Test: 88.15, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.1775, Train: 85.10%, Valid: 85.00%, Test: 85.23%
Epoch: 25, Loss: 359.9575, Train: 84.15%, Valid: 83.99%, Test: 84.31%
Epoch: 50, Loss: 5391.4307, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 75709.0859, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 45313.6836, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 24787.5664, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 443.8994, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 187.9912, Train: 15.42%, Valid: 15.55%, Test: 15.29%
Epoch: 200, Loss: 15848.1279, Train: 83.94%, Valid: 83.91%, Test: 84.19%
Epoch: 225, Loss: 796.9363, Train: 15.36%, Valid: 15.49%, Test: 15.36%
Epoch: 250, Loss: 326.3728, Train: 15.33%, Valid: 15.47%, Test: 15.32%
Epoch: 275, Loss: 23.5606, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 307.0096, Train: 15.32%, Valid: 15.47%, Test: 15.31%
Epoch: 325, Loss: 18291.4980, Train: 84.00%, Valid: 83.96%, Test: 84.24%
Epoch: 350, Loss: 207.9947, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 271.7105, Train: 15.95%, Valid: 16.01%, Test: 15.71%
Epoch: 400, Loss: 29.2086, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 1011.7750, Train: 15.98%, Valid: 16.03%, Test: 15.74%
Epoch: 450, Loss: 24785.6855, Train: 85.65%, Valid: 85.53%, Test: 85.62%
Epoch: 475, Loss: 3147.7156, Train: 16.09%, Valid: 16.14%, Test: 15.86%
Run 01:
Highest Train: 86.68
Highest Valid: 86.67
  Final Train: 86.68
   Final Test: 86.84
All runs:
Highest Train: 86.68, nan
Highest Valid: 86.67, nan
  Final Train: 86.68, nan
   Final Test: 86.84, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.4224, Train: 26.88%, Valid: 26.85%, Test: 26.51%
Epoch: 25, Loss: 133.4565, Train: 17.52%, Valid: 17.29%, Test: 17.34%
Epoch: 50, Loss: 118.6073, Train: 15.80%, Valid: 16.02%, Test: 15.70%
Epoch: 75, Loss: 116.9954, Train: 15.52%, Valid: 15.70%, Test: 15.38%
Epoch: 100, Loss: 118.8840, Train: 15.50%, Valid: 15.68%, Test: 15.37%
Epoch: 125, Loss: 100.4531, Train: 14.55%, Valid: 14.69%, Test: 14.48%
Epoch: 150, Loss: 91.8319, Train: 14.90%, Valid: 15.05%, Test: 14.81%
Epoch: 175, Loss: 84.9956, Train: 14.86%, Valid: 15.02%, Test: 14.78%
Epoch: 200, Loss: 80.6714, Train: 14.93%, Valid: 15.11%, Test: 14.86%
Epoch: 225, Loss: 77.8804, Train: 13.04%, Valid: 12.98%, Test: 12.97%
Epoch: 250, Loss: 76.0140, Train: 12.93%, Valid: 12.90%, Test: 12.89%
Epoch: 275, Loss: 75.0504, Train: 12.96%, Valid: 12.93%, Test: 12.94%
Epoch: 300, Loss: 74.6849, Train: 13.01%, Valid: 12.99%, Test: 12.99%
Epoch: 325, Loss: 74.2431, Train: 13.06%, Valid: 13.03%, Test: 13.03%
Epoch: 350, Loss: 73.9080, Train: 13.12%, Valid: 13.11%, Test: 13.09%
Epoch: 375, Loss: 73.6866, Train: 13.20%, Valid: 13.21%, Test: 13.18%
Epoch: 400, Loss: 73.4804, Train: 13.31%, Valid: 13.32%, Test: 13.30%
Epoch: 425, Loss: 73.3180, Train: 13.46%, Valid: 13.48%, Test: 13.46%
Epoch: 450, Loss: 73.1627, Train: 13.62%, Valid: 13.64%, Test: 13.63%
Epoch: 475, Loss: 72.9902, Train: 13.86%, Valid: 13.85%, Test: 13.87%
Run 01:
Highest Train: 86.20
Highest Valid: 86.24
  Final Train: 86.20
   Final Test: 86.29
All runs:
Highest Train: 86.20, nan
Highest Valid: 86.24, nan
  Final Train: 86.20, nan
   Final Test: 86.29, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.5796, Train: 85.39%, Valid: 85.39%, Test: 85.54%
Epoch: 25, Loss: 40.4266, Train: 13.65%, Valid: 13.54%, Test: 13.68%
Epoch: 50, Loss: 54.6011, Train: 13.63%, Valid: 13.55%, Test: 13.68%
Epoch: 75, Loss: 54.4210, Train: 13.60%, Valid: 13.51%, Test: 13.64%
Epoch: 100, Loss: 55.8628, Train: 13.60%, Valid: 13.52%, Test: 13.64%
Epoch: 125, Loss: 54.5662, Train: 13.59%, Valid: 13.50%, Test: 13.62%
Epoch: 150, Loss: 57.2199, Train: 13.57%, Valid: 13.48%, Test: 13.60%
Epoch: 175, Loss: 62.6672, Train: 13.65%, Valid: 13.54%, Test: 13.70%
Epoch: 200, Loss: 56.2263, Train: 13.66%, Valid: 13.56%, Test: 13.71%
Epoch: 225, Loss: 53.5113, Train: 13.67%, Valid: 13.57%, Test: 13.72%
Epoch: 250, Loss: 58.2802, Train: 13.69%, Valid: 13.59%, Test: 13.74%
Epoch: 275, Loss: 56.9870, Train: 13.70%, Valid: 13.59%, Test: 13.75%
Epoch: 300, Loss: 55.1809, Train: 13.71%, Valid: 13.60%, Test: 13.76%
Epoch: 325, Loss: 58.3756, Train: 13.70%, Valid: 13.59%, Test: 13.75%
Epoch: 350, Loss: 61.7997, Train: 13.70%, Valid: 13.59%, Test: 13.76%
Epoch: 375, Loss: 54.9629, Train: 13.70%, Valid: 13.60%, Test: 13.76%
Epoch: 400, Loss: 56.6046, Train: 13.71%, Valid: 13.60%, Test: 13.76%
Epoch: 425, Loss: 66.9287, Train: 13.70%, Valid: 13.60%, Test: 13.76%
Epoch: 450, Loss: 60.1081, Train: 13.70%, Valid: 13.60%, Test: 13.76%
Epoch: 475, Loss: 58.4022, Train: 13.71%, Valid: 13.60%, Test: 13.77%
Run 01:
Highest Train: 87.69
Highest Valid: 87.73
  Final Train: 87.69
   Final Test: 87.66
All runs:
Highest Train: 87.69, nan
Highest Valid: 87.73, nan
  Final Train: 87.69, nan
   Final Test: 87.66, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.7002, Train: 85.43%, Valid: 85.21%, Test: 85.45%
Epoch: 25, Loss: 0.9040, Train: 85.24%, Valid: 85.16%, Test: 85.26%
Epoch: 50, Loss: 0.9569, Train: 85.24%, Valid: 85.05%, Test: 85.26%
Epoch: 75, Loss: 0.9393, Train: 85.68%, Valid: 85.47%, Test: 85.75%
Epoch: 100, Loss: 0.9177, Train: 85.82%, Valid: 85.63%, Test: 85.91%
Epoch: 125, Loss: 0.9453, Train: 85.78%, Valid: 85.59%, Test: 85.87%
Epoch: 150, Loss: 0.9357, Train: 85.88%, Valid: 85.74%, Test: 85.96%
Epoch: 175, Loss: 0.9221, Train: 85.86%, Valid: 85.72%, Test: 85.94%
Epoch: 200, Loss: 0.9834, Train: 85.79%, Valid: 85.64%, Test: 85.88%
Epoch: 225, Loss: 1.1750, Train: 86.02%, Valid: 85.88%, Test: 86.13%
Epoch: 250, Loss: 641.9771, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 1509676285952.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 963647104.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 7120586752.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 525744224.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 30569228.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 2003858304.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 174082736.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 1528166016.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 302909792.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.21
Highest Valid: 86.14
  Final Train: 86.21
   Final Test: 86.27
All runs:
Highest Train: 86.21, nan
Highest Valid: 86.14, nan
  Final Train: 86.21, nan
   Final Test: 86.27, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.9091, Train: 85.03%, Valid: 84.88%, Test: 85.11%
Epoch: 25, Loss: 411.8409, Train: 83.12%, Valid: 83.27%, Test: 83.30%
Epoch: 50, Loss: 184.6002, Train: 15.46%, Valid: 15.60%, Test: 15.37%
Epoch: 75, Loss: 401.3799, Train: 87.75%, Valid: 87.74%, Test: 87.68%
Epoch: 100, Loss: 571367292928.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 461671239589950356717568.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 1755244.3750, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 22474661916835840.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 2806670080.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 96710256754688.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 705017151488.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 16242669739048960.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 51949532115632128.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 200503854104576.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 103239982776320.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 7429554700288.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 58206189518848.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 6575496888320.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 12621185123589554176.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 6928871194624.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 87.75
Highest Valid: 87.74
  Final Train: 87.75
   Final Test: 87.68
All runs:
Highest Train: 87.75, nan
Highest Valid: 87.74, nan
  Final Train: 87.75, nan
   Final Test: 87.68, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.0754, Train: 86.58%, Valid: 86.51%, Test: 86.64%
Epoch: 25, Loss: 58.2135, Train: 15.02%, Valid: 15.09%, Test: 14.87%
Epoch: 50, Loss: 58.5470, Train: 15.92%, Valid: 16.05%, Test: 15.82%
Epoch: 75, Loss: 80.9544, Train: 15.65%, Valid: 15.79%, Test: 15.55%
Epoch: 100, Loss: 82.8835, Train: 15.70%, Valid: 15.81%, Test: 15.60%
Epoch: 125, Loss: 82.6243, Train: 15.66%, Valid: 15.79%, Test: 15.54%
Epoch: 150, Loss: 83.1912, Train: 15.67%, Valid: 15.79%, Test: 15.57%
Epoch: 175, Loss: 83.4356, Train: 15.64%, Valid: 15.78%, Test: 15.52%
Epoch: 200, Loss: 83.2859, Train: 15.41%, Valid: 15.52%, Test: 15.30%
Epoch: 225, Loss: 82.5430, Train: 15.69%, Valid: 15.81%, Test: 15.59%
Epoch: 250, Loss: 83.5428, Train: 15.66%, Valid: 15.80%, Test: 15.55%
Epoch: 275, Loss: 83.3320, Train: 15.70%, Valid: 15.81%, Test: 15.60%
Epoch: 300, Loss: 83.4073, Train: 15.65%, Valid: 15.78%, Test: 15.53%
Epoch: 325, Loss: 82.7452, Train: 15.52%, Valid: 15.62%, Test: 15.42%
Epoch: 350, Loss: 82.8140, Train: 15.64%, Valid: 15.77%, Test: 15.51%
Epoch: 375, Loss: 83.2679, Train: 15.45%, Valid: 15.56%, Test: 15.34%
Epoch: 400, Loss: 83.2357, Train: 15.67%, Valid: 15.80%, Test: 15.55%
Epoch: 425, Loss: 82.5704, Train: 15.47%, Valid: 15.58%, Test: 15.37%
Epoch: 450, Loss: 83.1047, Train: 15.63%, Valid: 15.76%, Test: 15.50%
Epoch: 475, Loss: 83.5039, Train: 15.35%, Valid: 15.46%, Test: 15.24%
Run 01:
Highest Train: 87.19
Highest Valid: 87.21
  Final Train: 87.19
   Final Test: 87.17
All runs:
Highest Train: 87.19, nan
Highest Valid: 87.21, nan
  Final Train: 87.19, nan
   Final Test: 87.17, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.8231, Train: 82.35%, Valid: 82.38%, Test: 82.54%
Epoch: 25, Loss: 0.7289, Train: 86.48%, Valid: 86.45%, Test: 86.49%
Epoch: 50, Loss: 0.7756, Train: 85.39%, Valid: 85.35%, Test: 85.44%
Epoch: 75, Loss: 0.8710, Train: 85.42%, Valid: 85.43%, Test: 85.45%
Epoch: 100, Loss: 0.8856, Train: 85.88%, Valid: 85.88%, Test: 85.92%
Epoch: 125, Loss: 0.8881, Train: 85.88%, Valid: 85.88%, Test: 85.92%
Epoch: 150, Loss: 0.8876, Train: 85.88%, Valid: 85.88%, Test: 85.92%
Epoch: 175, Loss: 0.8874, Train: 85.88%, Valid: 85.88%, Test: 85.92%
Epoch: 200, Loss: 0.8900, Train: 85.88%, Valid: 85.88%, Test: 85.92%
Epoch: 225, Loss: 0.8881, Train: 85.88%, Valid: 85.88%, Test: 85.92%
Epoch: 250, Loss: 0.8875, Train: 85.88%, Valid: 85.88%, Test: 85.92%
Epoch: 275, Loss: 0.8891, Train: 85.88%, Valid: 85.88%, Test: 85.92%
Epoch: 300, Loss: 0.8894, Train: 85.88%, Valid: 85.88%, Test: 85.92%
Epoch: 325, Loss: 0.8869, Train: 85.88%, Valid: 85.88%, Test: 85.92%
Epoch: 350, Loss: 0.8885, Train: 85.88%, Valid: 85.88%, Test: 85.92%
Epoch: 375, Loss: 0.8870, Train: 85.88%, Valid: 85.88%, Test: 85.92%
Epoch: 400, Loss: 0.8873, Train: 85.88%, Valid: 85.88%, Test: 85.92%
Epoch: 425, Loss: 0.8883, Train: 85.88%, Valid: 85.88%, Test: 85.92%
Epoch: 450, Loss: 0.8893, Train: 85.88%, Valid: 85.88%, Test: 85.92%
Epoch: 475, Loss: 0.8890, Train: 85.88%, Valid: 85.88%, Test: 85.92%
Run 01:
Highest Train: 86.70
Highest Valid: 86.53
  Final Train: 86.70
   Final Test: 86.75
All runs:
Highest Train: 86.70, nan
Highest Valid: 86.53, nan
  Final Train: 86.70, nan
   Final Test: 86.75, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.8745, Train: 78.50%, Valid: 78.58%, Test: 78.88%
Epoch: 25, Loss: 0.8426, Train: 85.89%, Valid: 85.76%, Test: 85.96%
Epoch: 50, Loss: 2.3491, Train: 85.85%, Valid: 85.72%, Test: 85.91%
Epoch: 75, Loss: 803916274991104.0000, Train: 50.01%, Valid: 50.00%, Test: 50.01%
Epoch: 100, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 5957.9888, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 49125953536.0000, Train: 84.81%, Valid: 84.85%, Test: 84.90%
Epoch: 200, Loss: 2.8451, Train: 86.65%, Valid: 86.70%, Test: 86.71%
Epoch: 225, Loss: 9264.9053, Train: 85.51%, Valid: 85.31%, Test: 85.56%
Epoch: 250, Loss: 2.6225, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 2.4281, Train: 50.04%, Valid: 50.04%, Test: 50.06%
Epoch: 300, Loss: 5803467264.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 2.7394, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 2.5705, Train: 15.14%, Valid: 15.34%, Test: 15.04%
Epoch: 375, Loss: 13304544100352.0000, Train: 85.29%, Valid: 85.33%, Test: 85.35%
Epoch: 400, Loss: 897615872.0000, Train: 86.54%, Valid: 86.56%, Test: 86.61%
Epoch: 425, Loss: 199.2450, Train: 86.23%, Valid: 86.23%, Test: 86.30%
Epoch: 450, Loss: 3.0205, Train: 85.95%, Valid: 85.95%, Test: 86.00%
Epoch: 475, Loss: 48.3151, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.71
Highest Valid: 86.75
  Final Train: 86.71
   Final Test: 86.80
All runs:
Highest Train: 86.71, nan
Highest Valid: 86.75, nan
  Final Train: 86.71, nan
   Final Test: 86.80, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.9638, Train: 84.93%, Valid: 84.77%, Test: 85.07%
Epoch: 25, Loss: 1205.6115, Train: 87.89%, Valid: 87.93%, Test: 87.89%
Epoch: 50, Loss: 2052.6863, Train: 83.64%, Valid: 83.47%, Test: 83.76%
Epoch: 75, Loss: 197.5649, Train: 84.24%, Valid: 83.99%, Test: 84.32%
Epoch: 100, Loss: 163.4165, Train: 84.37%, Valid: 84.15%, Test: 84.50%
Epoch: 125, Loss: 28.8006, Train: 84.34%, Valid: 84.11%, Test: 84.45%
Epoch: 150, Loss: 170.5821, Train: 84.38%, Valid: 84.15%, Test: 84.50%
Epoch: 175, Loss: 204.5874, Train: 84.33%, Valid: 84.09%, Test: 84.44%
Epoch: 200, Loss: 85.9533, Train: 84.41%, Valid: 84.19%, Test: 84.52%
Epoch: 225, Loss: 83.9183, Train: 84.30%, Valid: 84.05%, Test: 84.41%
Epoch: 250, Loss: 134.6984, Train: 84.39%, Valid: 84.17%, Test: 84.51%
Epoch: 275, Loss: 104.1119, Train: 84.36%, Valid: 84.13%, Test: 84.48%
Epoch: 300, Loss: 106.0851, Train: 84.42%, Valid: 84.19%, Test: 84.54%
Epoch: 325, Loss: 141.7452, Train: 84.34%, Valid: 84.12%, Test: 84.46%
Epoch: 350, Loss: 121.7614, Train: 84.35%, Valid: 84.13%, Test: 84.47%
Epoch: 375, Loss: 80.8637, Train: 84.41%, Valid: 84.18%, Test: 84.52%
Epoch: 400, Loss: 106.7518, Train: 84.36%, Valid: 84.14%, Test: 84.48%
Epoch: 425, Loss: 150.1645, Train: 84.37%, Valid: 84.14%, Test: 84.49%
Epoch: 450, Loss: 81.3706, Train: 84.37%, Valid: 84.16%, Test: 84.49%
Epoch: 475, Loss: 147.6359, Train: 84.42%, Valid: 84.19%, Test: 84.52%
Run 01:
Highest Train: 88.06
Highest Valid: 88.14
  Final Train: 88.06
   Final Test: 88.05
All runs:
Highest Train: 88.06, nan
Highest Valid: 88.14, nan
  Final Train: 88.06, nan
   Final Test: 88.05, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.4398, Train: 87.08%, Valid: 87.08%, Test: 87.06%
Epoch: 25, Loss: 160.1589, Train: 85.90%, Valid: 85.87%, Test: 85.95%
Epoch: 50, Loss: 312.0499, Train: 86.89%, Valid: 86.86%, Test: 86.93%
Epoch: 75, Loss: 2.4714, Train: 16.80%, Valid: 16.91%, Test: 16.65%
Epoch: 100, Loss: 6869.1758, Train: 85.01%, Valid: 84.95%, Test: 85.08%
Epoch: 125, Loss: 409.8595, Train: 14.98%, Valid: 15.06%, Test: 14.92%
Epoch: 150, Loss: 399.4738, Train: 16.52%, Valid: 16.66%, Test: 16.36%
Epoch: 175, Loss: 403.0770, Train: 16.50%, Valid: 16.63%, Test: 16.34%
Epoch: 200, Loss: 398.4818, Train: 16.48%, Valid: 16.61%, Test: 16.31%
Epoch: 225, Loss: 391.7023, Train: 16.57%, Valid: 16.70%, Test: 16.41%
Epoch: 250, Loss: 389.2802, Train: 17.11%, Valid: 17.23%, Test: 16.94%
Epoch: 275, Loss: 396.4062, Train: 16.48%, Valid: 16.60%, Test: 16.30%
Epoch: 300, Loss: 390.0117, Train: 16.50%, Valid: 16.62%, Test: 16.33%
Epoch: 325, Loss: 398.9790, Train: 16.47%, Valid: 16.60%, Test: 16.30%
Epoch: 350, Loss: 405.6407, Train: 16.53%, Valid: 16.66%, Test: 16.37%
Epoch: 375, Loss: 400.6149, Train: 16.42%, Valid: 16.55%, Test: 16.24%
Epoch: 400, Loss: 395.8940, Train: 16.48%, Valid: 16.60%, Test: 16.31%
Epoch: 425, Loss: 410.3393, Train: 16.43%, Valid: 16.55%, Test: 16.24%
Epoch: 450, Loss: 406.1889, Train: 16.53%, Valid: 16.66%, Test: 16.37%
Epoch: 475, Loss: 397.9568, Train: 17.03%, Valid: 17.15%, Test: 16.86%
Run 01:
Highest Train: 87.26
Highest Valid: 87.20
  Final Train: 87.26
   Final Test: 87.28
All runs:
Highest Train: 87.26, nan
Highest Valid: 87.20, nan
  Final Train: 87.26, nan
   Final Test: 87.28, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 7.7662, Train: 86.31%, Valid: 86.18%, Test: 86.26%
Epoch: 25, Loss: 2.8903, Train: 85.96%, Valid: 85.82%, Test: 85.98%
Epoch: 50, Loss: 8.2698, Train: 85.16%, Valid: 84.97%, Test: 85.23%
Epoch: 75, Loss: 11.0046, Train: 85.29%, Valid: 85.06%, Test: 85.28%
Epoch: 100, Loss: 10.3626, Train: 85.15%, Valid: 84.91%, Test: 85.18%
Epoch: 125, Loss: 8.6502, Train: 85.16%, Valid: 84.92%, Test: 85.21%
Epoch: 150, Loss: 8.7188, Train: 85.18%, Valid: 84.94%, Test: 85.23%
Epoch: 175, Loss: 6.6557, Train: 85.15%, Valid: 84.90%, Test: 85.20%
Epoch: 200, Loss: 7.2343, Train: 85.16%, Valid: 84.91%, Test: 85.20%
Epoch: 225, Loss: 4.7463, Train: 85.17%, Valid: 84.92%, Test: 85.22%
Epoch: 250, Loss: 4.6289, Train: 85.18%, Valid: 84.96%, Test: 85.24%
Epoch: 275, Loss: 13.3032, Train: 85.25%, Valid: 85.05%, Test: 85.31%
Epoch: 300, Loss: 8.6889, Train: 85.22%, Valid: 85.02%, Test: 85.28%
Epoch: 325, Loss: 7.7765, Train: 85.07%, Valid: 84.88%, Test: 85.15%
Epoch: 350, Loss: 4.2002, Train: 85.12%, Valid: 84.92%, Test: 85.20%
Epoch: 375, Loss: 6.0773, Train: 85.10%, Valid: 84.89%, Test: 85.17%
Epoch: 400, Loss: 3.8985, Train: 85.14%, Valid: 84.93%, Test: 85.20%
Epoch: 425, Loss: 9.1119, Train: 85.14%, Valid: 84.92%, Test: 85.20%
Epoch: 450, Loss: 8.4227, Train: 85.12%, Valid: 84.91%, Test: 85.19%
Epoch: 475, Loss: 7.6582, Train: 85.14%, Valid: 84.93%, Test: 85.21%
Run 01:
Highest Train: 86.68
Highest Valid: 86.68
  Final Train: 86.68
   Final Test: 86.81
All runs:
Highest Train: 86.68, nan
Highest Valid: 86.68, nan
  Final Train: 86.68, nan
   Final Test: 86.81, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 3.7257, Train: 86.13%, Valid: 86.05%, Test: 86.22%
Epoch: 25, Loss: 2.6719, Train: 84.28%, Valid: 84.04%, Test: 84.39%
Epoch: 50, Loss: 3321456230400.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 55847.0977, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 27.7851, Train: 86.87%, Valid: 86.91%, Test: 86.95%
Epoch: 125, Loss: 553.9023, Train: 86.87%, Valid: 86.90%, Test: 86.93%
Epoch: 150, Loss: 42.6311, Train: 86.87%, Valid: 86.90%, Test: 86.94%
Epoch: 175, Loss: 116.4374, Train: 86.88%, Valid: 86.91%, Test: 86.94%
Epoch: 200, Loss: 40.7138, Train: 86.87%, Valid: 86.90%, Test: 86.94%
Epoch: 225, Loss: 44.4113, Train: 86.87%, Valid: 86.90%, Test: 86.93%
Epoch: 250, Loss: 46.6591, Train: 86.87%, Valid: 86.90%, Test: 86.93%
Epoch: 275, Loss: 44.1260, Train: 86.87%, Valid: 86.90%, Test: 86.94%
Epoch: 300, Loss: 42.4761, Train: 86.87%, Valid: 86.90%, Test: 86.94%
Epoch: 325, Loss: 36.2499, Train: 86.87%, Valid: 86.90%, Test: 86.93%
Epoch: 350, Loss: 820.8134, Train: 87.15%, Valid: 87.15%, Test: 87.25%
Epoch: 375, Loss: 43.8203, Train: 86.87%, Valid: 86.90%, Test: 86.93%
Epoch: 400, Loss: 49.1335, Train: 86.88%, Valid: 86.91%, Test: 86.95%
Epoch: 425, Loss: 34.5379, Train: 86.87%, Valid: 86.90%, Test: 86.94%
Epoch: 450, Loss: 5.1417, Train: 86.87%, Valid: 86.90%, Test: 86.93%
Epoch: 475, Loss: 45.2071, Train: 86.88%, Valid: 86.90%, Test: 86.94%
Run 01:
Highest Train: 87.96
Highest Valid: 88.02
  Final Train: 87.96
   Final Test: 88.00
All runs:
Highest Train: 87.96, nan
Highest Valid: 88.02, nan
  Final Train: 87.96, nan
   Final Test: 88.00, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 3.1876, Train: 85.92%, Valid: 85.89%, Test: 85.95%
Epoch: 25, Loss: 8589.1162, Train: 15.77%, Valid: 15.94%, Test: 15.58%
Epoch: 50, Loss: 3.4228, Train: 86.51%, Valid: 86.51%, Test: 86.57%
Epoch: 75, Loss: 22.1404, Train: 85.62%, Valid: 85.60%, Test: 85.68%
Epoch: 100, Loss: 26.0120, Train: 85.62%, Valid: 85.60%, Test: 85.67%
Epoch: 125, Loss: 26.0206, Train: 85.63%, Valid: 85.61%, Test: 85.69%
Epoch: 150, Loss: 26.1645, Train: 85.64%, Valid: 85.61%, Test: 85.70%
Epoch: 175, Loss: 25.8188, Train: 85.63%, Valid: 85.61%, Test: 85.69%
Epoch: 200, Loss: 26.1452, Train: 85.63%, Valid: 85.61%, Test: 85.69%
Epoch: 225, Loss: 26.6928, Train: 85.61%, Valid: 85.59%, Test: 85.67%
Epoch: 250, Loss: 27.9392, Train: 85.60%, Valid: 85.58%, Test: 85.65%
Epoch: 275, Loss: 26.9177, Train: 85.59%, Valid: 85.57%, Test: 85.65%
Epoch: 300, Loss: 26.5697, Train: 85.61%, Valid: 85.60%, Test: 85.67%
Epoch: 325, Loss: 26.3302, Train: 85.64%, Valid: 85.61%, Test: 85.69%
Epoch: 350, Loss: 26.0348, Train: 85.64%, Valid: 85.62%, Test: 85.70%
Epoch: 375, Loss: 25.9652, Train: 85.64%, Valid: 85.62%, Test: 85.69%
Epoch: 400, Loss: 26.0365, Train: 85.63%, Valid: 85.61%, Test: 85.69%
Epoch: 425, Loss: 25.7507, Train: 85.64%, Valid: 85.62%, Test: 85.70%
Epoch: 450, Loss: 25.9192, Train: 85.64%, Valid: 85.62%, Test: 85.70%
Epoch: 475, Loss: 25.6082, Train: 85.65%, Valid: 85.63%, Test: 85.71%
Run 01:
Highest Train: 86.58
Highest Valid: 86.59
  Final Train: 86.58
   Final Test: 86.64
All runs:
Highest Train: 86.58, nan
Highest Valid: 86.59, nan
  Final Train: 86.58, nan
   Final Test: 86.64, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 7.5811, Train: 81.17%, Valid: 81.27%, Test: 81.51%
Epoch: 25, Loss: 5.0526, Train: 14.97%, Valid: 15.08%, Test: 14.91%
Epoch: 50, Loss: 598634496.0000, Train: 49.98%, Valid: 49.97%, Test: 49.96%
Epoch: 75, Loss: 3068777216.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 16656474112.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 2661566.7500, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 301323872.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 3494580992.0000, Train: 50.00%, Valid: 49.99%, Test: 50.00%
Epoch: 200, Loss: 1787.2922, Train: 84.02%, Valid: 83.94%, Test: 84.10%
Epoch: 225, Loss: 2011556.5000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 12728509.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 93830408.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 5573094400.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 53238.9492, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 212554.9219, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 6901528.5000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 13367791.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 32969222.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 52493588.0000, Train: 50.01%, Valid: 50.01%, Test: 50.00%
Epoch: 475, Loss: 15162328.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.80
Highest Valid: 86.85
  Final Train: 86.80
   Final Test: 86.89
All runs:
Highest Train: 86.80, nan
Highest Valid: 86.85, nan
  Final Train: 86.80, nan
   Final Test: 86.89, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.0879, Train: 86.33%, Valid: 86.34%, Test: 86.33%
Epoch: 25, Loss: 29.5100, Train: 86.53%, Valid: 86.56%, Test: 86.57%
Epoch: 50, Loss: 544071995952292404691730432.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 99918916091904.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 4628279197696.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 1413943503456567296.0000, Train: 83.52%, Valid: 83.33%, Test: 83.66%
Epoch: 150, Loss: 1693145.6250, Train: 86.82%, Valid: 86.87%, Test: 86.87%
Epoch: 175, Loss: 24049347251404800.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 20.5270, Train: 86.48%, Valid: 86.54%, Test: 86.50%
Epoch: 225, Loss: 1080048746496.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 2503265269783199744.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 5993210052608.0000, Train: 85.65%, Valid: 85.78%, Test: 85.70%
Epoch: 300, Loss: 30670850.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 1954963.8750, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 1659943936.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 100075025445912066861826048.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 780046.8750, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 894198103534469120.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 155057897231389556736.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 8755313377280.0000, Train: 14.63%, Valid: 14.84%, Test: 14.54%
Run 01:
Highest Train: 86.91
Highest Valid: 86.93
  Final Train: 86.91
   Final Test: 86.95
All runs:
Highest Train: 86.91, nan
Highest Valid: 86.93, nan
  Final Train: 86.91, nan
   Final Test: 86.95, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.8721, Train: 85.70%, Valid: 85.55%, Test: 85.73%
Epoch: 25, Loss: 2.3890, Train: 86.43%, Valid: 86.33%, Test: 86.47%
Epoch: 50, Loss: 2.7199, Train: 86.44%, Valid: 86.34%, Test: 86.47%
Epoch: 75, Loss: 2.7384, Train: 86.43%, Valid: 86.32%, Test: 86.46%
Epoch: 100, Loss: 2.7244, Train: 86.43%, Valid: 86.32%, Test: 86.46%
Epoch: 125, Loss: 2.7305, Train: 86.43%, Valid: 86.32%, Test: 86.46%
Epoch: 150, Loss: 2.7329, Train: 86.44%, Valid: 86.33%, Test: 86.46%
Epoch: 175, Loss: 2.7219, Train: 86.44%, Valid: 86.33%, Test: 86.47%
Epoch: 200, Loss: 2.7282, Train: 86.45%, Valid: 86.34%, Test: 86.47%
Epoch: 225, Loss: 2.7183, Train: 86.44%, Valid: 86.33%, Test: 86.46%
Epoch: 250, Loss: 2.7305, Train: 86.44%, Valid: 86.33%, Test: 86.46%
Epoch: 275, Loss: 2.7410, Train: 86.44%, Valid: 86.33%, Test: 86.46%
Epoch: 300, Loss: 2.7494, Train: 86.43%, Valid: 86.32%, Test: 86.46%
Epoch: 325, Loss: 2.7383, Train: 86.45%, Valid: 86.33%, Test: 86.47%
Epoch: 350, Loss: 2.7343, Train: 86.45%, Valid: 86.34%, Test: 86.47%
Epoch: 375, Loss: 2.7451, Train: 86.46%, Valid: 86.34%, Test: 86.47%
Epoch: 400, Loss: 2.7351, Train: 86.45%, Valid: 86.34%, Test: 86.47%
Epoch: 425, Loss: 2.7336, Train: 86.46%, Valid: 86.35%, Test: 86.47%
Epoch: 450, Loss: 2.7200, Train: 86.47%, Valid: 86.35%, Test: 86.48%
Epoch: 475, Loss: 2.7165, Train: 86.48%, Valid: 86.35%, Test: 86.48%
Run 01:
Highest Train: 87.01
Highest Valid: 86.96
  Final Train: 87.01
   Final Test: 86.98
All runs:
Highest Train: 87.01, nan
Highest Valid: 86.96, nan
  Final Train: 87.01, nan
   Final Test: 86.98, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 19.4856, Train: 85.48%, Valid: 85.32%, Test: 85.56%
Epoch: 25, Loss: 2.6686, Train: 85.82%, Valid: 85.65%, Test: 85.89%
Epoch: 50, Loss: 1.6228, Train: 85.84%, Valid: 85.68%, Test: 85.91%
Epoch: 75, Loss: 0.6546, Train: 85.67%, Valid: 85.52%, Test: 85.75%
Epoch: 100, Loss: 0.5402, Train: 85.22%, Valid: 85.04%, Test: 85.26%
Epoch: 125, Loss: 0.9870, Train: 85.52%, Valid: 85.34%, Test: 85.58%
Epoch: 150, Loss: 0.6335, Train: 85.52%, Valid: 85.35%, Test: 85.57%
Epoch: 175, Loss: 0.4555, Train: 85.42%, Valid: 85.23%, Test: 85.46%
Epoch: 200, Loss: 0.4082, Train: 85.58%, Valid: 85.38%, Test: 85.64%
Epoch: 225, Loss: 1.2617, Train: 85.60%, Valid: 85.42%, Test: 85.63%
Epoch: 250, Loss: 0.5286, Train: 85.38%, Valid: 85.20%, Test: 85.45%
Epoch: 275, Loss: 0.4053, Train: 86.16%, Valid: 86.10%, Test: 86.28%
Epoch: 300, Loss: 0.3585, Train: 85.70%, Valid: 85.55%, Test: 85.79%
Epoch: 325, Loss: 0.3534, Train: 85.71%, Valid: 85.54%, Test: 85.79%
Epoch: 350, Loss: 0.3481, Train: 85.79%, Valid: 85.63%, Test: 85.88%
Epoch: 375, Loss: 0.3416, Train: 86.22%, Valid: 86.06%, Test: 86.32%
Epoch: 400, Loss: 0.3328, Train: 85.86%, Valid: 85.67%, Test: 85.93%
Epoch: 425, Loss: 0.5305, Train: 85.56%, Valid: 85.40%, Test: 85.64%
Epoch: 450, Loss: 1.1349, Train: 86.94%, Valid: 86.93%, Test: 86.97%
Epoch: 475, Loss: 2.4191, Train: 85.36%, Valid: 85.18%, Test: 85.44%
Run 01:
Highest Train: 87.02
Highest Valid: 87.04
  Final Train: 87.02
   Final Test: 87.05
All runs:
Highest Train: 87.02, nan
Highest Valid: 87.04, nan
  Final Train: 87.02, nan
   Final Test: 87.05, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 8.4647, Train: 86.78%, Valid: 86.63%, Test: 86.82%
Epoch: 25, Loss: 47.7927, Train: 86.91%, Valid: 86.99%, Test: 86.97%
Epoch: 50, Loss: 6.1876, Train: 86.02%, Valid: 86.08%, Test: 86.13%
Epoch: 75, Loss: 6.2379, Train: 86.62%, Valid: 86.70%, Test: 86.67%
Epoch: 100, Loss: 5.8595, Train: 87.04%, Valid: 87.09%, Test: 87.16%
Epoch: 125, Loss: 4.9958, Train: 86.48%, Valid: 86.56%, Test: 86.61%
Epoch: 150, Loss: 2.2803, Train: 84.85%, Valid: 84.75%, Test: 84.96%
Epoch: 175, Loss: 3.1623, Train: 86.05%, Valid: 86.11%, Test: 86.11%
Epoch: 200, Loss: 2.9617, Train: 86.00%, Valid: 86.05%, Test: 86.08%
Epoch: 225, Loss: 0.9201, Train: 85.61%, Valid: 85.67%, Test: 85.75%
Epoch: 250, Loss: 0.3797, Train: 86.98%, Valid: 87.00%, Test: 87.04%
Epoch: 275, Loss: 0.3596, Train: 87.10%, Valid: 87.14%, Test: 87.16%
Epoch: 300, Loss: 0.3521, Train: 85.77%, Valid: 85.59%, Test: 85.85%
Epoch: 325, Loss: 0.3460, Train: 85.71%, Valid: 85.52%, Test: 85.78%
Epoch: 350, Loss: 0.3415, Train: 85.75%, Valid: 85.73%, Test: 85.83%
Epoch: 375, Loss: 3.4791, Train: 85.99%, Valid: 86.01%, Test: 86.08%
Epoch: 400, Loss: 0.6596, Train: 80.62%, Valid: 80.72%, Test: 80.82%
Epoch: 425, Loss: 3.1850, Train: 85.69%, Valid: 85.73%, Test: 85.79%
Epoch: 450, Loss: 2.1147, Train: 85.51%, Valid: 85.54%, Test: 85.59%
Epoch: 475, Loss: 0.8967, Train: 85.32%, Valid: 85.35%, Test: 85.43%
Run 01:
Highest Train: 88.09
Highest Valid: 88.16
  Final Train: 88.09
   Final Test: 88.08
All runs:
Highest Train: 88.09, nan
Highest Valid: 88.16, nan
  Final Train: 88.09, nan
   Final Test: 88.08, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 8.3013, Train: 85.74%, Valid: 85.64%, Test: 85.81%
Epoch: 25, Loss: 7.5562, Train: 86.12%, Valid: 85.98%, Test: 86.17%
Epoch: 50, Loss: 10.2917, Train: 85.79%, Valid: 85.69%, Test: 85.85%
Epoch: 75, Loss: 9.7568, Train: 85.92%, Valid: 85.80%, Test: 85.98%
Epoch: 100, Loss: 8.3942, Train: 85.89%, Valid: 85.80%, Test: 85.95%
Epoch: 125, Loss: 3.0081, Train: 85.27%, Valid: 85.19%, Test: 85.39%
Epoch: 150, Loss: 6.8678, Train: 85.88%, Valid: 85.73%, Test: 85.97%
Epoch: 175, Loss: 2.8370, Train: 85.64%, Valid: 85.53%, Test: 85.75%
Epoch: 200, Loss: 2.2357, Train: 85.89%, Valid: 85.70%, Test: 85.98%
Epoch: 225, Loss: 3.4986, Train: 85.72%, Valid: 85.52%, Test: 85.83%
Epoch: 250, Loss: 3.8539, Train: 85.93%, Valid: 85.85%, Test: 85.99%
Epoch: 275, Loss: 2.3526, Train: 86.21%, Valid: 86.03%, Test: 86.28%
Epoch: 300, Loss: 0.8673, Train: 87.20%, Valid: 87.21%, Test: 87.17%
Epoch: 325, Loss: 0.5464, Train: 85.68%, Valid: 85.58%, Test: 85.64%
Epoch: 350, Loss: 0.4616, Train: 85.41%, Valid: 85.46%, Test: 85.46%
Epoch: 375, Loss: 0.4143, Train: 86.01%, Valid: 85.96%, Test: 86.12%
Epoch: 400, Loss: 0.3704, Train: 85.99%, Valid: 85.81%, Test: 86.11%
Epoch: 425, Loss: 3.5375, Train: 86.44%, Valid: 86.49%, Test: 86.50%
Epoch: 450, Loss: 12.6879, Train: 85.37%, Valid: 85.21%, Test: 85.51%
Epoch: 475, Loss: 14.7280, Train: 83.82%, Valid: 83.70%, Test: 84.00%
Run 01:
Highest Train: 87.48
Highest Valid: 87.45
  Final Train: 87.48
   Final Test: 87.42
All runs:
Highest Train: 87.48, nan
Highest Valid: 87.45, nan
  Final Train: 87.48, nan
   Final Test: 87.42, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 286.4448, Train: 78.38%, Valid: 78.42%, Test: 78.64%
Epoch: 25, Loss: 2.4762, Train: 85.15%, Valid: 85.17%, Test: 85.26%
Epoch: 50, Loss: 0.9922, Train: 85.29%, Valid: 85.29%, Test: 85.41%
Epoch: 75, Loss: 3.8636, Train: 86.09%, Valid: 86.01%, Test: 86.22%
Epoch: 100, Loss: 1.5180, Train: 86.04%, Valid: 85.96%, Test: 86.17%
Epoch: 125, Loss: 1.9397, Train: 86.20%, Valid: 86.15%, Test: 86.31%
Epoch: 150, Loss: 1.5530, Train: 86.05%, Valid: 86.05%, Test: 86.23%
Epoch: 175, Loss: 3.3525, Train: 85.99%, Valid: 85.87%, Test: 86.02%
Epoch: 200, Loss: 1.8872, Train: 86.21%, Valid: 86.14%, Test: 86.29%
Epoch: 225, Loss: 1.2003, Train: 86.20%, Valid: 86.12%, Test: 86.27%
Epoch: 250, Loss: 0.7271, Train: 86.50%, Valid: 86.35%, Test: 86.51%
Epoch: 275, Loss: 0.6076, Train: 85.62%, Valid: 85.45%, Test: 85.71%
Epoch: 300, Loss: 4.7108, Train: 86.11%, Valid: 86.00%, Test: 86.20%
Epoch: 325, Loss: 1.8277, Train: 86.02%, Valid: 85.91%, Test: 86.12%
Epoch: 350, Loss: 6.4311, Train: 86.14%, Valid: 86.03%, Test: 86.22%
Epoch: 375, Loss: 5.6352, Train: 86.12%, Valid: 86.00%, Test: 86.20%
Epoch: 400, Loss: 4.0605, Train: 86.08%, Valid: 85.97%, Test: 86.17%
Epoch: 425, Loss: 2.7021, Train: 86.05%, Valid: 85.94%, Test: 86.14%
Epoch: 450, Loss: 0.9453, Train: 84.99%, Valid: 84.85%, Test: 85.05%
Epoch: 475, Loss: 0.4827, Train: 84.82%, Valid: 84.68%, Test: 84.91%
Run 01:
Highest Train: 87.92
Highest Valid: 87.77
  Final Train: 87.92
   Final Test: 87.88
All runs:
Highest Train: 87.92, nan
Highest Valid: 87.77, nan
  Final Train: 87.92, nan
   Final Test: 87.88, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.5738, Train: 86.90%, Valid: 86.89%, Test: 86.95%
Epoch: 25, Loss: 3.5527, Train: 86.90%, Valid: 86.95%, Test: 86.96%
Epoch: 50, Loss: 1.7435, Train: 85.75%, Valid: 85.79%, Test: 85.78%
Epoch: 75, Loss: 2.3939, Train: 86.91%, Valid: 86.80%, Test: 87.02%
Epoch: 100, Loss: 0.8678, Train: 86.97%, Valid: 86.84%, Test: 87.01%
Epoch: 125, Loss: 0.3866, Train: 85.93%, Valid: 85.97%, Test: 86.01%
Epoch: 150, Loss: 0.3463, Train: 85.74%, Valid: 85.68%, Test: 85.84%
Epoch: 175, Loss: 0.3410, Train: 85.86%, Valid: 85.85%, Test: 85.95%
Epoch: 200, Loss: 0.3741, Train: 86.00%, Valid: 86.01%, Test: 86.08%
Epoch: 225, Loss: 0.3335, Train: 86.05%, Valid: 86.03%, Test: 86.15%
Epoch: 250, Loss: 0.3291, Train: 86.03%, Valid: 86.02%, Test: 86.15%
Epoch: 275, Loss: 1.6060, Train: 86.88%, Valid: 86.94%, Test: 86.97%
Epoch: 300, Loss: 2.0527, Train: 86.98%, Valid: 86.86%, Test: 86.98%
Epoch: 325, Loss: 0.5524, Train: 86.81%, Valid: 86.64%, Test: 86.72%
Epoch: 350, Loss: 0.3644, Train: 86.39%, Valid: 86.33%, Test: 86.39%
Epoch: 375, Loss: 0.3370, Train: 86.02%, Valid: 86.00%, Test: 86.12%
Epoch: 400, Loss: 0.4425, Train: 87.51%, Valid: 87.34%, Test: 87.54%
Epoch: 425, Loss: 0.3594, Train: 86.89%, Valid: 86.63%, Test: 86.99%
Epoch: 450, Loss: 0.3289, Train: 85.94%, Valid: 85.93%, Test: 86.02%
Epoch: 475, Loss: 1.1521, Train: 87.17%, Valid: 87.02%, Test: 87.17%
Run 01:
Highest Train: 88.36
Highest Valid: 88.42
  Final Train: 88.36
   Final Test: 88.41
All runs:
Highest Train: 88.36, nan
Highest Valid: 88.42, nan
  Final Train: 88.36, nan
   Final Test: 88.41, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 10.6698, Train: 85.55%, Valid: 85.42%, Test: 85.62%
Epoch: 25, Loss: 3.0311, Train: 85.75%, Valid: 85.65%, Test: 85.91%
Epoch: 50, Loss: 3.7553, Train: 86.20%, Valid: 86.18%, Test: 86.27%
Epoch: 75, Loss: 2.4835, Train: 86.31%, Valid: 86.30%, Test: 86.39%
Epoch: 100, Loss: 1.2093, Train: 87.29%, Valid: 87.28%, Test: 87.28%
Epoch: 125, Loss: 0.4379, Train: 86.35%, Valid: 86.30%, Test: 86.38%
Epoch: 150, Loss: 0.3547, Train: 85.92%, Valid: 85.75%, Test: 86.04%
Epoch: 175, Loss: 0.3398, Train: 85.87%, Valid: 85.69%, Test: 85.99%
Epoch: 200, Loss: 0.3322, Train: 85.95%, Valid: 85.79%, Test: 86.08%
Epoch: 225, Loss: 0.3432, Train: 86.06%, Valid: 85.89%, Test: 86.18%
Epoch: 250, Loss: 0.3338, Train: 86.03%, Valid: 85.86%, Test: 86.16%
Epoch: 275, Loss: 0.3295, Train: 86.03%, Valid: 85.85%, Test: 86.15%
Epoch: 300, Loss: 0.4281, Train: 86.12%, Valid: 86.05%, Test: 86.29%
Epoch: 325, Loss: 0.3463, Train: 86.08%, Valid: 85.91%, Test: 86.20%
Epoch: 350, Loss: 0.3390, Train: 86.07%, Valid: 85.92%, Test: 86.19%
Epoch: 375, Loss: 0.3301, Train: 86.00%, Valid: 85.83%, Test: 86.12%
Epoch: 400, Loss: 0.3297, Train: 85.95%, Valid: 85.77%, Test: 86.08%
Epoch: 425, Loss: 0.3263, Train: 86.05%, Valid: 85.89%, Test: 86.17%
Epoch: 450, Loss: 0.3417, Train: 86.16%, Valid: 85.97%, Test: 86.28%
Epoch: 475, Loss: 0.3339, Train: 86.10%, Valid: 85.93%, Test: 86.22%
Run 01:
Highest Train: 87.56
Highest Valid: 87.52
  Final Train: 87.56
   Final Test: 87.51
All runs:
Highest Train: 87.56, nan
Highest Valid: 87.52, nan
  Final Train: 87.56, nan
   Final Test: 87.51, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 32.8456, Train: 20.53%, Valid: 20.33%, Test: 20.16%
Epoch: 25, Loss: 6.6245, Train: 85.46%, Valid: 85.30%, Test: 85.50%
Epoch: 50, Loss: 7.4154, Train: 85.46%, Valid: 85.31%, Test: 85.51%
Epoch: 75, Loss: 5.9831, Train: 85.38%, Valid: 85.21%, Test: 85.43%
Epoch: 100, Loss: 6.0651, Train: 85.35%, Valid: 85.17%, Test: 85.41%
Epoch: 125, Loss: 7.6917, Train: 85.39%, Valid: 85.19%, Test: 85.42%
Epoch: 150, Loss: 5.4563, Train: 85.38%, Valid: 85.19%, Test: 85.43%
Epoch: 175, Loss: 1.1339, Train: 85.72%, Valid: 85.63%, Test: 85.81%
Epoch: 200, Loss: 1.9213, Train: 86.67%, Valid: 86.57%, Test: 86.62%
Epoch: 225, Loss: 3.3453, Train: 85.54%, Valid: 85.40%, Test: 85.61%
Epoch: 250, Loss: 1.8496, Train: 85.60%, Valid: 85.45%, Test: 85.65%
Epoch: 275, Loss: 8.8225, Train: 86.72%, Valid: 86.59%, Test: 86.62%
Epoch: 300, Loss: 7.2745, Train: 85.55%, Valid: 85.41%, Test: 85.60%
Epoch: 325, Loss: 3.5123, Train: 86.00%, Valid: 85.96%, Test: 86.18%
Epoch: 350, Loss: 2.2534, Train: 85.94%, Valid: 85.82%, Test: 86.02%
Epoch: 375, Loss: 0.5891, Train: 84.29%, Valid: 84.19%, Test: 84.41%
Epoch: 400, Loss: 0.8933, Train: 85.49%, Valid: 85.35%, Test: 85.55%
Epoch: 425, Loss: 0.4613, Train: 85.91%, Valid: 85.78%, Test: 86.01%
Epoch: 450, Loss: 1.8854, Train: 86.46%, Valid: 86.33%, Test: 86.43%
Epoch: 475, Loss: 6.4998, Train: 86.55%, Valid: 86.43%, Test: 86.55%
Run 01:
Highest Train: 87.76
Highest Valid: 87.70
  Final Train: 87.76
   Final Test: 87.72
All runs:
Highest Train: 87.76, nan
Highest Valid: 87.70, nan
  Final Train: 87.76, nan
   Final Test: 87.72, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.3989, Train: 86.00%, Valid: 85.93%, Test: 86.12%
Epoch: 25, Loss: 4.8331, Train: 84.61%, Valid: 84.48%, Test: 84.73%
Epoch: 50, Loss: 6.3032, Train: 84.82%, Valid: 84.67%, Test: 84.90%
Epoch: 75, Loss: 6.3931, Train: 84.76%, Valid: 84.63%, Test: 84.83%
Epoch: 100, Loss: 5.2693, Train: 84.79%, Valid: 84.65%, Test: 84.85%
Epoch: 125, Loss: 8.4893, Train: 85.75%, Valid: 85.59%, Test: 85.85%
Epoch: 150, Loss: 2.3263, Train: 85.80%, Valid: 85.66%, Test: 85.91%
Epoch: 175, Loss: 2.3175, Train: 85.78%, Valid: 85.62%, Test: 85.89%
Epoch: 200, Loss: 2.1587, Train: 85.74%, Valid: 85.57%, Test: 85.83%
Epoch: 225, Loss: 1.9703, Train: 85.62%, Valid: 85.43%, Test: 85.71%
Epoch: 250, Loss: 1.7599, Train: 85.61%, Valid: 85.43%, Test: 85.70%
Epoch: 275, Loss: 1.5263, Train: 85.62%, Valid: 85.43%, Test: 85.69%
Epoch: 300, Loss: 1.2671, Train: 85.64%, Valid: 85.47%, Test: 85.72%
Epoch: 325, Loss: 0.9794, Train: 85.67%, Valid: 85.49%, Test: 85.77%
Epoch: 350, Loss: 0.6601, Train: 85.66%, Valid: 85.49%, Test: 85.77%
Epoch: 375, Loss: 0.3879, Train: 86.03%, Valid: 86.12%, Test: 86.11%
Epoch: 400, Loss: 0.3648, Train: 86.10%, Valid: 86.17%, Test: 86.17%
Epoch: 425, Loss: 0.3616, Train: 86.06%, Valid: 86.14%, Test: 86.13%
Epoch: 450, Loss: 0.3613, Train: 86.00%, Valid: 86.10%, Test: 86.07%
Epoch: 475, Loss: 0.3610, Train: 85.95%, Valid: 86.03%, Test: 86.01%
Run 01:
Highest Train: 88.06
Highest Valid: 88.10
  Final Train: 88.06
   Final Test: 88.05
All runs:
Highest Train: 88.06, nan
Highest Valid: 88.10, nan
  Final Train: 88.06, nan
   Final Test: 88.05, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 10.4703, Train: 85.80%, Valid: 85.65%, Test: 85.85%
Epoch: 25, Loss: 3.2599, Train: 85.37%, Valid: 85.24%, Test: 85.44%
Epoch: 50, Loss: 4.8012, Train: 86.32%, Valid: 86.21%, Test: 86.41%
Epoch: 75, Loss: 2.2352, Train: 86.52%, Valid: 86.50%, Test: 86.58%
Epoch: 100, Loss: 0.9813, Train: 87.18%, Valid: 87.07%, Test: 87.23%
Epoch: 125, Loss: 0.3695, Train: 85.99%, Valid: 85.81%, Test: 86.13%
Epoch: 150, Loss: 0.3389, Train: 85.98%, Valid: 85.82%, Test: 86.11%
Epoch: 175, Loss: 1.5112, Train: 85.55%, Valid: 85.46%, Test: 85.66%
Epoch: 200, Loss: 1.9547, Train: 86.55%, Valid: 86.55%, Test: 86.61%
Epoch: 225, Loss: 1.1710, Train: 86.20%, Valid: 86.16%, Test: 86.25%
Epoch: 250, Loss: 0.5617, Train: 86.37%, Valid: 86.35%, Test: 86.41%
Epoch: 275, Loss: 1.0422, Train: 86.08%, Valid: 86.06%, Test: 86.16%
Epoch: 300, Loss: 0.3631, Train: 85.87%, Valid: 85.71%, Test: 85.97%
Epoch: 325, Loss: 0.3410, Train: 86.08%, Valid: 85.88%, Test: 86.18%
Epoch: 350, Loss: 0.3660, Train: 86.56%, Valid: 86.44%, Test: 86.60%
Epoch: 375, Loss: 0.3414, Train: 86.07%, Valid: 85.88%, Test: 86.18%
Epoch: 400, Loss: 0.3648, Train: 86.11%, Valid: 85.93%, Test: 86.20%
Epoch: 425, Loss: 0.3419, Train: 86.15%, Valid: 85.96%, Test: 86.26%
Epoch: 450, Loss: 0.3302, Train: 86.18%, Valid: 85.98%, Test: 86.28%
Epoch: 475, Loss: 0.3646, Train: 86.09%, Valid: 85.90%, Test: 86.20%
Run 01:
Highest Train: 87.24
Highest Valid: 87.14
  Final Train: 87.24
   Final Test: 87.30
All runs:
Highest Train: 87.24, nan
Highest Valid: 87.14, nan
  Final Train: 87.24, nan
   Final Test: 87.30, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 27.8463, Train: 80.78%, Valid: 80.89%, Test: 81.05%
Epoch: 25, Loss: 4.9004, Train: 85.81%, Valid: 85.64%, Test: 85.88%
Epoch: 50, Loss: 3.8151, Train: 85.86%, Valid: 85.71%, Test: 85.93%
Epoch: 75, Loss: 2.2551, Train: 85.82%, Valid: 85.67%, Test: 85.89%
Epoch: 100, Loss: 1.1234, Train: 85.70%, Valid: 85.55%, Test: 85.77%
Epoch: 125, Loss: 1.0826, Train: 85.72%, Valid: 85.57%, Test: 85.79%
Epoch: 150, Loss: 0.7218, Train: 85.61%, Valid: 85.43%, Test: 85.66%
Epoch: 175, Loss: 0.5980, Train: 85.61%, Valid: 85.44%, Test: 85.66%
Epoch: 200, Loss: 0.3824, Train: 86.80%, Valid: 86.72%, Test: 86.84%
Epoch: 225, Loss: 0.3490, Train: 85.73%, Valid: 85.55%, Test: 85.81%
Epoch: 250, Loss: 0.3516, Train: 85.29%, Valid: 85.23%, Test: 85.48%
Epoch: 275, Loss: 0.3382, Train: 85.66%, Valid: 85.52%, Test: 85.80%
Epoch: 300, Loss: 0.3369, Train: 86.06%, Valid: 85.91%, Test: 86.20%
Epoch: 325, Loss: 2.1791, Train: 86.88%, Valid: 86.82%, Test: 87.01%
Epoch: 350, Loss: 1.8360, Train: 86.50%, Valid: 86.51%, Test: 86.59%
Epoch: 375, Loss: 0.8405, Train: 85.92%, Valid: 85.80%, Test: 86.01%
Epoch: 400, Loss: 0.3902, Train: 85.98%, Valid: 85.82%, Test: 86.12%
Epoch: 425, Loss: 0.3495, Train: 87.25%, Valid: 87.03%, Test: 87.37%
Epoch: 450, Loss: 0.3345, Train: 86.25%, Valid: 86.03%, Test: 86.37%
Epoch: 475, Loss: 0.6519, Train: 85.99%, Valid: 85.83%, Test: 85.91%
Run 01:
Highest Train: 87.56
Highest Valid: 87.35
  Final Train: 87.56
   Final Test: 87.64
All runs:
Highest Train: 87.56, nan
Highest Valid: 87.35, nan
  Final Train: 87.56, nan
   Final Test: 87.64, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.7484, Train: 86.67%, Valid: 86.60%, Test: 86.70%
Epoch: 25, Loss: 0.4513, Train: 84.17%, Valid: 83.96%, Test: 84.26%
Epoch: 50, Loss: 1.8824, Train: 86.17%, Valid: 86.19%, Test: 86.24%
Epoch: 75, Loss: 2.6280, Train: 86.21%, Valid: 86.23%, Test: 86.27%
Epoch: 100, Loss: 0.6207, Train: 86.05%, Valid: 86.07%, Test: 86.11%
Epoch: 125, Loss: 4.5571, Train: 75.48%, Valid: 75.48%, Test: 75.72%
Epoch: 150, Loss: 208.8673, Train: 16.32%, Valid: 16.47%, Test: 16.18%
Epoch: 175, Loss: 667.1531, Train: 85.33%, Valid: 85.37%, Test: 85.44%
Epoch: 200, Loss: 1.0932, Train: 85.64%, Valid: 85.70%, Test: 85.74%
Epoch: 225, Loss: 1.1215, Train: 85.85%, Valid: 85.87%, Test: 85.94%
Epoch: 250, Loss: 1.1790, Train: 85.86%, Valid: 85.88%, Test: 85.95%
Epoch: 275, Loss: 1.3991, Train: 85.89%, Valid: 85.91%, Test: 85.97%
Epoch: 300, Loss: 1.1921, Train: 85.89%, Valid: 85.91%, Test: 85.97%
Epoch: 325, Loss: 1.5544, Train: 85.89%, Valid: 85.91%, Test: 85.97%
Epoch: 350, Loss: 1.1063, Train: 85.89%, Valid: 85.91%, Test: 85.97%
Epoch: 375, Loss: 1.1576, Train: 85.89%, Valid: 85.91%, Test: 85.97%
Epoch: 400, Loss: 1.4903, Train: 85.89%, Valid: 85.91%, Test: 85.97%
Epoch: 425, Loss: 1.1391, Train: 85.89%, Valid: 85.92%, Test: 85.98%
Epoch: 450, Loss: 0.9277, Train: 85.90%, Valid: 85.92%, Test: 85.98%
Epoch: 475, Loss: 1.3266, Train: 85.90%, Valid: 85.92%, Test: 85.98%
Run 01:
Highest Train: 86.67
Highest Valid: 86.60
  Final Train: 86.67
   Final Test: 86.70
All runs:
Highest Train: 86.67, nan
Highest Valid: 86.60, nan
  Final Train: 86.67, nan
   Final Test: 86.70, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 3.5701, Train: 85.40%, Valid: 85.36%, Test: 85.40%
Epoch: 25, Loss: 0.5289, Train: 86.59%, Valid: 86.50%, Test: 86.59%
Epoch: 50, Loss: 0.5517, Train: 85.75%, Valid: 85.70%, Test: 85.76%
Epoch: 75, Loss: 0.6404, Train: 85.81%, Valid: 85.77%, Test: 85.86%
Epoch: 100, Loss: 1.6408, Train: 86.02%, Valid: 85.95%, Test: 86.08%
Epoch: 125, Loss: 0.6976, Train: 86.07%, Valid: 86.00%, Test: 86.11%
Epoch: 150, Loss: 0.6963, Train: 86.08%, Valid: 86.00%, Test: 86.12%
Epoch: 175, Loss: 0.6823, Train: 86.08%, Valid: 86.00%, Test: 86.11%
Epoch: 200, Loss: 0.6724, Train: 86.08%, Valid: 86.00%, Test: 86.11%
Epoch: 225, Loss: 0.6754, Train: 86.08%, Valid: 86.00%, Test: 86.11%
Epoch: 250, Loss: 0.6954, Train: 86.08%, Valid: 86.00%, Test: 86.12%
Epoch: 275, Loss: 0.6720, Train: 86.08%, Valid: 86.00%, Test: 86.12%
Epoch: 300, Loss: 0.6920, Train: 86.08%, Valid: 86.00%, Test: 86.12%
Epoch: 325, Loss: 0.6903, Train: 86.07%, Valid: 86.00%, Test: 86.12%
Epoch: 350, Loss: 0.6809, Train: 86.07%, Valid: 86.00%, Test: 86.12%
Epoch: 375, Loss: 0.6740, Train: 86.06%, Valid: 85.99%, Test: 86.10%
Epoch: 400, Loss: 0.6770, Train: 86.05%, Valid: 85.98%, Test: 86.08%
Epoch: 425, Loss: 0.6798, Train: 85.99%, Valid: 85.93%, Test: 86.02%
Epoch: 450, Loss: 0.5188, Train: 85.88%, Valid: 85.83%, Test: 85.91%
Epoch: 475, Loss: 691.8245, Train: 85.20%, Valid: 85.15%, Test: 85.28%
Run 01:
Highest Train: 86.91
Highest Valid: 86.95
  Final Train: 86.91
   Final Test: 86.92
All runs:
Highest Train: 86.91, nan
Highest Valid: 86.95, nan
  Final Train: 86.91, nan
   Final Test: 86.92, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 8.9314, Train: 15.04%, Valid: 15.00%, Test: 14.83%
Epoch: 25, Loss: 12.8079, Train: 16.42%, Valid: 16.46%, Test: 16.33%
Epoch: 50, Loss: 13.0727, Train: 16.41%, Valid: 16.50%, Test: 16.33%
Epoch: 75, Loss: 12.1045, Train: 16.67%, Valid: 16.72%, Test: 16.59%
Epoch: 100, Loss: 11.4511, Train: 17.02%, Valid: 17.02%, Test: 16.91%
Epoch: 125, Loss: 10.8114, Train: 17.20%, Valid: 17.19%, Test: 17.10%
Epoch: 150, Loss: 10.1200, Train: 17.34%, Valid: 17.32%, Test: 17.24%
Epoch: 175, Loss: 9.4669, Train: 17.46%, Valid: 17.41%, Test: 17.32%
Epoch: 200, Loss: 8.8673, Train: 17.58%, Valid: 17.54%, Test: 17.46%
Epoch: 225, Loss: 8.3954, Train: 17.89%, Valid: 17.93%, Test: 17.79%
Epoch: 250, Loss: 6.9087, Train: 18.46%, Valid: 18.37%, Test: 18.30%
Epoch: 275, Loss: 7.4022, Train: 18.73%, Valid: 18.73%, Test: 18.56%
Epoch: 300, Loss: 4.6421, Train: 20.27%, Valid: 20.16%, Test: 20.03%
Epoch: 325, Loss: 0.5147, Train: 87.91%, Valid: 87.76%, Test: 87.94%
Epoch: 350, Loss: 0.4601, Train: 87.93%, Valid: 87.78%, Test: 87.95%
Epoch: 375, Loss: 0.5019, Train: 86.28%, Valid: 86.29%, Test: 86.37%
Epoch: 400, Loss: 0.4474, Train: 86.19%, Valid: 86.22%, Test: 86.27%
Epoch: 425, Loss: 0.4461, Train: 86.46%, Valid: 86.49%, Test: 86.54%
Epoch: 450, Loss: 0.4228, Train: 86.55%, Valid: 86.58%, Test: 86.64%
Epoch: 475, Loss: 0.4200, Train: 86.59%, Valid: 86.61%, Test: 86.68%
Run 01:
Highest Train: 87.95
Highest Valid: 87.80
  Final Train: 87.95
   Final Test: 87.97
All runs:
Highest Train: 87.95, nan
Highest Valid: 87.80, nan
  Final Train: 87.95, nan
   Final Test: 87.97, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.5496, Train: 85.49%, Valid: 85.36%, Test: 85.50%
Epoch: 25, Loss: 3.3763, Train: 86.62%, Valid: 86.65%, Test: 86.66%
Epoch: 50, Loss: 0.4774, Train: 85.24%, Valid: 85.24%, Test: 85.34%
Epoch: 75, Loss: 0.4361, Train: 84.32%, Valid: 84.14%, Test: 84.42%
Epoch: 100, Loss: 0.4352, Train: 84.31%, Valid: 84.12%, Test: 84.40%
Epoch: 125, Loss: 0.4340, Train: 84.33%, Valid: 84.14%, Test: 84.42%
Epoch: 150, Loss: 0.4336, Train: 84.32%, Valid: 84.14%, Test: 84.41%
Epoch: 175, Loss: 0.4334, Train: 84.31%, Valid: 84.13%, Test: 84.41%
Epoch: 200, Loss: 0.4345, Train: 84.31%, Valid: 84.13%, Test: 84.41%
Epoch: 225, Loss: 0.4336, Train: 84.31%, Valid: 84.13%, Test: 84.42%
Epoch: 250, Loss: 0.4331, Train: 84.32%, Valid: 84.13%, Test: 84.42%
Epoch: 275, Loss: 0.4325, Train: 84.32%, Valid: 84.14%, Test: 84.42%
Epoch: 300, Loss: 0.4351, Train: 84.32%, Valid: 84.14%, Test: 84.42%
Epoch: 325, Loss: 0.4345, Train: 85.19%, Valid: 85.20%, Test: 85.30%
Epoch: 350, Loss: 0.4310, Train: 85.21%, Valid: 85.21%, Test: 85.31%
Epoch: 375, Loss: 0.4305, Train: 85.20%, Valid: 85.21%, Test: 85.31%
Epoch: 400, Loss: 0.4306, Train: 85.19%, Valid: 85.19%, Test: 85.29%
Epoch: 425, Loss: 0.4227, Train: 85.19%, Valid: 85.19%, Test: 85.29%
Epoch: 450, Loss: 0.4205, Train: 85.18%, Valid: 85.18%, Test: 85.28%
Epoch: 475, Loss: 0.4208, Train: 85.18%, Valid: 85.19%, Test: 85.29%
Run 01:
Highest Train: 86.65
Highest Valid: 86.70
  Final Train: 86.65
   Final Test: 86.71
All runs:
Highest Train: 86.65, nan
Highest Valid: 86.70, nan
  Final Train: 86.65, nan
   Final Test: 86.71, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.5773, Train: 87.31%, Valid: 87.23%, Test: 87.28%
Epoch: 25, Loss: 0.3760, Train: 85.64%, Valid: 85.49%, Test: 85.74%
Epoch: 50, Loss: 1.4459, Train: 86.02%, Valid: 85.87%, Test: 86.12%
Epoch: 75, Loss: 0.4112, Train: 13.58%, Valid: 13.63%, Test: 13.55%
Epoch: 100, Loss: 2.8273, Train: 85.50%, Valid: 85.34%, Test: 85.61%
Epoch: 125, Loss: 0.7067, Train: 13.63%, Valid: 13.63%, Test: 13.63%
Epoch: 150, Loss: 4.5100, Train: 84.33%, Valid: 84.25%, Test: 84.56%
Epoch: 175, Loss: 3723.3911, Train: 84.63%, Valid: 84.63%, Test: 84.74%
Epoch: 200, Loss: 13520.8076, Train: 16.04%, Valid: 16.08%, Test: 15.93%
Epoch: 225, Loss: 91194.4375, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 3542.1868, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 16618.2578, Train: 86.45%, Valid: 86.45%, Test: 86.50%
Epoch: 300, Loss: 10.9099, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 676.8444, Train: 86.39%, Valid: 86.39%, Test: 86.45%
Epoch: 350, Loss: 4188.5020, Train: 15.38%, Valid: 15.58%, Test: 15.23%
Epoch: 375, Loss: 2.6944, Train: 15.52%, Valid: 15.63%, Test: 15.38%
Epoch: 400, Loss: 12595.7969, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 0.4571, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 3.3782, Train: 86.45%, Valid: 86.44%, Test: 86.50%
Epoch: 475, Loss: 2962.8401, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 87.31
Highest Valid: 87.23
  Final Train: 87.31
   Final Test: 87.28
All runs:
Highest Train: 87.31, nan
Highest Valid: 87.23, nan
  Final Train: 87.31, nan
   Final Test: 87.28, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.0582, Train: 81.54%, Valid: 81.68%, Test: 81.84%
Epoch: 25, Loss: 0.8835, Train: 84.12%, Valid: 84.23%, Test: 84.32%
Epoch: 50, Loss: 1.2100, Train: 84.91%, Valid: 84.97%, Test: 85.00%
Epoch: 75, Loss: 0.8041, Train: 85.27%, Valid: 85.34%, Test: 85.35%
Epoch: 100, Loss: 0.8210, Train: 85.72%, Valid: 85.74%, Test: 85.74%
Epoch: 125, Loss: 43181.2969, Train: 13.93%, Valid: 14.04%, Test: 13.84%
Epoch: 150, Loss: 250.5495, Train: 83.13%, Valid: 83.26%, Test: 83.36%
Epoch: 175, Loss: 1.2678, Train: 84.10%, Valid: 84.20%, Test: 84.29%
Epoch: 200, Loss: 1.1894, Train: 85.24%, Valid: 85.28%, Test: 85.32%
Epoch: 225, Loss: 1.1988, Train: 85.28%, Valid: 85.31%, Test: 85.34%
Epoch: 250, Loss: 1.1769, Train: 85.05%, Valid: 85.10%, Test: 85.14%
Epoch: 275, Loss: 1.1416, Train: 84.89%, Valid: 84.93%, Test: 84.99%
Epoch: 300, Loss: 1.1804, Train: 85.19%, Valid: 85.23%, Test: 85.27%
Epoch: 325, Loss: 1.1766, Train: 85.07%, Valid: 85.13%, Test: 85.16%
Epoch: 350, Loss: 1.2281, Train: 85.37%, Valid: 85.39%, Test: 85.42%
Epoch: 375, Loss: 1.1410, Train: 84.75%, Valid: 84.78%, Test: 84.86%
Epoch: 400, Loss: 1.1862, Train: 85.11%, Valid: 85.16%, Test: 85.21%
Epoch: 425, Loss: 1.1898, Train: 85.09%, Valid: 85.14%, Test: 85.19%
Epoch: 450, Loss: 1.1847, Train: 84.85%, Valid: 84.89%, Test: 84.95%
Epoch: 475, Loss: 1.2020, Train: 85.08%, Valid: 85.13%, Test: 85.17%
Run 01:
Highest Train: 87.08
Highest Valid: 87.08
  Final Train: 87.08
   Final Test: 87.06
All runs:
Highest Train: 87.08, nan
Highest Valid: 87.08, nan
  Final Train: 87.08, nan
   Final Test: 87.06, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.1042, Train: 85.57%, Valid: 85.53%, Test: 85.69%
Epoch: 25, Loss: 0.4494, Train: 84.40%, Valid: 84.20%, Test: 84.52%
Epoch: 50, Loss: 0.4663, Train: 85.33%, Valid: 85.31%, Test: 85.46%
Epoch: 75, Loss: 0.4362, Train: 85.33%, Valid: 85.31%, Test: 85.46%
Epoch: 100, Loss: 0.4642, Train: 85.39%, Valid: 85.38%, Test: 85.52%
Epoch: 125, Loss: 0.3940, Train: 85.39%, Valid: 85.39%, Test: 85.51%
Epoch: 150, Loss: 0.4514, Train: 85.94%, Valid: 86.01%, Test: 86.02%
Epoch: 175, Loss: 0.4245, Train: 85.96%, Valid: 86.03%, Test: 86.05%
Epoch: 200, Loss: 0.4022, Train: 85.97%, Valid: 86.04%, Test: 86.06%
Epoch: 225, Loss: 0.3825, Train: 85.86%, Valid: 85.93%, Test: 85.95%
Epoch: 250, Loss: 0.3791, Train: 85.82%, Valid: 85.90%, Test: 85.92%
Epoch: 275, Loss: 0.3754, Train: 85.81%, Valid: 85.88%, Test: 85.90%
Epoch: 300, Loss: 0.3743, Train: 85.82%, Valid: 85.87%, Test: 85.90%
Epoch: 325, Loss: 0.3698, Train: 85.79%, Valid: 85.84%, Test: 85.87%
Epoch: 350, Loss: 0.3615, Train: 85.78%, Valid: 85.81%, Test: 85.87%
Epoch: 375, Loss: 0.3503, Train: 85.80%, Valid: 85.82%, Test: 85.88%
Epoch: 400, Loss: 0.3440, Train: 85.66%, Valid: 85.70%, Test: 85.80%
Epoch: 425, Loss: 0.3400, Train: 86.07%, Valid: 85.84%, Test: 86.06%
Epoch: 450, Loss: 0.3357, Train: 85.60%, Valid: 85.62%, Test: 85.74%
Epoch: 475, Loss: 0.3324, Train: 85.51%, Valid: 85.52%, Test: 85.65%
Run 01:
Highest Train: 87.43
Highest Valid: 87.46
  Final Train: 87.43
   Final Test: 87.41
All runs:
Highest Train: 87.43, nan
Highest Valid: 87.46, nan
  Final Train: 87.43, nan
   Final Test: 87.41, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.8125, Train: 14.05%, Valid: 14.13%, Test: 14.06%
Epoch: 25, Loss: 0.4858, Train: 85.43%, Valid: 85.31%, Test: 85.56%
Epoch: 50, Loss: 0.4441, Train: 85.50%, Valid: 85.36%, Test: 85.61%
Epoch: 75, Loss: 0.4306, Train: 85.55%, Valid: 85.41%, Test: 85.66%
Epoch: 100, Loss: 0.4200, Train: 85.62%, Valid: 85.49%, Test: 85.73%
Epoch: 125, Loss: 0.4124, Train: 85.78%, Valid: 85.66%, Test: 85.89%
Epoch: 150, Loss: 0.4462, Train: 85.95%, Valid: 85.85%, Test: 86.08%
Epoch: 175, Loss: 0.4468, Train: 86.30%, Valid: 86.25%, Test: 86.40%
Epoch: 200, Loss: 0.4347, Train: 86.41%, Valid: 86.40%, Test: 86.51%
Epoch: 225, Loss: 0.4153, Train: 86.39%, Valid: 86.36%, Test: 86.48%
Epoch: 250, Loss: 0.4140, Train: 86.43%, Valid: 86.41%, Test: 86.53%
Epoch: 275, Loss: 0.4224, Train: 86.36%, Valid: 86.33%, Test: 86.47%
Epoch: 300, Loss: 0.4188, Train: 86.18%, Valid: 86.08%, Test: 86.31%
Epoch: 325, Loss: 0.4117, Train: 85.97%, Valid: 85.85%, Test: 86.13%
Epoch: 350, Loss: 0.4989, Train: 85.96%, Valid: 85.86%, Test: 86.14%
Epoch: 375, Loss: 32823263209979904.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 0.6611, Train: 85.80%, Valid: 85.67%, Test: 85.94%
Epoch: 425, Loss: 594082493628416.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 87.09
Highest Valid: 86.94
  Final Train: 87.09
   Final Test: 87.21
All runs:
Highest Train: 87.09, nan
Highest Valid: 86.94, nan
  Final Train: 87.09, nan
   Final Test: 87.21, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.2516, Train: 78.75%, Valid: 78.83%, Test: 79.14%
Epoch: 25, Loss: 0.4422, Train: 85.30%, Valid: 85.13%, Test: 85.36%
Epoch: 50, Loss: 0.4509, Train: 85.27%, Valid: 85.09%, Test: 85.33%
Epoch: 75, Loss: 0.4500, Train: 85.30%, Valid: 85.12%, Test: 85.36%
Epoch: 100, Loss: 0.5208, Train: 85.27%, Valid: 85.09%, Test: 85.33%
Epoch: 125, Loss: 0.5344, Train: 85.27%, Valid: 85.09%, Test: 85.33%
Epoch: 150, Loss: 0.5311, Train: 85.27%, Valid: 85.09%, Test: 85.33%
Epoch: 175, Loss: 0.5255, Train: 85.26%, Valid: 85.09%, Test: 85.32%
Epoch: 200, Loss: 0.5219, Train: 85.26%, Valid: 85.08%, Test: 85.32%
Epoch: 225, Loss: 0.5189, Train: 85.25%, Valid: 85.08%, Test: 85.32%
Epoch: 250, Loss: 0.5163, Train: 85.25%, Valid: 85.07%, Test: 85.31%
Epoch: 275, Loss: 0.5140, Train: 85.24%, Valid: 85.07%, Test: 85.31%
Epoch: 300, Loss: 0.5121, Train: 85.24%, Valid: 85.06%, Test: 85.30%
Epoch: 325, Loss: 0.5103, Train: 85.23%, Valid: 85.05%, Test: 85.30%
Epoch: 350, Loss: 0.5092, Train: 85.23%, Valid: 85.05%, Test: 85.30%
Epoch: 375, Loss: 0.5085, Train: 85.23%, Valid: 85.05%, Test: 85.30%
Epoch: 400, Loss: 0.5070, Train: 85.23%, Valid: 85.05%, Test: 85.30%
Epoch: 425, Loss: 0.5066, Train: 85.23%, Valid: 85.05%, Test: 85.29%
Epoch: 450, Loss: 0.5050, Train: 85.22%, Valid: 85.04%, Test: 85.29%
Epoch: 475, Loss: 0.5043, Train: 85.22%, Valid: 85.04%, Test: 85.29%
Run 01:
Highest Train: 86.07
Highest Valid: 86.06
  Final Train: 86.07
   Final Test: 86.13
All runs:
Highest Train: 86.07, nan
Highest Valid: 86.06, nan
  Final Train: 86.07, nan
   Final Test: 86.13, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.5745, Train: 86.44%, Valid: 86.28%, Test: 86.49%
Epoch: 25, Loss: 0.4954, Train: 86.45%, Valid: 86.50%, Test: 86.50%
Epoch: 50, Loss: 0.5076, Train: 85.89%, Valid: 85.92%, Test: 85.99%
Epoch: 75, Loss: 0.4748, Train: 85.59%, Valid: 85.66%, Test: 85.71%
Epoch: 100, Loss: 0.4472, Train: 85.43%, Valid: 85.48%, Test: 85.53%
Epoch: 125, Loss: 0.3713, Train: 85.45%, Valid: 85.46%, Test: 85.54%
Epoch: 150, Loss: 0.3522, Train: 85.24%, Valid: 85.20%, Test: 85.37%
Epoch: 175, Loss: 0.3458, Train: 85.62%, Valid: 85.61%, Test: 85.72%
Epoch: 200, Loss: 0.3365, Train: 85.35%, Valid: 85.33%, Test: 85.46%
Epoch: 225, Loss: 0.3323, Train: 85.61%, Valid: 85.61%, Test: 85.71%
Epoch: 250, Loss: 0.3293, Train: 85.54%, Valid: 85.54%, Test: 85.62%
Epoch: 275, Loss: 0.3269, Train: 85.62%, Valid: 85.63%, Test: 85.72%
Epoch: 300, Loss: 0.5017, Train: 85.40%, Valid: 85.41%, Test: 85.48%
Epoch: 325, Loss: 1.0555, Train: 85.36%, Valid: 85.39%, Test: 85.45%
Epoch: 350, Loss: 0.9471, Train: 85.36%, Valid: 85.39%, Test: 85.44%
Epoch: 375, Loss: 0.9200, Train: 85.36%, Valid: 85.38%, Test: 85.44%
Epoch: 400, Loss: 0.8720, Train: 85.33%, Valid: 85.36%, Test: 85.42%
Epoch: 425, Loss: 0.7604, Train: 85.34%, Valid: 85.36%, Test: 85.42%
Epoch: 450, Loss: 0.6024, Train: 85.37%, Valid: 85.39%, Test: 85.44%
Epoch: 475, Loss: 0.4227, Train: 85.35%, Valid: 85.38%, Test: 85.43%
Run 01:
Highest Train: 86.71
Highest Valid: 86.78
  Final Train: 86.71
   Final Test: 86.77
All runs:
Highest Train: 86.71, nan
Highest Valid: 86.78, nan
  Final Train: 86.71, nan
   Final Test: 86.77, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 7.7475, Train: 84.23%, Valid: 84.24%, Test: 84.36%
Epoch: 25, Loss: 0.5609, Train: 86.25%, Valid: 86.21%, Test: 86.31%
Epoch: 50, Loss: 0.6437, Train: 86.28%, Valid: 86.24%, Test: 86.35%
Epoch: 75, Loss: 1.3002, Train: 86.19%, Valid: 86.08%, Test: 86.32%
Epoch: 100, Loss: 0.6374, Train: 85.97%, Valid: 85.83%, Test: 86.09%
Epoch: 125, Loss: 0.6391, Train: 86.00%, Valid: 85.86%, Test: 86.12%
Epoch: 150, Loss: 0.6367, Train: 85.92%, Valid: 85.78%, Test: 86.01%
Epoch: 175, Loss: 0.6172, Train: 85.88%, Valid: 85.74%, Test: 85.97%
Epoch: 200, Loss: 0.5969, Train: 85.85%, Valid: 85.70%, Test: 85.94%
Epoch: 225, Loss: 0.5450, Train: 85.82%, Valid: 85.67%, Test: 85.91%
Epoch: 250, Loss: 0.3884, Train: 85.81%, Valid: 85.65%, Test: 85.91%
Epoch: 275, Loss: 0.3450, Train: 85.88%, Valid: 85.72%, Test: 85.99%
Epoch: 300, Loss: 0.3383, Train: 85.94%, Valid: 85.78%, Test: 86.04%
Epoch: 325, Loss: 0.3293, Train: 85.93%, Valid: 85.74%, Test: 86.04%
Epoch: 350, Loss: 0.6221, Train: 85.81%, Valid: 85.61%, Test: 85.90%
Epoch: 375, Loss: 0.7063, Train: 87.43%, Valid: 87.32%, Test: 87.51%
Epoch: 400, Loss: 0.6704, Train: 87.27%, Valid: 87.14%, Test: 87.37%
Epoch: 425, Loss: 0.6116, Train: 87.34%, Valid: 87.22%, Test: 87.43%
Epoch: 450, Loss: 0.5295, Train: 87.37%, Valid: 87.26%, Test: 87.46%
Epoch: 475, Loss: 0.4155, Train: 86.02%, Valid: 85.89%, Test: 86.05%
Run 01:
Highest Train: 87.43
Highest Valid: 87.32
  Final Train: 87.43
   Final Test: 87.52
All runs:
Highest Train: 87.43, nan
Highest Valid: 87.32, nan
  Final Train: 87.43, nan
   Final Test: 87.52, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.4827, Train: 84.82%, Valid: 84.68%, Test: 84.90%
Epoch: 25, Loss: 0.3670, Train: 85.54%, Valid: 85.40%, Test: 85.59%
Epoch: 50, Loss: 0.3631, Train: 85.58%, Valid: 85.44%, Test: 85.61%
Epoch: 75, Loss: 0.3585, Train: 85.50%, Valid: 85.35%, Test: 85.55%
Epoch: 100, Loss: 0.3522, Train: 85.51%, Valid: 85.35%, Test: 85.56%
Epoch: 125, Loss: 0.3420, Train: 85.29%, Valid: 85.11%, Test: 85.36%
Epoch: 150, Loss: 0.3359, Train: 85.26%, Valid: 85.08%, Test: 85.34%
Epoch: 175, Loss: 0.3335, Train: 85.51%, Valid: 85.34%, Test: 85.57%
Epoch: 200, Loss: 0.3508, Train: 85.56%, Valid: 85.41%, Test: 85.66%
Epoch: 225, Loss: 0.3384, Train: 85.77%, Valid: 85.59%, Test: 85.84%
Epoch: 250, Loss: 0.3305, Train: 85.65%, Valid: 85.45%, Test: 85.72%
Epoch: 275, Loss: 0.3265, Train: 85.81%, Valid: 85.59%, Test: 85.86%
Epoch: 300, Loss: 0.5360, Train: 85.35%, Valid: 85.21%, Test: 85.43%
Epoch: 325, Loss: 0.9632, Train: 85.59%, Valid: 85.46%, Test: 85.70%
Epoch: 350, Loss: 1.0015, Train: 85.66%, Valid: 85.52%, Test: 85.76%
Epoch: 375, Loss: 0.9994, Train: 85.59%, Valid: 85.44%, Test: 85.68%
Epoch: 400, Loss: 0.9552, Train: 85.57%, Valid: 85.42%, Test: 85.67%
Epoch: 425, Loss: 0.6322, Train: 85.58%, Valid: 85.42%, Test: 85.67%
Epoch: 450, Loss: 0.3678, Train: 85.77%, Valid: 85.54%, Test: 85.83%
Epoch: 475, Loss: 0.3429, Train: 86.79%, Valid: 86.66%, Test: 86.83%
Run 01:
Highest Train: 87.90
Highest Valid: 87.76
  Final Train: 87.89
   Final Test: 87.90
All runs:
Highest Train: 87.90, nan
Highest Valid: 87.76, nan
  Final Train: 87.89, nan
   Final Test: 87.90, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.3853, Train: 83.43%, Valid: 83.26%, Test: 83.56%
Epoch: 25, Loss: 0.4031, Train: 88.32%, Valid: 88.39%, Test: 88.32%
Epoch: 50, Loss: 0.4472, Train: 86.52%, Valid: 86.53%, Test: 86.51%
Epoch: 75, Loss: 0.6050, Train: 86.54%, Valid: 86.55%, Test: 86.51%
Epoch: 100, Loss: 0.5754, Train: 86.56%, Valid: 86.59%, Test: 86.54%
Epoch: 125, Loss: 0.6028, Train: 86.48%, Valid: 86.50%, Test: 86.46%
Epoch: 150, Loss: 0.5906, Train: 86.32%, Valid: 86.34%, Test: 86.34%
Epoch: 175, Loss: 0.5757, Train: 86.22%, Valid: 86.27%, Test: 86.29%
Epoch: 200, Loss: 0.5693, Train: 86.10%, Valid: 86.16%, Test: 86.18%
Epoch: 225, Loss: 0.5648, Train: 86.01%, Valid: 86.10%, Test: 86.10%
Epoch: 250, Loss: 0.5630, Train: 85.94%, Valid: 86.04%, Test: 86.02%
Epoch: 275, Loss: 0.5585, Train: 85.86%, Valid: 85.93%, Test: 85.94%
Epoch: 300, Loss: 0.5522, Train: 85.81%, Valid: 85.89%, Test: 85.89%
Epoch: 325, Loss: 0.5443, Train: 85.78%, Valid: 85.86%, Test: 85.87%
Epoch: 350, Loss: 0.5017, Train: 85.75%, Valid: 85.83%, Test: 85.82%
Epoch: 375, Loss: 0.3885, Train: 85.66%, Valid: 85.73%, Test: 85.73%
Epoch: 400, Loss: 0.3931, Train: 85.67%, Valid: 85.73%, Test: 85.73%
Epoch: 425, Loss: 0.3661, Train: 85.60%, Valid: 85.64%, Test: 85.66%
Epoch: 450, Loss: 0.3524, Train: 85.52%, Valid: 85.60%, Test: 85.62%
Epoch: 475, Loss: 0.3491, Train: 85.82%, Valid: 85.84%, Test: 85.84%
Run 01:
Highest Train: 88.32
Highest Valid: 88.39
  Final Train: 88.32
   Final Test: 88.32
All runs:
Highest Train: 88.32, nan
Highest Valid: 88.39, nan
  Final Train: 88.32, nan
   Final Test: 88.32, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 13.9085, Train: 14.40%, Valid: 14.31%, Test: 14.30%
Epoch: 25, Loss: 0.4834, Train: 85.83%, Valid: 85.69%, Test: 85.92%
Epoch: 50, Loss: 0.4983, Train: 86.01%, Valid: 85.87%, Test: 86.11%
Epoch: 75, Loss: 0.4700, Train: 85.87%, Valid: 85.74%, Test: 85.96%
Epoch: 100, Loss: 0.4577, Train: 85.78%, Valid: 85.64%, Test: 85.87%
Epoch: 125, Loss: 0.4335, Train: 85.72%, Valid: 85.59%, Test: 85.81%
Epoch: 150, Loss: 0.3920, Train: 85.82%, Valid: 85.69%, Test: 85.90%
Epoch: 175, Loss: 0.3534, Train: 85.70%, Valid: 85.54%, Test: 85.82%
Epoch: 200, Loss: 0.3441, Train: 85.87%, Valid: 85.74%, Test: 85.97%
Epoch: 225, Loss: 0.3327, Train: 85.64%, Valid: 85.48%, Test: 85.77%
Epoch: 250, Loss: 0.3400, Train: 86.19%, Valid: 86.01%, Test: 86.30%
Epoch: 275, Loss: 0.3306, Train: 86.07%, Valid: 85.90%, Test: 86.17%
Epoch: 300, Loss: 0.3278, Train: 86.16%, Valid: 85.99%, Test: 86.28%
Epoch: 325, Loss: 1.8957, Train: 85.95%, Valid: 85.81%, Test: 86.07%
Epoch: 350, Loss: 0.9716, Train: 85.90%, Valid: 85.76%, Test: 86.02%
Epoch: 375, Loss: 0.9397, Train: 85.88%, Valid: 85.74%, Test: 86.00%
Epoch: 400, Loss: 0.7671, Train: 85.88%, Valid: 85.74%, Test: 86.01%
Epoch: 425, Loss: 0.4736, Train: 85.92%, Valid: 85.77%, Test: 86.04%
Epoch: 450, Loss: 0.3924, Train: 85.96%, Valid: 85.81%, Test: 86.08%
Epoch: 475, Loss: 0.3849, Train: 85.99%, Valid: 85.84%, Test: 86.11%
Run 01:
Highest Train: 86.84
Highest Valid: 86.87
  Final Train: 86.84
   Final Test: 86.89
All runs:
Highest Train: 86.84, nan
Highest Valid: 86.87, nan
  Final Train: 86.84, nan
   Final Test: 86.89, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.1198, Train: 86.22%, Valid: 86.18%, Test: 86.28%
Epoch: 25, Loss: 0.5761, Train: 85.43%, Valid: 85.26%, Test: 85.48%
Epoch: 50, Loss: 0.5585, Train: 85.39%, Valid: 85.20%, Test: 85.44%
Epoch: 75, Loss: 0.5477, Train: 85.51%, Valid: 85.33%, Test: 85.55%
Epoch: 100, Loss: 0.5218, Train: 85.50%, Valid: 85.33%, Test: 85.55%
Epoch: 125, Loss: 0.4925, Train: 85.50%, Valid: 85.33%, Test: 85.55%
Epoch: 150, Loss: 0.4421, Train: 85.51%, Valid: 85.33%, Test: 85.55%
Epoch: 175, Loss: 0.3615, Train: 85.99%, Valid: 85.80%, Test: 86.01%
Epoch: 200, Loss: 0.3553, Train: 85.48%, Valid: 85.29%, Test: 85.54%
Epoch: 225, Loss: 0.3489, Train: 85.50%, Valid: 85.31%, Test: 85.56%
Epoch: 250, Loss: 0.3396, Train: 85.56%, Valid: 85.37%, Test: 85.61%
Epoch: 275, Loss: 0.3328, Train: 85.56%, Valid: 85.35%, Test: 85.62%
Epoch: 300, Loss: 0.3282, Train: 85.66%, Valid: 85.44%, Test: 85.72%
Epoch: 325, Loss: 0.3265, Train: 85.75%, Valid: 85.53%, Test: 85.81%
Epoch: 350, Loss: 0.3264, Train: 85.41%, Valid: 85.17%, Test: 85.48%
Epoch: 375, Loss: 0.3302, Train: 85.82%, Valid: 85.58%, Test: 85.89%
Epoch: 400, Loss: 0.3311, Train: 85.85%, Valid: 85.62%, Test: 85.91%
Epoch: 425, Loss: 0.3395, Train: 85.87%, Valid: 85.64%, Test: 85.93%
Epoch: 450, Loss: 0.3285, Train: 85.81%, Valid: 85.59%, Test: 85.87%
Epoch: 475, Loss: 0.3259, Train: 85.75%, Valid: 85.53%, Test: 85.82%
Run 01:
Highest Train: 86.63
Highest Valid: 86.69
  Final Train: 86.63
   Final Test: 86.73
All runs:
Highest Train: 86.63, nan
Highest Valid: 86.69, nan
  Final Train: 86.63, nan
   Final Test: 86.73, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 5.6503, Train: 83.37%, Valid: 83.41%, Test: 83.46%
Epoch: 25, Loss: 0.4602, Train: 86.34%, Valid: 86.36%, Test: 86.39%
Epoch: 50, Loss: 0.5451, Train: 86.37%, Valid: 86.41%, Test: 86.44%
Epoch: 75, Loss: 0.4465, Train: 86.08%, Valid: 86.15%, Test: 86.11%
Epoch: 100, Loss: 0.4305, Train: 86.29%, Valid: 86.32%, Test: 86.34%
Epoch: 125, Loss: 0.4126, Train: 86.27%, Valid: 86.29%, Test: 86.30%
Epoch: 150, Loss: 0.3936, Train: 86.13%, Valid: 86.18%, Test: 86.16%
Epoch: 175, Loss: 0.3737, Train: 85.83%, Valid: 85.88%, Test: 85.89%
Epoch: 200, Loss: 0.3615, Train: 86.88%, Valid: 86.93%, Test: 86.97%
Epoch: 225, Loss: 0.3564, Train: 87.68%, Valid: 87.72%, Test: 87.75%
Epoch: 250, Loss: 0.3527, Train: 87.07%, Valid: 87.07%, Test: 87.16%
Epoch: 275, Loss: 0.3490, Train: 87.11%, Valid: 87.14%, Test: 87.19%
Epoch: 300, Loss: 0.3453, Train: 87.28%, Valid: 87.28%, Test: 87.35%
Epoch: 325, Loss: 0.3389, Train: 85.88%, Valid: 85.72%, Test: 85.91%
Epoch: 350, Loss: 0.3287, Train: 86.03%, Valid: 86.02%, Test: 86.11%
Epoch: 375, Loss: 0.3614, Train: 85.43%, Valid: 85.25%, Test: 85.50%
Epoch: 400, Loss: 0.3431, Train: 86.96%, Valid: 86.90%, Test: 87.01%
Epoch: 425, Loss: 0.3405, Train: 86.59%, Valid: 86.55%, Test: 86.72%
Epoch: 450, Loss: 0.3375, Train: 86.28%, Valid: 86.24%, Test: 86.38%
Epoch: 475, Loss: 0.3340, Train: 86.22%, Valid: 86.14%, Test: 86.30%
Run 01:
Highest Train: 88.03
Highest Valid: 88.07
  Final Train: 88.03
   Final Test: 88.10
All runs:
Highest Train: 88.03, nan
Highest Valid: 88.07, nan
  Final Train: 88.03, nan
   Final Test: 88.10, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.4752, Train: 86.08%, Valid: 86.01%, Test: 86.14%
Epoch: 25, Loss: 0.4300, Train: 86.17%, Valid: 86.08%, Test: 86.22%
Epoch: 50, Loss: 0.4031, Train: 86.03%, Valid: 85.91%, Test: 86.13%
Epoch: 75, Loss: 0.3761, Train: 85.68%, Valid: 85.54%, Test: 85.77%
Epoch: 100, Loss: 0.3587, Train: 85.65%, Valid: 85.50%, Test: 85.76%
Epoch: 125, Loss: 0.3538, Train: 85.87%, Valid: 85.71%, Test: 85.98%
Epoch: 150, Loss: 0.3453, Train: 85.50%, Valid: 85.39%, Test: 85.62%
Epoch: 175, Loss: 0.3338, Train: 85.66%, Valid: 85.51%, Test: 85.77%
Epoch: 200, Loss: 0.3300, Train: 85.82%, Valid: 85.66%, Test: 85.92%
Epoch: 225, Loss: 0.3264, Train: 86.06%, Valid: 85.89%, Test: 86.17%
Epoch: 250, Loss: 0.3744, Train: 87.32%, Valid: 87.21%, Test: 87.42%
Epoch: 275, Loss: 0.3380, Train: 86.60%, Valid: 86.45%, Test: 86.69%
Epoch: 300, Loss: 0.3325, Train: 86.13%, Valid: 85.93%, Test: 86.25%
Epoch: 325, Loss: 0.3296, Train: 86.10%, Valid: 85.94%, Test: 86.23%
Epoch: 350, Loss: 0.3273, Train: 86.14%, Valid: 85.98%, Test: 86.28%
Epoch: 375, Loss: 0.3420, Train: 86.26%, Valid: 86.08%, Test: 86.37%
Epoch: 400, Loss: 0.7910, Train: 86.21%, Valid: 86.09%, Test: 86.32%
Epoch: 425, Loss: 0.6059, Train: 86.11%, Valid: 85.99%, Test: 86.22%
Epoch: 450, Loss: 0.4660, Train: 86.20%, Valid: 86.07%, Test: 86.29%
Epoch: 475, Loss: 0.3727, Train: 86.48%, Valid: 86.35%, Test: 86.57%
Run 01:
Highest Train: 87.60
Highest Valid: 87.46
  Final Train: 87.60
   Final Test: 87.69
All runs:
Highest Train: 87.60, nan
Highest Valid: 87.46, nan
  Final Train: 87.60, nan
   Final Test: 87.69, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 8.4731, Train: 17.42%, Valid: 17.33%, Test: 17.11%
Epoch: 25, Loss: 0.4445, Train: 85.41%, Valid: 85.26%, Test: 85.48%
Epoch: 50, Loss: 0.4392, Train: 85.40%, Valid: 85.24%, Test: 85.46%
Epoch: 75, Loss: 0.4304, Train: 85.45%, Valid: 85.30%, Test: 85.50%
Epoch: 100, Loss: 0.4208, Train: 85.47%, Valid: 85.30%, Test: 85.51%
Epoch: 125, Loss: 0.4093, Train: 85.35%, Valid: 85.19%, Test: 85.42%
Epoch: 150, Loss: 0.3933, Train: 85.52%, Valid: 85.33%, Test: 85.56%
Epoch: 175, Loss: 0.3648, Train: 85.53%, Valid: 85.35%, Test: 85.59%
Epoch: 200, Loss: 0.3580, Train: 85.55%, Valid: 85.37%, Test: 85.61%
Epoch: 225, Loss: 0.3553, Train: 85.54%, Valid: 85.37%, Test: 85.61%
Epoch: 250, Loss: 0.3529, Train: 85.52%, Valid: 85.35%, Test: 85.58%
Epoch: 275, Loss: 0.3502, Train: 85.46%, Valid: 85.30%, Test: 85.53%
Epoch: 300, Loss: 0.3452, Train: 85.44%, Valid: 85.28%, Test: 85.51%
Epoch: 325, Loss: 0.3385, Train: 85.32%, Valid: 85.14%, Test: 85.39%
Epoch: 350, Loss: 0.3340, Train: 85.38%, Valid: 85.20%, Test: 85.45%
Epoch: 375, Loss: 0.3318, Train: 85.44%, Valid: 85.25%, Test: 85.51%
Epoch: 400, Loss: 0.3294, Train: 85.62%, Valid: 85.43%, Test: 85.68%
Epoch: 425, Loss: 0.3281, Train: 85.74%, Valid: 85.54%, Test: 85.81%
Epoch: 450, Loss: 0.3292, Train: 85.84%, Valid: 85.64%, Test: 85.90%
Epoch: 475, Loss: 0.3585, Train: 85.48%, Valid: 85.29%, Test: 85.55%
Run 01:
Highest Train: 85.84
Highest Valid: 85.64
  Final Train: 85.84
   Final Test: 85.90
All runs:
Highest Train: 85.84, nan
Highest Valid: 85.64, nan
  Final Train: 85.84, nan
   Final Test: 85.90, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.4433, Train: 85.35%, Valid: 85.35%, Test: 85.43%
Epoch: 25, Loss: 0.3642, Train: 85.72%, Valid: 85.58%, Test: 85.76%
Epoch: 50, Loss: 0.3595, Train: 86.81%, Valid: 86.83%, Test: 86.87%
Epoch: 75, Loss: 0.3528, Train: 85.97%, Valid: 85.93%, Test: 86.01%
Epoch: 100, Loss: 0.3427, Train: 85.29%, Valid: 85.26%, Test: 85.38%
Epoch: 125, Loss: 0.3461, Train: 85.47%, Valid: 85.49%, Test: 85.57%
Epoch: 150, Loss: 0.6890, Train: 85.32%, Valid: 85.35%, Test: 85.42%
Epoch: 175, Loss: 0.3557, Train: 85.31%, Valid: 85.30%, Test: 85.35%
Epoch: 200, Loss: 0.3394, Train: 85.66%, Valid: 85.67%, Test: 85.76%
Epoch: 225, Loss: 0.3335, Train: 85.77%, Valid: 85.61%, Test: 85.84%
Epoch: 250, Loss: 0.3299, Train: 85.96%, Valid: 85.80%, Test: 86.08%
Epoch: 275, Loss: 0.3265, Train: 85.58%, Valid: 85.54%, Test: 85.67%
Epoch: 300, Loss: 1.0818, Train: 85.33%, Valid: 85.36%, Test: 85.42%
Epoch: 325, Loss: 1.1547, Train: 85.33%, Valid: 85.37%, Test: 85.42%
Epoch: 350, Loss: 0.8537, Train: 85.35%, Valid: 85.36%, Test: 85.43%
Epoch: 375, Loss: 0.6448, Train: 85.36%, Valid: 85.38%, Test: 85.44%
Epoch: 400, Loss: 0.3963, Train: 86.05%, Valid: 86.07%, Test: 86.12%
Epoch: 425, Loss: 0.3421, Train: 85.85%, Valid: 85.73%, Test: 85.93%
Epoch: 450, Loss: 0.3351, Train: 86.00%, Valid: 85.96%, Test: 86.05%
Epoch: 475, Loss: 0.3286, Train: 85.47%, Valid: 85.45%, Test: 85.56%
Run 01:
Highest Train: 87.09
Highest Valid: 87.12
  Final Train: 87.09
   Final Test: 87.16
All runs:
Highest Train: 87.09, nan
Highest Valid: 87.12, nan
  Final Train: 87.09, nan
   Final Test: 87.16, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 14.7604, Train: 16.45%, Valid: 16.29%, Test: 16.39%
Epoch: 25, Loss: 0.5353, Train: 86.19%, Valid: 86.11%, Test: 86.22%
Epoch: 50, Loss: 0.5246, Train: 86.07%, Valid: 85.97%, Test: 86.13%
Epoch: 75, Loss: 0.4473, Train: 86.03%, Valid: 85.91%, Test: 86.13%
Epoch: 100, Loss: 0.3573, Train: 85.90%, Valid: 85.73%, Test: 85.98%
Epoch: 125, Loss: 0.3525, Train: 85.84%, Valid: 85.66%, Test: 85.92%
Epoch: 150, Loss: 0.3446, Train: 86.39%, Valid: 86.27%, Test: 86.53%
Epoch: 175, Loss: 0.3327, Train: 85.68%, Valid: 85.50%, Test: 85.78%
Epoch: 200, Loss: 0.7623, Train: 85.76%, Valid: 85.62%, Test: 85.87%
Epoch: 225, Loss: 0.5871, Train: 85.78%, Valid: 85.64%, Test: 85.89%
Epoch: 250, Loss: 0.4321, Train: 85.83%, Valid: 85.68%, Test: 85.93%
Epoch: 275, Loss: 0.3409, Train: 87.37%, Valid: 87.33%, Test: 87.44%
Epoch: 300, Loss: 0.3337, Train: 85.92%, Valid: 85.73%, Test: 86.03%
Epoch: 325, Loss: 0.3292, Train: 85.51%, Valid: 85.33%, Test: 85.64%
Epoch: 350, Loss: 0.3288, Train: 86.02%, Valid: 85.83%, Test: 86.14%
Epoch: 375, Loss: 0.3275, Train: 86.14%, Valid: 85.96%, Test: 86.26%
Epoch: 400, Loss: 0.3271, Train: 86.13%, Valid: 85.94%, Test: 86.25%
Epoch: 425, Loss: 0.7529, Train: 85.83%, Valid: 85.70%, Test: 85.96%
Epoch: 450, Loss: 1.0921, Train: 85.94%, Valid: 85.80%, Test: 86.07%
Epoch: 475, Loss: 1.2007, Train: 85.99%, Valid: 85.86%, Test: 86.10%
Run 01:
Highest Train: 87.83
Highest Valid: 87.72
  Final Train: 87.83
   Final Test: 87.88
All runs:
Highest Train: 87.83, nan
Highest Valid: 87.72, nan
  Final Train: 87.83, nan
   Final Test: 87.88, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 20.3515, Train: 20.60%, Valid: 20.54%, Test: 20.25%
Epoch: 25, Loss: 0.5573, Train: 85.44%, Valid: 85.30%, Test: 85.46%
Epoch: 50, Loss: 0.5360, Train: 86.90%, Valid: 86.75%, Test: 86.85%
Epoch: 75, Loss: 0.4084, Train: 86.82%, Valid: 86.68%, Test: 86.79%
Epoch: 100, Loss: 0.3682, Train: 85.40%, Valid: 85.27%, Test: 85.47%
Epoch: 125, Loss: 0.3655, Train: 85.37%, Valid: 85.25%, Test: 85.44%
Epoch: 150, Loss: 0.3622, Train: 85.34%, Valid: 85.22%, Test: 85.41%
Epoch: 175, Loss: 0.3569, Train: 85.27%, Valid: 85.13%, Test: 85.34%
Epoch: 200, Loss: 0.3525, Train: 85.22%, Valid: 85.07%, Test: 85.30%
Epoch: 225, Loss: 0.3482, Train: 85.15%, Valid: 84.99%, Test: 85.24%
Epoch: 250, Loss: 0.3437, Train: 85.04%, Valid: 84.88%, Test: 85.14%
Epoch: 275, Loss: 0.3392, Train: 85.02%, Valid: 84.85%, Test: 85.12%
Epoch: 300, Loss: 0.3362, Train: 85.00%, Valid: 84.83%, Test: 85.10%
Epoch: 325, Loss: 0.3405, Train: 85.32%, Valid: 85.15%, Test: 85.40%
Epoch: 350, Loss: 0.3336, Train: 85.32%, Valid: 85.15%, Test: 85.40%
Epoch: 375, Loss: 0.3321, Train: 85.32%, Valid: 85.14%, Test: 85.40%
Epoch: 400, Loss: 0.3306, Train: 85.35%, Valid: 85.17%, Test: 85.42%
Epoch: 425, Loss: 0.3297, Train: 85.54%, Valid: 85.33%, Test: 85.61%
Epoch: 450, Loss: 0.3285, Train: 85.68%, Valid: 85.48%, Test: 85.75%
Epoch: 475, Loss: 0.3351, Train: 85.71%, Valid: 85.49%, Test: 85.78%
Run 01:
Highest Train: 86.93
Highest Valid: 86.78
  Final Train: 86.93
   Final Test: 86.87
All runs:
Highest Train: 86.93, nan
Highest Valid: 86.78, nan
  Final Train: 86.93, nan
   Final Test: 86.87, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 16.1364, Train: 14.99%, Valid: 14.96%, Test: 14.85%
Epoch: 25, Loss: 0.5278, Train: 86.32%, Valid: 86.36%, Test: 86.34%
Epoch: 50, Loss: 0.5273, Train: 86.39%, Valid: 86.40%, Test: 86.39%
Epoch: 75, Loss: 0.4982, Train: 86.54%, Valid: 86.60%, Test: 86.56%
Epoch: 100, Loss: 0.4392, Train: 86.05%, Valid: 86.11%, Test: 86.14%
Epoch: 125, Loss: 0.3476, Train: 85.35%, Valid: 85.32%, Test: 85.49%
Epoch: 150, Loss: 0.3351, Train: 85.70%, Valid: 85.67%, Test: 85.76%
Epoch: 175, Loss: 0.6029, Train: 86.43%, Valid: 86.46%, Test: 86.51%
Epoch: 200, Loss: 0.3841, Train: 85.57%, Valid: 85.59%, Test: 85.62%
Epoch: 225, Loss: 0.3363, Train: 87.37%, Valid: 87.24%, Test: 87.43%
Epoch: 250, Loss: 0.3287, Train: 85.72%, Valid: 85.74%, Test: 85.83%
Epoch: 275, Loss: 0.3339, Train: 85.85%, Valid: 85.89%, Test: 85.94%
Epoch: 300, Loss: 0.5975, Train: 85.63%, Valid: 85.66%, Test: 85.70%
Epoch: 325, Loss: 0.7388, Train: 85.63%, Valid: 85.67%, Test: 85.73%
Epoch: 350, Loss: 0.8608, Train: 85.59%, Valid: 85.64%, Test: 85.70%
Epoch: 375, Loss: 0.6937, Train: 85.61%, Valid: 85.66%, Test: 85.71%
Epoch: 400, Loss: 0.5867, Train: 85.60%, Valid: 85.64%, Test: 85.70%
Epoch: 425, Loss: 0.4962, Train: 85.60%, Valid: 85.64%, Test: 85.70%
Epoch: 450, Loss: 0.4081, Train: 85.63%, Valid: 85.66%, Test: 85.73%
Epoch: 475, Loss: 0.3417, Train: 86.00%, Valid: 86.01%, Test: 86.12%
Run 01:
Highest Train: 88.20
Highest Valid: 88.25
  Final Train: 88.20
   Final Test: 88.28
All runs:
Highest Train: 88.20, nan
Highest Valid: 88.25, nan
  Final Train: 88.20, nan
   Final Test: 88.28, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.3877, Train: 84.81%, Valid: 84.73%, Test: 84.90%
Epoch: 25, Loss: 0.3644, Train: 85.06%, Valid: 84.98%, Test: 85.21%
Epoch: 50, Loss: 0.3583, Train: 85.22%, Valid: 85.13%, Test: 85.36%
Epoch: 75, Loss: 0.3506, Train: 85.19%, Valid: 85.09%, Test: 85.32%
Epoch: 100, Loss: 0.3425, Train: 85.59%, Valid: 85.44%, Test: 85.70%
Epoch: 125, Loss: 0.3417, Train: 85.53%, Valid: 85.45%, Test: 85.63%
Epoch: 150, Loss: 0.3444, Train: 86.08%, Valid: 85.94%, Test: 86.16%
Epoch: 175, Loss: 0.3285, Train: 85.96%, Valid: 85.79%, Test: 86.05%
Epoch: 200, Loss: 0.6911, Train: 86.30%, Valid: 86.16%, Test: 86.40%
Epoch: 225, Loss: 0.9417, Train: 85.87%, Valid: 85.72%, Test: 85.98%
Epoch: 250, Loss: 0.6452, Train: 85.91%, Valid: 85.75%, Test: 86.02%
Epoch: 275, Loss: 0.3550, Train: 85.90%, Valid: 85.71%, Test: 86.02%
Epoch: 300, Loss: 0.3388, Train: 86.13%, Valid: 85.95%, Test: 86.22%
Epoch: 325, Loss: 0.3324, Train: 86.13%, Valid: 85.94%, Test: 86.23%
Epoch: 350, Loss: 0.3291, Train: 86.11%, Valid: 85.92%, Test: 86.22%
Epoch: 375, Loss: 0.3261, Train: 86.20%, Valid: 86.02%, Test: 86.31%
Epoch: 400, Loss: 0.3250, Train: 86.23%, Valid: 86.05%, Test: 86.34%
Epoch: 425, Loss: 0.4788, Train: 85.85%, Valid: 85.70%, Test: 85.94%
Epoch: 450, Loss: 1.2126, Train: 85.84%, Valid: 85.70%, Test: 85.97%
Epoch: 475, Loss: 0.8380, Train: 85.87%, Valid: 85.74%, Test: 86.00%
Run 01:
Highest Train: 86.96
Highest Valid: 86.92
  Final Train: 86.96
   Final Test: 86.96
All runs:
Highest Train: 86.96, nan
Highest Valid: 86.92, nan
  Final Train: 86.96, nan
   Final Test: 86.96, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 4.0268, Train: 84.55%, Valid: 84.63%, Test: 84.67%
Epoch: 25, Loss: 0.7322, Train: 85.48%, Valid: 85.29%, Test: 85.53%
Epoch: 50, Loss: 0.5915, Train: 86.02%, Valid: 85.88%, Test: 86.06%
Epoch: 75, Loss: 0.5164, Train: 85.72%, Valid: 85.58%, Test: 85.74%
Epoch: 100, Loss: 0.4665, Train: 85.42%, Valid: 85.25%, Test: 85.46%
Epoch: 125, Loss: 0.8840, Train: 84.32%, Valid: 84.29%, Test: 84.45%
Epoch: 150, Loss: 0.4535, Train: 85.87%, Valid: 85.83%, Test: 85.93%
Epoch: 175, Loss: 0.4476, Train: 85.56%, Valid: 85.39%, Test: 85.60%
Epoch: 200, Loss: 0.3916, Train: 85.59%, Valid: 85.42%, Test: 85.63%
Epoch: 225, Loss: 0.3739, Train: 85.59%, Valid: 85.41%, Test: 85.64%
Epoch: 250, Loss: 0.3655, Train: 85.59%, Valid: 85.41%, Test: 85.65%
Epoch: 275, Loss: 0.3583, Train: 85.58%, Valid: 85.40%, Test: 85.64%
Epoch: 300, Loss: 0.3539, Train: 85.58%, Valid: 85.40%, Test: 85.64%
Epoch: 325, Loss: 0.3505, Train: 85.57%, Valid: 85.39%, Test: 85.64%
Epoch: 350, Loss: 0.3477, Train: 85.53%, Valid: 85.35%, Test: 85.61%
Epoch: 375, Loss: 0.3454, Train: 85.51%, Valid: 85.32%, Test: 85.59%
Epoch: 400, Loss: 0.3432, Train: 85.50%, Valid: 85.30%, Test: 85.57%
Epoch: 425, Loss: 0.3410, Train: 85.54%, Valid: 85.34%, Test: 85.61%
Epoch: 450, Loss: 0.3378, Train: 85.55%, Valid: 85.36%, Test: 85.63%
Epoch: 475, Loss: 0.3355, Train: 85.56%, Valid: 85.37%, Test: 85.64%
Run 01:
Highest Train: 86.73
Highest Valid: 86.77
  Final Train: 86.73
   Final Test: 86.81
All runs:
Highest Train: 86.73, nan
Highest Valid: 86.77, nan
  Final Train: 86.73, nan
   Final Test: 86.81, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.2294, Train: 84.02%, Valid: 83.82%, Test: 84.00%
Epoch: 25, Loss: 0.4944, Train: 86.49%, Valid: 86.55%, Test: 86.50%
Epoch: 50, Loss: 0.4065, Train: 86.18%, Valid: 86.23%, Test: 86.23%
Epoch: 75, Loss: 0.3581, Train: 85.59%, Valid: 85.56%, Test: 85.64%
Epoch: 100, Loss: 0.3537, Train: 86.16%, Valid: 86.06%, Test: 86.17%
Epoch: 125, Loss: 0.3422, Train: 85.56%, Valid: 85.41%, Test: 85.56%
Epoch: 150, Loss: 0.4285, Train: 85.57%, Valid: 85.64%, Test: 85.68%
Epoch: 175, Loss: 0.9414, Train: 85.65%, Valid: 85.69%, Test: 85.73%
Epoch: 200, Loss: 0.5088, Train: 85.67%, Valid: 85.72%, Test: 85.76%
Epoch: 225, Loss: 0.3464, Train: 85.87%, Valid: 85.91%, Test: 85.95%
Epoch: 250, Loss: 0.3379, Train: 85.91%, Valid: 85.73%, Test: 86.02%
Epoch: 275, Loss: 0.3326, Train: 85.85%, Valid: 85.60%, Test: 85.89%
Epoch: 300, Loss: 0.3281, Train: 85.94%, Valid: 85.93%, Test: 86.04%
Epoch: 325, Loss: 0.3925, Train: 85.72%, Valid: 85.76%, Test: 85.81%
Epoch: 350, Loss: 1.0795, Train: 85.74%, Valid: 85.78%, Test: 85.85%
Epoch: 375, Loss: 1.6287, Train: 85.72%, Valid: 85.76%, Test: 85.82%
Epoch: 400, Loss: 1.5550, Train: 85.71%, Valid: 85.74%, Test: 85.81%
Epoch: 425, Loss: 1.3782, Train: 85.68%, Valid: 85.72%, Test: 85.78%
Epoch: 450, Loss: 0.8928, Train: 85.68%, Valid: 85.72%, Test: 85.79%
Epoch: 475, Loss: 0.4399, Train: 85.68%, Valid: 85.73%, Test: 85.78%
Run 01:
Highest Train: 87.77
Highest Valid: 87.79
  Final Train: 87.77
   Final Test: 87.84
All runs:
Highest Train: 87.77, nan
Highest Valid: 87.79, nan
  Final Train: 87.77, nan
   Final Test: 87.84, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.5028, Train: 85.90%, Valid: 85.90%, Test: 85.95%
Epoch: 25, Loss: 0.3745, Train: 85.90%, Valid: 85.78%, Test: 85.98%
Epoch: 50, Loss: 0.3604, Train: 85.87%, Valid: 85.67%, Test: 85.93%
Epoch: 75, Loss: 0.3537, Train: 86.28%, Valid: 86.11%, Test: 86.38%
Epoch: 100, Loss: 0.3433, Train: 85.67%, Valid: 85.49%, Test: 85.78%
Epoch: 125, Loss: 0.3359, Train: 85.46%, Valid: 85.27%, Test: 85.60%
Epoch: 150, Loss: 0.3331, Train: 85.70%, Valid: 85.53%, Test: 85.82%
Epoch: 175, Loss: 0.3310, Train: 85.88%, Valid: 85.71%, Test: 85.99%
Epoch: 200, Loss: 0.7444, Train: 85.87%, Valid: 85.74%, Test: 85.99%
Epoch: 225, Loss: 0.5285, Train: 86.10%, Valid: 85.97%, Test: 86.17%
Epoch: 250, Loss: 0.3401, Train: 85.97%, Valid: 85.79%, Test: 86.09%
Epoch: 275, Loss: 0.3341, Train: 86.17%, Valid: 86.00%, Test: 86.29%
Epoch: 300, Loss: 0.3302, Train: 86.14%, Valid: 85.96%, Test: 86.25%
Epoch: 325, Loss: 0.4397, Train: 85.64%, Valid: 85.52%, Test: 85.67%
Epoch: 350, Loss: 0.8139, Train: 85.91%, Valid: 85.76%, Test: 86.04%
Epoch: 375, Loss: 0.7487, Train: 86.55%, Valid: 86.34%, Test: 86.64%
Epoch: 400, Loss: 0.4820, Train: 85.99%, Valid: 85.84%, Test: 86.10%
Epoch: 425, Loss: 0.3437, Train: 87.04%, Valid: 86.84%, Test: 87.16%
Epoch: 450, Loss: 0.3386, Train: 86.47%, Valid: 86.26%, Test: 86.54%
Epoch: 475, Loss: 0.3340, Train: 86.26%, Valid: 86.10%, Test: 86.37%
Run 01:
Highest Train: 87.10
Highest Valid: 86.93
  Final Train: 87.10
   Final Test: 87.22
All runs:
Highest Train: 87.10, nan
Highest Valid: 86.93, nan
  Final Train: 87.10, nan
   Final Test: 87.22, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 9.3397, Train: 28.90%, Valid: 28.63%, Test: 28.50%
Epoch: 25, Loss: 0.5871, Train: 85.59%, Valid: 85.45%, Test: 85.64%
Epoch: 50, Loss: 0.5623, Train: 85.59%, Valid: 85.44%, Test: 85.63%
Epoch: 75, Loss: 0.4712, Train: 85.48%, Valid: 85.31%, Test: 85.53%
Epoch: 100, Loss: 0.3624, Train: 85.67%, Valid: 85.47%, Test: 85.72%
Epoch: 125, Loss: 0.3539, Train: 85.71%, Valid: 85.53%, Test: 85.76%
Epoch: 150, Loss: 0.3478, Train: 85.65%, Valid: 85.48%, Test: 85.71%
Epoch: 175, Loss: 0.3408, Train: 85.54%, Valid: 85.36%, Test: 85.61%
Epoch: 200, Loss: 0.3355, Train: 85.40%, Valid: 85.22%, Test: 85.47%
Epoch: 225, Loss: 0.3344, Train: 85.46%, Valid: 85.27%, Test: 85.52%
Epoch: 250, Loss: 0.3293, Train: 85.52%, Valid: 85.31%, Test: 85.59%
Epoch: 275, Loss: 0.5060, Train: 85.52%, Valid: 85.42%, Test: 85.65%
Epoch: 300, Loss: 0.3513, Train: 85.86%, Valid: 85.67%, Test: 85.92%
Epoch: 325, Loss: 0.3388, Train: 85.86%, Valid: 85.66%, Test: 85.92%
Epoch: 350, Loss: 0.3318, Train: 85.69%, Valid: 85.48%, Test: 85.76%
Epoch: 375, Loss: 0.3294, Train: 85.61%, Valid: 85.39%, Test: 85.68%
Epoch: 400, Loss: 0.7908, Train: 86.09%, Valid: 86.06%, Test: 86.22%
Epoch: 425, Loss: 0.6448, Train: 85.55%, Valid: 85.43%, Test: 85.68%
Epoch: 450, Loss: 0.4697, Train: 85.50%, Valid: 85.38%, Test: 85.64%
Epoch: 475, Loss: 0.3432, Train: 85.72%, Valid: 85.51%, Test: 85.80%
Run 01:
Highest Train: 86.31
Highest Valid: 86.31
  Final Train: 86.31
   Final Test: 86.43
All runs:
Highest Train: 86.31, nan
Highest Valid: 86.31, nan
  Final Train: 86.31, nan
   Final Test: 86.43, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.6131, Train: 86.31%, Valid: 86.42%, Test: 86.34%
Epoch: 25, Loss: 0.3984, Train: 85.27%, Valid: 85.30%, Test: 85.38%
Epoch: 50, Loss: 0.5369, Train: 87.52%, Valid: 87.65%, Test: 87.55%
Epoch: 75, Loss: 0.4565, Train: 86.45%, Valid: 86.55%, Test: 86.49%
Epoch: 100, Loss: 0.5441, Train: 86.44%, Valid: 86.52%, Test: 86.47%
Epoch: 125, Loss: 0.5626, Train: 86.41%, Valid: 86.49%, Test: 86.45%
Epoch: 150, Loss: 0.5511, Train: 86.20%, Valid: 86.27%, Test: 86.22%
Epoch: 175, Loss: 0.5055, Train: 86.09%, Valid: 86.15%, Test: 86.13%
Epoch: 200, Loss: 0.5123, Train: 85.88%, Valid: 85.93%, Test: 85.97%
Epoch: 225, Loss: 0.4700, Train: 85.64%, Valid: 85.71%, Test: 85.76%
Epoch: 250, Loss: 0.4406, Train: 85.45%, Valid: 85.49%, Test: 85.54%
Epoch: 275, Loss: 0.4204, Train: 85.44%, Valid: 85.48%, Test: 85.54%
Epoch: 300, Loss: 0.4098, Train: 85.48%, Valid: 85.53%, Test: 85.58%
Epoch: 325, Loss: 0.3884, Train: 85.45%, Valid: 85.47%, Test: 85.54%
Epoch: 350, Loss: 0.3644, Train: 85.46%, Valid: 85.48%, Test: 85.57%
Epoch: 375, Loss: 0.3527, Train: 85.56%, Valid: 85.57%, Test: 85.68%
Epoch: 400, Loss: 0.3494, Train: 85.42%, Valid: 85.43%, Test: 85.54%
Epoch: 425, Loss: 0.3436, Train: 85.52%, Valid: 85.54%, Test: 85.63%
Epoch: 450, Loss: 0.3350, Train: 85.57%, Valid: 85.57%, Test: 85.68%
Epoch: 475, Loss: 0.3299, Train: 85.62%, Valid: 85.62%, Test: 85.73%
Run 01:
Highest Train: 87.62
Highest Valid: 87.76
  Final Train: 87.62
   Final Test: 87.65
All runs:
Highest Train: 87.62, nan
Highest Valid: 87.76, nan
  Final Train: 87.62, nan
   Final Test: 87.65, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6705, Train: 85.56%, Valid: 85.40%, Test: 85.71%
Epoch: 25, Loss: 0.4136, Train: 86.47%, Valid: 86.40%, Test: 86.57%
Epoch: 50, Loss: 0.4741, Train: 85.79%, Valid: 85.77%, Test: 85.91%
Epoch: 75, Loss: 0.4720, Train: 85.12%, Valid: 85.09%, Test: 85.27%
Epoch: 100, Loss: 0.5195, Train: 86.03%, Valid: 86.18%, Test: 86.13%
Epoch: 125, Loss: 0.4697, Train: 86.48%, Valid: 86.52%, Test: 86.53%
Epoch: 150, Loss: 0.5730, Train: 85.75%, Valid: 85.72%, Test: 85.81%
Epoch: 175, Loss: 0.5089, Train: 86.33%, Valid: 86.29%, Test: 86.40%
Epoch: 200, Loss: 0.4860, Train: 86.31%, Valid: 86.28%, Test: 86.40%
Epoch: 225, Loss: 0.4778, Train: 86.10%, Valid: 86.00%, Test: 86.23%
Epoch: 250, Loss: 0.4562, Train: 86.00%, Valid: 85.88%, Test: 86.11%
Epoch: 275, Loss: 0.4410, Train: 85.95%, Valid: 85.83%, Test: 86.06%
Epoch: 300, Loss: 0.4217, Train: 85.93%, Valid: 85.79%, Test: 86.02%
Epoch: 325, Loss: 0.4077, Train: 85.85%, Valid: 85.70%, Test: 85.93%
Epoch: 350, Loss: 0.3966, Train: 85.89%, Valid: 85.74%, Test: 85.97%
Epoch: 375, Loss: 0.3755, Train: 85.86%, Valid: 85.72%, Test: 85.94%
Epoch: 400, Loss: 0.3518, Train: 85.83%, Valid: 85.67%, Test: 85.92%
Epoch: 425, Loss: 0.3466, Train: 85.80%, Valid: 85.63%, Test: 85.90%
Epoch: 450, Loss: 0.3423, Train: 85.74%, Valid: 85.58%, Test: 85.85%
Epoch: 475, Loss: 0.3387, Train: 85.78%, Valid: 85.62%, Test: 85.89%
Run 01:
Highest Train: 87.12
Highest Valid: 87.09
  Final Train: 87.12
   Final Test: 87.14
All runs:
Highest Train: 87.12, nan
Highest Valid: 87.09, nan
  Final Train: 87.12, nan
   Final Test: 87.14, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 4.6243, Train: 15.70%, Valid: 15.72%, Test: 15.56%
Epoch: 25, Loss: 0.4436, Train: 85.88%, Valid: 85.78%, Test: 85.87%
Epoch: 50, Loss: 0.4649, Train: 87.02%, Valid: 86.82%, Test: 87.00%
Epoch: 75, Loss: 0.4927, Train: 85.36%, Valid: 85.17%, Test: 85.40%
Epoch: 100, Loss: 0.4604, Train: 85.38%, Valid: 85.21%, Test: 85.44%
Epoch: 125, Loss: 0.4530, Train: 85.46%, Valid: 85.29%, Test: 85.49%
Epoch: 150, Loss: 0.4298, Train: 85.47%, Valid: 85.30%, Test: 85.50%
Epoch: 175, Loss: 0.4139, Train: 85.58%, Valid: 85.44%, Test: 85.64%
Epoch: 200, Loss: 0.3965, Train: 86.11%, Valid: 85.97%, Test: 86.14%
Epoch: 225, Loss: 0.3779, Train: 86.04%, Valid: 85.91%, Test: 86.09%
Epoch: 250, Loss: 0.3618, Train: 86.09%, Valid: 85.93%, Test: 86.14%
Epoch: 275, Loss: 0.3570, Train: 85.61%, Valid: 85.43%, Test: 85.71%
Epoch: 300, Loss: 0.3538, Train: 85.50%, Valid: 85.29%, Test: 85.58%
Epoch: 325, Loss: 0.3500, Train: 85.35%, Valid: 85.14%, Test: 85.43%
Epoch: 350, Loss: 0.3458, Train: 85.31%, Valid: 85.11%, Test: 85.40%
Epoch: 375, Loss: 0.3412, Train: 85.22%, Valid: 85.02%, Test: 85.32%
Epoch: 400, Loss: 0.3370, Train: 85.17%, Valid: 84.97%, Test: 85.27%
Epoch: 425, Loss: 0.3336, Train: 85.16%, Valid: 84.99%, Test: 85.28%
Epoch: 450, Loss: 0.3314, Train: 85.26%, Valid: 85.10%, Test: 85.36%
Epoch: 475, Loss: 0.3301, Train: 85.40%, Valid: 85.23%, Test: 85.49%
Run 01:
Highest Train: 87.09
Highest Valid: 86.89
  Final Train: 87.09
   Final Test: 87.04
All runs:
Highest Train: 87.09, nan
Highest Valid: 86.89, nan
  Final Train: 87.09, nan
   Final Test: 87.04, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.3937, Train: 18.31%, Valid: 18.15%, Test: 18.20%
Epoch: 25, Loss: 0.4007, Train: 84.51%, Valid: 84.25%, Test: 84.61%
Epoch: 50, Loss: 0.4066, Train: 85.29%, Valid: 85.28%, Test: 85.43%
Epoch: 75, Loss: 0.3835, Train: 85.31%, Valid: 85.30%, Test: 85.42%
Epoch: 100, Loss: 0.3740, Train: 85.63%, Valid: 85.68%, Test: 85.71%
Epoch: 125, Loss: 0.3695, Train: 85.68%, Valid: 85.74%, Test: 85.77%
Epoch: 150, Loss: 0.3694, Train: 85.74%, Valid: 85.81%, Test: 85.83%
Epoch: 175, Loss: 0.3707, Train: 85.74%, Valid: 85.78%, Test: 85.82%
Epoch: 200, Loss: 0.3719, Train: 85.76%, Valid: 85.81%, Test: 85.83%
Epoch: 225, Loss: 0.3712, Train: 85.81%, Valid: 85.87%, Test: 85.88%
Epoch: 250, Loss: 0.3673, Train: 85.82%, Valid: 85.89%, Test: 85.90%
Epoch: 275, Loss: 0.3652, Train: 85.82%, Valid: 85.89%, Test: 85.89%
Epoch: 300, Loss: 0.3634, Train: 85.70%, Valid: 85.76%, Test: 85.78%
Epoch: 325, Loss: 0.3579, Train: 85.78%, Valid: 85.81%, Test: 85.85%
Epoch: 350, Loss: 0.3528, Train: 85.79%, Valid: 85.79%, Test: 85.88%
Epoch: 375, Loss: 0.3506, Train: 85.79%, Valid: 85.78%, Test: 85.88%
Epoch: 400, Loss: 0.3491, Train: 85.80%, Valid: 85.78%, Test: 85.90%
Epoch: 425, Loss: 0.3459, Train: 85.85%, Valid: 85.83%, Test: 85.94%
Epoch: 450, Loss: 0.3427, Train: 85.93%, Valid: 85.93%, Test: 86.01%
Epoch: 475, Loss: 0.3393, Train: 85.87%, Valid: 85.85%, Test: 85.93%
Run 01:
Highest Train: 87.10
Highest Valid: 86.99
  Final Train: 87.10
   Final Test: 87.12
All runs:
Highest Train: 87.10, nan
Highest Valid: 86.99, nan
  Final Train: 87.10, nan
   Final Test: 87.12, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.3343, Train: 15.30%, Valid: 15.21%, Test: 15.26%
Epoch: 25, Loss: 12.8733, Train: 14.10%, Valid: 14.04%, Test: 14.15%
Epoch: 50, Loss: 22.0433, Train: 15.50%, Valid: 15.52%, Test: 15.34%
Epoch: 75, Loss: 13.9327, Train: 14.72%, Valid: 14.75%, Test: 14.74%
Epoch: 100, Loss: 14.5039, Train: 15.24%, Valid: 15.23%, Test: 15.24%
Epoch: 125, Loss: 11.6911, Train: 15.51%, Valid: 15.48%, Test: 15.51%
Epoch: 150, Loss: 11.0191, Train: 14.77%, Valid: 14.79%, Test: 14.79%
Epoch: 175, Loss: 14.3044, Train: 14.37%, Valid: 14.39%, Test: 14.42%
Epoch: 200, Loss: 12.1502, Train: 14.18%, Valid: 14.20%, Test: 14.21%
Epoch: 225, Loss: 12.2103, Train: 14.98%, Valid: 14.94%, Test: 14.93%
Epoch: 250, Loss: 12.4774, Train: 14.81%, Valid: 14.80%, Test: 14.77%
Epoch: 275, Loss: 11.8663, Train: 14.23%, Valid: 14.20%, Test: 14.22%
Epoch: 300, Loss: 13.4505, Train: 14.14%, Valid: 14.10%, Test: 14.14%
Epoch: 325, Loss: 13.3748, Train: 14.13%, Valid: 14.09%, Test: 14.12%
Epoch: 350, Loss: 12.0957, Train: 14.16%, Valid: 14.14%, Test: 14.16%
Epoch: 375, Loss: 13.6576, Train: 14.17%, Valid: 14.15%, Test: 14.17%
Epoch: 400, Loss: 12.2614, Train: 14.21%, Valid: 14.18%, Test: 14.20%
Epoch: 425, Loss: 12.0333, Train: 14.20%, Valid: 14.16%, Test: 14.19%
Epoch: 450, Loss: 13.8862, Train: 14.21%, Valid: 14.17%, Test: 14.21%
Epoch: 475, Loss: 11.9674, Train: 14.19%, Valid: 14.15%, Test: 14.19%
Run 01:
Highest Train: 83.45
Highest Valid: 83.46
  Final Train: 83.45
   Final Test: 83.56
All runs:
Highest Train: 83.45, nan
Highest Valid: 83.46, nan
  Final Train: 83.45, nan
   Final Test: 83.56, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 3.7796, Train: 20.78%, Valid: 20.64%, Test: 20.50%
Epoch: 25, Loss: 0.6348, Train: 84.81%, Valid: 84.66%, Test: 84.92%
Epoch: 50, Loss: 0.4810, Train: 84.84%, Valid: 84.68%, Test: 84.95%
Epoch: 75, Loss: 0.4918, Train: 84.90%, Valid: 84.71%, Test: 84.99%
Epoch: 100, Loss: 0.4789, Train: 85.08%, Valid: 84.93%, Test: 85.17%
Epoch: 125, Loss: 0.4713, Train: 85.40%, Valid: 85.32%, Test: 85.45%
Epoch: 150, Loss: 0.4662, Train: 85.80%, Valid: 85.78%, Test: 85.85%
Epoch: 175, Loss: 1.5378, Train: 85.85%, Valid: 85.75%, Test: 85.90%
Epoch: 200, Loss: 0.5295, Train: 86.22%, Valid: 86.12%, Test: 86.27%
Epoch: 225, Loss: 0.5346, Train: 86.25%, Valid: 86.15%, Test: 86.30%
Epoch: 250, Loss: 0.5196, Train: 86.31%, Valid: 86.22%, Test: 86.36%
Epoch: 275, Loss: 0.5250, Train: 86.33%, Valid: 86.24%, Test: 86.37%
Epoch: 300, Loss: 0.5247, Train: 86.32%, Valid: 86.23%, Test: 86.36%
Epoch: 325, Loss: 0.5309, Train: 86.35%, Valid: 86.27%, Test: 86.40%
Epoch: 350, Loss: 0.5254, Train: 86.35%, Valid: 86.26%, Test: 86.40%
Epoch: 375, Loss: 0.4851, Train: 86.37%, Valid: 86.29%, Test: 86.42%
Epoch: 400, Loss: 0.5378, Train: 86.37%, Valid: 86.29%, Test: 86.43%
Epoch: 425, Loss: 0.5311, Train: 86.31%, Valid: 86.23%, Test: 86.36%
Epoch: 450, Loss: 0.4860, Train: 86.36%, Valid: 86.28%, Test: 86.41%
Epoch: 475, Loss: 0.4946, Train: 86.35%, Valid: 86.27%, Test: 86.40%
Run 01:
Highest Train: 86.42
Highest Valid: 86.34
  Final Train: 86.42
   Final Test: 86.47
All runs:
Highest Train: 86.42, nan
Highest Valid: 86.34, nan
  Final Train: 86.42, nan
   Final Test: 86.47, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.1379, Train: 84.73%, Valid: 84.72%, Test: 84.87%
Epoch: 25, Loss: 0.3910, Train: 84.20%, Valid: 83.99%, Test: 84.31%
Epoch: 50, Loss: 2.2208, Train: 84.80%, Valid: 84.72%, Test: 84.94%
Epoch: 75, Loss: 0.4648, Train: 84.86%, Valid: 84.78%, Test: 85.00%
Epoch: 100, Loss: 0.4151, Train: 84.93%, Valid: 84.85%, Test: 85.07%
Epoch: 125, Loss: 0.4380, Train: 85.13%, Valid: 85.11%, Test: 85.27%
Epoch: 150, Loss: 0.4425, Train: 85.18%, Valid: 85.17%, Test: 85.32%
Epoch: 175, Loss: 0.4428, Train: 85.25%, Valid: 85.25%, Test: 85.39%
Epoch: 200, Loss: 0.4400, Train: 85.58%, Valid: 85.63%, Test: 85.68%
Epoch: 225, Loss: 0.4417, Train: 85.79%, Valid: 85.86%, Test: 85.90%
Epoch: 250, Loss: 0.4405, Train: 86.12%, Valid: 86.16%, Test: 86.21%
Epoch: 275, Loss: 0.4348, Train: 86.27%, Valid: 86.27%, Test: 86.33%
Epoch: 300, Loss: 0.4358, Train: 86.32%, Valid: 86.34%, Test: 86.40%
Epoch: 325, Loss: 0.4390, Train: 86.32%, Valid: 86.34%, Test: 86.38%
Epoch: 350, Loss: 0.4381, Train: 86.27%, Valid: 86.29%, Test: 86.34%
Epoch: 375, Loss: 0.4366, Train: 86.21%, Valid: 86.24%, Test: 86.29%
Epoch: 400, Loss: 0.4325, Train: 86.21%, Valid: 86.25%, Test: 86.29%
Epoch: 425, Loss: 0.4328, Train: 86.08%, Valid: 86.14%, Test: 86.18%
Epoch: 450, Loss: 0.4322, Train: 86.01%, Valid: 86.08%, Test: 86.11%
Epoch: 475, Loss: 0.4300, Train: 85.99%, Valid: 86.07%, Test: 86.09%
Run 01:
Highest Train: 88.20
Highest Valid: 88.27
  Final Train: 88.20
   Final Test: 88.23
All runs:
Highest Train: 88.20, nan
Highest Valid: 88.27, nan
  Final Train: 88.20, nan
   Final Test: 88.23, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.0052, Train: 83.73%, Valid: 83.74%, Test: 83.91%
Epoch: 25, Loss: 0.3799, Train: 86.85%, Valid: 86.65%, Test: 86.80%
Epoch: 50, Loss: 0.3821, Train: 85.85%, Valid: 85.80%, Test: 85.94%
Epoch: 75, Loss: 0.3900, Train: 85.91%, Valid: 85.80%, Test: 86.01%
Epoch: 100, Loss: 0.3751, Train: 85.99%, Valid: 85.89%, Test: 86.08%
Epoch: 125, Loss: 0.3684, Train: 85.90%, Valid: 85.78%, Test: 85.98%
Epoch: 150, Loss: 0.3641, Train: 85.87%, Valid: 85.74%, Test: 85.95%
Epoch: 175, Loss: 0.3609, Train: 85.87%, Valid: 85.74%, Test: 85.95%
Epoch: 200, Loss: 0.3588, Train: 85.84%, Valid: 85.71%, Test: 85.91%
Epoch: 225, Loss: 0.3555, Train: 85.82%, Valid: 85.68%, Test: 85.91%
Epoch: 250, Loss: 0.3491, Train: 85.76%, Valid: 85.61%, Test: 85.85%
Epoch: 275, Loss: 0.3421, Train: 85.55%, Valid: 85.40%, Test: 85.65%
Epoch: 300, Loss: 0.3362, Train: 85.47%, Valid: 85.32%, Test: 85.58%
Epoch: 325, Loss: 0.3318, Train: 85.63%, Valid: 85.48%, Test: 85.75%
Epoch: 350, Loss: 0.3291, Train: 85.92%, Valid: 85.77%, Test: 86.03%
Epoch: 375, Loss: 0.3271, Train: 85.94%, Valid: 85.78%, Test: 86.06%
Epoch: 400, Loss: 0.3273, Train: 86.08%, Valid: 85.92%, Test: 86.22%
Epoch: 425, Loss: 0.3345, Train: 86.83%, Valid: 86.66%, Test: 86.94%
Epoch: 450, Loss: 0.3256, Train: 86.33%, Valid: 86.16%, Test: 86.45%
Epoch: 475, Loss: 0.3245, Train: 85.96%, Valid: 85.76%, Test: 86.08%
Run 01:
Highest Train: 87.61
Highest Valid: 87.41
  Final Train: 87.61
   Final Test: 87.63
All runs:
Highest Train: 87.61, nan
Highest Valid: 87.41, nan
  Final Train: 87.61, nan
   Final Test: 87.63, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.8500, Train: 84.19%, Valid: 84.14%, Test: 84.19%
Epoch: 25, Loss: 0.5130, Train: 84.85%, Valid: 84.67%, Test: 84.93%
Epoch: 50, Loss: 0.7342, Train: 85.36%, Valid: 85.20%, Test: 85.43%
Epoch: 75, Loss: 0.5309, Train: 85.78%, Valid: 85.67%, Test: 85.84%
Epoch: 100, Loss: 0.4089, Train: 86.05%, Valid: 85.91%, Test: 86.08%
Epoch: 125, Loss: 0.4089, Train: 85.68%, Valid: 85.58%, Test: 85.76%
Epoch: 150, Loss: 0.4070, Train: 85.74%, Valid: 85.63%, Test: 85.79%
Epoch: 175, Loss: 0.4056, Train: 85.63%, Valid: 85.53%, Test: 85.72%
Epoch: 200, Loss: 0.4050, Train: 86.00%, Valid: 85.88%, Test: 86.06%
Epoch: 225, Loss: 0.4064, Train: 86.18%, Valid: 86.06%, Test: 86.22%
Epoch: 250, Loss: 0.4053, Train: 85.73%, Valid: 85.63%, Test: 85.81%
Epoch: 275, Loss: 0.4074, Train: 85.54%, Valid: 85.43%, Test: 85.61%
Epoch: 300, Loss: 0.4058, Train: 85.78%, Valid: 85.67%, Test: 85.86%
Epoch: 325, Loss: 0.4056, Train: 85.57%, Valid: 85.46%, Test: 85.65%
Epoch: 350, Loss: 0.4100, Train: 85.59%, Valid: 85.48%, Test: 85.68%
Epoch: 375, Loss: 0.4130, Train: 85.74%, Valid: 85.63%, Test: 85.82%
Epoch: 400, Loss: 0.4116, Train: 85.72%, Valid: 85.61%, Test: 85.80%
Epoch: 425, Loss: 0.4130, Train: 85.85%, Valid: 85.72%, Test: 85.90%
Epoch: 450, Loss: 0.4138, Train: 85.41%, Valid: 85.26%, Test: 85.43%
Epoch: 475, Loss: 0.4135, Train: 85.86%, Valid: 85.75%, Test: 85.92%
Run 01:
Highest Train: 87.53
Highest Valid: 87.36
  Final Train: 87.53
   Final Test: 87.48
All runs:
Highest Train: 87.53, nan
Highest Valid: 87.36, nan
  Final Train: 87.53, nan
   Final Test: 87.48, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.4629, Train: 88.03%, Valid: 88.05%, Test: 88.05%
Epoch: 25, Loss: 0.3714, Train: 88.25%, Valid: 88.30%, Test: 88.28%
Epoch: 50, Loss: 0.3611, Train: 85.06%, Valid: 85.05%, Test: 85.20%
Epoch: 75, Loss: 0.3553, Train: 85.46%, Valid: 85.48%, Test: 85.57%
Epoch: 100, Loss: 0.3501, Train: 85.55%, Valid: 85.55%, Test: 85.65%
Epoch: 125, Loss: 0.3395, Train: 85.50%, Valid: 85.47%, Test: 85.64%
Epoch: 150, Loss: 0.3348, Train: 85.32%, Valid: 85.31%, Test: 85.40%
Epoch: 175, Loss: 0.4978, Train: 85.39%, Valid: 85.42%, Test: 85.45%
Epoch: 200, Loss: 0.6707, Train: 85.42%, Valid: 85.41%, Test: 85.46%
Epoch: 225, Loss: 0.5915, Train: 85.37%, Valid: 85.39%, Test: 85.41%
Epoch: 250, Loss: 0.4764, Train: 86.17%, Valid: 86.10%, Test: 86.15%
Epoch: 275, Loss: 0.3544, Train: 85.47%, Valid: 85.45%, Test: 85.50%
Epoch: 300, Loss: 0.3376, Train: 85.53%, Valid: 85.53%, Test: 85.61%
Epoch: 325, Loss: 0.3324, Train: 85.43%, Valid: 85.43%, Test: 85.53%
Epoch: 350, Loss: 0.3290, Train: 85.41%, Valid: 85.43%, Test: 85.50%
Epoch: 375, Loss: 0.3270, Train: 85.52%, Valid: 85.53%, Test: 85.60%
Epoch: 400, Loss: 0.3276, Train: 85.65%, Valid: 85.65%, Test: 85.73%
Epoch: 425, Loss: 0.3256, Train: 85.55%, Valid: 85.52%, Test: 85.62%
Epoch: 450, Loss: 0.3248, Train: 85.39%, Valid: 85.33%, Test: 85.48%
Epoch: 475, Loss: 0.3295, Train: 85.71%, Valid: 85.73%, Test: 85.80%
Run 01:
Highest Train: 88.27
Highest Valid: 88.32
  Final Train: 88.27
   Final Test: 88.31
All runs:
Highest Train: 88.27, nan
Highest Valid: 88.32, nan
  Final Train: 88.27, nan
   Final Test: 88.31, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.4120, Train: 86.17%, Valid: 85.89%, Test: 86.18%
Epoch: 25, Loss: 0.5237, Train: 86.19%, Valid: 86.16%, Test: 86.25%
Epoch: 50, Loss: 0.5131, Train: 86.22%, Valid: 86.20%, Test: 86.28%
Epoch: 75, Loss: 0.4520, Train: 86.02%, Valid: 85.90%, Test: 86.11%
Epoch: 100, Loss: 0.3575, Train: 85.57%, Valid: 85.45%, Test: 85.70%
Epoch: 125, Loss: 0.3482, Train: 85.90%, Valid: 85.76%, Test: 86.04%
Epoch: 150, Loss: 0.3391, Train: 85.85%, Valid: 85.67%, Test: 85.83%
Epoch: 175, Loss: 0.6559, Train: 85.88%, Valid: 85.75%, Test: 85.99%
Epoch: 200, Loss: 1.0704, Train: 85.88%, Valid: 85.76%, Test: 86.01%
Epoch: 225, Loss: 6.8793, Train: 85.87%, Valid: 85.73%, Test: 85.99%
Epoch: 250, Loss: 1.2659, Train: 85.91%, Valid: 85.76%, Test: 86.02%
Epoch: 275, Loss: 1.2414, Train: 85.89%, Valid: 85.75%, Test: 86.01%
Epoch: 300, Loss: 1.1010, Train: 85.89%, Valid: 85.74%, Test: 86.01%
Epoch: 325, Loss: 1.0656, Train: 85.89%, Valid: 85.74%, Test: 86.01%
Epoch: 350, Loss: 0.9900, Train: 85.89%, Valid: 85.75%, Test: 86.01%
Epoch: 375, Loss: 0.7014, Train: 85.77%, Valid: 85.63%, Test: 85.89%
Epoch: 400, Loss: 0.3672, Train: 85.99%, Valid: 85.83%, Test: 86.11%
Epoch: 425, Loss: 0.3420, Train: 85.80%, Valid: 85.63%, Test: 85.89%
Epoch: 450, Loss: 0.3384, Train: 86.22%, Valid: 86.07%, Test: 86.31%
Epoch: 475, Loss: 0.3353, Train: 86.04%, Valid: 85.95%, Test: 86.12%
Run 01:
Highest Train: 86.90
Highest Valid: 86.95
  Final Train: 86.90
   Final Test: 86.92
All runs:
Highest Train: 86.90, nan
Highest Valid: 86.95, nan
  Final Train: 86.90, nan
   Final Test: 86.92, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.7402, Train: 85.77%, Valid: 85.76%, Test: 85.84%
Epoch: 25, Loss: 0.4664, Train: 85.47%, Valid: 85.32%, Test: 85.48%
Epoch: 50, Loss: 0.4250, Train: 85.42%, Valid: 85.28%, Test: 85.44%
Epoch: 75, Loss: 0.3911, Train: 85.36%, Valid: 85.22%, Test: 85.41%
Epoch: 100, Loss: 0.3664, Train: 85.35%, Valid: 85.24%, Test: 85.41%
Epoch: 125, Loss: 0.3640, Train: 85.40%, Valid: 85.28%, Test: 85.46%
Epoch: 150, Loss: 0.3609, Train: 85.41%, Valid: 85.29%, Test: 85.48%
Epoch: 175, Loss: 0.3575, Train: 85.42%, Valid: 85.28%, Test: 85.49%
Epoch: 200, Loss: 0.3525, Train: 85.44%, Valid: 85.29%, Test: 85.51%
Epoch: 225, Loss: 0.3470, Train: 85.45%, Valid: 85.30%, Test: 85.52%
Epoch: 250, Loss: 0.3418, Train: 85.46%, Valid: 85.30%, Test: 85.54%
Epoch: 275, Loss: 0.3370, Train: 85.29%, Valid: 85.11%, Test: 85.36%
Epoch: 300, Loss: 0.3347, Train: 85.28%, Valid: 85.10%, Test: 85.36%
Epoch: 325, Loss: 0.3342, Train: 85.45%, Valid: 85.27%, Test: 85.53%
Epoch: 350, Loss: 0.3312, Train: 85.57%, Valid: 85.38%, Test: 85.65%
Epoch: 375, Loss: 0.3299, Train: 85.59%, Valid: 85.39%, Test: 85.65%
Epoch: 400, Loss: 0.3290, Train: 85.63%, Valid: 85.43%, Test: 85.69%
Epoch: 425, Loss: 0.3359, Train: 85.70%, Valid: 85.49%, Test: 85.78%
Epoch: 450, Loss: 0.3300, Train: 85.61%, Valid: 85.40%, Test: 85.68%
Epoch: 475, Loss: 0.3282, Train: 85.48%, Valid: 85.26%, Test: 85.54%
Run 01:
Highest Train: 85.82
Highest Valid: 85.76
  Final Train: 85.77
   Final Test: 85.84
All runs:
Highest Train: 85.82, nan
Highest Valid: 85.76, nan
  Final Train: 85.77, nan
   Final Test: 85.84, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 6.9047, Train: 85.27%, Valid: 85.13%, Test: 85.26%
Epoch: 25, Loss: 0.5281, Train: 87.03%, Valid: 87.07%, Test: 87.08%
Epoch: 50, Loss: 0.5617, Train: 86.96%, Valid: 86.99%, Test: 87.01%
Epoch: 75, Loss: 0.5593, Train: 86.66%, Valid: 86.67%, Test: 86.71%
Epoch: 100, Loss: 0.5462, Train: 86.58%, Valid: 86.60%, Test: 86.62%
Epoch: 125, Loss: 0.5146, Train: 86.30%, Valid: 86.32%, Test: 86.36%
Epoch: 150, Loss: 0.4361, Train: 86.10%, Valid: 86.17%, Test: 86.19%
Epoch: 175, Loss: 0.3554, Train: 85.89%, Valid: 85.93%, Test: 85.97%
Epoch: 200, Loss: 0.3477, Train: 86.37%, Valid: 86.41%, Test: 86.46%
Epoch: 225, Loss: 0.3394, Train: 85.43%, Valid: 85.46%, Test: 85.50%
Epoch: 250, Loss: 0.3335, Train: 85.34%, Valid: 85.37%, Test: 85.42%
Epoch: 275, Loss: 0.4099, Train: 85.73%, Valid: 85.78%, Test: 85.81%
Epoch: 300, Loss: 0.3414, Train: 86.04%, Valid: 86.05%, Test: 86.12%
Epoch: 325, Loss: 0.3324, Train: 86.82%, Valid: 86.82%, Test: 86.91%
Epoch: 350, Loss: 0.3295, Train: 85.84%, Valid: 85.82%, Test: 85.93%
Epoch: 375, Loss: 0.3266, Train: 85.74%, Valid: 85.74%, Test: 85.82%
Epoch: 400, Loss: 0.3250, Train: 85.99%, Valid: 85.98%, Test: 86.08%
Epoch: 425, Loss: 0.3957, Train: 85.77%, Valid: 85.78%, Test: 85.85%
Epoch: 450, Loss: 0.3565, Train: 86.61%, Valid: 86.60%, Test: 86.65%
Epoch: 475, Loss: 0.3394, Train: 86.07%, Valid: 86.07%, Test: 86.16%
Run 01:
Highest Train: 87.31
Highest Valid: 87.34
  Final Train: 87.31
   Final Test: 87.43
All runs:
Highest Train: 87.31, nan
Highest Valid: 87.34, nan
  Final Train: 87.31, nan
   Final Test: 87.43, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.3978, Train: 86.14%, Valid: 86.10%, Test: 86.16%
Epoch: 25, Loss: 0.3658, Train: 85.73%, Valid: 85.56%, Test: 85.83%
Epoch: 50, Loss: 0.3605, Train: 85.76%, Valid: 85.61%, Test: 85.85%
Epoch: 75, Loss: 0.3532, Train: 85.71%, Valid: 85.56%, Test: 85.79%
Epoch: 100, Loss: 0.3444, Train: 85.43%, Valid: 85.27%, Test: 85.55%
Epoch: 125, Loss: 0.3379, Train: 85.33%, Valid: 85.18%, Test: 85.47%
Epoch: 150, Loss: 0.3663, Train: 85.91%, Valid: 85.78%, Test: 86.01%
Epoch: 175, Loss: 0.3408, Train: 85.82%, Valid: 85.64%, Test: 85.96%
Epoch: 200, Loss: 0.3330, Train: 85.80%, Valid: 85.65%, Test: 85.91%
Epoch: 225, Loss: 0.3330, Train: 86.10%, Valid: 85.90%, Test: 86.21%
Epoch: 250, Loss: 0.9941, Train: 85.89%, Valid: 85.76%, Test: 86.01%
Epoch: 275, Loss: 0.9419, Train: 85.91%, Valid: 85.76%, Test: 86.03%
Epoch: 300, Loss: 0.8522, Train: 85.92%, Valid: 85.76%, Test: 86.03%
Epoch: 325, Loss: 0.6731, Train: 85.94%, Valid: 85.78%, Test: 86.05%
Epoch: 350, Loss: 0.3540, Train: 86.27%, Valid: 86.09%, Test: 86.38%
Epoch: 375, Loss: 0.3383, Train: 85.91%, Valid: 85.89%, Test: 86.02%
Epoch: 400, Loss: 0.3349, Train: 86.25%, Valid: 86.06%, Test: 86.38%
Epoch: 425, Loss: 0.3304, Train: 86.25%, Valid: 86.06%, Test: 86.37%
Epoch: 450, Loss: 0.3315, Train: 86.25%, Valid: 86.06%, Test: 86.37%
Epoch: 475, Loss: 0.3279, Train: 86.30%, Valid: 86.12%, Test: 86.42%
Run 01:
Highest Train: 87.03
Highest Valid: 87.02
  Final Train: 87.03
   Final Test: 87.18
All runs:
Highest Train: 87.03, nan
Highest Valid: 87.02, nan
  Final Train: 87.03, nan
   Final Test: 87.18, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 12.3037, Train: 18.71%, Valid: 18.59%, Test: 18.39%
Epoch: 25, Loss: 0.5332, Train: 85.86%, Valid: 85.74%, Test: 85.92%
Epoch: 50, Loss: 0.5253, Train: 85.95%, Valid: 85.85%, Test: 85.97%
Epoch: 75, Loss: 0.5136, Train: 85.94%, Valid: 85.84%, Test: 85.95%
Epoch: 100, Loss: 0.5011, Train: 85.84%, Valid: 85.75%, Test: 85.86%
Epoch: 125, Loss: 0.4835, Train: 85.63%, Valid: 85.53%, Test: 85.68%
Epoch: 150, Loss: 0.4759, Train: 85.18%, Valid: 85.03%, Test: 85.24%
Epoch: 175, Loss: 0.4905, Train: 85.18%, Valid: 85.01%, Test: 85.23%
Epoch: 200, Loss: 0.4505, Train: 85.28%, Valid: 85.11%, Test: 85.33%
Epoch: 225, Loss: 0.3901, Train: 85.38%, Valid: 85.21%, Test: 85.44%
Epoch: 250, Loss: 0.3716, Train: 85.42%, Valid: 85.26%, Test: 85.49%
Epoch: 275, Loss: 0.3653, Train: 85.43%, Valid: 85.26%, Test: 85.50%
Epoch: 300, Loss: 0.3612, Train: 85.42%, Valid: 85.25%, Test: 85.50%
Epoch: 325, Loss: 0.3572, Train: 85.44%, Valid: 85.26%, Test: 85.51%
Epoch: 350, Loss: 0.3528, Train: 85.50%, Valid: 85.32%, Test: 85.57%
Epoch: 375, Loss: 0.3485, Train: 85.46%, Valid: 85.28%, Test: 85.53%
Epoch: 400, Loss: 0.3444, Train: 85.40%, Valid: 85.23%, Test: 85.48%
Epoch: 425, Loss: 0.3403, Train: 85.34%, Valid: 85.18%, Test: 85.42%
Epoch: 450, Loss: 0.3364, Train: 85.25%, Valid: 85.08%, Test: 85.34%
Epoch: 475, Loss: 0.3352, Train: 85.24%, Valid: 85.06%, Test: 85.32%
Run 01:
Highest Train: 85.96
Highest Valid: 85.86
  Final Train: 85.96
   Final Test: 85.98
All runs:
Highest Train: 85.96, nan
Highest Valid: 85.86, nan
  Final Train: 85.96, nan
   Final Test: 85.98, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.4352, Train: 85.31%, Valid: 85.23%, Test: 85.42%
Epoch: 25, Loss: 0.4059, Train: 85.37%, Valid: 85.35%, Test: 85.49%
Epoch: 50, Loss: 0.3631, Train: 85.54%, Valid: 85.54%, Test: 85.72%
Epoch: 75, Loss: 0.3597, Train: 85.87%, Valid: 85.88%, Test: 86.03%
Epoch: 100, Loss: 0.3545, Train: 85.43%, Valid: 85.45%, Test: 85.50%
Epoch: 125, Loss: 0.3463, Train: 85.57%, Valid: 85.60%, Test: 85.66%
Epoch: 150, Loss: 0.3378, Train: 85.19%, Valid: 85.24%, Test: 85.27%
Epoch: 175, Loss: 0.3344, Train: 85.32%, Valid: 85.33%, Test: 85.41%
Epoch: 200, Loss: 0.3287, Train: 85.02%, Valid: 84.99%, Test: 85.10%
Epoch: 225, Loss: 0.3262, Train: 85.48%, Valid: 85.43%, Test: 85.58%
Epoch: 250, Loss: 0.3292, Train: 85.87%, Valid: 85.87%, Test: 85.97%
Epoch: 275, Loss: 0.8720, Train: 85.69%, Valid: 85.72%, Test: 85.80%
Epoch: 300, Loss: 0.6991, Train: 85.68%, Valid: 85.71%, Test: 85.78%
Epoch: 325, Loss: 0.5490, Train: 85.67%, Valid: 85.71%, Test: 85.77%
Epoch: 350, Loss: 0.4037, Train: 85.57%, Valid: 85.61%, Test: 85.67%
Epoch: 375, Loss: 0.3350, Train: 85.95%, Valid: 85.94%, Test: 86.04%
Epoch: 400, Loss: 0.3293, Train: 85.94%, Valid: 85.94%, Test: 86.05%
Epoch: 425, Loss: 0.3274, Train: 85.95%, Valid: 85.95%, Test: 86.04%
Epoch: 450, Loss: 0.3263, Train: 86.08%, Valid: 86.07%, Test: 86.16%
Epoch: 475, Loss: 0.3255, Train: 86.08%, Valid: 86.07%, Test: 86.16%
Run 01:
Highest Train: 87.76
Highest Valid: 87.74
  Final Train: 87.76
   Final Test: 87.77
All runs:
Highest Train: 87.76, nan
Highest Valid: 87.74, nan
  Final Train: 87.76, nan
   Final Test: 87.77, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 3.3099, Train: 84.00%, Valid: 84.14%, Test: 84.15%
Epoch: 25, Loss: 0.4059, Train: 85.99%, Valid: 85.89%, Test: 85.95%
Epoch: 50, Loss: 0.3665, Train: 85.38%, Valid: 85.24%, Test: 85.52%
Epoch: 75, Loss: 0.3630, Train: 85.82%, Valid: 85.63%, Test: 85.92%
Epoch: 100, Loss: 0.3584, Train: 85.59%, Valid: 85.45%, Test: 85.69%
Epoch: 125, Loss: 0.3520, Train: 85.60%, Valid: 85.44%, Test: 85.69%
Epoch: 150, Loss: 0.3440, Train: 85.58%, Valid: 85.43%, Test: 85.68%
Epoch: 175, Loss: 0.3364, Train: 85.51%, Valid: 85.37%, Test: 85.64%
Epoch: 200, Loss: 0.3310, Train: 85.52%, Valid: 85.36%, Test: 85.65%
Epoch: 225, Loss: 0.3417, Train: 86.23%, Valid: 86.08%, Test: 86.34%
Epoch: 250, Loss: 0.3334, Train: 86.32%, Valid: 86.16%, Test: 86.46%
Epoch: 275, Loss: 0.3289, Train: 85.97%, Valid: 85.79%, Test: 86.10%
Epoch: 300, Loss: 0.3261, Train: 85.86%, Valid: 85.71%, Test: 85.98%
Epoch: 325, Loss: 0.5988, Train: 85.97%, Valid: 85.82%, Test: 86.09%
Epoch: 350, Loss: 0.4042, Train: 85.99%, Valid: 85.84%, Test: 86.10%
Epoch: 375, Loss: 0.3412, Train: 86.11%, Valid: 85.90%, Test: 86.22%
Epoch: 400, Loss: 0.3361, Train: 86.17%, Valid: 86.02%, Test: 86.30%
Epoch: 425, Loss: 0.3332, Train: 86.13%, Valid: 85.98%, Test: 86.25%
Epoch: 450, Loss: 0.3294, Train: 86.16%, Valid: 86.00%, Test: 86.28%
Epoch: 475, Loss: 0.3269, Train: 86.14%, Valid: 85.98%, Test: 86.25%
Run 01:
Highest Train: 87.40
Highest Valid: 87.36
  Final Train: 87.40
   Final Test: 87.44
All runs:
Highest Train: 87.40, nan
Highest Valid: 87.36, nan
  Final Train: 87.40, nan
   Final Test: 87.44, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 9.5598, Train: 24.74%, Valid: 24.57%, Test: 24.36%
Epoch: 25, Loss: 0.5638, Train: 85.32%, Valid: 85.17%, Test: 85.38%
Epoch: 50, Loss: 0.5563, Train: 85.35%, Valid: 85.21%, Test: 85.41%
Epoch: 75, Loss: 0.5402, Train: 85.37%, Valid: 85.22%, Test: 85.43%
Epoch: 100, Loss: 0.5223, Train: 85.34%, Valid: 85.18%, Test: 85.40%
Epoch: 125, Loss: 0.5040, Train: 85.28%, Valid: 85.12%, Test: 85.34%
Epoch: 150, Loss: 0.4953, Train: 85.32%, Valid: 85.17%, Test: 85.39%
Epoch: 175, Loss: 0.4732, Train: 85.29%, Valid: 85.13%, Test: 85.34%
Epoch: 200, Loss: 0.4242, Train: 85.34%, Valid: 85.18%, Test: 85.40%
Epoch: 225, Loss: 0.3788, Train: 85.45%, Valid: 85.28%, Test: 85.52%
Epoch: 250, Loss: 0.3636, Train: 85.48%, Valid: 85.31%, Test: 85.55%
Epoch: 275, Loss: 0.3550, Train: 85.47%, Valid: 85.29%, Test: 85.55%
Epoch: 300, Loss: 0.3515, Train: 85.44%, Valid: 85.25%, Test: 85.52%
Epoch: 325, Loss: 0.3481, Train: 85.41%, Valid: 85.22%, Test: 85.49%
Epoch: 350, Loss: 0.3443, Train: 85.43%, Valid: 85.24%, Test: 85.51%
Epoch: 375, Loss: 0.3482, Train: 85.58%, Valid: 85.38%, Test: 85.64%
Epoch: 400, Loss: 0.3406, Train: 85.56%, Valid: 85.37%, Test: 85.63%
Epoch: 425, Loss: 0.3374, Train: 85.57%, Valid: 85.38%, Test: 85.64%
Epoch: 450, Loss: 0.3355, Train: 85.57%, Valid: 85.37%, Test: 85.65%
Epoch: 475, Loss: 0.3343, Train: 85.58%, Valid: 85.37%, Test: 85.66%
Run 01:
Highest Train: 85.60
Highest Valid: 85.40
  Final Train: 85.59
   Final Test: 85.66
All runs:
Highest Train: 85.60, nan
Highest Valid: 85.40, nan
  Final Train: 85.59, nan
   Final Test: 85.66, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.3003, Train: 83.57%, Valid: 83.36%, Test: 83.63%
Epoch: 25, Loss: 0.4949, Train: 86.01%, Valid: 86.02%, Test: 86.12%
Epoch: 50, Loss: 0.4269, Train: 85.62%, Valid: 85.65%, Test: 85.73%
Epoch: 75, Loss: 0.3560, Train: 85.64%, Valid: 85.59%, Test: 85.73%
Epoch: 100, Loss: 0.3470, Train: 85.89%, Valid: 85.86%, Test: 85.97%
Epoch: 125, Loss: 0.3373, Train: 85.00%, Valid: 84.97%, Test: 85.13%
Epoch: 150, Loss: 0.4210, Train: 85.46%, Valid: 85.46%, Test: 85.54%
Epoch: 175, Loss: 0.8827, Train: 85.40%, Valid: 85.41%, Test: 85.48%
Epoch: 200, Loss: 0.7868, Train: 85.35%, Valid: 85.39%, Test: 85.41%
Epoch: 225, Loss: 0.4813, Train: 85.34%, Valid: 85.36%, Test: 85.39%
Epoch: 250, Loss: 0.3458, Train: 87.21%, Valid: 87.25%, Test: 87.28%
Epoch: 275, Loss: 0.3353, Train: 85.50%, Valid: 85.48%, Test: 85.58%
Epoch: 300, Loss: 0.3299, Train: 85.53%, Valid: 85.51%, Test: 85.62%
Epoch: 325, Loss: 0.3266, Train: 85.44%, Valid: 85.44%, Test: 85.54%
Epoch: 350, Loss: 0.6954, Train: 85.37%, Valid: 85.39%, Test: 85.45%
Epoch: 375, Loss: 0.8963, Train: 87.93%, Valid: 87.96%, Test: 87.97%
Epoch: 400, Loss: 0.6356, Train: 87.08%, Valid: 87.15%, Test: 87.14%
Epoch: 425, Loss: 0.4113, Train: 85.41%, Valid: 85.43%, Test: 85.49%
Epoch: 450, Loss: 0.3463, Train: 85.65%, Valid: 85.65%, Test: 85.75%
Epoch: 475, Loss: 0.3373, Train: 85.58%, Valid: 85.56%, Test: 85.67%
Run 01:
Highest Train: 88.22
Highest Valid: 88.28
  Final Train: 88.22
   Final Test: 88.28
All runs:
Highest Train: 88.22, nan
Highest Valid: 88.28, nan
  Final Train: 88.22, nan
   Final Test: 88.28, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 4.2574, Train: 85.01%, Valid: 84.85%, Test: 85.13%
Epoch: 25, Loss: 0.5178, Train: 86.83%, Valid: 86.66%, Test: 86.89%
Epoch: 50, Loss: 0.4453, Train: 86.44%, Valid: 86.36%, Test: 86.56%
Epoch: 75, Loss: 0.3621, Train: 85.76%, Valid: 85.59%, Test: 85.87%
Epoch: 100, Loss: 0.3578, Train: 85.77%, Valid: 85.59%, Test: 85.90%
Epoch: 125, Loss: 0.3528, Train: 85.80%, Valid: 85.62%, Test: 85.91%
Epoch: 150, Loss: 0.3450, Train: 85.92%, Valid: 85.80%, Test: 86.04%
Epoch: 175, Loss: 0.3565, Train: 85.75%, Valid: 85.60%, Test: 85.86%
Epoch: 200, Loss: 0.3355, Train: 85.77%, Valid: 85.59%, Test: 85.90%
Epoch: 225, Loss: 0.3304, Train: 85.61%, Valid: 85.45%, Test: 85.74%
Epoch: 250, Loss: 0.3481, Train: 86.10%, Valid: 85.97%, Test: 86.21%
Epoch: 275, Loss: 0.3345, Train: 86.04%, Valid: 85.85%, Test: 86.15%
Epoch: 300, Loss: 0.3313, Train: 86.06%, Valid: 85.88%, Test: 86.19%
Epoch: 325, Loss: 0.3287, Train: 86.01%, Valid: 85.82%, Test: 86.13%
Epoch: 350, Loss: 0.3266, Train: 86.00%, Valid: 85.82%, Test: 86.13%
Epoch: 375, Loss: 0.3265, Train: 86.12%, Valid: 85.94%, Test: 86.25%
Epoch: 400, Loss: 0.3253, Train: 86.09%, Valid: 85.93%, Test: 86.22%
Epoch: 425, Loss: 0.3268, Train: 86.17%, Valid: 86.01%, Test: 86.30%
Epoch: 450, Loss: 0.5010, Train: 85.88%, Valid: 85.74%, Test: 85.99%
Epoch: 475, Loss: 0.3448, Train: 86.70%, Valid: 86.50%, Test: 86.78%
Run 01:
Highest Train: 88.08
Highest Valid: 87.94
  Final Train: 88.08
   Final Test: 88.19
All runs:
Highest Train: 88.08, nan
Highest Valid: 87.94, nan
  Final Train: 88.08, nan
   Final Test: 88.19, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 27.9143, Train: 14.79%, Valid: 14.89%, Test: 14.78%
Epoch: 25, Loss: 0.7076, Train: 84.98%, Valid: 84.88%, Test: 85.03%
Epoch: 50, Loss: 0.6322, Train: 85.32%, Valid: 85.20%, Test: 85.39%
Epoch: 75, Loss: 0.4749, Train: 86.96%, Valid: 86.79%, Test: 86.89%
Epoch: 100, Loss: 0.4189, Train: 86.79%, Valid: 86.66%, Test: 86.76%
Epoch: 125, Loss: 0.4030, Train: 86.53%, Valid: 86.41%, Test: 86.48%
Epoch: 150, Loss: 0.3867, Train: 85.79%, Valid: 85.67%, Test: 85.81%
Epoch: 175, Loss: 0.3764, Train: 85.42%, Valid: 85.26%, Test: 85.46%
Epoch: 200, Loss: 0.3703, Train: 85.28%, Valid: 85.14%, Test: 85.35%
Epoch: 225, Loss: 0.3662, Train: 85.24%, Valid: 85.09%, Test: 85.32%
Epoch: 250, Loss: 0.3631, Train: 85.17%, Valid: 85.03%, Test: 85.26%
Epoch: 275, Loss: 0.3599, Train: 85.09%, Valid: 84.95%, Test: 85.18%
Epoch: 300, Loss: 0.3568, Train: 85.00%, Valid: 84.86%, Test: 85.10%
Epoch: 325, Loss: 0.3539, Train: 84.90%, Valid: 84.77%, Test: 85.01%
Epoch: 350, Loss: 0.3513, Train: 84.78%, Valid: 84.64%, Test: 84.90%
Epoch: 375, Loss: 0.3498, Train: 84.83%, Valid: 84.68%, Test: 84.94%
Epoch: 400, Loss: 0.3518, Train: 84.76%, Valid: 84.60%, Test: 84.87%
Epoch: 425, Loss: 1.1310, Train: 86.25%, Valid: 86.14%, Test: 86.34%
Epoch: 450, Loss: 0.8877, Train: 86.83%, Valid: 86.76%, Test: 86.86%
Epoch: 475, Loss: 0.5976, Train: 86.90%, Valid: 86.72%, Test: 86.93%
Run 01:
Highest Train: 87.58
Highest Valid: 87.53
  Final Train: 87.58
   Final Test: 87.59
All runs:
Highest Train: 87.58, nan
Highest Valid: 87.53, nan
  Final Train: 87.58, nan
   Final Test: 87.59, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 6.9561, Train: 85.99%, Valid: 86.05%, Test: 86.14%
Epoch: 25, Loss: 0.4773, Train: 85.81%, Valid: 85.86%, Test: 85.87%
Epoch: 50, Loss: 0.3629, Train: 85.68%, Valid: 85.75%, Test: 85.74%
Epoch: 75, Loss: 0.3574, Train: 87.14%, Valid: 86.96%, Test: 87.17%
Epoch: 100, Loss: 0.3518, Train: 85.51%, Valid: 85.57%, Test: 85.59%
Epoch: 125, Loss: 0.3446, Train: 85.47%, Valid: 85.52%, Test: 85.56%
Epoch: 150, Loss: 0.3365, Train: 85.48%, Valid: 85.52%, Test: 85.58%
Epoch: 175, Loss: 0.4610, Train: 85.59%, Valid: 85.62%, Test: 85.67%
Epoch: 200, Loss: 0.8122, Train: 85.63%, Valid: 85.66%, Test: 85.72%
Epoch: 225, Loss: 0.7704, Train: 85.67%, Valid: 85.69%, Test: 85.76%
Epoch: 250, Loss: 0.6032, Train: 85.69%, Valid: 85.72%, Test: 85.78%
Epoch: 275, Loss: 0.4653, Train: 85.70%, Valid: 85.71%, Test: 85.79%
Epoch: 300, Loss: 0.3485, Train: 85.80%, Valid: 85.81%, Test: 85.92%
Epoch: 325, Loss: 0.3385, Train: 85.96%, Valid: 85.97%, Test: 86.05%
Epoch: 350, Loss: 0.3334, Train: 86.00%, Valid: 86.00%, Test: 86.09%
Epoch: 375, Loss: 0.3364, Train: 85.93%, Valid: 85.93%, Test: 86.04%
Epoch: 400, Loss: 0.8792, Train: 85.68%, Valid: 85.73%, Test: 85.76%
Epoch: 425, Loss: 0.4715, Train: 85.76%, Valid: 85.80%, Test: 85.84%
Epoch: 450, Loss: 0.8370, Train: 85.75%, Valid: 85.79%, Test: 85.84%
Epoch: 475, Loss: 0.7248, Train: 85.74%, Valid: 85.78%, Test: 85.83%
Run 01:
Highest Train: 88.27
Highest Valid: 88.34
  Final Train: 88.27
   Final Test: 88.34
All runs:
Highest Train: 88.27, nan
Highest Valid: 88.34, nan
  Final Train: 88.27, nan
   Final Test: 88.34, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 13.3157, Train: 85.37%, Valid: 85.33%, Test: 85.37%
Epoch: 25, Loss: 0.5596, Train: 86.42%, Valid: 86.31%, Test: 86.45%
Epoch: 50, Loss: 0.4017, Train: 86.51%, Valid: 86.41%, Test: 86.53%
Epoch: 75, Loss: 0.3625, Train: 86.24%, Valid: 86.16%, Test: 86.27%
Epoch: 100, Loss: 0.3582, Train: 85.76%, Valid: 85.58%, Test: 85.87%
Epoch: 125, Loss: 0.3519, Train: 86.24%, Valid: 86.03%, Test: 86.25%
Epoch: 150, Loss: 0.3427, Train: 85.62%, Valid: 85.45%, Test: 85.73%
Epoch: 175, Loss: 0.3363, Train: 85.51%, Valid: 85.36%, Test: 85.64%
Epoch: 200, Loss: 0.3309, Train: 85.60%, Valid: 85.45%, Test: 85.72%
Epoch: 225, Loss: 0.9355, Train: 86.14%, Valid: 86.02%, Test: 86.25%
Epoch: 250, Loss: 0.6264, Train: 85.74%, Valid: 85.62%, Test: 85.87%
Epoch: 275, Loss: 0.3555, Train: 85.80%, Valid: 85.65%, Test: 85.91%
Epoch: 300, Loss: 0.3368, Train: 85.92%, Valid: 85.76%, Test: 86.04%
Epoch: 325, Loss: 0.3325, Train: 85.99%, Valid: 85.81%, Test: 86.11%
Epoch: 350, Loss: 0.3294, Train: 86.01%, Valid: 85.80%, Test: 86.12%
Epoch: 375, Loss: 0.3268, Train: 86.07%, Valid: 85.89%, Test: 86.19%
Epoch: 400, Loss: 0.3251, Train: 86.10%, Valid: 85.91%, Test: 86.21%
Epoch: 425, Loss: 0.3296, Train: 86.23%, Valid: 86.05%, Test: 86.34%
Epoch: 450, Loss: 0.6303, Train: 85.89%, Valid: 85.74%, Test: 86.00%
Epoch: 475, Loss: 1.1031, Train: 86.17%, Valid: 86.04%, Test: 86.29%
Run 01:
Highest Train: 87.15
Highest Valid: 87.02
  Final Train: 87.15
   Final Test: 87.24
All runs:
Highest Train: 87.15, nan
Highest Valid: 87.02, nan
  Final Train: 87.15, nan
   Final Test: 87.24, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 14.1432, Train: 80.07%, Valid: 80.10%, Test: 80.30%
Epoch: 25, Loss: 1.4677, Train: 86.14%, Valid: 86.03%, Test: 86.24%
Epoch: 50, Loss: 1.1431, Train: 86.25%, Valid: 86.12%, Test: 86.30%
Epoch: 75, Loss: 1.0951, Train: 85.96%, Valid: 85.84%, Test: 86.10%
Epoch: 100, Loss: 0.8077, Train: 86.09%, Valid: 85.98%, Test: 86.20%
Epoch: 125, Loss: 0.7044, Train: 85.69%, Valid: 85.58%, Test: 85.83%
Epoch: 150, Loss: 0.6434, Train: 85.73%, Valid: 85.60%, Test: 85.78%
Epoch: 175, Loss: 0.5905, Train: 85.46%, Valid: 85.33%, Test: 85.47%
Epoch: 200, Loss: 0.5518, Train: 85.13%, Valid: 84.98%, Test: 85.17%
Epoch: 225, Loss: 0.5295, Train: 84.92%, Valid: 84.76%, Test: 84.97%
Epoch: 250, Loss: 0.5940, Train: 85.12%, Valid: 84.96%, Test: 85.18%
Epoch: 275, Loss: 0.5154, Train: 84.95%, Valid: 84.78%, Test: 85.01%
Epoch: 300, Loss: 0.5530, Train: 84.98%, Valid: 84.82%, Test: 85.04%
Epoch: 325, Loss: 0.5372, Train: 85.02%, Valid: 84.85%, Test: 85.08%
Epoch: 350, Loss: 0.4889, Train: 84.96%, Valid: 84.78%, Test: 85.02%
Epoch: 375, Loss: 0.5618, Train: 85.07%, Valid: 84.90%, Test: 85.14%
Epoch: 400, Loss: 0.4771, Train: 84.89%, Valid: 84.72%, Test: 84.97%
Epoch: 425, Loss: 0.4686, Train: 84.90%, Valid: 84.72%, Test: 84.97%
Epoch: 450, Loss: 0.4408, Train: 84.87%, Valid: 84.69%, Test: 84.93%
Epoch: 475, Loss: 0.5444, Train: 85.12%, Valid: 84.95%, Test: 85.19%
Run 01:
Highest Train: 87.25
Highest Valid: 87.17
  Final Train: 87.25
   Final Test: 87.32
All runs:
Highest Train: 87.25, nan
Highest Valid: 87.17, nan
  Final Train: 87.25, nan
   Final Test: 87.32, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 3.9825, Train: 85.48%, Valid: 85.60%, Test: 85.48%
Epoch: 25, Loss: 0.5124, Train: 86.33%, Valid: 86.36%, Test: 86.32%
Epoch: 50, Loss: 0.3911, Train: 85.83%, Valid: 85.76%, Test: 85.81%
Epoch: 75, Loss: 0.3607, Train: 86.48%, Valid: 86.59%, Test: 86.65%
Epoch: 100, Loss: 0.3544, Train: 86.20%, Valid: 86.19%, Test: 86.32%
Epoch: 125, Loss: 0.3462, Train: 85.98%, Valid: 85.95%, Test: 86.11%
Epoch: 150, Loss: 0.3413, Train: 85.62%, Valid: 85.63%, Test: 85.67%
Epoch: 175, Loss: 0.3334, Train: 85.49%, Valid: 85.52%, Test: 85.60%
Epoch: 200, Loss: 0.3293, Train: 85.63%, Valid: 85.63%, Test: 85.75%
Epoch: 225, Loss: 0.3283, Train: 85.64%, Valid: 85.65%, Test: 85.76%
Epoch: 250, Loss: 1.0820, Train: 85.50%, Valid: 85.48%, Test: 85.64%
Epoch: 275, Loss: 1.2541, Train: 85.75%, Valid: 85.79%, Test: 85.86%
Epoch: 300, Loss: 0.7803, Train: 85.71%, Valid: 85.75%, Test: 85.81%
Epoch: 325, Loss: 0.5291, Train: 85.74%, Valid: 85.77%, Test: 85.84%
Epoch: 350, Loss: 0.3417, Train: 86.01%, Valid: 85.95%, Test: 86.18%
Epoch: 375, Loss: 0.3302, Train: 86.18%, Valid: 86.14%, Test: 86.27%
Epoch: 400, Loss: 0.3991, Train: 85.43%, Valid: 85.21%, Test: 85.54%
Epoch: 425, Loss: 0.5009, Train: 85.65%, Valid: 85.68%, Test: 85.74%
Epoch: 450, Loss: 1.0524, Train: 85.71%, Valid: 85.74%, Test: 85.81%
Epoch: 475, Loss: 0.6779, Train: 85.71%, Valid: 85.74%, Test: 85.81%
Run 01:
Highest Train: 87.70
Highest Valid: 87.72
  Final Train: 87.70
   Final Test: 87.73
All runs:
Highest Train: 87.70, nan
Highest Valid: 87.72, nan
  Final Train: 87.70, nan
   Final Test: 87.73, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.3962, Train: 86.23%, Valid: 86.14%, Test: 86.21%
Epoch: 25, Loss: 0.3642, Train: 85.85%, Valid: 85.63%, Test: 85.91%
Epoch: 50, Loss: 0.3589, Train: 85.28%, Valid: 85.20%, Test: 85.43%
Epoch: 75, Loss: 0.3526, Train: 85.61%, Valid: 85.45%, Test: 85.73%
Epoch: 100, Loss: 0.3476, Train: 85.52%, Valid: 85.37%, Test: 85.63%
Epoch: 125, Loss: 0.3401, Train: 85.33%, Valid: 85.18%, Test: 85.45%
Epoch: 150, Loss: 0.3399, Train: 85.67%, Valid: 85.52%, Test: 85.79%
Epoch: 175, Loss: 0.9249, Train: 85.85%, Valid: 85.69%, Test: 85.96%
Epoch: 200, Loss: 0.4396, Train: 85.81%, Valid: 85.67%, Test: 85.91%
Epoch: 225, Loss: 0.3434, Train: 85.96%, Valid: 85.81%, Test: 86.06%
Epoch: 250, Loss: 0.3382, Train: 85.98%, Valid: 85.78%, Test: 86.08%
Epoch: 275, Loss: 0.3343, Train: 85.86%, Valid: 85.68%, Test: 85.99%
Epoch: 300, Loss: 0.3287, Train: 85.94%, Valid: 85.74%, Test: 86.06%
Epoch: 325, Loss: 0.3261, Train: 85.91%, Valid: 85.73%, Test: 86.04%
Epoch: 350, Loss: 1.0122, Train: 85.97%, Valid: 85.87%, Test: 86.09%
Epoch: 375, Loss: 0.7231, Train: 86.28%, Valid: 86.14%, Test: 86.39%
Epoch: 400, Loss: 0.5417, Train: 86.20%, Valid: 86.06%, Test: 86.31%
Epoch: 425, Loss: 0.4218, Train: 86.10%, Valid: 85.96%, Test: 86.21%
Epoch: 450, Loss: 0.3485, Train: 86.20%, Valid: 86.04%, Test: 86.32%
Epoch: 475, Loss: 0.3399, Train: 86.08%, Valid: 85.90%, Test: 86.21%
Run 01:
Highest Train: 87.28
Highest Valid: 87.16
  Final Train: 87.28
   Final Test: 87.24
All runs:
Highest Train: 87.28, nan
Highest Valid: 87.16, nan
  Final Train: 87.28, nan
   Final Test: 87.24, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 11.0164, Train: 82.87%, Valid: 82.78%, Test: 82.89%
Epoch: 25, Loss: 0.7129, Train: 85.20%, Valid: 85.04%, Test: 85.23%
Epoch: 50, Loss: 0.7009, Train: 85.24%, Valid: 85.09%, Test: 85.31%
Epoch: 75, Loss: 0.6592, Train: 85.16%, Valid: 85.02%, Test: 85.23%
Epoch: 100, Loss: 2.0089, Train: 84.56%, Valid: 84.53%, Test: 84.65%
Epoch: 125, Loss: 1.9609, Train: 85.59%, Valid: 85.57%, Test: 85.70%
Epoch: 150, Loss: 1.2468, Train: 85.39%, Valid: 85.21%, Test: 85.45%
Epoch: 175, Loss: 0.8654, Train: 85.34%, Valid: 85.18%, Test: 85.40%
Epoch: 200, Loss: 0.6319, Train: 85.30%, Valid: 85.14%, Test: 85.36%
Epoch: 225, Loss: 0.5883, Train: 85.25%, Valid: 85.09%, Test: 85.31%
Epoch: 250, Loss: 0.5464, Train: 85.17%, Valid: 85.01%, Test: 85.23%
Epoch: 275, Loss: 0.5041, Train: 85.03%, Valid: 84.86%, Test: 85.08%
Epoch: 300, Loss: 0.4836, Train: 85.13%, Valid: 84.96%, Test: 85.19%
Epoch: 325, Loss: 0.4218, Train: 85.24%, Valid: 85.07%, Test: 85.30%
Epoch: 350, Loss: 0.3857, Train: 85.31%, Valid: 85.14%, Test: 85.39%
Epoch: 375, Loss: 0.3747, Train: 85.33%, Valid: 85.16%, Test: 85.40%
Epoch: 400, Loss: 0.3679, Train: 85.33%, Valid: 85.15%, Test: 85.41%
Epoch: 425, Loss: 0.3618, Train: 85.32%, Valid: 85.12%, Test: 85.40%
Epoch: 450, Loss: 0.3578, Train: 85.33%, Valid: 85.13%, Test: 85.41%
Epoch: 475, Loss: 0.3549, Train: 85.35%, Valid: 85.14%, Test: 85.43%
Run 01:
Highest Train: 86.51
Highest Valid: 86.49
  Final Train: 86.51
   Final Test: 86.58
All runs:
Highest Train: 86.51, nan
Highest Valid: 86.49, nan
  Final Train: 86.51, nan
   Final Test: 86.58, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.5723, Train: 85.95%, Valid: 85.86%, Test: 85.91%
Epoch: 25, Loss: 0.4055, Train: 87.12%, Valid: 87.16%, Test: 87.14%
Epoch: 50, Loss: 0.3770, Train: 86.49%, Valid: 86.56%, Test: 86.51%
Epoch: 75, Loss: 0.3631, Train: 85.29%, Valid: 85.30%, Test: 85.38%
Epoch: 100, Loss: 0.3582, Train: 85.38%, Valid: 85.41%, Test: 85.46%
Epoch: 125, Loss: 0.3531, Train: 85.59%, Valid: 85.60%, Test: 85.66%
Epoch: 150, Loss: 0.3470, Train: 85.81%, Valid: 85.79%, Test: 85.87%
Epoch: 175, Loss: 0.3393, Train: 85.92%, Valid: 85.86%, Test: 85.95%
Epoch: 200, Loss: 0.3326, Train: 85.38%, Valid: 85.41%, Test: 85.48%
Epoch: 225, Loss: 0.3268, Train: 85.58%, Valid: 85.59%, Test: 85.67%
Epoch: 250, Loss: 0.3276, Train: 85.55%, Valid: 85.55%, Test: 85.60%
Epoch: 275, Loss: 0.3259, Train: 85.98%, Valid: 85.96%, Test: 86.00%
Epoch: 300, Loss: 0.3395, Train: 87.06%, Valid: 87.07%, Test: 87.14%
Epoch: 325, Loss: 0.3310, Train: 85.71%, Valid: 85.68%, Test: 85.79%
Epoch: 350, Loss: 0.3270, Train: 85.87%, Valid: 85.80%, Test: 85.93%
Epoch: 375, Loss: 0.3247, Train: 86.75%, Valid: 86.78%, Test: 86.86%
Epoch: 400, Loss: 0.3336, Train: 86.16%, Valid: 86.12%, Test: 86.22%
Epoch: 425, Loss: 0.3262, Train: 86.75%, Valid: 86.77%, Test: 86.81%
Epoch: 450, Loss: 0.3298, Train: 87.48%, Valid: 87.30%, Test: 87.53%
Epoch: 475, Loss: 0.4297, Train: 85.42%, Valid: 85.41%, Test: 85.49%
Run 01:
Highest Train: 88.35
Highest Valid: 88.37
  Final Train: 88.35
   Final Test: 88.38
All runs:
Highest Train: 88.35, nan
Highest Valid: 88.37, nan
  Final Train: 88.35, nan
   Final Test: 88.38, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 5.3393, Train: 16.23%, Valid: 16.28%, Test: 16.07%
Epoch: 25, Loss: 0.3973, Train: 85.69%, Valid: 85.45%, Test: 85.70%
Epoch: 50, Loss: 0.4261, Train: 85.88%, Valid: 85.69%, Test: 85.87%
Epoch: 75, Loss: 0.4656, Train: 85.84%, Valid: 85.67%, Test: 85.90%
Epoch: 100, Loss: 0.4705, Train: 85.32%, Valid: 85.14%, Test: 85.43%
Epoch: 125, Loss: 0.7462, Train: 85.45%, Valid: 85.30%, Test: 85.57%
Epoch: 150, Loss: 0.6181, Train: 85.36%, Valid: 85.22%, Test: 85.47%
Epoch: 175, Loss: 0.5941, Train: 85.38%, Valid: 85.22%, Test: 85.49%
Epoch: 200, Loss: 0.5542, Train: 85.41%, Valid: 85.25%, Test: 85.52%
Epoch: 225, Loss: 0.5734, Train: 85.33%, Valid: 85.18%, Test: 85.44%
Epoch: 250, Loss: 0.5560, Train: 85.47%, Valid: 85.33%, Test: 85.58%
Epoch: 275, Loss: 0.5407, Train: 86.19%, Valid: 86.03%, Test: 86.27%
Epoch: 300, Loss: 0.5237, Train: 86.11%, Valid: 85.96%, Test: 86.22%
Epoch: 325, Loss: 0.5973, Train: 85.56%, Valid: 85.35%, Test: 85.67%
Epoch: 350, Loss: 0.5787, Train: 86.40%, Valid: 86.45%, Test: 86.44%
Epoch: 375, Loss: 0.6061, Train: 86.13%, Valid: 86.17%, Test: 86.17%
Epoch: 400, Loss: 0.6178, Train: 86.01%, Valid: 85.82%, Test: 86.13%
Epoch: 425, Loss: 0.6838, Train: 86.21%, Valid: 86.02%, Test: 86.32%
Epoch: 450, Loss: 0.5996, Train: 86.38%, Valid: 86.22%, Test: 86.51%
Epoch: 475, Loss: 0.5905, Train: 86.56%, Valid: 86.42%, Test: 86.75%
Run 01:
Highest Train: 87.29
Highest Valid: 87.20
  Final Train: 87.29
   Final Test: 87.26
All runs:
Highest Train: 87.29, nan
Highest Valid: 87.20, nan
  Final Train: 87.29, nan
   Final Test: 87.26, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.2138, Train: 78.52%, Valid: 78.45%, Test: 78.71%
Epoch: 25, Loss: 0.8750, Train: 84.69%, Valid: 84.52%, Test: 84.77%
Epoch: 50, Loss: 0.5846, Train: 84.82%, Valid: 84.65%, Test: 84.88%
Epoch: 75, Loss: 0.5406, Train: 84.86%, Valid: 84.70%, Test: 84.93%
Epoch: 100, Loss: 0.5043, Train: 84.92%, Valid: 84.77%, Test: 84.99%
Epoch: 125, Loss: 0.4743, Train: 85.01%, Valid: 84.86%, Test: 85.09%
Epoch: 150, Loss: 0.4476, Train: 85.08%, Valid: 84.93%, Test: 85.18%
Epoch: 175, Loss: 0.4254, Train: 85.15%, Valid: 85.00%, Test: 85.26%
Epoch: 200, Loss: 0.4076, Train: 85.15%, Valid: 85.00%, Test: 85.26%
Epoch: 225, Loss: 0.3941, Train: 85.13%, Valid: 84.99%, Test: 85.24%
Epoch: 250, Loss: 0.3843, Train: 85.11%, Valid: 84.96%, Test: 85.23%
Epoch: 275, Loss: 0.3778, Train: 85.09%, Valid: 84.94%, Test: 85.21%
Epoch: 300, Loss: 0.3734, Train: 85.08%, Valid: 84.92%, Test: 85.19%
Epoch: 325, Loss: 0.3696, Train: 85.08%, Valid: 84.91%, Test: 85.19%
Epoch: 350, Loss: 0.3656, Train: 85.15%, Valid: 84.97%, Test: 85.26%
Epoch: 375, Loss: 0.3603, Train: 85.31%, Valid: 85.15%, Test: 85.42%
Epoch: 400, Loss: 0.3560, Train: 85.44%, Valid: 85.27%, Test: 85.54%
Epoch: 425, Loss: 0.3521, Train: 85.44%, Valid: 85.27%, Test: 85.52%
Epoch: 450, Loss: 0.3481, Train: 85.43%, Valid: 85.26%, Test: 85.51%
Epoch: 475, Loss: 0.3452, Train: 85.40%, Valid: 85.22%, Test: 85.47%
Run 01:
Highest Train: 85.47
Highest Valid: 85.30
  Final Train: 85.47
   Final Test: 85.54
All runs:
Highest Train: 85.47, nan
Highest Valid: 85.30, nan
  Final Train: 85.47, nan
   Final Test: 85.54, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.3626, Train: 86.71%, Valid: 86.66%, Test: 86.67%
Epoch: 25, Loss: 0.4491, Train: 86.69%, Valid: 86.74%, Test: 86.68%
Epoch: 50, Loss: 0.3989, Train: 85.67%, Valid: 85.69%, Test: 85.76%
Epoch: 75, Loss: 0.3913, Train: 86.92%, Valid: 86.81%, Test: 86.95%
Epoch: 100, Loss: 6.1954, Train: 86.65%, Valid: 86.66%, Test: 86.63%
Epoch: 125, Loss: 2.6227, Train: 86.68%, Valid: 86.69%, Test: 86.67%
Epoch: 150, Loss: 1.2809, Train: 86.47%, Valid: 86.49%, Test: 86.48%
Epoch: 175, Loss: 0.5553, Train: 86.46%, Valid: 86.49%, Test: 86.46%
Epoch: 200, Loss: 0.5334, Train: 86.48%, Valid: 86.49%, Test: 86.48%
Epoch: 225, Loss: 0.5170, Train: 86.44%, Valid: 86.46%, Test: 86.45%
Epoch: 250, Loss: 0.4928, Train: 86.40%, Valid: 86.43%, Test: 86.43%
Epoch: 275, Loss: 0.5021, Train: 86.40%, Valid: 86.42%, Test: 86.43%
Epoch: 300, Loss: 0.4876, Train: 86.37%, Valid: 86.39%, Test: 86.40%
Epoch: 325, Loss: 0.5056, Train: 86.40%, Valid: 86.41%, Test: 86.42%
Epoch: 350, Loss: 0.5101, Train: 86.41%, Valid: 86.42%, Test: 86.43%
Epoch: 375, Loss: 0.5040, Train: 86.41%, Valid: 86.43%, Test: 86.44%
Epoch: 400, Loss: 0.4957, Train: 86.55%, Valid: 86.57%, Test: 86.60%
Epoch: 425, Loss: 0.4937, Train: 86.54%, Valid: 86.56%, Test: 86.59%
Epoch: 450, Loss: 0.4995, Train: 86.49%, Valid: 86.51%, Test: 86.55%
Epoch: 475, Loss: 0.5022, Train: 86.47%, Valid: 86.48%, Test: 86.52%
Run 01:
Highest Train: 88.09
Highest Valid: 88.16
  Final Train: 88.08
   Final Test: 88.08
All runs:
Highest Train: 88.09, nan
Highest Valid: 88.16, nan
  Final Train: 88.08, nan
   Final Test: 88.08, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.0475, Train: 18.10%, Valid: 18.14%, Test: 17.91%
Epoch: 25, Loss: 0.4150, Train: 85.37%, Valid: 85.24%, Test: 85.46%
Epoch: 50, Loss: 0.3733, Train: 85.46%, Valid: 85.33%, Test: 85.57%
Epoch: 75, Loss: 0.3693, Train: 85.47%, Valid: 85.33%, Test: 85.58%
Epoch: 100, Loss: 0.3676, Train: 85.47%, Valid: 85.33%, Test: 85.59%
Epoch: 125, Loss: 0.3666, Train: 85.51%, Valid: 85.37%, Test: 85.64%
Epoch: 150, Loss: 0.3654, Train: 85.55%, Valid: 85.41%, Test: 85.68%
Epoch: 175, Loss: 0.3640, Train: 85.51%, Valid: 85.37%, Test: 85.64%
Epoch: 200, Loss: 0.3623, Train: 85.46%, Valid: 85.33%, Test: 85.60%
Epoch: 225, Loss: 0.3601, Train: 85.42%, Valid: 85.29%, Test: 85.57%
Epoch: 250, Loss: 0.3574, Train: 85.42%, Valid: 85.28%, Test: 85.56%
Epoch: 275, Loss: 0.3542, Train: 85.32%, Valid: 85.19%, Test: 85.50%
Epoch: 300, Loss: 0.3506, Train: 85.31%, Valid: 85.14%, Test: 85.47%
Epoch: 325, Loss: 0.3466, Train: 85.51%, Valid: 85.34%, Test: 85.63%
Epoch: 350, Loss: 0.3423, Train: 85.39%, Valid: 85.24%, Test: 85.51%
Epoch: 375, Loss: 0.3381, Train: 85.35%, Valid: 85.20%, Test: 85.46%
Epoch: 400, Loss: 0.3346, Train: 85.40%, Valid: 85.25%, Test: 85.52%
Epoch: 425, Loss: 0.3324, Train: 85.46%, Valid: 85.30%, Test: 85.57%
Epoch: 450, Loss: 0.3301, Train: 85.42%, Valid: 85.27%, Test: 85.54%
Epoch: 475, Loss: 0.3286, Train: 85.70%, Valid: 85.53%, Test: 85.80%
Run 01:
Highest Train: 85.85
Highest Valid: 85.70
  Final Train: 85.85
   Final Test: 85.96
All runs:
Highest Train: 85.85, nan
Highest Valid: 85.70, nan
  Final Train: 85.85, nan
   Final Test: 85.96, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 7.4392, Train: 68.35%, Valid: 68.43%, Test: 68.64%
Epoch: 25, Loss: 0.6510, Train: 84.86%, Valid: 84.72%, Test: 84.92%
Epoch: 50, Loss: 0.5923, Train: 84.87%, Valid: 84.73%, Test: 84.93%
Epoch: 75, Loss: 0.5851, Train: 84.88%, Valid: 84.74%, Test: 84.94%
Epoch: 100, Loss: 0.5783, Train: 84.88%, Valid: 84.73%, Test: 84.94%
Epoch: 125, Loss: 0.5751, Train: 84.86%, Valid: 84.71%, Test: 84.91%
Epoch: 150, Loss: 0.5728, Train: 84.87%, Valid: 84.72%, Test: 84.93%
Epoch: 175, Loss: 0.5706, Train: 84.88%, Valid: 84.73%, Test: 84.93%
Epoch: 200, Loss: 0.5682, Train: 84.89%, Valid: 84.74%, Test: 84.94%
Epoch: 225, Loss: 0.5656, Train: 84.91%, Valid: 84.76%, Test: 84.96%
Epoch: 250, Loss: 0.5626, Train: 84.93%, Valid: 84.78%, Test: 84.98%
Epoch: 275, Loss: 0.5581, Train: 84.91%, Valid: 84.77%, Test: 84.96%
Epoch: 300, Loss: 1.4752, Train: 85.12%, Valid: 84.95%, Test: 85.18%
Epoch: 325, Loss: 0.6846, Train: 85.08%, Valid: 84.92%, Test: 85.14%
Epoch: 350, Loss: 0.6336, Train: 85.10%, Valid: 84.95%, Test: 85.16%
Epoch: 375, Loss: 0.6171, Train: 85.10%, Valid: 84.94%, Test: 85.16%
Epoch: 400, Loss: 0.6030, Train: 85.09%, Valid: 84.93%, Test: 85.15%
Epoch: 425, Loss: 0.5906, Train: 85.09%, Valid: 84.93%, Test: 85.14%
Epoch: 450, Loss: 0.5777, Train: 85.08%, Valid: 84.92%, Test: 85.14%
Epoch: 475, Loss: 0.5491, Train: 85.04%, Valid: 84.87%, Test: 85.09%
Run 01:
Highest Train: 85.12
Highest Valid: 84.96
  Final Train: 85.12
   Final Test: 85.17
All runs:
Highest Train: 85.12, nan
Highest Valid: 84.96, nan
  Final Train: 85.12, nan
   Final Test: 85.17, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.5929, Train: 85.54%, Valid: 85.62%, Test: 85.55%
Epoch: 25, Loss: 0.4022, Train: 85.10%, Valid: 85.05%, Test: 85.25%
Epoch: 50, Loss: 0.4026, Train: 86.21%, Valid: 86.26%, Test: 86.30%
Epoch: 75, Loss: 0.4010, Train: 86.52%, Valid: 86.54%, Test: 86.58%
Epoch: 100, Loss: 0.3956, Train: 86.46%, Valid: 86.48%, Test: 86.49%
Epoch: 125, Loss: 0.3726, Train: 85.92%, Valid: 85.99%, Test: 86.01%
Epoch: 150, Loss: 0.3636, Train: 85.79%, Valid: 85.83%, Test: 85.88%
Epoch: 175, Loss: 0.3582, Train: 85.83%, Valid: 85.84%, Test: 85.92%
Epoch: 200, Loss: 0.3545, Train: 85.73%, Valid: 85.75%, Test: 85.85%
Epoch: 225, Loss: 0.3511, Train: 85.85%, Valid: 85.86%, Test: 85.94%
Epoch: 250, Loss: 0.3459, Train: 85.86%, Valid: 85.86%, Test: 85.95%
Epoch: 275, Loss: 0.3398, Train: 85.72%, Valid: 85.70%, Test: 85.80%
Epoch: 300, Loss: 0.3347, Train: 85.64%, Valid: 85.64%, Test: 85.74%
Epoch: 325, Loss: 0.3308, Train: 85.64%, Valid: 85.68%, Test: 85.77%
Epoch: 350, Loss: 0.3283, Train: 85.80%, Valid: 85.81%, Test: 85.93%
Epoch: 375, Loss: 0.3266, Train: 85.69%, Valid: 85.61%, Test: 85.74%
Epoch: 400, Loss: 0.3252, Train: 85.73%, Valid: 85.76%, Test: 85.82%
Epoch: 425, Loss: 0.3242, Train: 85.38%, Valid: 85.34%, Test: 85.44%
Epoch: 450, Loss: 0.3285, Train: 86.01%, Valid: 86.00%, Test: 86.10%
Epoch: 475, Loss: 0.3250, Train: 85.83%, Valid: 85.82%, Test: 85.92%
Run 01:
Highest Train: 87.75
Highest Valid: 87.76
  Final Train: 87.75
   Final Test: 87.73
All runs:
Highest Train: 87.75, nan
Highest Valid: 87.76, nan
  Final Train: 87.75, nan
   Final Test: 87.73, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.5753, Train: 15.91%, Valid: 16.01%, Test: 15.85%
Epoch: 25, Loss: 0.4715, Train: 85.75%, Valid: 85.63%, Test: 85.75%
Epoch: 50, Loss: 0.4512, Train: 85.74%, Valid: 85.59%, Test: 85.79%
Epoch: 75, Loss: 0.4201, Train: 85.22%, Valid: 85.08%, Test: 85.35%
Epoch: 100, Loss: 0.4090, Train: 85.21%, Valid: 85.06%, Test: 85.34%
Epoch: 125, Loss: 0.4999, Train: 85.55%, Valid: 85.42%, Test: 85.64%
Epoch: 150, Loss: 0.4903, Train: 85.48%, Valid: 85.35%, Test: 85.59%
Epoch: 175, Loss: 0.4159, Train: 85.52%, Valid: 85.38%, Test: 85.64%
Epoch: 200, Loss: 0.3920, Train: 85.64%, Valid: 85.50%, Test: 85.72%
Epoch: 225, Loss: 0.3741, Train: 85.67%, Valid: 85.52%, Test: 85.75%
Epoch: 250, Loss: 0.3662, Train: 85.72%, Valid: 85.57%, Test: 85.80%
Epoch: 275, Loss: 0.3637, Train: 85.80%, Valid: 85.66%, Test: 85.87%
Epoch: 300, Loss: 0.3613, Train: 85.80%, Valid: 85.65%, Test: 85.87%
Epoch: 325, Loss: 0.3582, Train: 85.79%, Valid: 85.63%, Test: 85.87%
Epoch: 350, Loss: 0.3545, Train: 85.76%, Valid: 85.57%, Test: 85.84%
Epoch: 375, Loss: 0.3502, Train: 85.72%, Valid: 85.53%, Test: 85.82%
Epoch: 400, Loss: 0.3453, Train: 85.48%, Valid: 85.35%, Test: 85.62%
Epoch: 425, Loss: 0.3401, Train: 85.64%, Valid: 85.53%, Test: 85.77%
Epoch: 450, Loss: 0.3350, Train: 85.70%, Valid: 85.60%, Test: 85.85%
Epoch: 475, Loss: 0.3313, Train: 85.83%, Valid: 85.68%, Test: 85.96%
Run 01:
Highest Train: 87.15
Highest Valid: 86.94
  Final Train: 87.15
   Final Test: 87.15
All runs:
Highest Train: 87.15, nan
Highest Valid: 86.94, nan
  Final Train: 87.15, nan
   Final Test: 87.15, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 9.1637, Train: 16.14%, Valid: 16.26%, Test: 16.13%
Epoch: 25, Loss: 1.3990, Train: 84.15%, Valid: 84.03%, Test: 84.21%
Epoch: 50, Loss: 0.6978, Train: 84.37%, Valid: 84.25%, Test: 84.41%
Epoch: 75, Loss: 0.6560, Train: 84.55%, Valid: 84.42%, Test: 84.60%
Epoch: 100, Loss: 0.6254, Train: 84.72%, Valid: 84.58%, Test: 84.78%
Epoch: 125, Loss: 0.6072, Train: 84.81%, Valid: 84.65%, Test: 84.86%
Epoch: 150, Loss: 0.5954, Train: 84.87%, Valid: 84.72%, Test: 84.93%
Epoch: 175, Loss: 0.5808, Train: 84.94%, Valid: 84.78%, Test: 84.99%
Epoch: 200, Loss: 0.5718, Train: 84.95%, Valid: 84.80%, Test: 85.01%
Epoch: 225, Loss: 0.5673, Train: 84.96%, Valid: 84.80%, Test: 85.02%
Epoch: 250, Loss: 0.5638, Train: 84.97%, Valid: 84.80%, Test: 85.03%
Epoch: 275, Loss: 0.5605, Train: 84.98%, Valid: 84.81%, Test: 85.04%
Epoch: 300, Loss: 0.5570, Train: 84.98%, Valid: 84.81%, Test: 85.05%
Epoch: 325, Loss: 0.5534, Train: 84.99%, Valid: 84.82%, Test: 85.06%
Epoch: 350, Loss: 0.5497, Train: 85.00%, Valid: 84.82%, Test: 85.06%
Epoch: 375, Loss: 0.5458, Train: 85.00%, Valid: 84.83%, Test: 85.07%
Epoch: 400, Loss: 0.5415, Train: 85.00%, Valid: 84.83%, Test: 85.07%
Epoch: 425, Loss: 0.5372, Train: 85.00%, Valid: 84.83%, Test: 85.07%
Epoch: 450, Loss: 0.5327, Train: 85.00%, Valid: 84.82%, Test: 85.06%
Epoch: 475, Loss: 0.5278, Train: 85.00%, Valid: 84.82%, Test: 85.06%
Run 01:
Highest Train: 85.00
Highest Valid: 84.83
  Final Train: 85.00
   Final Test: 85.07
All runs:
Highest Train: 85.00, nan
Highest Valid: 84.83, nan
  Final Train: 85.00, nan
   Final Test: 85.07, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 4.9499, Train: 86.28%, Valid: 86.34%, Test: 86.36%
Epoch: 25, Loss: 0.4613, Train: 86.70%, Valid: 86.76%, Test: 86.75%
Epoch: 50, Loss: 0.4387, Train: 86.21%, Valid: 86.27%, Test: 86.29%
Epoch: 75, Loss: 0.3832, Train: 85.49%, Valid: 85.53%, Test: 85.59%
Epoch: 100, Loss: 0.3542, Train: 85.36%, Valid: 85.36%, Test: 85.47%
Epoch: 125, Loss: 0.3462, Train: 86.07%, Valid: 86.04%, Test: 86.14%
Epoch: 150, Loss: 0.3362, Train: 85.85%, Valid: 85.80%, Test: 85.89%
Epoch: 175, Loss: 0.3585, Train: 85.74%, Valid: 85.70%, Test: 85.84%
Epoch: 200, Loss: 0.3297, Train: 86.94%, Valid: 86.98%, Test: 87.04%
Epoch: 225, Loss: 0.3259, Train: 85.92%, Valid: 85.87%, Test: 86.02%
Epoch: 250, Loss: 0.7211, Train: 85.24%, Valid: 85.27%, Test: 85.35%
Epoch: 275, Loss: 0.4217, Train: 85.24%, Valid: 85.28%, Test: 85.34%
Epoch: 300, Loss: 0.3408, Train: 85.68%, Valid: 85.69%, Test: 85.78%
Epoch: 325, Loss: 0.3367, Train: 85.85%, Valid: 85.83%, Test: 85.90%
Epoch: 350, Loss: 0.3316, Train: 85.77%, Valid: 85.76%, Test: 85.85%
Epoch: 375, Loss: 0.3264, Train: 85.76%, Valid: 85.76%, Test: 85.86%
Epoch: 400, Loss: 0.3289, Train: 85.77%, Valid: 85.78%, Test: 85.87%
Epoch: 425, Loss: 0.3265, Train: 85.77%, Valid: 85.78%, Test: 85.87%
Epoch: 450, Loss: 0.3249, Train: 85.89%, Valid: 85.86%, Test: 85.98%
Epoch: 475, Loss: 0.3241, Train: 86.06%, Valid: 85.99%, Test: 86.11%
Run 01:
Highest Train: 86.94
Highest Valid: 86.98
  Final Train: 86.94
   Final Test: 87.04
All runs:
Highest Train: 86.94, nan
Highest Valid: 86.98, nan
  Final Train: 86.94, nan
   Final Test: 87.04, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 5.4358, Train: 83.59%, Valid: 83.42%, Test: 83.69%
Epoch: 25, Loss: 0.4762, Train: 84.85%, Valid: 84.72%, Test: 84.92%
Epoch: 50, Loss: 0.4640, Train: 85.28%, Valid: 85.11%, Test: 85.36%
Epoch: 75, Loss: 0.4553, Train: 85.30%, Valid: 85.19%, Test: 85.38%
Epoch: 100, Loss: 0.4098, Train: 85.45%, Valid: 85.35%, Test: 85.53%
Epoch: 125, Loss: 0.3779, Train: 85.73%, Valid: 85.58%, Test: 85.80%
Epoch: 150, Loss: 0.3626, Train: 85.68%, Valid: 85.53%, Test: 85.75%
Epoch: 175, Loss: 0.3593, Train: 85.90%, Valid: 85.74%, Test: 85.97%
Epoch: 200, Loss: 0.3550, Train: 85.95%, Valid: 85.78%, Test: 85.99%
Epoch: 225, Loss: 0.3467, Train: 85.72%, Valid: 85.58%, Test: 85.82%
Epoch: 250, Loss: 0.3399, Train: 85.59%, Valid: 85.44%, Test: 85.69%
Epoch: 275, Loss: 0.3360, Train: 85.54%, Valid: 85.39%, Test: 85.64%
Epoch: 300, Loss: 0.3339, Train: 85.65%, Valid: 85.49%, Test: 85.76%
Epoch: 325, Loss: 0.3304, Train: 85.54%, Valid: 85.38%, Test: 85.65%
Epoch: 350, Loss: 0.3272, Train: 85.80%, Valid: 85.64%, Test: 85.91%
Epoch: 375, Loss: 0.3259, Train: 85.97%, Valid: 85.81%, Test: 86.08%
Epoch: 400, Loss: 0.3250, Train: 85.85%, Valid: 85.68%, Test: 85.96%
Epoch: 425, Loss: 0.7721, Train: 85.84%, Valid: 85.68%, Test: 85.95%
Epoch: 450, Loss: 0.4641, Train: 86.47%, Valid: 86.37%, Test: 86.57%
Epoch: 475, Loss: 0.3496, Train: 87.42%, Valid: 87.31%, Test: 87.51%
Run 01:
Highest Train: 88.00
Highest Valid: 88.00
  Final Train: 88.00
   Final Test: 88.11
All runs:
Highest Train: 88.00, nan
Highest Valid: 88.00, nan
  Final Train: 88.00, nan
   Final Test: 88.11, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 3.5385, Train: 73.58%, Valid: 73.61%, Test: 73.87%
Epoch: 25, Loss: 0.8477, Train: 84.32%, Valid: 84.18%, Test: 84.39%
Epoch: 50, Loss: 0.6644, Train: 84.53%, Valid: 84.35%, Test: 84.62%
Epoch: 75, Loss: 0.5696, Train: 84.59%, Valid: 84.43%, Test: 84.72%
Epoch: 100, Loss: 0.4207, Train: 85.30%, Valid: 85.19%, Test: 85.36%
Epoch: 125, Loss: 0.3994, Train: 85.32%, Valid: 85.19%, Test: 85.40%
Epoch: 150, Loss: 0.3925, Train: 85.25%, Valid: 85.12%, Test: 85.34%
Epoch: 175, Loss: 0.3880, Train: 85.22%, Valid: 85.09%, Test: 85.31%
Epoch: 200, Loss: 0.3846, Train: 85.19%, Valid: 85.06%, Test: 85.28%
Epoch: 225, Loss: 0.3804, Train: 85.16%, Valid: 85.03%, Test: 85.27%
Epoch: 250, Loss: 0.3773, Train: 85.14%, Valid: 85.02%, Test: 85.25%
Epoch: 275, Loss: 0.3742, Train: 85.13%, Valid: 85.00%, Test: 85.24%
Epoch: 300, Loss: 0.3720, Train: 85.12%, Valid: 84.99%, Test: 85.22%
Epoch: 325, Loss: 0.3708, Train: 85.11%, Valid: 84.98%, Test: 85.21%
Epoch: 350, Loss: 0.3698, Train: 85.10%, Valid: 84.97%, Test: 85.21%
Epoch: 375, Loss: 0.3687, Train: 85.09%, Valid: 84.95%, Test: 85.19%
Epoch: 400, Loss: 0.3675, Train: 85.07%, Valid: 84.93%, Test: 85.17%
Epoch: 425, Loss: 0.3665, Train: 85.06%, Valid: 84.92%, Test: 85.16%
Epoch: 450, Loss: 0.3653, Train: 85.04%, Valid: 84.90%, Test: 85.14%
Epoch: 475, Loss: 0.3641, Train: 85.02%, Valid: 84.88%, Test: 85.12%
Run 01:
Highest Train: 86.21
Highest Valid: 86.12
  Final Train: 86.21
   Final Test: 86.21
All runs:
Highest Train: 86.21, nan
Highest Valid: 86.12, nan
  Final Train: 86.21, nan
   Final Test: 86.21, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.3797, Train: 85.08%, Valid: 85.06%, Test: 85.18%
Epoch: 25, Loss: 0.3645, Train: 86.32%, Valid: 86.35%, Test: 86.43%
Epoch: 50, Loss: 0.3594, Train: 86.67%, Valid: 86.69%, Test: 86.77%
Epoch: 75, Loss: 0.3531, Train: 86.82%, Valid: 86.88%, Test: 86.94%
Epoch: 100, Loss: 0.3499, Train: 85.24%, Valid: 85.13%, Test: 85.27%
Epoch: 125, Loss: 0.3395, Train: 85.44%, Valid: 85.42%, Test: 85.52%
Epoch: 150, Loss: 0.5432, Train: 85.67%, Valid: 85.73%, Test: 85.75%
Epoch: 175, Loss: 0.6597, Train: 85.69%, Valid: 85.73%, Test: 85.78%
Epoch: 200, Loss: 0.4599, Train: 85.67%, Valid: 85.71%, Test: 85.75%
Epoch: 225, Loss: 0.3451, Train: 85.79%, Valid: 85.82%, Test: 85.87%
Epoch: 250, Loss: 0.3356, Train: 85.48%, Valid: 85.53%, Test: 85.62%
Epoch: 275, Loss: 0.3296, Train: 85.50%, Valid: 85.54%, Test: 85.62%
Epoch: 300, Loss: 0.3513, Train: 86.48%, Valid: 86.33%, Test: 86.56%
Epoch: 325, Loss: 0.4491, Train: 85.66%, Valid: 85.70%, Test: 85.75%
Epoch: 350, Loss: 0.3377, Train: 86.53%, Valid: 86.26%, Test: 86.54%
Epoch: 375, Loss: 0.3344, Train: 86.83%, Valid: 86.62%, Test: 86.82%
Epoch: 400, Loss: 0.3298, Train: 85.87%, Valid: 85.84%, Test: 85.95%
Epoch: 425, Loss: 0.3275, Train: 85.78%, Valid: 85.76%, Test: 85.86%
Epoch: 450, Loss: 0.3261, Train: 86.12%, Valid: 86.05%, Test: 86.18%
Epoch: 475, Loss: 0.3251, Train: 86.03%, Valid: 85.79%, Test: 86.05%
Run 01:
Highest Train: 87.08
Highest Valid: 87.13
  Final Train: 87.08
   Final Test: 87.17
All runs:
Highest Train: 87.08, nan
Highest Valid: 87.13, nan
  Final Train: 87.08, nan
   Final Test: 87.17, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 11.6055, Train: 14.91%, Valid: 14.88%, Test: 14.88%
Epoch: 25, Loss: 0.4706, Train: 86.28%, Valid: 86.17%, Test: 86.37%
Epoch: 50, Loss: 0.3845, Train: 86.20%, Valid: 86.14%, Test: 86.24%
Epoch: 75, Loss: 0.3666, Train: 86.18%, Valid: 86.08%, Test: 86.20%
Epoch: 100, Loss: 0.3627, Train: 85.76%, Valid: 85.58%, Test: 85.87%
Epoch: 125, Loss: 0.3592, Train: 85.77%, Valid: 85.59%, Test: 85.88%
Epoch: 150, Loss: 0.3551, Train: 85.77%, Valid: 85.59%, Test: 85.87%
Epoch: 175, Loss: 0.3500, Train: 85.75%, Valid: 85.59%, Test: 85.85%
Epoch: 200, Loss: 0.3422, Train: 85.61%, Valid: 85.44%, Test: 85.72%
Epoch: 225, Loss: 0.3359, Train: 85.53%, Valid: 85.38%, Test: 85.65%
Epoch: 250, Loss: 0.3314, Train: 85.66%, Valid: 85.52%, Test: 85.78%
Epoch: 275, Loss: 0.3415, Train: 85.78%, Valid: 85.63%, Test: 85.90%
Epoch: 300, Loss: 0.3299, Train: 85.99%, Valid: 85.84%, Test: 86.13%
Epoch: 325, Loss: 0.3275, Train: 85.93%, Valid: 85.77%, Test: 86.06%
Epoch: 350, Loss: 0.3265, Train: 85.96%, Valid: 85.79%, Test: 86.08%
Epoch: 375, Loss: 0.3260, Train: 86.01%, Valid: 85.84%, Test: 86.13%
Epoch: 400, Loss: 0.3426, Train: 86.09%, Valid: 85.91%, Test: 86.21%
Epoch: 425, Loss: 0.3412, Train: 86.13%, Valid: 85.93%, Test: 86.26%
Epoch: 450, Loss: 0.3365, Train: 86.06%, Valid: 85.85%, Test: 86.18%
Epoch: 475, Loss: 0.3325, Train: 86.11%, Valid: 85.93%, Test: 86.24%
Run 01:
Highest Train: 87.41
Highest Valid: 87.29
  Final Train: 87.41
   Final Test: 87.50
All runs:
Highest Train: 87.41, nan
Highest Valid: 87.29, nan
  Final Train: 87.41, nan
   Final Test: 87.50, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 10.0736, Train: 84.68%, Valid: 84.62%, Test: 84.71%
Epoch: 25, Loss: 0.6772, Train: 85.21%, Valid: 85.11%, Test: 85.24%
Epoch: 50, Loss: 0.6604, Train: 85.06%, Valid: 84.93%, Test: 85.13%
Epoch: 75, Loss: 0.6431, Train: 85.01%, Valid: 84.87%, Test: 85.08%
Epoch: 100, Loss: 0.5943, Train: 84.76%, Valid: 84.62%, Test: 84.84%
Epoch: 125, Loss: 0.6309, Train: 85.06%, Valid: 84.91%, Test: 85.13%
Epoch: 150, Loss: 0.6158, Train: 85.05%, Valid: 84.89%, Test: 85.12%
Epoch: 175, Loss: 0.6006, Train: 85.03%, Valid: 84.87%, Test: 85.10%
Epoch: 200, Loss: 0.5863, Train: 85.01%, Valid: 84.85%, Test: 85.08%
Epoch: 225, Loss: 0.5740, Train: 84.99%, Valid: 84.83%, Test: 85.06%
Epoch: 250, Loss: 0.5622, Train: 84.99%, Valid: 84.83%, Test: 85.06%
Epoch: 275, Loss: 0.5455, Train: 84.96%, Valid: 84.78%, Test: 85.02%
Epoch: 300, Loss: 0.5310, Train: 84.93%, Valid: 84.76%, Test: 85.00%
Epoch: 325, Loss: 0.5286, Train: 85.03%, Valid: 84.85%, Test: 85.09%
Epoch: 350, Loss: 0.5467, Train: 85.04%, Valid: 84.86%, Test: 85.11%
Epoch: 375, Loss: 0.5368, Train: 85.04%, Valid: 84.86%, Test: 85.11%
Epoch: 400, Loss: 0.4986, Train: 84.96%, Valid: 84.79%, Test: 85.05%
Epoch: 425, Loss: 0.4956, Train: 84.96%, Valid: 84.79%, Test: 85.05%
Epoch: 450, Loss: 0.5067, Train: 84.95%, Valid: 84.77%, Test: 85.03%
Epoch: 475, Loss: 0.4873, Train: 84.93%, Valid: 84.76%, Test: 85.03%
Run 01:
Highest Train: 85.53
Highest Valid: 85.42
  Final Train: 85.53
   Final Test: 85.56
All runs:
Highest Train: 85.53, nan
Highest Valid: 85.42, nan
  Final Train: 85.53, nan
   Final Test: 85.56, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.3801, Train: 87.64%, Valid: 87.69%, Test: 87.70%
Epoch: 25, Loss: 0.3646, Train: 85.27%, Valid: 85.28%, Test: 85.33%
Epoch: 50, Loss: 0.3598, Train: 85.52%, Valid: 85.51%, Test: 85.61%
Epoch: 75, Loss: 0.3529, Train: 85.66%, Valid: 85.65%, Test: 85.78%
Epoch: 100, Loss: 0.3483, Train: 85.81%, Valid: 85.77%, Test: 85.89%
Epoch: 125, Loss: 0.3378, Train: 85.20%, Valid: 85.17%, Test: 85.30%
Epoch: 150, Loss: 0.4305, Train: 85.66%, Valid: 85.71%, Test: 85.74%
Epoch: 175, Loss: 0.3416, Train: 85.22%, Valid: 85.22%, Test: 85.35%
Epoch: 200, Loss: 0.3355, Train: 85.80%, Valid: 85.79%, Test: 85.86%
Epoch: 225, Loss: 0.3288, Train: 85.59%, Valid: 85.60%, Test: 85.66%
Epoch: 250, Loss: 0.6538, Train: 85.68%, Valid: 85.71%, Test: 85.77%
Epoch: 275, Loss: 0.4737, Train: 85.68%, Valid: 85.70%, Test: 85.77%
Epoch: 300, Loss: 0.3423, Train: 85.97%, Valid: 85.99%, Test: 86.07%
Epoch: 325, Loss: 0.3363, Train: 85.91%, Valid: 85.93%, Test: 86.01%
Epoch: 350, Loss: 0.3322, Train: 85.91%, Valid: 85.89%, Test: 85.99%
Epoch: 375, Loss: 0.3294, Train: 85.82%, Valid: 85.82%, Test: 85.90%
Epoch: 400, Loss: 0.3268, Train: 85.80%, Valid: 85.82%, Test: 85.89%
Epoch: 425, Loss: 0.3254, Train: 85.80%, Valid: 85.84%, Test: 85.88%
Epoch: 450, Loss: 0.3245, Train: 85.84%, Valid: 85.88%, Test: 85.93%
Epoch: 475, Loss: 0.3253, Train: 85.70%, Valid: 85.69%, Test: 85.79%
Run 01:
Highest Train: 87.64
Highest Valid: 87.69
  Final Train: 87.64
   Final Test: 87.70
All runs:
Highest Train: 87.64, nan
Highest Valid: 87.69, nan
  Final Train: 87.64, nan
   Final Test: 87.70, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.8312, Train: 83.26%, Valid: 83.29%, Test: 83.45%
Epoch: 25, Loss: 0.4444, Train: 86.01%, Valid: 85.93%, Test: 86.07%
Epoch: 50, Loss: 0.4021, Train: 85.25%, Valid: 85.21%, Test: 85.37%
Epoch: 75, Loss: 0.3893, Train: 85.39%, Valid: 85.34%, Test: 85.49%
Epoch: 100, Loss: 0.3942, Train: 85.78%, Valid: 85.71%, Test: 85.86%
Epoch: 125, Loss: 0.3818, Train: 85.15%, Valid: 85.14%, Test: 85.30%
Epoch: 150, Loss: 0.3755, Train: 84.88%, Valid: 84.83%, Test: 85.14%
Epoch: 175, Loss: 0.3701, Train: 84.75%, Valid: 84.76%, Test: 85.08%
Epoch: 200, Loss: 0.3705, Train: 85.28%, Valid: 85.21%, Test: 85.50%
Epoch: 225, Loss: 0.3788, Train: 85.75%, Valid: 85.65%, Test: 85.92%
Epoch: 250, Loss: 0.3627, Train: 84.59%, Valid: 84.53%, Test: 84.82%
Epoch: 275, Loss: 0.3706, Train: 85.54%, Valid: 85.41%, Test: 85.70%
Epoch: 300, Loss: 0.3604, Train: 85.05%, Valid: 84.90%, Test: 85.21%
Epoch: 325, Loss: 0.3555, Train: 85.17%, Valid: 85.24%, Test: 85.44%
Epoch: 350, Loss: 0.3579, Train: 85.31%, Valid: 85.22%, Test: 85.54%
Epoch: 375, Loss: 0.3512, Train: 84.79%, Valid: 84.76%, Test: 85.07%
Epoch: 400, Loss: 0.3482, Train: 84.95%, Valid: 84.91%, Test: 85.20%
Epoch: 425, Loss: 0.3570, Train: 85.65%, Valid: 85.50%, Test: 85.80%
Epoch: 450, Loss: 0.3407, Train: 85.36%, Valid: 85.26%, Test: 85.49%
Epoch: 475, Loss: 0.3345, Train: 85.67%, Valid: 85.52%, Test: 85.79%
Run 01:
Highest Train: 86.64
Highest Valid: 86.50
  Final Train: 86.64
   Final Test: 86.72
All runs:
Highest Train: 86.64, nan
Highest Valid: 86.50, nan
  Final Train: 86.64, nan
   Final Test: 86.72, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 7.7515, Train: 82.15%, Valid: 82.22%, Test: 82.41%
Epoch: 25, Loss: 0.8499, Train: 85.13%, Valid: 84.98%, Test: 85.17%
Epoch: 50, Loss: 0.7736, Train: 85.17%, Valid: 85.02%, Test: 85.22%
Epoch: 75, Loss: 0.7506, Train: 85.18%, Valid: 85.03%, Test: 85.23%
Epoch: 100, Loss: 0.7220, Train: 85.14%, Valid: 84.99%, Test: 85.19%
Epoch: 125, Loss: 0.6832, Train: 85.09%, Valid: 84.93%, Test: 85.15%
Epoch: 150, Loss: 0.6477, Train: 85.05%, Valid: 84.88%, Test: 85.11%
Epoch: 175, Loss: 0.6139, Train: 84.96%, Valid: 84.80%, Test: 85.02%
Epoch: 200, Loss: 0.5927, Train: 84.91%, Valid: 84.74%, Test: 84.97%
Epoch: 225, Loss: 0.5785, Train: 84.93%, Valid: 84.76%, Test: 84.99%
Epoch: 250, Loss: 0.5575, Train: 84.90%, Valid: 84.72%, Test: 84.97%
Epoch: 275, Loss: 0.5364, Train: 84.89%, Valid: 84.71%, Test: 84.96%
Epoch: 300, Loss: 0.5723, Train: 85.06%, Valid: 84.89%, Test: 85.13%
Epoch: 325, Loss: 0.5170, Train: 84.97%, Valid: 84.78%, Test: 85.04%
Epoch: 350, Loss: 0.4984, Train: 84.95%, Valid: 84.77%, Test: 85.02%
Epoch: 375, Loss: 0.5477, Train: 85.12%, Valid: 84.93%, Test: 85.18%
Epoch: 400, Loss: 0.4890, Train: 85.11%, Valid: 84.91%, Test: 85.16%
Epoch: 425, Loss: 0.6927, Train: 85.26%, Valid: 85.06%, Test: 85.32%
Epoch: 450, Loss: 0.5450, Train: 85.17%, Valid: 84.97%, Test: 85.22%
Epoch: 475, Loss: 0.4952, Train: 85.06%, Valid: 84.87%, Test: 85.12%
Run 01:
Highest Train: 85.27
Highest Valid: 85.08
  Final Train: 85.27
   Final Test: 85.33
All runs:
Highest Train: 85.27, nan
Highest Valid: 85.08, nan
  Final Train: 85.27, nan
   Final Test: 85.33, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 16.3929, Train: 14.94%, Valid: 14.83%, Test: 14.98%
Epoch: 25, Loss: 0.5049, Train: 86.06%, Valid: 86.08%, Test: 86.18%
Epoch: 50, Loss: 0.4921, Train: 86.07%, Valid: 86.07%, Test: 86.13%
Epoch: 75, Loss: 0.4030, Train: 86.07%, Valid: 86.06%, Test: 86.14%
Epoch: 100, Loss: 0.3558, Train: 86.03%, Valid: 85.96%, Test: 86.08%
Epoch: 125, Loss: 0.3495, Train: 86.68%, Valid: 86.67%, Test: 86.76%
Epoch: 150, Loss: 0.3424, Train: 85.33%, Valid: 85.30%, Test: 85.43%
Epoch: 175, Loss: 0.3332, Train: 85.45%, Valid: 85.45%, Test: 85.55%
Epoch: 200, Loss: 0.3286, Train: 84.02%, Valid: 84.00%, Test: 84.12%
Epoch: 225, Loss: 0.8192, Train: 85.32%, Valid: 85.33%, Test: 85.39%
Epoch: 250, Loss: 0.9266, Train: 85.12%, Valid: 85.09%, Test: 85.24%
Epoch: 275, Loss: 0.6113, Train: 85.40%, Valid: 85.44%, Test: 85.49%
Epoch: 300, Loss: 0.4387, Train: 85.43%, Valid: 85.46%, Test: 85.51%
Epoch: 325, Loss: 0.3466, Train: 86.49%, Valid: 86.42%, Test: 86.51%
Epoch: 350, Loss: 0.3401, Train: 85.74%, Valid: 85.70%, Test: 85.84%
Epoch: 375, Loss: 0.3331, Train: 85.69%, Valid: 85.64%, Test: 85.80%
Epoch: 400, Loss: 0.4508, Train: 85.45%, Valid: 85.45%, Test: 85.53%
Epoch: 425, Loss: 0.3467, Train: 85.72%, Valid: 85.71%, Test: 85.82%
Epoch: 450, Loss: 0.3318, Train: 85.73%, Valid: 85.72%, Test: 85.80%
Epoch: 475, Loss: 0.3293, Train: 85.57%, Valid: 85.59%, Test: 85.65%
Run 01:
Highest Train: 87.34
Highest Valid: 87.18
  Final Train: 87.34
   Final Test: 87.35
All runs:
Highest Train: 87.34, nan
Highest Valid: 87.18, nan
  Final Train: 87.34, nan
   Final Test: 87.35, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 17.2285, Train: 17.53%, Valid: 17.48%, Test: 17.26%
Epoch: 25, Loss: 0.6127, Train: 85.25%, Valid: 85.27%, Test: 85.36%
Epoch: 50, Loss: 0.5367, Train: 85.37%, Valid: 85.38%, Test: 85.47%
Epoch: 75, Loss: 0.4608, Train: 85.69%, Valid: 85.65%, Test: 85.74%
Epoch: 100, Loss: 0.3798, Train: 85.53%, Valid: 85.41%, Test: 85.61%
Epoch: 125, Loss: 0.3614, Train: 85.58%, Valid: 85.44%, Test: 85.69%
Epoch: 150, Loss: 0.3569, Train: 85.64%, Valid: 85.48%, Test: 85.75%
Epoch: 175, Loss: 0.3506, Train: 85.68%, Valid: 85.53%, Test: 85.78%
Epoch: 200, Loss: 0.3418, Train: 85.53%, Valid: 85.38%, Test: 85.65%
Epoch: 225, Loss: 0.3358, Train: 85.49%, Valid: 85.34%, Test: 85.62%
Epoch: 250, Loss: 0.3327, Train: 85.48%, Valid: 85.35%, Test: 85.60%
Epoch: 275, Loss: 0.3308, Train: 85.57%, Valid: 85.42%, Test: 85.70%
Epoch: 300, Loss: 0.3296, Train: 85.68%, Valid: 85.52%, Test: 85.80%
Epoch: 325, Loss: 0.3285, Train: 85.82%, Valid: 85.66%, Test: 85.94%
Epoch: 350, Loss: 0.3282, Train: 85.77%, Valid: 85.60%, Test: 85.89%
Epoch: 375, Loss: 0.3276, Train: 85.88%, Valid: 85.71%, Test: 86.01%
Epoch: 400, Loss: 0.3275, Train: 85.94%, Valid: 85.75%, Test: 86.08%
Epoch: 425, Loss: 0.3391, Train: 86.08%, Valid: 85.93%, Test: 86.22%
Epoch: 450, Loss: 0.6221, Train: 86.01%, Valid: 85.89%, Test: 86.16%
Epoch: 475, Loss: 0.5813, Train: 85.81%, Valid: 85.69%, Test: 85.94%
Run 01:
Highest Train: 87.51
Highest Valid: 87.33
  Final Train: 87.51
   Final Test: 87.60
All runs:
Highest Train: 87.51, nan
Highest Valid: 87.33, nan
  Final Train: 87.51, nan
   Final Test: 87.60, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 76.9646, Train: 61.20%, Valid: 61.31%, Test: 61.66%
Epoch: 25, Loss: 2.4906, Train: 84.08%, Valid: 83.93%, Test: 84.11%
Epoch: 50, Loss: 1.8424, Train: 84.32%, Valid: 84.14%, Test: 84.34%
Epoch: 75, Loss: 1.3520, Train: 84.51%, Valid: 84.31%, Test: 84.54%
Epoch: 100, Loss: 0.7736, Train: 86.70%, Valid: 86.46%, Test: 86.68%
Epoch: 125, Loss: 0.7173, Train: 86.49%, Valid: 86.28%, Test: 86.49%
Epoch: 150, Loss: 0.6265, Train: 85.57%, Valid: 85.39%, Test: 85.64%
Epoch: 175, Loss: 0.5867, Train: 85.76%, Valid: 85.59%, Test: 85.80%
Epoch: 200, Loss: 0.5353, Train: 85.38%, Valid: 85.21%, Test: 85.42%
Epoch: 225, Loss: 0.4757, Train: 84.87%, Valid: 84.70%, Test: 84.92%
Epoch: 250, Loss: 0.4420, Train: 84.78%, Valid: 84.60%, Test: 84.83%
Epoch: 275, Loss: 0.4288, Train: 84.92%, Valid: 84.75%, Test: 84.97%
Epoch: 300, Loss: 0.4256, Train: 84.97%, Valid: 84.81%, Test: 85.03%
Epoch: 325, Loss: 0.4242, Train: 85.00%, Valid: 84.84%, Test: 85.06%
Epoch: 350, Loss: 0.4231, Train: 85.02%, Valid: 84.86%, Test: 85.08%
Epoch: 375, Loss: 0.4222, Train: 85.04%, Valid: 84.88%, Test: 85.09%
Epoch: 400, Loss: 0.4213, Train: 85.05%, Valid: 84.89%, Test: 85.11%
Epoch: 425, Loss: 0.4204, Train: 85.06%, Valid: 84.91%, Test: 85.12%
Epoch: 450, Loss: 0.4195, Train: 85.08%, Valid: 84.92%, Test: 85.13%
Epoch: 475, Loss: 0.4188, Train: 85.09%, Valid: 84.93%, Test: 85.14%
Run 01:
Highest Train: 86.76
Highest Valid: 86.50
  Final Train: 86.76
   Final Test: 86.73
All runs:
Highest Train: 86.76, nan
Highest Valid: 86.50, nan
  Final Train: 86.76, nan
   Final Test: 86.73, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 10.0101, Train: 19.03%, Valid: 18.99%, Test: 18.86%
Epoch: 25, Loss: 0.5126, Train: 86.30%, Valid: 86.34%, Test: 86.29%
Epoch: 50, Loss: 0.3670, Train: 87.32%, Valid: 87.38%, Test: 87.33%
Epoch: 75, Loss: 0.3612, Train: 86.06%, Valid: 86.11%, Test: 86.25%
Epoch: 100, Loss: 0.3550, Train: 85.60%, Valid: 85.62%, Test: 85.70%
Epoch: 125, Loss: 0.3463, Train: 85.96%, Valid: 85.98%, Test: 86.03%
Epoch: 150, Loss: 0.3354, Train: 84.93%, Valid: 84.93%, Test: 85.06%
Epoch: 175, Loss: 0.3429, Train: 85.42%, Valid: 85.48%, Test: 85.57%
Epoch: 200, Loss: 0.3323, Train: 85.18%, Valid: 85.20%, Test: 85.32%
Epoch: 225, Loss: 0.3288, Train: 85.46%, Valid: 85.46%, Test: 85.60%
Epoch: 250, Loss: 0.5246, Train: 85.60%, Valid: 85.65%, Test: 85.72%
Epoch: 275, Loss: 0.3703, Train: 85.88%, Valid: 85.90%, Test: 85.99%
Epoch: 300, Loss: 0.3390, Train: 85.71%, Valid: 85.62%, Test: 85.83%
Epoch: 325, Loss: 0.3343, Train: 85.70%, Valid: 85.66%, Test: 85.81%
Epoch: 350, Loss: 0.3303, Train: 85.63%, Valid: 85.62%, Test: 85.76%
Epoch: 375, Loss: 0.3300, Train: 85.04%, Valid: 85.01%, Test: 85.13%
Epoch: 400, Loss: 0.3276, Train: 85.61%, Valid: 85.59%, Test: 85.74%
Epoch: 425, Loss: 0.3256, Train: 85.88%, Valid: 85.87%, Test: 85.99%
Epoch: 450, Loss: 0.3246, Train: 86.07%, Valid: 86.04%, Test: 86.17%
Epoch: 475, Loss: 0.7967, Train: 85.54%, Valid: 85.57%, Test: 85.68%
Run 01:
Highest Train: 87.70
Highest Valid: 87.68
  Final Train: 87.70
   Final Test: 87.73
All runs:
Highest Train: 87.70, nan
Highest Valid: 87.68, nan
  Final Train: 87.70, nan
   Final Test: 87.73, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 3.7316, Train: 84.05%, Valid: 84.10%, Test: 84.15%
Epoch: 25, Loss: 0.7065, Train: 87.71%, Valid: 87.64%, Test: 87.68%
Epoch: 50, Loss: 0.5596, Train: 87.97%, Valid: 87.93%, Test: 88.00%
Epoch: 75, Loss: 0.5038, Train: 86.30%, Valid: 86.32%, Test: 86.34%
Epoch: 100, Loss: 0.4747, Train: 85.66%, Valid: 85.60%, Test: 85.78%
Epoch: 125, Loss: 0.4426, Train: 85.16%, Valid: 85.05%, Test: 85.30%
Epoch: 150, Loss: 0.3979, Train: 85.53%, Valid: 85.42%, Test: 85.67%
Epoch: 175, Loss: 0.3741, Train: 85.68%, Valid: 85.53%, Test: 85.80%
Epoch: 200, Loss: 0.3627, Train: 85.75%, Valid: 85.59%, Test: 85.86%
Epoch: 225, Loss: 0.3558, Train: 85.77%, Valid: 85.62%, Test: 85.88%
Epoch: 250, Loss: 0.3505, Train: 85.76%, Valid: 85.60%, Test: 85.87%
Epoch: 275, Loss: 0.3460, Train: 85.71%, Valid: 85.54%, Test: 85.81%
Epoch: 300, Loss: 0.3415, Train: 85.57%, Valid: 85.41%, Test: 85.69%
Epoch: 325, Loss: 0.3377, Train: 85.59%, Valid: 85.43%, Test: 85.70%
Epoch: 350, Loss: 0.3342, Train: 85.63%, Valid: 85.47%, Test: 85.75%
Epoch: 375, Loss: 0.3338, Train: 85.72%, Valid: 85.57%, Test: 85.83%
Epoch: 400, Loss: 0.5691, Train: 86.15%, Valid: 86.10%, Test: 86.25%
Epoch: 425, Loss: 0.3774, Train: 86.24%, Valid: 86.22%, Test: 86.36%
Epoch: 450, Loss: 0.3415, Train: 86.12%, Valid: 85.98%, Test: 86.27%
Epoch: 475, Loss: 0.3386, Train: 86.11%, Valid: 85.96%, Test: 86.25%
Run 01:
Highest Train: 87.97
Highest Valid: 87.93
  Final Train: 87.97
   Final Test: 88.00
All runs:
Highest Train: 87.97, nan
Highest Valid: 87.93, nan
  Final Train: 87.97, nan
   Final Test: 88.00, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 4.0877, Train: 84.51%, Valid: 84.46%, Test: 84.58%
Epoch: 25, Loss: 0.9374, Train: 84.94%, Valid: 84.84%, Test: 84.94%
Epoch: 50, Loss: 0.8888, Train: 84.97%, Valid: 84.81%, Test: 84.99%
Epoch: 75, Loss: 0.8397, Train: 84.96%, Valid: 84.80%, Test: 85.02%
Epoch: 100, Loss: 0.8037, Train: 84.95%, Valid: 84.80%, Test: 85.00%
Epoch: 125, Loss: 0.7684, Train: 84.96%, Valid: 84.80%, Test: 85.01%
Epoch: 150, Loss: 0.7304, Train: 84.95%, Valid: 84.78%, Test: 85.00%
Epoch: 175, Loss: 0.6992, Train: 84.95%, Valid: 84.79%, Test: 85.01%
Epoch: 200, Loss: 0.6665, Train: 84.93%, Valid: 84.75%, Test: 85.00%
Epoch: 225, Loss: 0.6389, Train: 84.90%, Valid: 84.72%, Test: 84.98%
Epoch: 250, Loss: 0.7214, Train: 85.08%, Valid: 84.90%, Test: 85.15%
Epoch: 275, Loss: 0.6356, Train: 85.03%, Valid: 84.84%, Test: 85.10%
Epoch: 300, Loss: 0.5945, Train: 84.97%, Valid: 84.78%, Test: 85.04%
Epoch: 325, Loss: 0.5617, Train: 84.95%, Valid: 84.76%, Test: 85.03%
Epoch: 350, Loss: 0.5984, Train: 85.00%, Valid: 84.81%, Test: 85.08%
Epoch: 375, Loss: 0.5201, Train: 84.91%, Valid: 84.72%, Test: 84.99%
Epoch: 400, Loss: 0.4633, Train: 84.73%, Valid: 84.56%, Test: 84.86%
Epoch: 425, Loss: 0.5918, Train: 85.10%, Valid: 84.91%, Test: 85.17%
Epoch: 450, Loss: 0.5234, Train: 85.11%, Valid: 84.92%, Test: 85.19%
Epoch: 475, Loss: 0.4633, Train: 85.08%, Valid: 84.90%, Test: 85.16%
Run 01:
Highest Train: 87.53
Highest Valid: 87.40
  Final Train: 87.53
   Final Test: 87.52
All runs:
Highest Train: 87.53, nan
Highest Valid: 87.40, nan
  Final Train: 87.53, nan
   Final Test: 87.52, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.4363, Train: 87.46%, Valid: 87.43%, Test: 87.43%
Epoch: 25, Loss: 0.3649, Train: 85.78%, Valid: 85.83%, Test: 85.88%
Epoch: 50, Loss: 0.3601, Train: 85.53%, Valid: 85.52%, Test: 85.65%
Epoch: 75, Loss: 0.3553, Train: 85.64%, Valid: 85.65%, Test: 85.75%
Epoch: 100, Loss: 0.3466, Train: 85.45%, Valid: 85.46%, Test: 85.58%
Epoch: 125, Loss: 0.3388, Train: 85.33%, Valid: 85.35%, Test: 85.43%
Epoch: 150, Loss: 0.3379, Train: 85.70%, Valid: 85.75%, Test: 85.83%
Epoch: 175, Loss: 0.9846, Train: 85.60%, Valid: 85.64%, Test: 85.70%
Epoch: 200, Loss: 0.7817, Train: 85.70%, Valid: 85.74%, Test: 85.80%
Epoch: 225, Loss: 0.4500, Train: 85.56%, Valid: 85.59%, Test: 85.66%
Epoch: 250, Loss: 0.3415, Train: 85.63%, Valid: 85.67%, Test: 85.77%
Epoch: 275, Loss: 0.3369, Train: 85.79%, Valid: 85.82%, Test: 85.92%
Epoch: 300, Loss: 0.3318, Train: 85.82%, Valid: 85.85%, Test: 85.94%
Epoch: 325, Loss: 0.3278, Train: 85.73%, Valid: 85.75%, Test: 85.85%
Epoch: 350, Loss: 0.3298, Train: 85.67%, Valid: 85.69%, Test: 85.75%
Epoch: 375, Loss: 0.5561, Train: 86.95%, Valid: 86.96%, Test: 87.02%
Epoch: 400, Loss: 0.7641, Train: 85.43%, Valid: 85.49%, Test: 85.54%
Epoch: 425, Loss: 0.6431, Train: 85.63%, Valid: 85.67%, Test: 85.73%
Epoch: 450, Loss: 0.4863, Train: 85.64%, Valid: 85.68%, Test: 85.73%
Epoch: 475, Loss: 0.3842, Train: 85.83%, Valid: 85.87%, Test: 85.93%
Run 01:
Highest Train: 87.46
Highest Valid: 87.43
  Final Train: 87.46
   Final Test: 87.43
All runs:
Highest Train: 87.46, nan
Highest Valid: 87.43, nan
  Final Train: 87.46, nan
   Final Test: 87.43, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 17.7471, Train: 14.53%, Valid: 14.53%, Test: 14.44%
Epoch: 25, Loss: 0.7356, Train: 86.02%, Valid: 85.92%, Test: 86.04%
Epoch: 50, Loss: 0.5584, Train: 85.99%, Valid: 85.87%, Test: 86.00%
Epoch: 75, Loss: 0.4148, Train: 85.70%, Valid: 85.58%, Test: 85.77%
Epoch: 100, Loss: 0.3674, Train: 85.69%, Valid: 85.53%, Test: 85.81%
Epoch: 125, Loss: 0.3600, Train: 85.68%, Valid: 85.49%, Test: 85.80%
Epoch: 150, Loss: 0.3527, Train: 85.66%, Valid: 85.48%, Test: 85.79%
Epoch: 175, Loss: 0.3458, Train: 85.42%, Valid: 85.23%, Test: 85.55%
Epoch: 200, Loss: 0.3416, Train: 85.43%, Valid: 85.24%, Test: 85.56%
Epoch: 225, Loss: 0.3378, Train: 85.25%, Valid: 85.09%, Test: 85.39%
Epoch: 250, Loss: 0.3347, Train: 85.27%, Valid: 85.11%, Test: 85.42%
Epoch: 275, Loss: 0.3329, Train: 85.13%, Valid: 84.98%, Test: 85.27%
Epoch: 300, Loss: 0.3359, Train: 85.45%, Valid: 85.28%, Test: 85.59%
Epoch: 325, Loss: 0.3308, Train: 85.50%, Valid: 85.34%, Test: 85.64%
Epoch: 350, Loss: 0.3337, Train: 85.12%, Valid: 85.01%, Test: 85.26%
Epoch: 375, Loss: 0.9360, Train: 86.82%, Valid: 86.75%, Test: 86.93%
Epoch: 400, Loss: 0.6692, Train: 86.25%, Valid: 86.19%, Test: 86.36%
Epoch: 425, Loss: 0.4581, Train: 86.00%, Valid: 85.90%, Test: 86.10%
Epoch: 450, Loss: 0.3446, Train: 85.67%, Valid: 85.49%, Test: 85.81%
Epoch: 475, Loss: 0.3369, Train: 85.76%, Valid: 85.57%, Test: 85.89%
Run 01:
Highest Train: 87.09
Highest Valid: 86.99
  Final Train: 87.09
   Final Test: 87.20
All runs:
Highest Train: 87.09, nan
Highest Valid: 86.99, nan
  Final Train: 87.09, nan
   Final Test: 87.20, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 10.0120, Train: 82.23%, Valid: 82.10%, Test: 82.23%
Epoch: 25, Loss: 0.9973, Train: 84.86%, Valid: 84.75%, Test: 84.97%
Epoch: 50, Loss: 0.8866, Train: 84.87%, Valid: 84.76%, Test: 84.97%
Epoch: 75, Loss: 0.8029, Train: 84.67%, Valid: 84.60%, Test: 84.80%
Epoch: 100, Loss: 0.7620, Train: 84.37%, Valid: 84.22%, Test: 84.48%
Epoch: 125, Loss: 0.7069, Train: 84.55%, Valid: 84.44%, Test: 84.67%
Epoch: 150, Loss: 1.3554, Train: 84.76%, Valid: 84.60%, Test: 84.83%
Epoch: 175, Loss: 0.8698, Train: 84.98%, Valid: 84.81%, Test: 85.04%
Epoch: 200, Loss: 0.8219, Train: 84.96%, Valid: 84.80%, Test: 85.03%
Epoch: 225, Loss: 0.7660, Train: 84.91%, Valid: 84.75%, Test: 84.98%
Epoch: 250, Loss: 0.7076, Train: 84.89%, Valid: 84.73%, Test: 84.96%
Epoch: 275, Loss: 0.6381, Train: 84.96%, Valid: 84.79%, Test: 85.03%
Epoch: 300, Loss: 0.5457, Train: 85.09%, Valid: 84.91%, Test: 85.15%
Epoch: 325, Loss: 0.3774, Train: 86.62%, Valid: 86.63%, Test: 86.70%
Epoch: 350, Loss: 0.3550, Train: 86.52%, Valid: 86.48%, Test: 86.62%
Epoch: 375, Loss: 0.3516, Train: 86.96%, Valid: 86.84%, Test: 87.08%
Epoch: 400, Loss: 0.3498, Train: 86.38%, Valid: 86.15%, Test: 86.47%
Epoch: 425, Loss: 0.3477, Train: 87.01%, Valid: 86.80%, Test: 87.10%
Epoch: 450, Loss: 0.3455, Train: 86.76%, Valid: 86.53%, Test: 86.87%
Epoch: 475, Loss: 0.3431, Train: 86.54%, Valid: 86.30%, Test: 86.63%
Run 01:
Highest Train: 87.21
Highest Valid: 87.04
  Final Train: 87.21
   Final Test: 87.33
All runs:
Highest Train: 87.21, nan
Highest Valid: 87.04, nan
  Final Train: 87.21, nan
   Final Test: 87.33, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.0137, Train: 84.49%, Valid: 84.36%, Test: 84.63%
Epoch: 25, Loss: 27.8489, Train: 15.07%, Valid: 15.22%, Test: 14.98%
Epoch: 50, Loss: 31.0859, Train: 15.08%, Valid: 15.24%, Test: 15.02%
Epoch: 75, Loss: 28.9513, Train: 12.99%, Valid: 12.90%, Test: 12.95%
Epoch: 100, Loss: 26.7366, Train: 13.23%, Valid: 13.14%, Test: 13.19%
Epoch: 125, Loss: 24.8389, Train: 15.11%, Valid: 15.26%, Test: 15.08%
Epoch: 150, Loss: 23.2134, Train: 15.39%, Valid: 15.54%, Test: 15.35%
Epoch: 175, Loss: 21.7155, Train: 15.55%, Valid: 15.72%, Test: 15.52%
Epoch: 200, Loss: 20.3816, Train: 16.11%, Valid: 16.27%, Test: 16.08%
Epoch: 225, Loss: 19.1973, Train: 16.29%, Valid: 16.44%, Test: 16.23%
Epoch: 250, Loss: 1.0683, Train: 84.39%, Valid: 84.29%, Test: 84.47%
Epoch: 275, Loss: 2083.4128, Train: 16.44%, Valid: 16.42%, Test: 16.38%
Epoch: 300, Loss: 65.0130, Train: 16.43%, Valid: 16.38%, Test: 16.36%
Epoch: 325, Loss: 168.2147, Train: 17.15%, Valid: 17.30%, Test: 17.09%
Epoch: 350, Loss: 39.2425, Train: 16.85%, Valid: 17.00%, Test: 16.78%
Epoch: 375, Loss: 37.9544, Train: 16.81%, Valid: 16.95%, Test: 16.74%
Epoch: 400, Loss: 40.2188, Train: 16.85%, Valid: 17.00%, Test: 16.79%
Epoch: 425, Loss: 41.3418, Train: 16.82%, Valid: 16.96%, Test: 16.76%
Epoch: 450, Loss: 32.9838, Train: 16.85%, Valid: 17.00%, Test: 16.79%
Epoch: 475, Loss: 37.5042, Train: 16.84%, Valid: 16.99%, Test: 16.78%
Run 01:
Highest Train: 87.16
Highest Valid: 87.22
  Final Train: 87.16
   Final Test: 87.21
All runs:
Highest Train: 87.16, nan
Highest Valid: 87.22, nan
  Final Train: 87.16, nan
   Final Test: 87.21, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.6126, Train: 86.28%, Valid: 86.25%, Test: 86.45%
Epoch: 25, Loss: 0.5712, Train: 85.96%, Valid: 85.92%, Test: 85.99%
Epoch: 50, Loss: 28.2359, Train: 86.14%, Valid: 86.10%, Test: 86.14%
Epoch: 75, Loss: 69.1110, Train: 85.92%, Valid: 85.88%, Test: 85.94%
Epoch: 100, Loss: 58.8024, Train: 85.88%, Valid: 85.85%, Test: 85.91%
Epoch: 125, Loss: 10.7111, Train: 85.86%, Valid: 85.82%, Test: 85.90%
Epoch: 150, Loss: 55.3202, Train: 15.06%, Valid: 15.10%, Test: 14.99%
Epoch: 175, Loss: 156.7488, Train: 85.78%, Valid: 85.72%, Test: 85.82%
Epoch: 200, Loss: 80.2918, Train: 86.05%, Valid: 86.06%, Test: 86.08%
Epoch: 225, Loss: 82.6685, Train: 86.46%, Valid: 86.45%, Test: 86.52%
Epoch: 250, Loss: 0.4077, Train: 86.10%, Valid: 86.11%, Test: 86.14%
Epoch: 275, Loss: 1.0763, Train: 16.08%, Valid: 16.13%, Test: 16.02%
Epoch: 300, Loss: 14.7159, Train: 85.83%, Valid: 85.85%, Test: 85.88%
Epoch: 325, Loss: 4.4872, Train: 15.26%, Valid: 15.35%, Test: 15.18%
Epoch: 350, Loss: 2.8947, Train: 15.24%, Valid: 15.34%, Test: 15.17%
Epoch: 375, Loss: 21.3968, Train: 15.12%, Valid: 15.23%, Test: 15.03%
Epoch: 400, Loss: 0.4890, Train: 86.02%, Valid: 86.05%, Test: 86.05%
Epoch: 425, Loss: 24.7478, Train: 15.24%, Valid: 15.33%, Test: 15.15%
Epoch: 450, Loss: 57.5451, Train: 15.19%, Valid: 15.28%, Test: 15.13%
Epoch: 475, Loss: 36.5742, Train: 15.22%, Valid: 15.32%, Test: 15.14%
Run 01:
Highest Train: 87.07
Highest Valid: 87.16
  Final Train: 87.07
   Final Test: 87.27
All runs:
Highest Train: 87.07, nan
Highest Valid: 87.16, nan
  Final Train: 87.07, nan
   Final Test: 87.27, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.6463, Train: 85.38%, Valid: 85.21%, Test: 85.36%
Epoch: 25, Loss: 4.1293, Train: 16.86%, Valid: 16.97%, Test: 16.78%
Epoch: 50, Loss: 7283.0107, Train: 15.55%, Valid: 15.62%, Test: 15.47%
Epoch: 75, Loss: 6486.7319, Train: 87.02%, Valid: 87.08%, Test: 87.07%
Epoch: 100, Loss: 1053.2358, Train: 15.28%, Valid: 15.49%, Test: 15.17%
Epoch: 125, Loss: 1787.7432, Train: 15.22%, Valid: 15.44%, Test: 15.10%
Epoch: 150, Loss: 889.8930, Train: 86.82%, Valid: 86.86%, Test: 86.85%
Epoch: 175, Loss: 6.9157, Train: 86.43%, Valid: 86.43%, Test: 86.46%
Epoch: 200, Loss: 92.9861, Train: 86.78%, Valid: 86.82%, Test: 86.82%
Epoch: 225, Loss: 0.9543, Train: 15.24%, Valid: 15.43%, Test: 15.12%
Epoch: 250, Loss: 32.3497, Train: 86.80%, Valid: 86.84%, Test: 86.85%
Epoch: 275, Loss: 3.2119, Train: 86.84%, Valid: 86.89%, Test: 86.90%
Epoch: 300, Loss: 48.9352, Train: 14.38%, Valid: 14.55%, Test: 14.30%
Epoch: 325, Loss: 6.6103, Train: 15.24%, Valid: 15.43%, Test: 15.11%
Epoch: 350, Loss: 2.3271, Train: 15.24%, Valid: 15.43%, Test: 15.12%
Epoch: 375, Loss: 118.6486, Train: 14.38%, Valid: 14.55%, Test: 14.30%
Epoch: 400, Loss: 188.8098, Train: 14.74%, Valid: 14.94%, Test: 14.66%
Epoch: 425, Loss: 16.0966, Train: 86.85%, Valid: 86.90%, Test: 86.91%
Epoch: 450, Loss: 42.9369, Train: 86.77%, Valid: 86.83%, Test: 86.83%
Epoch: 475, Loss: 267.6731, Train: 13.24%, Valid: 13.20%, Test: 13.20%
Run 01:
Highest Train: 87.26
Highest Valid: 87.29
  Final Train: 87.24
   Final Test: 87.27
All runs:
Highest Train: 87.26, nan
Highest Valid: 87.29, nan
  Final Train: 87.24, nan
   Final Test: 87.27, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.3390, Train: 85.68%, Valid: 85.51%, Test: 85.66%
Epoch: 25, Loss: 16.6229, Train: 13.34%, Valid: 13.24%, Test: 13.38%
Epoch: 50, Loss: 19.4087, Train: 13.38%, Valid: 13.29%, Test: 13.41%
Epoch: 75, Loss: 19.8777, Train: 13.38%, Valid: 13.29%, Test: 13.41%
Epoch: 100, Loss: 19.8428, Train: 13.38%, Valid: 13.29%, Test: 13.41%
Epoch: 125, Loss: 19.7543, Train: 13.38%, Valid: 13.29%, Test: 13.41%
Epoch: 150, Loss: 20.1011, Train: 13.38%, Valid: 13.29%, Test: 13.41%
Epoch: 175, Loss: 19.4375, Train: 13.39%, Valid: 13.29%, Test: 13.41%
Epoch: 200, Loss: 19.2653, Train: 13.39%, Valid: 13.29%, Test: 13.41%
Epoch: 225, Loss: 19.3233, Train: 13.39%, Valid: 13.29%, Test: 13.41%
Epoch: 250, Loss: 18.9239, Train: 13.39%, Valid: 13.29%, Test: 13.42%
Epoch: 275, Loss: 18.8800, Train: 13.40%, Valid: 13.30%, Test: 13.43%
Epoch: 300, Loss: 18.5630, Train: 13.42%, Valid: 13.31%, Test: 13.44%
Epoch: 325, Loss: 18.6509, Train: 13.46%, Valid: 13.36%, Test: 13.48%
Epoch: 350, Loss: 18.3734, Train: 13.61%, Valid: 13.52%, Test: 13.64%
Epoch: 375, Loss: 16.0833, Train: 13.84%, Valid: 13.74%, Test: 13.85%
Epoch: 400, Loss: 16.0549, Train: 16.47%, Valid: 16.58%, Test: 16.43%
Epoch: 425, Loss: 15.8035, Train: 16.52%, Valid: 16.63%, Test: 16.44%
Epoch: 450, Loss: 15.0458, Train: 16.64%, Valid: 16.75%, Test: 16.57%
Epoch: 475, Loss: 13.8439, Train: 16.63%, Valid: 16.70%, Test: 16.54%
Run 01:
Highest Train: 86.69
Highest Valid: 86.65
  Final Train: 86.69
   Final Test: 86.84
All runs:
Highest Train: 86.69, nan
Highest Valid: 86.65, nan
  Final Train: 86.69, nan
   Final Test: 86.84, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.1522, Train: 85.91%, Valid: 85.84%, Test: 85.94%
Epoch: 25, Loss: 6.3612, Train: 16.53%, Valid: 16.62%, Test: 16.31%
Epoch: 50, Loss: 0.5920, Train: 85.68%, Valid: 85.53%, Test: 85.76%
Epoch: 75, Loss: 1.0633, Train: 85.69%, Valid: 85.54%, Test: 85.78%
Epoch: 100, Loss: 1.1067, Train: 85.69%, Valid: 85.54%, Test: 85.78%
Epoch: 125, Loss: 1.1165, Train: 85.69%, Valid: 85.54%, Test: 85.78%
Epoch: 150, Loss: 1.1064, Train: 85.69%, Valid: 85.54%, Test: 85.78%
Epoch: 175, Loss: 1.1271, Train: 85.69%, Valid: 85.54%, Test: 85.78%
Epoch: 200, Loss: 1.0887, Train: 85.69%, Valid: 85.54%, Test: 85.78%
Epoch: 225, Loss: 1.1399, Train: 85.70%, Valid: 85.54%, Test: 85.78%
Epoch: 250, Loss: 1.0963, Train: 85.69%, Valid: 85.54%, Test: 85.78%
Epoch: 275, Loss: 1.1053, Train: 85.69%, Valid: 85.54%, Test: 85.78%
Epoch: 300, Loss: 1.0898, Train: 85.69%, Valid: 85.54%, Test: 85.78%
Epoch: 325, Loss: 1.1063, Train: 85.69%, Valid: 85.54%, Test: 85.78%
Epoch: 350, Loss: 1.1221, Train: 85.69%, Valid: 85.54%, Test: 85.78%
Epoch: 375, Loss: 1.1163, Train: 85.69%, Valid: 85.54%, Test: 85.78%
Epoch: 400, Loss: 1.0932, Train: 85.69%, Valid: 85.54%, Test: 85.78%
Epoch: 425, Loss: 1.0971, Train: 85.69%, Valid: 85.54%, Test: 85.78%
Epoch: 450, Loss: 1.0713, Train: 85.69%, Valid: 85.54%, Test: 85.78%
Epoch: 475, Loss: 1.1080, Train: 85.69%, Valid: 85.54%, Test: 85.78%
Run 01:
Highest Train: 86.28
Highest Valid: 86.22
  Final Train: 86.28
   Final Test: 86.43
All runs:
Highest Train: 86.28, nan
Highest Valid: 86.22, nan
  Final Train: 86.28, nan
   Final Test: 86.43, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.7410, Train: 85.28%, Valid: 85.24%, Test: 85.39%
Epoch: 25, Loss: 26071788.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 3121365.5000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 558872832.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 4485499392.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 220258385920.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 944.9769, Train: 16.42%, Valid: 16.43%, Test: 16.30%
Epoch: 175, Loss: 31.2695, Train: 16.91%, Valid: 16.96%, Test: 16.81%
Epoch: 200, Loss: 54.6181, Train: 16.29%, Valid: 16.16%, Test: 16.06%
Epoch: 225, Loss: 19.7065, Train: 84.99%, Valid: 84.83%, Test: 85.05%
Epoch: 250, Loss: 5.5910, Train: 27.96%, Valid: 27.75%, Test: 27.64%
Epoch: 275, Loss: 2.4981, Train: 14.89%, Valid: 15.05%, Test: 14.84%
Epoch: 300, Loss: 6.1911, Train: 85.51%, Valid: 85.32%, Test: 85.56%
Epoch: 325, Loss: 1.2217, Train: 85.25%, Valid: 85.09%, Test: 85.32%
Epoch: 350, Loss: 40.9825, Train: 16.78%, Valid: 16.83%, Test: 16.68%
Epoch: 375, Loss: 23.7774, Train: 16.72%, Valid: 16.77%, Test: 16.61%
Epoch: 400, Loss: 40.8950, Train: 14.38%, Valid: 14.39%, Test: 14.30%
Epoch: 425, Loss: 83.7376, Train: 15.09%, Valid: 15.06%, Test: 14.98%
Epoch: 450, Loss: 0.9117, Train: 14.40%, Valid: 14.41%, Test: 14.33%
Epoch: 475, Loss: 11.2126, Train: 85.00%, Valid: 84.84%, Test: 85.06%
Run 01:
Highest Train: 86.19
Highest Valid: 86.20
  Final Train: 86.19
   Final Test: 86.26
All runs:
Highest Train: 86.19, nan
Highest Valid: 86.20, nan
  Final Train: 86.19, nan
   Final Test: 86.26, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.2151, Train: 85.64%, Valid: 85.64%, Test: 85.65%
Epoch: 25, Loss: 11898941603840.0000, Train: 86.58%, Valid: 86.62%, Test: 86.57%
Epoch: 50, Loss: 40347.2852, Train: 87.70%, Valid: 87.73%, Test: 87.83%
Epoch: 75, Loss: 167809724593761129857024.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 68462311374848.0000, Train: 14.40%, Valid: 14.39%, Test: 14.28%
Epoch: 125, Loss: 53957308416.0000, Train: 84.96%, Valid: 84.86%, Test: 84.94%
Epoch: 150, Loss: 3024058.7500, Train: 49.97%, Valid: 49.96%, Test: 49.97%
Epoch: 175, Loss: 18100.2168, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 1522560133995954176.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 6582026240.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 58184980.0000, Train: 85.54%, Valid: 85.41%, Test: 85.51%
Epoch: 275, Loss: 58695364403923773292544.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 765787072.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 1205249.5000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 6062041202688.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 62.2040, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 8.5110, Train: 85.54%, Valid: 85.42%, Test: 85.52%
Epoch: 425, Loss: 18852258582102016.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 2925973.2500, Train: 14.72%, Valid: 14.70%, Test: 14.58%
Epoch: 475, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 87.70
Highest Valid: 87.73
  Final Train: 87.70
   Final Test: 87.83
All runs:
Highest Train: 87.70, nan
Highest Valid: 87.73, nan
  Final Train: 87.70, nan
   Final Test: 87.83, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.5879, Train: 86.74%, Valid: 86.68%, Test: 86.81%
Epoch: 25, Loss: 3291123548160.0000, Train: 86.00%, Valid: 85.98%, Test: 86.04%
Epoch: 50, Loss: 14.0528, Train: 14.87%, Valid: 14.82%, Test: 14.89%
Epoch: 75, Loss: 14.5737, Train: 14.87%, Valid: 14.82%, Test: 14.86%
Epoch: 100, Loss: 14.7611, Train: 14.96%, Valid: 14.91%, Test: 14.93%
Epoch: 125, Loss: 14.7286, Train: 14.94%, Valid: 14.89%, Test: 14.92%
Epoch: 150, Loss: 14.7938, Train: 14.94%, Valid: 14.89%, Test: 14.92%
Epoch: 175, Loss: 14.7191, Train: 14.94%, Valid: 14.89%, Test: 14.92%
Epoch: 200, Loss: 14.7880, Train: 14.94%, Valid: 14.89%, Test: 14.92%
Epoch: 225, Loss: 14.7551, Train: 14.94%, Valid: 14.89%, Test: 14.92%
Epoch: 250, Loss: 14.7403, Train: 14.94%, Valid: 14.89%, Test: 14.92%
Epoch: 275, Loss: 14.7133, Train: 14.94%, Valid: 14.89%, Test: 14.92%
Epoch: 300, Loss: 14.6689, Train: 14.94%, Valid: 14.89%, Test: 14.92%
Epoch: 325, Loss: 14.7596, Train: 14.94%, Valid: 14.89%, Test: 14.92%
Epoch: 350, Loss: 14.7256, Train: 14.94%, Valid: 14.89%, Test: 14.92%
Epoch: 375, Loss: 14.7123, Train: 14.95%, Valid: 14.89%, Test: 14.92%
Epoch: 400, Loss: 14.7537, Train: 14.95%, Valid: 14.90%, Test: 14.92%
Epoch: 425, Loss: 14.7609, Train: 14.95%, Valid: 14.90%, Test: 14.93%
Epoch: 450, Loss: 14.7456, Train: 14.95%, Valid: 14.90%, Test: 14.92%
Epoch: 475, Loss: 14.7537, Train: 14.95%, Valid: 14.90%, Test: 14.93%
Run 01:
Highest Train: 86.74
Highest Valid: 86.68
  Final Train: 86.74
   Final Test: 86.81
All runs:
Highest Train: 86.74, nan
Highest Valid: 86.68, nan
  Final Train: 86.74, nan
   Final Test: 86.81, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.3820, Train: 85.68%, Valid: 85.54%, Test: 85.75%
Epoch: 25, Loss: 0.8943, Train: 85.31%, Valid: 85.14%, Test: 85.37%
Epoch: 50, Loss: 2.4157, Train: 85.36%, Valid: 85.19%, Test: 85.42%
Epoch: 75, Loss: 3.7970, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 3.7919, Train: 83.72%, Valid: 83.68%, Test: 83.85%
Epoch: 125, Loss: 2466.0925, Train: 85.61%, Valid: 85.43%, Test: 85.65%
Epoch: 150, Loss: 25274752.0000, Train: 83.80%, Valid: 83.78%, Test: 83.94%
Epoch: 175, Loss: 399.9052, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 4.3343, Train: 85.27%, Valid: 85.11%, Test: 85.33%
Epoch: 225, Loss: 4738525.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 1704311296.0000, Train: 85.40%, Valid: 85.24%, Test: 85.46%
Epoch: 275, Loss: 117680594944.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 5.0579, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 734339776.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 1.5947, Train: 85.23%, Valid: 85.01%, Test: 85.29%
Epoch: 375, Loss: 2168730.2500, Train: 85.21%, Valid: 85.06%, Test: 85.30%
Epoch: 400, Loss: 7.4719, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 8563.9023, Train: 84.76%, Valid: 84.55%, Test: 84.85%
Epoch: 450, Loss: 28.2847, Train: 13.79%, Valid: 13.77%, Test: 13.78%
Epoch: 475, Loss: 5.9195, Train: 13.57%, Valid: 13.61%, Test: 13.54%
Run 01:
Highest Train: 87.75
Highest Valid: 87.69
  Final Train: 87.75
   Final Test: 87.79
All runs:
Highest Train: 87.75, nan
Highest Valid: 87.69, nan
  Final Train: 87.75, nan
   Final Test: 87.79, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.9104, Train: 85.62%, Valid: 85.49%, Test: 85.68%
Epoch: 25, Loss: 0.5744, Train: 84.05%, Valid: 83.84%, Test: 84.15%
Epoch: 50, Loss: 48.1130, Train: 86.28%, Valid: 86.31%, Test: 86.32%
Epoch: 75, Loss: 1.1242, Train: 13.78%, Valid: 13.96%, Test: 13.68%
Epoch: 100, Loss: 14.3059, Train: 86.06%, Valid: 86.10%, Test: 86.14%
Epoch: 125, Loss: 29922.3926, Train: 83.86%, Valid: 83.66%, Test: 83.97%
Epoch: 150, Loss: 1854.7872, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 3104.7637, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: 5.1951, Train: 88.13%, Valid: 88.21%, Test: 88.19%
Epoch: 225, Loss: 964.8272, Train: 84.83%, Valid: 84.85%, Test: 84.97%
Epoch: 250, Loss: 756.6205, Train: 84.82%, Valid: 84.85%, Test: 84.96%
Epoch: 275, Loss: 446.8170, Train: 16.26%, Valid: 16.45%, Test: 16.15%
Epoch: 300, Loss: 805.5552, Train: 16.07%, Valid: 16.26%, Test: 15.94%
Epoch: 325, Loss: 3798.3135, Train: 84.85%, Valid: 84.87%, Test: 84.98%
Epoch: 350, Loss: 292.4758, Train: 16.18%, Valid: 16.39%, Test: 16.07%
Epoch: 375, Loss: 448.2328, Train: 15.56%, Valid: 15.66%, Test: 15.43%
Epoch: 400, Loss: 23.8287, Train: 84.84%, Valid: 84.86%, Test: 84.97%
Epoch: 425, Loss: 2179.1201, Train: 88.31%, Valid: 88.39%, Test: 88.36%
Epoch: 450, Loss: 1148.3298, Train: 16.14%, Valid: 16.35%, Test: 16.03%
Epoch: 475, Loss: 7189.7754, Train: 16.20%, Valid: 16.41%, Test: 16.09%
Run 01:
Highest Train: 88.33
Highest Valid: 88.40
  Final Train: 88.33
   Final Test: 88.37
All runs:
Highest Train: 88.33, nan
Highest Valid: 88.40, nan
  Final Train: 88.33, nan
   Final Test: 88.37, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 7.2263, Train: 84.98%, Valid: 84.83%, Test: 85.10%
Epoch: 25, Loss: 0.7736, Train: 86.06%, Valid: 85.94%, Test: 86.08%
Epoch: 50, Loss: 0.8223, Train: 86.13%, Valid: 86.01%, Test: 86.13%
Epoch: 75, Loss: 0.7407, Train: 86.43%, Valid: 86.29%, Test: 86.43%
Epoch: 100, Loss: 238.7916, Train: 16.61%, Valid: 16.70%, Test: 16.42%
Epoch: 125, Loss: 0.8973, Train: 16.20%, Valid: 16.35%, Test: 16.02%
Epoch: 150, Loss: 2.8691, Train: 86.45%, Valid: 86.40%, Test: 86.51%
Epoch: 175, Loss: 0.7356, Train: 86.12%, Valid: 86.06%, Test: 86.15%
Epoch: 200, Loss: 1.4851, Train: 86.33%, Valid: 86.28%, Test: 86.39%
Epoch: 225, Loss: 1.7545, Train: 86.47%, Valid: 86.42%, Test: 86.54%
Epoch: 250, Loss: 1.4063, Train: 86.39%, Valid: 86.34%, Test: 86.46%
Epoch: 275, Loss: 1.3107, Train: 86.52%, Valid: 86.47%, Test: 86.59%
Epoch: 300, Loss: 5.8635, Train: 86.59%, Valid: 86.55%, Test: 86.65%
Epoch: 325, Loss: 0.5480, Train: 86.11%, Valid: 86.12%, Test: 86.16%
Epoch: 350, Loss: 1.7342, Train: 86.57%, Valid: 86.53%, Test: 86.63%
Epoch: 375, Loss: 1.6156, Train: 86.30%, Valid: 86.23%, Test: 86.33%
Epoch: 400, Loss: 1.6241, Train: 86.43%, Valid: 86.39%, Test: 86.50%
Epoch: 425, Loss: 2.0735, Train: 86.49%, Valid: 86.44%, Test: 86.56%
Epoch: 450, Loss: 1.1271, Train: 86.53%, Valid: 86.48%, Test: 86.60%
Epoch: 475, Loss: 1.6972, Train: 86.33%, Valid: 86.28%, Test: 86.39%
Run 01:
Highest Train: 87.07
Highest Valid: 87.06
  Final Train: 87.07
   Final Test: 87.26
All runs:
Highest Train: 87.07, nan
Highest Valid: 87.06, nan
  Final Train: 87.07, nan
   Final Test: 87.26, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 7.2896, Train: 86.00%, Valid: 85.91%, Test: 86.11%
Epoch: 25, Loss: 0.9960, Train: 86.13%, Valid: 86.05%, Test: 86.24%
Epoch: 50, Loss: 2.8312, Train: 86.35%, Valid: 86.29%, Test: 86.41%
Epoch: 75, Loss: 4.9170, Train: 86.23%, Valid: 86.14%, Test: 86.27%
Epoch: 100, Loss: 4.3715, Train: 86.07%, Valid: 85.97%, Test: 86.16%
Epoch: 125, Loss: 3.3660, Train: 86.01%, Valid: 85.90%, Test: 86.10%
Epoch: 150, Loss: 3.0198, Train: 86.07%, Valid: 85.97%, Test: 86.15%
Epoch: 175, Loss: 2.8145, Train: 86.02%, Valid: 85.93%, Test: 86.12%
Epoch: 200, Loss: 2.8794, Train: 85.99%, Valid: 85.89%, Test: 86.08%
Epoch: 225, Loss: 2.2487, Train: 86.00%, Valid: 85.89%, Test: 86.09%
Epoch: 250, Loss: 2.0347, Train: 86.03%, Valid: 85.93%, Test: 86.10%
Epoch: 275, Loss: 2.0203, Train: 86.02%, Valid: 85.92%, Test: 86.09%
Epoch: 300, Loss: 1.9993, Train: 86.00%, Valid: 85.90%, Test: 86.07%
Epoch: 325, Loss: 1.9312, Train: 86.02%, Valid: 85.93%, Test: 86.08%
Epoch: 350, Loss: 0.9954, Train: 85.99%, Valid: 85.88%, Test: 86.08%
Epoch: 375, Loss: 1.4626, Train: 86.27%, Valid: 86.22%, Test: 86.32%
Epoch: 400, Loss: 1.0126, Train: 86.02%, Valid: 85.92%, Test: 86.09%
Epoch: 425, Loss: 0.9657, Train: 86.30%, Valid: 86.26%, Test: 86.35%
Epoch: 450, Loss: 0.9480, Train: 86.31%, Valid: 86.23%, Test: 86.32%
Epoch: 475, Loss: 0.9664, Train: 86.41%, Valid: 86.36%, Test: 86.48%
Run 01:
Highest Train: 86.46
Highest Valid: 86.41
  Final Train: 86.46
   Final Test: 86.52
All runs:
Highest Train: 86.46, nan
Highest Valid: 86.41, nan
  Final Train: 86.46, nan
   Final Test: 86.52, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 4.8255, Train: 86.44%, Valid: 86.48%, Test: 86.53%
Epoch: 25, Loss: 0.7380, Train: 85.71%, Valid: 85.78%, Test: 85.77%
Epoch: 50, Loss: 0.9259, Train: 86.15%, Valid: 86.16%, Test: 86.18%
Epoch: 75, Loss: 1.3499, Train: 86.21%, Valid: 86.23%, Test: 86.26%
Epoch: 100, Loss: 1.5009, Train: 86.56%, Valid: 86.60%, Test: 86.62%
Epoch: 125, Loss: 1.5003, Train: 86.54%, Valid: 86.59%, Test: 86.57%
Epoch: 150, Loss: 1.5252, Train: 86.55%, Valid: 86.59%, Test: 86.58%
Epoch: 175, Loss: 1.5850, Train: 86.49%, Valid: 86.52%, Test: 86.53%
Epoch: 200, Loss: 1.3777, Train: 86.54%, Valid: 86.57%, Test: 86.58%
Epoch: 225, Loss: 1.5280, Train: 86.54%, Valid: 86.57%, Test: 86.58%
Epoch: 250, Loss: 1.4415, Train: 86.55%, Valid: 86.57%, Test: 86.59%
Epoch: 275, Loss: 0.7745, Train: 86.53%, Valid: 86.57%, Test: 86.58%
Epoch: 300, Loss: 1.7665, Train: 86.53%, Valid: 86.57%, Test: 86.58%
Epoch: 325, Loss: 1.6972, Train: 86.48%, Valid: 86.51%, Test: 86.52%
Epoch: 350, Loss: 1.5087, Train: 86.48%, Valid: 86.51%, Test: 86.52%
Epoch: 375, Loss: 1.9937, Train: 86.53%, Valid: 86.57%, Test: 86.58%
Epoch: 400, Loss: 1.3903, Train: 86.48%, Valid: 86.51%, Test: 86.52%
Epoch: 425, Loss: 1.2529, Train: 86.48%, Valid: 86.51%, Test: 86.52%
Epoch: 450, Loss: 1.4399, Train: 86.48%, Valid: 86.51%, Test: 86.52%
Epoch: 475, Loss: 1.4195, Train: 86.47%, Valid: 86.50%, Test: 86.51%
Run 01:
Highest Train: 86.92
Highest Valid: 86.97
  Final Train: 86.92
   Final Test: 87.01
All runs:
Highest Train: 86.92, nan
Highest Valid: 86.97, nan
  Final Train: 86.92, nan
   Final Test: 87.01, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 5.9580, Train: 85.58%, Valid: 85.62%, Test: 85.70%
Epoch: 25, Loss: 17.1214, Train: 86.62%, Valid: 86.50%, Test: 86.67%
Epoch: 50, Loss: 2094.3723, Train: 86.38%, Valid: 86.33%, Test: 86.42%
Epoch: 75, Loss: 2.1739, Train: 85.57%, Valid: 85.47%, Test: 85.59%
Epoch: 100, Loss: 6.6234, Train: 85.57%, Valid: 85.47%, Test: 85.59%
Epoch: 125, Loss: 6.9878, Train: 85.57%, Valid: 85.47%, Test: 85.59%
Epoch: 150, Loss: 7.0164, Train: 85.57%, Valid: 85.47%, Test: 85.59%
Epoch: 175, Loss: 7.0731, Train: 85.57%, Valid: 85.48%, Test: 85.59%
Epoch: 200, Loss: 6.9641, Train: 85.57%, Valid: 85.48%, Test: 85.59%
Epoch: 225, Loss: 7.0723, Train: 85.57%, Valid: 85.47%, Test: 85.59%
Epoch: 250, Loss: 6.9572, Train: 85.57%, Valid: 85.47%, Test: 85.59%
Epoch: 275, Loss: 7.0063, Train: 85.57%, Valid: 85.47%, Test: 85.59%
Epoch: 300, Loss: 6.9988, Train: 85.57%, Valid: 85.47%, Test: 85.59%
Epoch: 325, Loss: 6.9841, Train: 85.57%, Valid: 85.47%, Test: 85.59%
Epoch: 350, Loss: 6.9806, Train: 85.57%, Valid: 85.48%, Test: 85.59%
Epoch: 375, Loss: 7.0368, Train: 85.57%, Valid: 85.47%, Test: 85.59%
Epoch: 400, Loss: 7.0451, Train: 85.57%, Valid: 85.47%, Test: 85.59%
Epoch: 425, Loss: 6.9528, Train: 85.56%, Valid: 85.47%, Test: 85.59%
Epoch: 450, Loss: 6.9533, Train: 85.57%, Valid: 85.48%, Test: 85.60%
Epoch: 475, Loss: 6.9049, Train: 85.57%, Valid: 85.47%, Test: 85.59%
Run 01:
Highest Train: 87.57
Highest Valid: 87.54
  Final Train: 87.57
   Final Test: 87.55
All runs:
Highest Train: 87.57, nan
Highest Valid: 87.54, nan
  Final Train: 87.57, nan
   Final Test: 87.55, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.5298, Train: 82.02%, Valid: 82.12%, Test: 82.32%
Epoch: 25, Loss: 0.4079, Train: 85.43%, Valid: 85.43%, Test: 85.58%
Epoch: 50, Loss: 0.3891, Train: 85.53%, Valid: 85.39%, Test: 85.62%
Epoch: 75, Loss: 0.3762, Train: 85.44%, Valid: 85.29%, Test: 85.52%
Epoch: 100, Loss: 0.3663, Train: 85.37%, Valid: 85.22%, Test: 85.44%
Epoch: 125, Loss: 0.3612, Train: 85.46%, Valid: 85.31%, Test: 85.53%
Epoch: 150, Loss: 0.3561, Train: 85.39%, Valid: 85.22%, Test: 85.47%
Epoch: 175, Loss: 0.3492, Train: 85.39%, Valid: 85.21%, Test: 85.47%
Epoch: 200, Loss: 0.3946, Train: 85.92%, Valid: 85.89%, Test: 86.04%
Epoch: 225, Loss: 5.7826, Train: 85.40%, Valid: 85.27%, Test: 85.44%
Epoch: 250, Loss: 15.5557, Train: 85.57%, Valid: 85.43%, Test: 85.68%
Epoch: 275, Loss: 2.3877, Train: 85.86%, Valid: 85.75%, Test: 85.91%
Epoch: 300, Loss: 1.2644, Train: 85.77%, Valid: 85.68%, Test: 85.82%
Epoch: 325, Loss: 1.2640, Train: 85.77%, Valid: 85.68%, Test: 85.82%
Epoch: 350, Loss: 1.2596, Train: 85.77%, Valid: 85.68%, Test: 85.81%
Epoch: 375, Loss: 1.2571, Train: 85.77%, Valid: 85.68%, Test: 85.82%
Epoch: 400, Loss: 1.2524, Train: 85.77%, Valid: 85.68%, Test: 85.82%
Epoch: 425, Loss: 1.2527, Train: 85.77%, Valid: 85.68%, Test: 85.82%
Epoch: 450, Loss: 1.2431, Train: 85.78%, Valid: 85.68%, Test: 85.82%
Epoch: 475, Loss: 1.2330, Train: 85.78%, Valid: 85.68%, Test: 85.82%
Run 01:
Highest Train: 87.16
Highest Valid: 87.23
  Final Train: 87.16
   Final Test: 87.23
All runs:
Highest Train: 87.16, nan
Highest Valid: 87.23, nan
  Final Train: 87.16, nan
   Final Test: 87.23, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 10.0822, Train: 87.51%, Valid: 87.55%, Test: 87.56%
Epoch: 25, Loss: 3.0071, Train: 87.67%, Valid: 87.68%, Test: 87.70%
Epoch: 50, Loss: 0.7813, Train: 87.84%, Valid: 87.85%, Test: 87.84%
Epoch: 75, Loss: 0.7794, Train: 87.86%, Valid: 87.87%, Test: 87.86%
Epoch: 100, Loss: 0.8063, Train: 87.82%, Valid: 87.82%, Test: 87.81%
Epoch: 125, Loss: 0.8078, Train: 87.82%, Valid: 87.82%, Test: 87.81%
Epoch: 150, Loss: 0.8051, Train: 87.83%, Valid: 87.83%, Test: 87.82%
Epoch: 175, Loss: 0.8076, Train: 87.83%, Valid: 87.83%, Test: 87.82%
Epoch: 200, Loss: 0.8058, Train: 87.83%, Valid: 87.84%, Test: 87.83%
Epoch: 225, Loss: 0.8073, Train: 87.86%, Valid: 87.87%, Test: 87.85%
Epoch: 250, Loss: 0.8045, Train: 87.86%, Valid: 87.87%, Test: 87.85%
Epoch: 275, Loss: 0.8045, Train: 87.86%, Valid: 87.87%, Test: 87.86%
Epoch: 300, Loss: 0.8037, Train: 87.86%, Valid: 87.88%, Test: 87.86%
Epoch: 325, Loss: 0.8049, Train: 87.86%, Valid: 87.88%, Test: 87.86%
Epoch: 350, Loss: 0.8037, Train: 87.87%, Valid: 87.88%, Test: 87.86%
Epoch: 375, Loss: 0.8030, Train: 87.86%, Valid: 87.88%, Test: 87.86%
Epoch: 400, Loss: 0.8041, Train: 87.86%, Valid: 87.87%, Test: 87.86%
Epoch: 425, Loss: 0.8019, Train: 87.86%, Valid: 87.87%, Test: 87.86%
Epoch: 450, Loss: 0.8005, Train: 87.86%, Valid: 87.87%, Test: 87.85%
Epoch: 475, Loss: 0.8023, Train: 87.85%, Valid: 87.87%, Test: 87.85%
Run 01:
Highest Train: 87.87
Highest Valid: 87.88
  Final Train: 87.86
   Final Test: 87.86
All runs:
Highest Train: 87.87, nan
Highest Valid: 87.88, nan
  Final Train: 87.86, nan
   Final Test: 87.86, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 3.7881, Train: 85.26%, Valid: 85.26%, Test: 85.33%
Epoch: 25, Loss: 0.7060, Train: 85.98%, Valid: 85.95%, Test: 86.06%
Epoch: 50, Loss: 0.6467, Train: 86.10%, Valid: 86.02%, Test: 86.15%
Epoch: 75, Loss: 0.5451, Train: 86.00%, Valid: 85.90%, Test: 86.09%
Epoch: 100, Loss: 0.4160, Train: 85.97%, Valid: 85.84%, Test: 86.04%
Epoch: 125, Loss: 0.3504, Train: 85.99%, Valid: 85.78%, Test: 86.05%
Epoch: 150, Loss: 0.3374, Train: 85.62%, Valid: 85.53%, Test: 85.77%
Epoch: 175, Loss: 0.3301, Train: 85.92%, Valid: 85.68%, Test: 85.91%
Epoch: 200, Loss: 0.3343, Train: 86.11%, Valid: 85.97%, Test: 86.25%
Epoch: 225, Loss: 0.3266, Train: 86.13%, Valid: 85.93%, Test: 86.24%
Epoch: 250, Loss: 0.6703, Train: 85.76%, Valid: 85.62%, Test: 85.90%
Epoch: 275, Loss: 0.3500, Train: 86.02%, Valid: 85.86%, Test: 86.17%
Epoch: 300, Loss: 0.3379, Train: 86.13%, Valid: 85.95%, Test: 86.26%
Epoch: 325, Loss: 0.3337, Train: 86.14%, Valid: 85.99%, Test: 86.28%
Epoch: 350, Loss: 0.3290, Train: 86.07%, Valid: 85.91%, Test: 86.20%
Epoch: 375, Loss: 0.3265, Train: 86.09%, Valid: 85.92%, Test: 86.22%
Epoch: 400, Loss: 0.3252, Train: 85.93%, Valid: 85.78%, Test: 86.08%
Epoch: 425, Loss: 0.3279, Train: 86.26%, Valid: 86.08%, Test: 86.39%
Epoch: 450, Loss: 0.3254, Train: 86.22%, Valid: 86.05%, Test: 86.36%
Epoch: 475, Loss: 0.3244, Train: 86.23%, Valid: 86.05%, Test: 86.36%
Run 01:
Highest Train: 87.36
Highest Valid: 87.42
  Final Train: 87.36
   Final Test: 87.49
All runs:
Highest Train: 87.36, nan
Highest Valid: 87.42, nan
  Final Train: 87.36, nan
   Final Test: 87.49, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6948, Train: 84.62%, Valid: 84.59%, Test: 84.73%
Epoch: 25, Loss: 0.9262, Train: 85.60%, Valid: 85.58%, Test: 85.68%
Epoch: 50, Loss: 24434120.0000, Train: 85.35%, Valid: 85.32%, Test: 85.44%
Epoch: 75, Loss: 3.6053, Train: 85.61%, Valid: 85.61%, Test: 85.72%
Epoch: 100, Loss: 2.3665, Train: 85.63%, Valid: 85.62%, Test: 85.74%
Epoch: 125, Loss: 2.2662, Train: 85.63%, Valid: 85.62%, Test: 85.74%
Epoch: 150, Loss: 2.2575, Train: 85.66%, Valid: 85.65%, Test: 85.77%
Epoch: 175, Loss: 1.7507, Train: 85.72%, Valid: 85.70%, Test: 85.82%
Epoch: 200, Loss: 1.6143, Train: 85.74%, Valid: 85.72%, Test: 85.84%
Epoch: 225, Loss: 1.5662, Train: 85.76%, Valid: 85.74%, Test: 85.86%
Epoch: 250, Loss: 1.5393, Train: 85.76%, Valid: 85.73%, Test: 85.85%
Epoch: 275, Loss: 1.5116, Train: 85.76%, Valid: 85.74%, Test: 85.86%
Epoch: 300, Loss: 1.4962, Train: 85.76%, Valid: 85.74%, Test: 85.86%
Epoch: 325, Loss: 1.4897, Train: 85.77%, Valid: 85.74%, Test: 85.86%
Epoch: 350, Loss: 1.4741, Train: 85.77%, Valid: 85.74%, Test: 85.87%
Epoch: 375, Loss: 1.4579, Train: 85.77%, Valid: 85.74%, Test: 85.87%
Epoch: 400, Loss: 1.4556, Train: 85.77%, Valid: 85.74%, Test: 85.87%
Epoch: 425, Loss: 1.4529, Train: 85.77%, Valid: 85.74%, Test: 85.87%
Epoch: 450, Loss: 1.4598, Train: 85.77%, Valid: 85.73%, Test: 85.86%
Epoch: 475, Loss: 1.4605, Train: 85.76%, Valid: 85.73%, Test: 85.86%
Run 01:
Highest Train: 85.99
Highest Valid: 86.00
  Final Train: 85.99
   Final Test: 86.07
All runs:
Highest Train: 85.99, nan
Highest Valid: 86.00, nan
  Final Train: 85.99, nan
   Final Test: 86.07, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.3742, Train: 16.01%, Valid: 16.16%, Test: 15.98%
Epoch: 25, Loss: 0.8897, Train: 86.19%, Valid: 86.25%, Test: 86.26%
Epoch: 50, Loss: 0.8891, Train: 85.91%, Valid: 85.95%, Test: 86.02%
Epoch: 75, Loss: 0.6373, Train: 85.77%, Valid: 85.77%, Test: 85.89%
Epoch: 100, Loss: 0.3533, Train: 85.86%, Valid: 85.81%, Test: 85.98%
Epoch: 125, Loss: 0.3409, Train: 85.54%, Valid: 85.54%, Test: 85.66%
Epoch: 150, Loss: 1.0290, Train: 85.58%, Valid: 85.55%, Test: 85.69%
Epoch: 175, Loss: 0.9025, Train: 85.45%, Valid: 85.44%, Test: 85.54%
Epoch: 200, Loss: 0.5289, Train: 85.29%, Valid: 85.29%, Test: 85.39%
Epoch: 225, Loss: 0.3333, Train: 85.82%, Valid: 85.81%, Test: 85.93%
Epoch: 250, Loss: 0.3280, Train: 85.73%, Valid: 85.69%, Test: 85.82%
Epoch: 275, Loss: 0.8088, Train: 85.31%, Valid: 85.33%, Test: 85.42%
Epoch: 300, Loss: 3.5078, Train: 85.36%, Valid: 85.39%, Test: 85.44%
Epoch: 325, Loss: 4.9097, Train: 85.34%, Valid: 85.37%, Test: 85.43%
Epoch: 350, Loss: 5.4685, Train: 85.36%, Valid: 85.38%, Test: 85.44%
Epoch: 375, Loss: 5.6892, Train: 85.36%, Valid: 85.39%, Test: 85.44%
Epoch: 400, Loss: 5.3476, Train: 85.36%, Valid: 85.38%, Test: 85.44%
Epoch: 425, Loss: 5.2192, Train: 85.36%, Valid: 85.38%, Test: 85.44%
Epoch: 450, Loss: 5.0030, Train: 85.36%, Valid: 85.38%, Test: 85.43%
Epoch: 475, Loss: 4.1320, Train: 85.26%, Valid: 85.28%, Test: 85.34%
Run 01:
Highest Train: 88.08
Highest Valid: 88.13
  Final Train: 88.08
   Final Test: 88.13
All runs:
Highest Train: 88.08, nan
Highest Valid: 88.13, nan
  Final Train: 88.08, nan
   Final Test: 88.13, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.4431, Train: 85.25%, Valid: 85.16%, Test: 85.42%
Epoch: 25, Loss: 1.0686, Train: 86.07%, Valid: 85.93%, Test: 86.14%
Epoch: 50, Loss: 1.1469, Train: 85.98%, Valid: 85.83%, Test: 86.06%
Epoch: 75, Loss: 0.9794, Train: 85.94%, Valid: 85.80%, Test: 86.02%
Epoch: 100, Loss: 0.4378, Train: 87.40%, Valid: 87.36%, Test: 87.48%
Epoch: 125, Loss: 0.3522, Train: 85.51%, Valid: 85.35%, Test: 85.63%
Epoch: 150, Loss: 0.3456, Train: 85.77%, Valid: 85.56%, Test: 85.81%
Epoch: 175, Loss: 0.3379, Train: 85.50%, Valid: 85.25%, Test: 85.57%
Epoch: 200, Loss: 0.3485, Train: 85.84%, Valid: 85.63%, Test: 85.93%
Epoch: 225, Loss: 0.3415, Train: 85.94%, Valid: 85.71%, Test: 86.05%
Epoch: 250, Loss: 0.3310, Train: 86.04%, Valid: 85.65%, Test: 86.08%
Epoch: 275, Loss: 0.7687, Train: 87.08%, Valid: 87.01%, Test: 87.22%
Epoch: 300, Loss: 1.2345, Train: 86.11%, Valid: 86.01%, Test: 86.21%
Epoch: 325, Loss: 1.2550, Train: 86.12%, Valid: 86.03%, Test: 86.25%
Epoch: 350, Loss: 1.2541, Train: 86.21%, Valid: 86.13%, Test: 86.31%
Epoch: 375, Loss: 0.9992, Train: 85.97%, Valid: 85.87%, Test: 86.09%
Epoch: 400, Loss: 0.9067, Train: 85.85%, Valid: 85.72%, Test: 85.97%
Epoch: 425, Loss: 0.8085, Train: 85.81%, Valid: 85.68%, Test: 85.94%
Epoch: 450, Loss: 0.6511, Train: 85.77%, Valid: 85.63%, Test: 85.88%
Epoch: 475, Loss: 0.3560, Train: 86.09%, Valid: 85.96%, Test: 86.23%
Run 01:
Highest Train: 87.93
Highest Valid: 87.94
  Final Train: 87.89
   Final Test: 88.00
All runs:
Highest Train: 87.93, nan
Highest Valid: 87.94, nan
  Final Train: 87.89, nan
   Final Test: 88.00, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 13.7803, Train: 83.08%, Valid: 83.21%, Test: 83.20%
Epoch: 25, Loss: 0.5788, Train: 85.34%, Valid: 85.19%, Test: 85.39%
Epoch: 50, Loss: 0.3776, Train: 85.29%, Valid: 85.14%, Test: 85.36%
Epoch: 75, Loss: 0.3627, Train: 85.34%, Valid: 85.19%, Test: 85.42%
Epoch: 100, Loss: 0.3564, Train: 85.47%, Valid: 85.32%, Test: 85.54%
Epoch: 125, Loss: 0.3494, Train: 85.38%, Valid: 85.22%, Test: 85.47%
Epoch: 150, Loss: 0.3423, Train: 85.26%, Valid: 85.09%, Test: 85.34%
Epoch: 175, Loss: 0.3508, Train: 85.42%, Valid: 85.24%, Test: 85.50%
Epoch: 200, Loss: 0.3365, Train: 85.36%, Valid: 85.18%, Test: 85.44%
Epoch: 225, Loss: 0.3368, Train: 84.96%, Valid: 84.80%, Test: 85.05%
Epoch: 250, Loss: 0.4112, Train: 86.88%, Valid: 86.70%, Test: 86.87%
Epoch: 275, Loss: 0.7728, Train: 85.42%, Valid: 85.25%, Test: 85.50%
Epoch: 300, Loss: 17.7299, Train: 85.68%, Valid: 85.58%, Test: 85.78%
Epoch: 325, Loss: 5.5746, Train: 85.33%, Valid: 85.09%, Test: 85.38%
Epoch: 350, Loss: 6.7037, Train: 86.80%, Valid: 86.75%, Test: 86.85%
Epoch: 375, Loss: 5.7957, Train: 86.79%, Valid: 86.75%, Test: 86.85%
Epoch: 400, Loss: 5.4785, Train: 86.77%, Valid: 86.73%, Test: 86.83%
Epoch: 425, Loss: 4.9976, Train: 86.75%, Valid: 86.70%, Test: 86.82%
Epoch: 450, Loss: 5.6567, Train: 86.73%, Valid: 86.67%, Test: 86.80%
Epoch: 475, Loss: 5.7159, Train: 86.69%, Valid: 86.62%, Test: 86.75%
Run 01:
Highest Train: 86.88
Highest Valid: 86.77
  Final Train: 86.83
   Final Test: 86.87
All runs:
Highest Train: 86.88, nan
Highest Valid: 86.77, nan
  Final Train: 86.83, nan
   Final Test: 86.87, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 12.3673, Train: 85.60%, Valid: 85.51%, Test: 85.62%
Epoch: 25, Loss: 1.0203, Train: 86.74%, Valid: 86.79%, Test: 86.76%
Epoch: 50, Loss: 1.0775, Train: 88.04%, Valid: 88.10%, Test: 88.08%
Epoch: 75, Loss: 1.0935, Train: 87.62%, Valid: 87.66%, Test: 87.69%
Epoch: 100, Loss: 0.8044, Train: 87.32%, Valid: 87.40%, Test: 87.37%
Epoch: 125, Loss: 0.3756, Train: 85.95%, Valid: 85.96%, Test: 86.01%
Epoch: 150, Loss: 0.3515, Train: 86.44%, Valid: 86.44%, Test: 86.50%
Epoch: 175, Loss: 0.3417, Train: 86.21%, Valid: 86.19%, Test: 86.29%
Epoch: 200, Loss: 0.5096, Train: 85.75%, Valid: 85.76%, Test: 85.84%
Epoch: 225, Loss: 0.3479, Train: 86.43%, Valid: 86.21%, Test: 86.51%
Epoch: 250, Loss: 0.3372, Train: 85.87%, Valid: 85.89%, Test: 85.98%
Epoch: 275, Loss: 0.3311, Train: 85.80%, Valid: 85.76%, Test: 85.90%
Epoch: 300, Loss: 0.8321, Train: 85.71%, Valid: 85.72%, Test: 85.82%
Epoch: 325, Loss: 1.3399, Train: 85.70%, Valid: 85.74%, Test: 85.79%
Epoch: 350, Loss: 1.5083, Train: 85.69%, Valid: 85.73%, Test: 85.78%
Epoch: 375, Loss: 1.1966, Train: 85.68%, Valid: 85.72%, Test: 85.78%
Epoch: 400, Loss: 0.8127, Train: 85.68%, Valid: 85.72%, Test: 85.78%
Epoch: 425, Loss: 0.4639, Train: 85.60%, Valid: 85.65%, Test: 85.70%
Epoch: 450, Loss: 0.3445, Train: 85.84%, Valid: 85.83%, Test: 85.92%
Epoch: 475, Loss: 0.3357, Train: 85.93%, Valid: 85.90%, Test: 86.02%
Run 01:
Highest Train: 88.49
Highest Valid: 88.53
  Final Train: 88.49
   Final Test: 88.57
All runs:
Highest Train: 88.49, nan
Highest Valid: 88.53, nan
  Final Train: 88.49, nan
   Final Test: 88.57, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.9189, Train: 85.47%, Valid: 85.45%, Test: 85.62%
Epoch: 25, Loss: 0.7562, Train: 87.34%, Valid: 87.37%, Test: 87.31%
Epoch: 50, Loss: 0.5033, Train: 86.76%, Valid: 86.67%, Test: 86.79%
Epoch: 75, Loss: 0.3572, Train: 86.93%, Valid: 86.77%, Test: 86.98%
Epoch: 100, Loss: 0.3510, Train: 87.14%, Valid: 87.01%, Test: 87.23%
Epoch: 125, Loss: 0.3381, Train: 86.29%, Valid: 86.18%, Test: 86.38%
Epoch: 150, Loss: 0.5643, Train: 85.86%, Valid: 85.74%, Test: 85.98%
Epoch: 175, Loss: 0.3474, Train: 86.05%, Valid: 85.87%, Test: 86.17%
Epoch: 200, Loss: 0.3364, Train: 86.12%, Valid: 85.95%, Test: 86.24%
Epoch: 225, Loss: 0.3314, Train: 86.02%, Valid: 85.85%, Test: 86.13%
Epoch: 250, Loss: 0.3274, Train: 85.90%, Valid: 85.72%, Test: 86.02%
Epoch: 275, Loss: 0.9734, Train: 85.82%, Valid: 85.69%, Test: 85.94%
Epoch: 300, Loss: 0.8810, Train: 87.06%, Valid: 87.09%, Test: 87.12%
Epoch: 325, Loss: 2.2163, Train: 85.92%, Valid: 85.79%, Test: 86.02%
Epoch: 350, Loss: 2.2280, Train: 85.87%, Valid: 85.74%, Test: 85.97%
Epoch: 375, Loss: 1.8578, Train: 85.86%, Valid: 85.75%, Test: 85.98%
Epoch: 400, Loss: 1.0080, Train: 85.88%, Valid: 85.76%, Test: 85.99%
Epoch: 425, Loss: 0.4130, Train: 86.49%, Valid: 86.41%, Test: 86.61%
Epoch: 450, Loss: 0.6070, Train: 85.75%, Valid: 85.63%, Test: 85.86%
Epoch: 475, Loss: 0.3515, Train: 85.97%, Valid: 85.80%, Test: 86.07%
Run 01:
Highest Train: 87.34
Highest Valid: 87.37
  Final Train: 87.34
   Final Test: 87.31
All runs:
Highest Train: 87.34, nan
Highest Valid: 87.37, nan
  Final Train: 87.34, nan
   Final Test: 87.31, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 7.4283, Train: 83.76%, Valid: 83.67%, Test: 83.83%
Epoch: 25, Loss: 0.7519, Train: 86.36%, Valid: 86.36%, Test: 86.44%
Epoch: 50, Loss: 0.4081, Train: 85.67%, Valid: 85.53%, Test: 85.77%
Epoch: 75, Loss: 0.3769, Train: 85.58%, Valid: 85.41%, Test: 85.65%
Epoch: 100, Loss: 0.3633, Train: 85.65%, Valid: 85.48%, Test: 85.71%
Epoch: 125, Loss: 0.3554, Train: 85.61%, Valid: 85.44%, Test: 85.68%
Epoch: 150, Loss: 0.3485, Train: 85.56%, Valid: 85.38%, Test: 85.62%
Epoch: 175, Loss: 0.3434, Train: 85.47%, Valid: 85.28%, Test: 85.55%
Epoch: 200, Loss: 0.7037, Train: 85.65%, Valid: 85.47%, Test: 85.70%
Epoch: 225, Loss: 0.3913, Train: 85.73%, Valid: 85.55%, Test: 85.79%
Epoch: 250, Loss: 0.3515, Train: 85.69%, Valid: 85.50%, Test: 85.76%
Epoch: 275, Loss: 0.3437, Train: 85.75%, Valid: 85.55%, Test: 85.82%
Epoch: 300, Loss: 0.3378, Train: 85.56%, Valid: 85.36%, Test: 85.63%
Epoch: 325, Loss: 0.9615, Train: 86.34%, Valid: 86.33%, Test: 86.45%
Epoch: 350, Loss: 0.6046, Train: 86.25%, Valid: 86.24%, Test: 86.33%
Epoch: 375, Loss: 1.2124, Train: 85.64%, Valid: 85.58%, Test: 85.79%
Epoch: 400, Loss: 1.0480, Train: 85.39%, Valid: 85.34%, Test: 85.49%
Epoch: 425, Loss: 0.7445, Train: 85.24%, Valid: 85.19%, Test: 85.33%
Epoch: 450, Loss: 0.5090, Train: 86.33%, Valid: 86.20%, Test: 86.42%
Epoch: 475, Loss: 0.3565, Train: 86.23%, Valid: 86.15%, Test: 86.31%
Run 01:
Highest Train: 87.15
Highest Valid: 87.09
  Final Train: 87.15
   Final Test: 87.21
All runs:
Highest Train: 87.15, nan
Highest Valid: 87.09, nan
  Final Train: 87.15, nan
   Final Test: 87.21, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.4271, Train: 86.13%, Valid: 86.11%, Test: 86.26%
Epoch: 25, Loss: 0.3676, Train: 87.06%, Valid: 87.14%, Test: 87.19%
Epoch: 50, Loss: 0.3568, Train: 85.60%, Valid: 85.57%, Test: 85.64%
Epoch: 75, Loss: 0.3484, Train: 86.74%, Valid: 86.73%, Test: 86.87%
Epoch: 100, Loss: 0.3466, Train: 85.71%, Valid: 85.72%, Test: 85.79%
Epoch: 125, Loss: 1.0530, Train: 85.73%, Valid: 85.76%, Test: 85.82%
Epoch: 150, Loss: 0.9786, Train: 85.70%, Valid: 85.74%, Test: 85.79%
Epoch: 175, Loss: 0.6315, Train: 85.72%, Valid: 85.74%, Test: 85.80%
Epoch: 200, Loss: 0.3565, Train: 85.89%, Valid: 85.90%, Test: 85.98%
Epoch: 225, Loss: 0.3354, Train: 85.97%, Valid: 85.97%, Test: 86.06%
Epoch: 250, Loss: 0.3389, Train: 86.02%, Valid: 86.02%, Test: 86.11%
Epoch: 275, Loss: 0.7999, Train: 85.76%, Valid: 85.76%, Test: 85.83%
Epoch: 300, Loss: 0.6893, Train: 85.84%, Valid: 85.88%, Test: 85.92%
Epoch: 325, Loss: 0.9529, Train: 85.68%, Valid: 85.71%, Test: 85.75%
Epoch: 350, Loss: 0.3545, Train: 86.09%, Valid: 86.08%, Test: 86.17%
Epoch: 375, Loss: 1.0216, Train: 85.70%, Valid: 85.75%, Test: 85.78%
Epoch: 400, Loss: 0.6428, Train: 85.69%, Valid: 85.71%, Test: 85.75%
Epoch: 425, Loss: 0.3451, Train: 86.04%, Valid: 86.02%, Test: 86.06%
Epoch: 450, Loss: 0.3370, Train: 87.54%, Valid: 87.51%, Test: 87.60%
Epoch: 475, Loss: 0.3310, Train: 86.89%, Valid: 86.82%, Test: 86.93%
Run 01:
Highest Train: 88.09
Highest Valid: 88.07
  Final Train: 88.09
   Final Test: 88.05
All runs:
Highest Train: 88.09, nan
Highest Valid: 88.07, nan
  Final Train: 88.09, nan
   Final Test: 88.05, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.5195, Train: 86.14%, Valid: 86.10%, Test: 86.18%
Epoch: 25, Loss: 0.3685, Train: 86.93%, Valid: 86.89%, Test: 86.92%
Epoch: 50, Loss: 0.3586, Train: 85.79%, Valid: 85.65%, Test: 85.87%
Epoch: 75, Loss: 0.3523, Train: 85.77%, Valid: 85.62%, Test: 85.87%
Epoch: 100, Loss: 0.3579, Train: 86.23%, Valid: 86.17%, Test: 86.15%
Epoch: 125, Loss: 0.3548, Train: 85.86%, Valid: 85.73%, Test: 85.98%
Epoch: 150, Loss: 0.3365, Train: 85.82%, Valid: 85.68%, Test: 85.95%
Epoch: 175, Loss: 0.7130, Train: 85.82%, Valid: 85.69%, Test: 85.92%
Epoch: 200, Loss: 1.2062, Train: 85.82%, Valid: 85.69%, Test: 85.93%
Epoch: 225, Loss: 0.6137, Train: 85.84%, Valid: 85.70%, Test: 85.95%
Epoch: 250, Loss: 0.3418, Train: 86.16%, Valid: 86.02%, Test: 86.29%
Epoch: 275, Loss: 0.3368, Train: 86.14%, Valid: 85.98%, Test: 86.26%
Epoch: 300, Loss: 0.3301, Train: 86.03%, Valid: 85.86%, Test: 86.14%
Epoch: 325, Loss: 0.3275, Train: 85.98%, Valid: 85.81%, Test: 86.10%
Epoch: 350, Loss: 1.0931, Train: 85.78%, Valid: 85.65%, Test: 85.91%
Epoch: 375, Loss: 1.8283, Train: 85.90%, Valid: 85.76%, Test: 86.02%
Epoch: 400, Loss: 1.1738, Train: 85.89%, Valid: 85.75%, Test: 86.01%
Epoch: 425, Loss: 0.8712, Train: 85.89%, Valid: 85.76%, Test: 86.01%
Epoch: 450, Loss: 0.3529, Train: 85.76%, Valid: 85.59%, Test: 85.93%
Epoch: 475, Loss: 0.3432, Train: 86.10%, Valid: 85.84%, Test: 86.20%
Run 01:
Highest Train: 87.69
Highest Valid: 87.52
  Final Train: 87.69
   Final Test: 87.75
All runs:
Highest Train: 87.69, nan
Highest Valid: 87.52, nan
  Final Train: 87.69, nan
   Final Test: 87.75, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 9.4737, Train: 85.36%, Valid: 85.23%, Test: 85.42%
Epoch: 25, Loss: 0.7460, Train: 85.91%, Valid: 85.75%, Test: 85.98%
Epoch: 50, Loss: 0.5702, Train: 85.97%, Valid: 85.84%, Test: 86.06%
Epoch: 75, Loss: 0.4189, Train: 85.87%, Valid: 85.69%, Test: 85.94%
Epoch: 100, Loss: 0.3774, Train: 85.58%, Valid: 85.40%, Test: 85.65%
Epoch: 125, Loss: 0.3640, Train: 85.62%, Valid: 85.44%, Test: 85.68%
Epoch: 150, Loss: 0.3569, Train: 85.61%, Valid: 85.43%, Test: 85.69%
Epoch: 175, Loss: 0.3553, Train: 85.62%, Valid: 85.44%, Test: 85.69%
Epoch: 200, Loss: 0.3503, Train: 85.58%, Valid: 85.41%, Test: 85.65%
Epoch: 225, Loss: 0.3985, Train: 86.23%, Valid: 86.11%, Test: 86.23%
Epoch: 250, Loss: 0.3531, Train: 85.72%, Valid: 85.54%, Test: 85.78%
Epoch: 275, Loss: 0.3433, Train: 85.70%, Valid: 85.52%, Test: 85.76%
Epoch: 300, Loss: 0.3362, Train: 85.65%, Valid: 85.46%, Test: 85.71%
Epoch: 325, Loss: 0.3327, Train: 85.64%, Valid: 85.44%, Test: 85.71%
Epoch: 350, Loss: 1.2678, Train: 85.52%, Valid: 85.31%, Test: 85.57%
Epoch: 375, Loss: 1.6915, Train: 86.98%, Valid: 87.06%, Test: 87.08%
Epoch: 400, Loss: 1.0205, Train: 85.69%, Valid: 85.74%, Test: 85.84%
Epoch: 425, Loss: 0.6268, Train: 86.30%, Valid: 86.20%, Test: 86.40%
Epoch: 450, Loss: 0.3615, Train: 85.32%, Valid: 85.17%, Test: 85.41%
Epoch: 475, Loss: 0.3501, Train: 85.47%, Valid: 85.30%, Test: 85.53%
Run 01:
Highest Train: 87.13
Highest Valid: 87.13
  Final Train: 87.05
   Final Test: 87.13
All runs:
Highest Train: 87.13, nan
Highest Valid: 87.13, nan
  Final Train: 87.05, nan
   Final Test: 87.13, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 6.2321, Train: 85.73%, Valid: 85.58%, Test: 85.78%
Epoch: 25, Loss: 1.3468, Train: 86.21%, Valid: 86.26%, Test: 86.28%
Epoch: 50, Loss: 6.3136, Train: 16.34%, Valid: 16.42%, Test: 16.24%
Epoch: 75, Loss: 6.0937, Train: 86.36%, Valid: 86.42%, Test: 86.43%
Epoch: 100, Loss: 30.4898, Train: 86.42%, Valid: 86.50%, Test: 86.49%
Epoch: 125, Loss: 11.0065, Train: 86.31%, Valid: 86.37%, Test: 86.37%
Epoch: 150, Loss: 12.2326, Train: 86.35%, Valid: 86.40%, Test: 86.41%
Epoch: 175, Loss: 11.1696, Train: 86.34%, Valid: 86.38%, Test: 86.40%
Epoch: 200, Loss: 9.0476, Train: 86.33%, Valid: 86.37%, Test: 86.40%
Epoch: 225, Loss: 4.3557, Train: 86.32%, Valid: 86.36%, Test: 86.38%
Epoch: 250, Loss: 3.3438, Train: 86.30%, Valid: 86.35%, Test: 86.37%
Epoch: 275, Loss: 2.8686, Train: 86.30%, Valid: 86.35%, Test: 86.36%
Epoch: 300, Loss: 2.9670, Train: 86.30%, Valid: 86.35%, Test: 86.36%
Epoch: 325, Loss: 2.6544, Train: 86.30%, Valid: 86.34%, Test: 86.36%
Epoch: 350, Loss: 2.4787, Train: 86.30%, Valid: 86.34%, Test: 86.36%
Epoch: 375, Loss: 2.2962, Train: 86.30%, Valid: 86.34%, Test: 86.36%
Epoch: 400, Loss: 2.3908, Train: 86.30%, Valid: 86.34%, Test: 86.36%
Epoch: 425, Loss: 2.3660, Train: 86.30%, Valid: 86.33%, Test: 86.36%
Epoch: 450, Loss: 2.3544, Train: 86.29%, Valid: 86.33%, Test: 86.36%
Epoch: 475, Loss: 2.3328, Train: 86.29%, Valid: 86.33%, Test: 86.35%
Run 01:
Highest Train: 87.12
Highest Valid: 87.10
  Final Train: 87.12
   Final Test: 87.14
All runs:
Highest Train: 87.12, nan
Highest Valid: 87.10, nan
  Final Train: 87.12, nan
   Final Test: 87.14, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.4245, Train: 85.70%, Valid: 85.57%, Test: 85.78%
Epoch: 25, Loss: 0.6002, Train: 86.19%, Valid: 86.12%, Test: 86.27%
Epoch: 50, Loss: 3.5035, Train: 86.33%, Valid: 86.26%, Test: 86.42%
Epoch: 75, Loss: 0.9259, Train: 85.63%, Valid: 85.46%, Test: 85.75%
Epoch: 100, Loss: 109.1188, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 111.0619, Train: 16.44%, Valid: 16.57%, Test: 16.28%
Epoch: 150, Loss: 3161.5859, Train: 85.13%, Valid: 85.07%, Test: 85.22%
Epoch: 175, Loss: 19972.3555, Train: 85.17%, Valid: 85.11%, Test: 85.26%
Epoch: 200, Loss: 7072.0420, Train: 85.85%, Valid: 85.79%, Test: 85.89%
Epoch: 225, Loss: 38558.2500, Train: 85.24%, Valid: 85.17%, Test: 85.33%
Epoch: 250, Loss: 343.7454, Train: 16.54%, Valid: 16.66%, Test: 16.39%
Epoch: 275, Loss: 23985.2168, Train: 16.44%, Valid: 16.57%, Test: 16.28%
Epoch: 300, Loss: 9727.0625, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 12600.2363, Train: 13.63%, Valid: 13.74%, Test: 13.55%
Epoch: 350, Loss: 1612.7178, Train: 85.10%, Valid: 85.05%, Test: 85.20%
Epoch: 375, Loss: 6642.9146, Train: 85.10%, Valid: 85.06%, Test: 85.21%
Epoch: 400, Loss: 640.0366, Train: 16.42%, Valid: 16.56%, Test: 16.27%
Epoch: 425, Loss: 8.5826, Train: 85.80%, Valid: 85.76%, Test: 85.86%
Epoch: 450, Loss: 26.8859, Train: 85.80%, Valid: 85.75%, Test: 85.86%
Epoch: 475, Loss: 368.9966, Train: 85.11%, Valid: 85.06%, Test: 85.21%
Run 01:
Highest Train: 86.93
Highest Valid: 86.90
  Final Train: 86.93
   Final Test: 86.98
All runs:
Highest Train: 86.93, nan
Highest Valid: 86.90, nan
  Final Train: 86.93, nan
   Final Test: 86.98, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 5.0771, Train: 81.13%, Valid: 81.26%, Test: 81.41%
Epoch: 25, Loss: 0.5695, Train: 85.24%, Valid: 85.19%, Test: 85.40%
Epoch: 50, Loss: 0.5188, Train: 86.12%, Valid: 85.96%, Test: 86.18%
Epoch: 75, Loss: 0.4987, Train: 86.51%, Valid: 86.41%, Test: 86.51%
Epoch: 100, Loss: 0.4778, Train: 86.47%, Valid: 86.36%, Test: 86.49%
Epoch: 125, Loss: 0.4576, Train: 86.33%, Valid: 86.21%, Test: 86.34%
Epoch: 150, Loss: 0.4380, Train: 86.22%, Valid: 86.08%, Test: 86.26%
Epoch: 175, Loss: 0.4179, Train: 85.97%, Valid: 85.85%, Test: 86.04%
Epoch: 200, Loss: 0.3898, Train: 85.44%, Valid: 85.30%, Test: 85.51%
Epoch: 225, Loss: 0.3576, Train: 85.49%, Valid: 85.29%, Test: 85.53%
Epoch: 250, Loss: 0.3473, Train: 85.39%, Valid: 85.17%, Test: 85.45%
Epoch: 275, Loss: 0.3382, Train: 85.20%, Valid: 84.95%, Test: 85.26%
Epoch: 300, Loss: 0.3311, Train: 85.28%, Valid: 85.11%, Test: 85.36%
Epoch: 325, Loss: 0.3295, Train: 85.61%, Valid: 85.45%, Test: 85.72%
Epoch: 350, Loss: 0.3268, Train: 85.62%, Valid: 85.44%, Test: 85.70%
Epoch: 375, Loss: 0.3257, Train: 85.62%, Valid: 85.43%, Test: 85.70%
Epoch: 400, Loss: 0.3250, Train: 85.61%, Valid: 85.38%, Test: 85.66%
Epoch: 425, Loss: 0.3249, Train: 85.67%, Valid: 85.42%, Test: 85.73%
Epoch: 450, Loss: 0.3245, Train: 85.51%, Valid: 85.26%, Test: 85.59%
Epoch: 475, Loss: 0.3572, Train: 87.78%, Valid: 87.62%, Test: 87.75%
Run 01:
Highest Train: 87.78
Highest Valid: 87.62
  Final Train: 87.78
   Final Test: 87.75
All runs:
Highest Train: 87.78, nan
Highest Valid: 87.62, nan
  Final Train: 87.78, nan
   Final Test: 87.75, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6151, Train: 84.87%, Valid: 84.71%, Test: 84.96%
Epoch: 25, Loss: 4.3989, Train: 86.71%, Valid: 86.70%, Test: 86.76%
Epoch: 50, Loss: 0.4531, Train: 86.69%, Valid: 86.74%, Test: 86.74%
Epoch: 75, Loss: 0.6194, Train: 86.37%, Valid: 86.34%, Test: 86.48%
Epoch: 100, Loss: 0.6368, Train: 86.37%, Valid: 86.34%, Test: 86.48%
Epoch: 125, Loss: 0.6404, Train: 86.36%, Valid: 86.33%, Test: 86.48%
Epoch: 150, Loss: 0.5894, Train: 86.38%, Valid: 86.35%, Test: 86.49%
Epoch: 175, Loss: 0.6269, Train: 86.36%, Valid: 86.33%, Test: 86.47%
Epoch: 200, Loss: 0.6218, Train: 86.36%, Valid: 86.33%, Test: 86.47%
Epoch: 225, Loss: 0.6215, Train: 86.35%, Valid: 86.33%, Test: 86.47%
Epoch: 250, Loss: 0.6199, Train: 86.35%, Valid: 86.32%, Test: 86.47%
Epoch: 275, Loss: 0.6254, Train: 86.35%, Valid: 86.32%, Test: 86.47%
Epoch: 300, Loss: 0.6280, Train: 86.35%, Valid: 86.32%, Test: 86.46%
Epoch: 325, Loss: 0.6210, Train: 86.34%, Valid: 86.31%, Test: 86.46%
Epoch: 350, Loss: 0.6183, Train: 86.33%, Valid: 86.30%, Test: 86.44%
Epoch: 375, Loss: 0.6186, Train: 86.31%, Valid: 86.28%, Test: 86.43%
Epoch: 400, Loss: 0.6000, Train: 86.28%, Valid: 86.24%, Test: 86.39%
Epoch: 425, Loss: 0.5785, Train: 86.31%, Valid: 86.28%, Test: 86.43%
Epoch: 450, Loss: 0.6368, Train: 86.34%, Valid: 86.30%, Test: 86.45%
Epoch: 475, Loss: 0.6524, Train: 86.33%, Valid: 86.30%, Test: 86.43%
Run 01:
Highest Train: 87.12
Highest Valid: 87.15
  Final Train: 87.12
   Final Test: 87.21
All runs:
Highest Train: 87.12, nan
Highest Valid: 87.15, nan
  Final Train: 87.12, nan
   Final Test: 87.21, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.0725, Train: 86.01%, Valid: 85.94%, Test: 85.98%
Epoch: 25, Loss: 0.4504, Train: 86.29%, Valid: 86.18%, Test: 86.33%
Epoch: 50, Loss: 0.8188, Train: 87.02%, Valid: 86.93%, Test: 87.04%
Epoch: 75, Loss: 0.4903, Train: 87.21%, Valid: 87.06%, Test: 87.15%
Epoch: 100, Loss: 0.4923, Train: 87.14%, Valid: 87.00%, Test: 87.11%
Epoch: 125, Loss: 0.4960, Train: 86.79%, Valid: 86.63%, Test: 86.81%
Epoch: 150, Loss: 0.4927, Train: 86.72%, Valid: 86.58%, Test: 86.73%
Epoch: 175, Loss: 0.4860, Train: 86.16%, Valid: 86.07%, Test: 86.16%
Epoch: 200, Loss: 0.4889, Train: 86.08%, Valid: 86.01%, Test: 86.09%
Epoch: 225, Loss: 0.4897, Train: 86.27%, Valid: 86.19%, Test: 86.31%
Epoch: 250, Loss: 0.4855, Train: 86.31%, Valid: 86.24%, Test: 86.37%
Epoch: 275, Loss: 0.4915, Train: 86.47%, Valid: 86.41%, Test: 86.54%
Epoch: 300, Loss: 0.4966, Train: 86.15%, Valid: 86.02%, Test: 86.24%
Epoch: 325, Loss: 0.4902, Train: 86.01%, Valid: 85.88%, Test: 86.11%
Epoch: 350, Loss: 0.4882, Train: 85.98%, Valid: 85.86%, Test: 86.09%
Epoch: 375, Loss: 0.4855, Train: 85.96%, Valid: 85.83%, Test: 86.05%
Epoch: 400, Loss: 0.4860, Train: 85.97%, Valid: 85.84%, Test: 86.06%
Epoch: 425, Loss: 0.4724, Train: 85.90%, Valid: 85.74%, Test: 85.99%
Epoch: 450, Loss: 0.4734, Train: 85.93%, Valid: 85.79%, Test: 86.02%
Epoch: 475, Loss: 0.4739, Train: 85.94%, Valid: 85.80%, Test: 86.03%
Run 01:
Highest Train: 87.23
Highest Valid: 87.07
  Final Train: 87.23
   Final Test: 87.16
All runs:
Highest Train: 87.23, nan
Highest Valid: 87.07, nan
  Final Train: 87.23, nan
   Final Test: 87.16, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 3.0927, Train: 83.93%, Valid: 83.86%, Test: 83.97%
Epoch: 25, Loss: 0.5301, Train: 85.38%, Valid: 85.24%, Test: 85.41%
Epoch: 50, Loss: 0.5225, Train: 85.37%, Valid: 85.23%, Test: 85.41%
Epoch: 75, Loss: 0.5279, Train: 85.37%, Valid: 85.23%, Test: 85.41%
Epoch: 100, Loss: 0.5337, Train: 85.37%, Valid: 85.23%, Test: 85.41%
Epoch: 125, Loss: 0.5360, Train: 85.37%, Valid: 85.24%, Test: 85.42%
Epoch: 150, Loss: 0.5380, Train: 85.38%, Valid: 85.25%, Test: 85.43%
Epoch: 175, Loss: 0.5396, Train: 85.40%, Valid: 85.26%, Test: 85.44%
Epoch: 200, Loss: 0.5395, Train: 85.41%, Valid: 85.28%, Test: 85.45%
Epoch: 225, Loss: 0.5288, Train: 85.44%, Valid: 85.31%, Test: 85.48%
Epoch: 250, Loss: 0.5122, Train: 85.48%, Valid: 85.35%, Test: 85.52%
Epoch: 275, Loss: 0.5164, Train: 85.48%, Valid: 85.35%, Test: 85.52%
Epoch: 300, Loss: 0.5192, Train: 85.47%, Valid: 85.34%, Test: 85.51%
Epoch: 325, Loss: 0.5719, Train: 85.45%, Valid: 85.31%, Test: 85.48%
Epoch: 350, Loss: 0.5688, Train: 85.47%, Valid: 85.34%, Test: 85.51%
Epoch: 375, Loss: 0.5599, Train: 85.49%, Valid: 85.37%, Test: 85.53%
Epoch: 400, Loss: 0.5523, Train: 85.51%, Valid: 85.39%, Test: 85.55%
Epoch: 425, Loss: 0.5438, Train: 85.54%, Valid: 85.42%, Test: 85.57%
Epoch: 450, Loss: 0.5321, Train: 85.57%, Valid: 85.45%, Test: 85.58%
Epoch: 475, Loss: 0.5323, Train: 85.58%, Valid: 85.46%, Test: 85.59%
Run 01:
Highest Train: 86.09
Highest Valid: 85.98
  Final Train: 86.09
   Final Test: 86.15
All runs:
Highest Train: 86.09, nan
Highest Valid: 85.98, nan
  Final Train: 86.09, nan
   Final Test: 86.15, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.7501, Train: 84.87%, Valid: 84.73%, Test: 85.03%
Epoch: 25, Loss: 0.4356, Train: 85.99%, Valid: 86.09%, Test: 86.09%
Epoch: 50, Loss: 0.6132, Train: 86.63%, Valid: 86.64%, Test: 86.63%
Epoch: 75, Loss: 0.5189, Train: 86.63%, Valid: 86.63%, Test: 86.62%
Epoch: 100, Loss: 0.4838, Train: 86.70%, Valid: 86.70%, Test: 86.71%
Epoch: 125, Loss: 0.5185, Train: 86.69%, Valid: 86.70%, Test: 86.72%
Epoch: 150, Loss: 0.4855, Train: 86.62%, Valid: 86.65%, Test: 86.66%
Epoch: 175, Loss: 0.5103, Train: 86.60%, Valid: 86.62%, Test: 86.62%
Epoch: 200, Loss: 0.5153, Train: 86.39%, Valid: 86.42%, Test: 86.45%
Epoch: 225, Loss: 0.4735, Train: 86.05%, Valid: 86.12%, Test: 86.13%
Epoch: 250, Loss: 0.4588, Train: 85.87%, Valid: 85.94%, Test: 85.95%
Epoch: 275, Loss: 0.4671, Train: 85.84%, Valid: 85.90%, Test: 85.94%
Epoch: 300, Loss: 0.4288, Train: 85.77%, Valid: 85.82%, Test: 85.85%
Epoch: 325, Loss: 0.4045, Train: 85.77%, Valid: 85.82%, Test: 85.86%
Epoch: 350, Loss: 0.3897, Train: 85.76%, Valid: 85.81%, Test: 85.85%
Epoch: 375, Loss: 0.3612, Train: 85.75%, Valid: 85.76%, Test: 85.86%
Epoch: 400, Loss: 0.3509, Train: 85.70%, Valid: 85.72%, Test: 85.80%
Epoch: 425, Loss: 0.3461, Train: 85.75%, Valid: 85.76%, Test: 85.84%
Epoch: 450, Loss: 0.3400, Train: 85.94%, Valid: 85.96%, Test: 86.00%
Epoch: 475, Loss: 0.3350, Train: 85.81%, Valid: 85.83%, Test: 85.89%
Run 01:
Highest Train: 86.74
Highest Valid: 86.79
  Final Train: 86.74
   Final Test: 86.79
All runs:
Highest Train: 86.74, nan
Highest Valid: 86.79, nan
  Final Train: 86.74, nan
   Final Test: 86.79, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.5867, Train: 85.63%, Valid: 85.51%, Test: 85.62%
Epoch: 25, Loss: 0.4503, Train: 86.04%, Valid: 85.99%, Test: 86.07%
Epoch: 50, Loss: 0.4602, Train: 86.19%, Valid: 86.14%, Test: 86.24%
Epoch: 75, Loss: 0.4221, Train: 86.29%, Valid: 86.21%, Test: 86.31%
Epoch: 100, Loss: 0.4602, Train: 86.12%, Valid: 86.01%, Test: 86.15%
Epoch: 125, Loss: 0.3917, Train: 86.00%, Valid: 85.90%, Test: 86.08%
Epoch: 150, Loss: 0.3592, Train: 85.87%, Valid: 85.72%, Test: 85.96%
Epoch: 175, Loss: 0.3542, Train: 85.87%, Valid: 85.70%, Test: 85.97%
Epoch: 200, Loss: 0.3485, Train: 85.91%, Valid: 85.75%, Test: 86.02%
Epoch: 225, Loss: 0.3407, Train: 85.92%, Valid: 85.69%, Test: 86.00%
Epoch: 250, Loss: 0.3330, Train: 85.57%, Valid: 85.40%, Test: 85.69%
Epoch: 275, Loss: 0.3283, Train: 85.81%, Valid: 85.65%, Test: 85.93%
Epoch: 300, Loss: 0.3448, Train: 87.47%, Valid: 87.35%, Test: 87.60%
Epoch: 325, Loss: 0.3370, Train: 86.16%, Valid: 85.98%, Test: 86.27%
Epoch: 350, Loss: 0.3320, Train: 86.06%, Valid: 85.89%, Test: 86.18%
Epoch: 375, Loss: 0.3284, Train: 86.01%, Valid: 85.84%, Test: 86.14%
Epoch: 400, Loss: 0.3358, Train: 86.31%, Valid: 86.16%, Test: 86.45%
Epoch: 425, Loss: 0.3293, Train: 86.14%, Valid: 85.94%, Test: 86.27%
Epoch: 450, Loss: 0.3249, Train: 86.30%, Valid: 86.11%, Test: 86.42%
Epoch: 475, Loss: 0.3584, Train: 85.76%, Valid: 85.57%, Test: 85.86%
Run 01:
Highest Train: 87.56
Highest Valid: 87.41
  Final Train: 87.56
   Final Test: 87.67
All runs:
Highest Train: 87.56, nan
Highest Valid: 87.41, nan
  Final Train: 87.56, nan
   Final Test: 87.67, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.8616, Train: 76.47%, Valid: 76.62%, Test: 76.86%
Epoch: 25, Loss: 0.5806, Train: 85.20%, Valid: 85.05%, Test: 85.27%
Epoch: 50, Loss: 0.4954, Train: 85.30%, Valid: 85.14%, Test: 85.36%
Epoch: 75, Loss: 0.4884, Train: 85.31%, Valid: 85.16%, Test: 85.38%
Epoch: 100, Loss: 0.4807, Train: 85.31%, Valid: 85.16%, Test: 85.38%
Epoch: 125, Loss: 0.4748, Train: 85.33%, Valid: 85.18%, Test: 85.39%
Epoch: 150, Loss: 0.6854, Train: 85.83%, Valid: 85.82%, Test: 85.89%
Epoch: 175, Loss: 0.4960, Train: 85.99%, Valid: 85.87%, Test: 86.03%
Epoch: 200, Loss: 0.5092, Train: 85.91%, Valid: 85.82%, Test: 85.98%
Epoch: 225, Loss: 0.5193, Train: 85.75%, Valid: 85.65%, Test: 85.80%
Epoch: 250, Loss: 0.5046, Train: 85.67%, Valid: 85.57%, Test: 85.71%
Epoch: 275, Loss: 0.5064, Train: 85.65%, Valid: 85.53%, Test: 85.66%
Epoch: 300, Loss: 0.4946, Train: 85.56%, Valid: 85.44%, Test: 85.60%
Epoch: 325, Loss: 0.4906, Train: 85.56%, Valid: 85.44%, Test: 85.58%
Epoch: 350, Loss: 0.4958, Train: 85.41%, Valid: 85.27%, Test: 85.47%
Epoch: 375, Loss: 0.4743, Train: 85.34%, Valid: 85.18%, Test: 85.40%
Epoch: 400, Loss: 2.6153, Train: 85.78%, Valid: 85.69%, Test: 85.82%
Epoch: 425, Loss: 652.5355, Train: 85.24%, Valid: 85.05%, Test: 85.32%
Epoch: 450, Loss: 3.5185, Train: 85.06%, Valid: 84.87%, Test: 85.13%
Epoch: 475, Loss: 1.8917, Train: 85.04%, Valid: 84.85%, Test: 85.11%
Run 01:
Highest Train: 87.45
Highest Valid: 87.46
  Final Train: 87.45
   Final Test: 87.51
All runs:
Highest Train: 87.45, nan
Highest Valid: 87.46, nan
  Final Train: 87.45, nan
   Final Test: 87.51, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 8.1277, Train: 84.73%, Valid: 84.72%, Test: 84.89%
Epoch: 25, Loss: 0.8258, Train: 86.49%, Valid: 86.49%, Test: 86.55%
Epoch: 50, Loss: 0.7601, Train: 86.05%, Valid: 86.07%, Test: 86.13%
Epoch: 75, Loss: 0.7164, Train: 85.72%, Valid: 85.79%, Test: 85.84%
Epoch: 100, Loss: 0.5943, Train: 85.52%, Valid: 85.57%, Test: 85.60%
Epoch: 125, Loss: 0.3746, Train: 87.63%, Valid: 87.48%, Test: 87.68%
Epoch: 150, Loss: 0.3468, Train: 85.55%, Valid: 85.56%, Test: 85.63%
Epoch: 175, Loss: 0.3353, Train: 85.79%, Valid: 85.58%, Test: 85.82%
Epoch: 200, Loss: 0.3397, Train: 85.86%, Valid: 85.84%, Test: 85.96%
Epoch: 225, Loss: 0.3269, Train: 85.65%, Valid: 85.56%, Test: 85.75%
Epoch: 250, Loss: 0.5725, Train: 85.59%, Valid: 85.61%, Test: 85.66%
Epoch: 275, Loss: 1.4570, Train: 85.37%, Valid: 85.40%, Test: 85.46%
Epoch: 300, Loss: 2.0738, Train: 85.41%, Valid: 85.43%, Test: 85.49%
Epoch: 325, Loss: 2.0638, Train: 85.39%, Valid: 85.42%, Test: 85.48%
Epoch: 350, Loss: 1.8112, Train: 85.39%, Valid: 85.42%, Test: 85.48%
Epoch: 375, Loss: 1.5525, Train: 85.35%, Valid: 85.37%, Test: 85.44%
Epoch: 400, Loss: 1.4096, Train: 85.35%, Valid: 85.36%, Test: 85.43%
Epoch: 425, Loss: 1.0325, Train: 85.21%, Valid: 85.23%, Test: 85.31%
Epoch: 450, Loss: 0.3896, Train: 85.16%, Valid: 85.21%, Test: 85.27%
Epoch: 475, Loss: 0.3419, Train: 85.79%, Valid: 85.76%, Test: 85.88%
Run 01:
Highest Train: 87.63
Highest Valid: 87.49
  Final Train: 87.63
   Final Test: 87.67
All runs:
Highest Train: 87.63, nan
Highest Valid: 87.49, nan
  Final Train: 87.63, nan
   Final Test: 87.67, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.5176, Train: 85.33%, Valid: 85.30%, Test: 85.36%
Epoch: 25, Loss: 0.7324, Train: 85.87%, Valid: 85.80%, Test: 85.88%
Epoch: 50, Loss: 0.8654, Train: 85.78%, Valid: 85.66%, Test: 85.75%
Epoch: 75, Loss: 0.8160, Train: 85.76%, Valid: 85.62%, Test: 85.87%
Epoch: 100, Loss: 0.7720, Train: 85.72%, Valid: 85.58%, Test: 85.81%
Epoch: 125, Loss: 0.6041, Train: 85.84%, Valid: 85.69%, Test: 85.92%
Epoch: 150, Loss: 0.3515, Train: 85.63%, Valid: 85.47%, Test: 85.77%
Epoch: 175, Loss: 0.3434, Train: 85.89%, Valid: 85.69%, Test: 85.99%
Epoch: 200, Loss: 0.3311, Train: 85.83%, Valid: 85.67%, Test: 85.96%
Epoch: 225, Loss: 0.3315, Train: 86.17%, Valid: 85.99%, Test: 86.28%
Epoch: 250, Loss: 0.3275, Train: 86.14%, Valid: 85.90%, Test: 86.23%
Epoch: 275, Loss: 0.4604, Train: 85.84%, Valid: 85.69%, Test: 85.96%
Epoch: 300, Loss: 0.7260, Train: 86.28%, Valid: 86.16%, Test: 86.38%
Epoch: 325, Loss: 0.7809, Train: 86.35%, Valid: 86.22%, Test: 86.46%
Epoch: 350, Loss: 0.7269, Train: 86.58%, Valid: 86.45%, Test: 86.68%
Epoch: 375, Loss: 0.5135, Train: 86.71%, Valid: 86.59%, Test: 86.81%
Epoch: 400, Loss: 0.5293, Train: 87.21%, Valid: 87.10%, Test: 87.30%
Epoch: 425, Loss: 0.5263, Train: 87.10%, Valid: 87.00%, Test: 87.22%
Epoch: 450, Loss: 0.4605, Train: 87.05%, Valid: 86.95%, Test: 87.14%
Epoch: 475, Loss: 0.3872, Train: 87.22%, Valid: 87.13%, Test: 87.31%
Run 01:
Highest Train: 87.52
Highest Valid: 87.41
  Final Train: 87.52
   Final Test: 87.63
All runs:
Highest Train: 87.52, nan
Highest Valid: 87.41, nan
  Final Train: 87.52, nan
   Final Test: 87.63, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 11.2940, Train: 77.87%, Valid: 78.10%, Test: 78.24%
Epoch: 25, Loss: 0.6980, Train: 86.16%, Valid: 86.12%, Test: 86.30%
Epoch: 50, Loss: 0.5600, Train: 85.84%, Valid: 85.79%, Test: 85.98%
Epoch: 75, Loss: 0.3773, Train: 85.97%, Valid: 85.84%, Test: 86.02%
Epoch: 100, Loss: 0.3653, Train: 85.58%, Valid: 85.44%, Test: 85.65%
Epoch: 125, Loss: 0.3624, Train: 85.52%, Valid: 85.37%, Test: 85.59%
Epoch: 150, Loss: 0.3589, Train: 85.49%, Valid: 85.35%, Test: 85.57%
Epoch: 175, Loss: 0.3542, Train: 85.48%, Valid: 85.33%, Test: 85.56%
Epoch: 200, Loss: 0.3490, Train: 85.39%, Valid: 85.24%, Test: 85.48%
Epoch: 225, Loss: 0.3439, Train: 85.27%, Valid: 85.11%, Test: 85.36%
Epoch: 250, Loss: 0.3381, Train: 85.29%, Valid: 85.12%, Test: 85.37%
Epoch: 275, Loss: 0.3347, Train: 85.40%, Valid: 85.23%, Test: 85.47%
Epoch: 300, Loss: 0.3328, Train: 85.46%, Valid: 85.28%, Test: 85.53%
Epoch: 325, Loss: 0.3340, Train: 85.51%, Valid: 85.33%, Test: 85.59%
Epoch: 350, Loss: 0.3477, Train: 85.69%, Valid: 85.49%, Test: 85.74%
Epoch: 375, Loss: 0.3374, Train: 85.71%, Valid: 85.52%, Test: 85.78%
Epoch: 400, Loss: 0.3327, Train: 85.59%, Valid: 85.40%, Test: 85.66%
Epoch: 425, Loss: 0.3303, Train: 85.60%, Valid: 85.41%, Test: 85.68%
Epoch: 450, Loss: 0.3346, Train: 85.63%, Valid: 85.45%, Test: 85.73%
Epoch: 475, Loss: 0.3338, Train: 85.67%, Valid: 85.46%, Test: 85.75%
Run 01:
Highest Train: 86.25
Highest Valid: 86.19
  Final Train: 86.22
   Final Test: 86.36
All runs:
Highest Train: 86.25, nan
Highest Valid: 86.19, nan
  Final Train: 86.22, nan
   Final Test: 86.36, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 5.2200, Train: 86.32%, Valid: 86.33%, Test: 86.32%
Epoch: 25, Loss: 1.0181, Train: 85.06%, Valid: 84.93%, Test: 85.11%
Epoch: 50, Loss: 0.6129, Train: 85.63%, Valid: 85.47%, Test: 85.70%
Epoch: 75, Loss: 0.6023, Train: 85.86%, Valid: 85.67%, Test: 85.94%
Epoch: 100, Loss: 0.6190, Train: 85.48%, Valid: 85.32%, Test: 85.57%
Epoch: 125, Loss: 0.6079, Train: 86.83%, Valid: 86.88%, Test: 86.91%
Epoch: 150, Loss: 0.5957, Train: 87.26%, Valid: 87.34%, Test: 87.22%
Epoch: 175, Loss: 0.5740, Train: 86.73%, Valid: 86.72%, Test: 86.81%
Epoch: 200, Loss: 0.5925, Train: 87.87%, Valid: 87.88%, Test: 87.87%
Epoch: 225, Loss: 0.6434, Train: 87.84%, Valid: 87.87%, Test: 87.82%
Epoch: 250, Loss: 0.6252, Train: 87.94%, Valid: 87.96%, Test: 87.90%
Epoch: 275, Loss: 0.6231, Train: 87.88%, Valid: 87.91%, Test: 87.85%
Epoch: 300, Loss: 0.6152, Train: 87.78%, Valid: 87.80%, Test: 87.73%
Epoch: 325, Loss: 0.6086, Train: 87.73%, Valid: 87.74%, Test: 87.67%
Epoch: 350, Loss: 0.6083, Train: 87.89%, Valid: 87.91%, Test: 87.85%
Epoch: 375, Loss: 0.6027, Train: 87.88%, Valid: 87.89%, Test: 87.83%
Epoch: 400, Loss: 0.5825, Train: 87.96%, Valid: 87.95%, Test: 87.90%
Epoch: 425, Loss: 0.5760, Train: 87.96%, Valid: 87.94%, Test: 87.90%
Epoch: 450, Loss: 0.5764, Train: 87.90%, Valid: 87.90%, Test: 87.85%
Epoch: 475, Loss: 0.5811, Train: 87.82%, Valid: 87.83%, Test: 87.77%
Run 01:
Highest Train: 87.99
Highest Valid: 87.99
  Final Train: 87.97
   Final Test: 87.92
All runs:
Highest Train: 87.99, nan
Highest Valid: 87.99, nan
  Final Train: 87.97, nan
   Final Test: 87.92, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 5.1727, Train: 87.00%, Valid: 86.85%, Test: 87.06%
Epoch: 25, Loss: 0.6571, Train: 85.92%, Valid: 85.81%, Test: 86.05%
Epoch: 50, Loss: 0.9818, Train: 86.02%, Valid: 85.89%, Test: 86.12%
Epoch: 75, Loss: 13.7110, Train: 86.28%, Valid: 86.26%, Test: 86.38%
Epoch: 100, Loss: 1.2889, Train: 85.93%, Valid: 85.86%, Test: 85.92%
Epoch: 125, Loss: 1.5761, Train: 85.99%, Valid: 85.93%, Test: 85.97%
Epoch: 150, Loss: 1.4656, Train: 86.06%, Valid: 86.00%, Test: 86.06%
Epoch: 175, Loss: 1.4376, Train: 86.00%, Valid: 85.95%, Test: 86.00%
Epoch: 200, Loss: 1.4635, Train: 85.99%, Valid: 85.94%, Test: 85.99%
Epoch: 225, Loss: 1.4674, Train: 85.99%, Valid: 85.94%, Test: 85.99%
Epoch: 250, Loss: 1.4800, Train: 85.96%, Valid: 85.91%, Test: 85.95%
Epoch: 275, Loss: 1.4852, Train: 85.96%, Valid: 85.91%, Test: 85.96%
Epoch: 300, Loss: 1.5306, Train: 85.97%, Valid: 85.92%, Test: 85.97%
Epoch: 325, Loss: 1.4944, Train: 85.98%, Valid: 85.93%, Test: 85.97%
Epoch: 350, Loss: 1.4791, Train: 85.97%, Valid: 85.92%, Test: 85.96%
Epoch: 375, Loss: 1.5779, Train: 85.94%, Valid: 85.88%, Test: 85.95%
Epoch: 400, Loss: 1.8468, Train: 86.97%, Valid: 86.96%, Test: 87.00%
Epoch: 425, Loss: 25.6284, Train: 86.97%, Valid: 86.94%, Test: 86.97%
Epoch: 450, Loss: 18.0874, Train: 87.03%, Valid: 87.02%, Test: 87.05%
Epoch: 475, Loss: 13.6958, Train: 86.96%, Valid: 86.94%, Test: 87.00%
Run 01:
Highest Train: 87.49
Highest Valid: 87.42
  Final Train: 87.49
   Final Test: 87.62
All runs:
Highest Train: 87.49, nan
Highest Valid: 87.42, nan
  Final Train: 87.49, nan
   Final Test: 87.62, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.7739, Train: 83.87%, Valid: 83.98%, Test: 84.08%
Epoch: 25, Loss: 46.4866, Train: 21.06%, Valid: 20.88%, Test: 20.71%
Epoch: 50, Loss: 14.8738, Train: 85.55%, Valid: 85.45%, Test: 85.56%
Epoch: 75, Loss: 10.9828, Train: 82.39%, Valid: 82.50%, Test: 82.65%
Epoch: 100, Loss: 2.6798, Train: 84.99%, Valid: 85.01%, Test: 85.03%
Epoch: 125, Loss: 1.9121, Train: 85.61%, Valid: 85.63%, Test: 85.65%
Epoch: 150, Loss: 1.8895, Train: 85.69%, Valid: 85.71%, Test: 85.71%
Epoch: 175, Loss: 1.8739, Train: 85.69%, Valid: 85.71%, Test: 85.72%
Epoch: 200, Loss: 1.8487, Train: 85.71%, Valid: 85.73%, Test: 85.74%
Epoch: 225, Loss: 1.8208, Train: 85.73%, Valid: 85.75%, Test: 85.75%
Epoch: 250, Loss: 1.8043, Train: 85.74%, Valid: 85.77%, Test: 85.76%
Epoch: 275, Loss: 1.7767, Train: 85.75%, Valid: 85.78%, Test: 85.78%
Epoch: 300, Loss: 1.7825, Train: 85.76%, Valid: 85.79%, Test: 85.78%
Epoch: 325, Loss: 1.7537, Train: 85.80%, Valid: 85.82%, Test: 85.81%
Epoch: 350, Loss: 1.7288, Train: 85.82%, Valid: 85.84%, Test: 85.84%
Epoch: 375, Loss: 1.6940, Train: 85.84%, Valid: 85.86%, Test: 85.85%
Epoch: 400, Loss: 1.6934, Train: 85.85%, Valid: 85.87%, Test: 85.86%
Epoch: 425, Loss: 1.6694, Train: 85.85%, Valid: 85.84%, Test: 85.83%
Epoch: 450, Loss: 1.6443, Train: 85.96%, Valid: 85.97%, Test: 85.95%
Epoch: 475, Loss: 1.5904, Train: 85.99%, Valid: 85.99%, Test: 85.97%
Run 01:
Highest Train: 86.13
Highest Valid: 86.08
  Final Train: 86.08
   Final Test: 86.09
All runs:
Highest Train: 86.13, nan
Highest Valid: 86.08, nan
  Final Train: 86.08, nan
   Final Test: 86.09, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 5.1622, Train: 85.99%, Valid: 86.07%, Test: 86.15%
Epoch: 25, Loss: 10.4924, Train: 88.21%, Valid: 88.21%, Test: 88.22%
Epoch: 50, Loss: 0.6737, Train: 88.23%, Valid: 88.32%, Test: 88.25%
Epoch: 75, Loss: 0.7818, Train: 86.86%, Valid: 86.93%, Test: 86.91%
Epoch: 100, Loss: 0.8286, Train: 86.84%, Valid: 86.90%, Test: 86.86%
Epoch: 125, Loss: 10.7986, Train: 87.40%, Valid: 87.45%, Test: 87.35%
Epoch: 150, Loss: 1.0911, Train: 86.89%, Valid: 86.94%, Test: 86.90%
Epoch: 175, Loss: 1771209490432.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 200, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 225, Loss: 14473831430066536448.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 250, Loss: 831477191392597705294792884224.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 61808358960660480.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 300, Loss: 337627510154148950346563584.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 2347740524707840.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 350, Loss: 81348511061994014507008.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: 129419081299263488.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 1346561109219248650252713984.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: 391010505935568874349002752.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: 2034186600266646012559360.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: 95814702130629505206779866251264.0000, Train: 83.47%, Valid: 83.30%, Test: 83.64%
Run 01:
Highest Train: 88.32
Highest Valid: 88.35
  Final Train: 88.31
   Final Test: 88.29
All runs:
Highest Train: 88.32, nan
Highest Valid: 88.35, nan
  Final Train: 88.31, nan
   Final Test: 88.29, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 4.4744, Train: 85.96%, Valid: 85.88%, Test: 85.96%
Epoch: 25, Loss: 0.6900, Train: 87.20%, Valid: 87.29%, Test: 87.27%
Epoch: 50, Loss: 0.7778, Train: 86.17%, Valid: 86.13%, Test: 86.22%
Epoch: 75, Loss: 0.7124, Train: 86.23%, Valid: 86.17%, Test: 86.27%
Epoch: 100, Loss: 0.5023, Train: 85.94%, Valid: 85.81%, Test: 86.01%
Epoch: 125, Loss: 0.3601, Train: 86.37%, Valid: 86.35%, Test: 86.45%
Epoch: 150, Loss: 0.3527, Train: 85.89%, Valid: 85.69%, Test: 85.98%
Epoch: 175, Loss: 0.3454, Train: 85.85%, Valid: 85.69%, Test: 85.96%
Epoch: 200, Loss: 0.3365, Train: 85.60%, Valid: 85.47%, Test: 85.75%
Epoch: 225, Loss: 0.3333, Train: 85.79%, Valid: 85.63%, Test: 85.94%
Epoch: 250, Loss: 0.3281, Train: 85.80%, Valid: 85.65%, Test: 85.95%
Epoch: 275, Loss: 0.3287, Train: 85.98%, Valid: 85.80%, Test: 86.10%
Epoch: 300, Loss: 0.9539, Train: 86.16%, Valid: 86.02%, Test: 86.28%
Epoch: 325, Loss: 1.6976, Train: 85.85%, Valid: 85.72%, Test: 85.97%
Epoch: 350, Loss: 1.6048, Train: 85.88%, Valid: 85.74%, Test: 85.99%
Epoch: 375, Loss: 1.3405, Train: 85.88%, Valid: 85.75%, Test: 86.00%
Epoch: 400, Loss: 0.7639, Train: 85.89%, Valid: 85.75%, Test: 86.00%
Epoch: 425, Loss: 0.5939, Train: 85.88%, Valid: 85.74%, Test: 86.00%
Epoch: 450, Loss: 0.5462, Train: 85.90%, Valid: 85.77%, Test: 86.01%
Epoch: 475, Loss: 0.3713, Train: 85.90%, Valid: 85.76%, Test: 86.01%
Run 01:
Highest Train: 87.45
Highest Valid: 87.36
  Final Train: 87.45
   Final Test: 87.57
All runs:
Highest Train: 87.45, nan
Highest Valid: 87.36, nan
  Final Train: 87.45, nan
   Final Test: 87.57, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 6.1545, Train: 85.17%, Valid: 85.09%, Test: 85.18%
Epoch: 25, Loss: 0.6618, Train: 85.79%, Valid: 85.64%, Test: 85.87%
Epoch: 50, Loss: 0.6050, Train: 85.91%, Valid: 85.77%, Test: 85.96%
Epoch: 75, Loss: 0.5240, Train: 85.71%, Valid: 85.60%, Test: 85.79%
Epoch: 100, Loss: 0.4789, Train: 85.25%, Valid: 85.08%, Test: 85.31%
Epoch: 125, Loss: 0.4598, Train: 85.23%, Valid: 85.05%, Test: 85.29%
Epoch: 150, Loss: 0.4424, Train: 85.26%, Valid: 85.08%, Test: 85.32%
Epoch: 175, Loss: 0.5873, Train: 85.48%, Valid: 85.29%, Test: 85.52%
Epoch: 200, Loss: 0.4776, Train: 85.44%, Valid: 85.25%, Test: 85.48%
Epoch: 225, Loss: 0.3703, Train: 85.53%, Valid: 85.34%, Test: 85.59%
Epoch: 250, Loss: 0.3512, Train: 85.51%, Valid: 85.35%, Test: 85.59%
Epoch: 275, Loss: 0.3464, Train: 85.42%, Valid: 85.25%, Test: 85.50%
Epoch: 300, Loss: 0.3416, Train: 85.29%, Valid: 85.11%, Test: 85.38%
Epoch: 325, Loss: 0.3378, Train: 85.13%, Valid: 84.95%, Test: 85.23%
Epoch: 350, Loss: 0.3352, Train: 85.29%, Valid: 85.11%, Test: 85.38%
Epoch: 375, Loss: 0.3336, Train: 85.41%, Valid: 85.21%, Test: 85.48%
Epoch: 400, Loss: 0.3322, Train: 85.53%, Valid: 85.34%, Test: 85.61%
Epoch: 425, Loss: 0.3364, Train: 85.50%, Valid: 85.29%, Test: 85.58%
Epoch: 450, Loss: 0.3483, Train: 85.77%, Valid: 85.60%, Test: 85.85%
Epoch: 475, Loss: 0.3442, Train: 85.81%, Valid: 85.63%, Test: 85.88%
Run 01:
Highest Train: 86.41
Highest Valid: 86.28
  Final Train: 86.41
   Final Test: 86.48
All runs:
Highest Train: 86.41, nan
Highest Valid: 86.28, nan
  Final Train: 86.41, nan
   Final Test: 86.48, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.3829, Train: 14.55%, Valid: 14.59%, Test: 14.54%
Epoch: 25, Loss: 0.9899, Train: 86.66%, Valid: 86.71%, Test: 86.71%
Epoch: 50, Loss: 0.9108, Train: 86.53%, Valid: 86.57%, Test: 86.61%
Epoch: 75, Loss: 0.5144, Train: 85.77%, Valid: 85.83%, Test: 85.89%
Epoch: 100, Loss: 0.3559, Train: 85.51%, Valid: 85.52%, Test: 85.62%
Epoch: 125, Loss: 0.3507, Train: 85.59%, Valid: 85.58%, Test: 85.69%
Epoch: 150, Loss: 0.3365, Train: 85.32%, Valid: 85.30%, Test: 85.43%
Epoch: 175, Loss: 0.8318, Train: 85.44%, Valid: 85.44%, Test: 85.53%
Epoch: 200, Loss: 2.4195, Train: 85.41%, Valid: 85.44%, Test: 85.49%
Epoch: 225, Loss: 2.6346, Train: 85.15%, Valid: 85.14%, Test: 85.26%
Epoch: 250, Loss: 2.4575, Train: 85.38%, Valid: 85.41%, Test: 85.46%
Epoch: 275, Loss: 2.4314, Train: 85.39%, Valid: 85.43%, Test: 85.49%
Epoch: 300, Loss: 2.3198, Train: 85.41%, Valid: 85.44%, Test: 85.50%
Epoch: 325, Loss: 2.2425, Train: 85.41%, Valid: 85.44%, Test: 85.49%
Epoch: 350, Loss: 1.8371, Train: 85.40%, Valid: 85.42%, Test: 85.48%
Epoch: 375, Loss: 0.4695, Train: 84.96%, Valid: 85.01%, Test: 85.07%
Epoch: 400, Loss: 0.7510, Train: 85.40%, Valid: 85.43%, Test: 85.48%
Epoch: 425, Loss: 0.6945, Train: 85.40%, Valid: 85.43%, Test: 85.49%
Epoch: 450, Loss: 0.5948, Train: 85.41%, Valid: 85.43%, Test: 85.49%
Epoch: 475, Loss: 0.4942, Train: 85.41%, Valid: 85.43%, Test: 85.49%
Run 01:
Highest Train: 87.61
Highest Valid: 87.47
  Final Train: 87.61
   Final Test: 87.65
All runs:
Highest Train: 87.61, nan
Highest Valid: 87.47, nan
  Final Train: 87.61, nan
   Final Test: 87.65, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.5290, Train: 85.52%, Valid: 85.41%, Test: 85.66%
Epoch: 25, Loss: 0.3910, Train: 85.82%, Valid: 85.76%, Test: 85.93%
Epoch: 50, Loss: 0.3608, Train: 85.79%, Valid: 85.60%, Test: 85.90%
Epoch: 75, Loss: 0.3546, Train: 85.83%, Valid: 85.68%, Test: 85.92%
Epoch: 100, Loss: 0.3471, Train: 85.78%, Valid: 85.63%, Test: 85.88%
Epoch: 125, Loss: 0.4022, Train: 86.54%, Valid: 86.37%, Test: 86.58%
Epoch: 150, Loss: 0.9838, Train: 86.02%, Valid: 85.96%, Test: 86.14%
Epoch: 175, Loss: 0.5277, Train: 86.44%, Valid: 86.37%, Test: 86.54%
Epoch: 200, Loss: 0.6844, Train: 85.92%, Valid: 85.86%, Test: 86.06%
Epoch: 225, Loss: 0.3720, Train: 86.12%, Valid: 86.06%, Test: 86.24%
Epoch: 250, Loss: 0.3437, Train: 86.81%, Valid: 86.62%, Test: 86.92%
Epoch: 275, Loss: 0.3388, Train: 85.86%, Valid: 85.70%, Test: 85.98%
Epoch: 300, Loss: 0.3384, Train: 85.81%, Valid: 85.59%, Test: 85.90%
Epoch: 325, Loss: 1.5679, Train: 85.81%, Valid: 85.69%, Test: 85.93%
Epoch: 350, Loss: 1.2299, Train: 85.85%, Valid: 85.73%, Test: 85.97%
Epoch: 375, Loss: 0.8487, Train: 85.87%, Valid: 85.75%, Test: 85.99%
Epoch: 400, Loss: 0.5356, Train: 85.87%, Valid: 85.74%, Test: 85.97%
Epoch: 425, Loss: 0.3821, Train: 86.08%, Valid: 85.96%, Test: 86.20%
Epoch: 450, Loss: 0.3358, Train: 86.14%, Valid: 85.95%, Test: 86.26%
Epoch: 475, Loss: 0.3313, Train: 86.00%, Valid: 85.81%, Test: 86.11%
Run 01:
Highest Train: 87.07
Highest Valid: 86.94
  Final Train: 87.03
   Final Test: 87.09
All runs:
Highest Train: 87.07, nan
Highest Valid: 86.94, nan
  Final Train: 87.03, nan
   Final Test: 87.09, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 10.2308, Train: 85.44%, Valid: 85.25%, Test: 85.59%
Epoch: 25, Loss: 0.4797, Train: 85.64%, Valid: 85.54%, Test: 85.79%
Epoch: 50, Loss: 0.4086, Train: 85.40%, Valid: 85.26%, Test: 85.46%
Epoch: 75, Loss: 0.3739, Train: 85.44%, Valid: 85.29%, Test: 85.51%
Epoch: 100, Loss: 0.3657, Train: 85.45%, Valid: 85.30%, Test: 85.52%
Epoch: 125, Loss: 0.3598, Train: 85.32%, Valid: 85.18%, Test: 85.41%
Epoch: 150, Loss: 0.3549, Train: 85.21%, Valid: 85.06%, Test: 85.30%
Epoch: 175, Loss: 0.4129, Train: 85.30%, Valid: 85.11%, Test: 85.37%
Epoch: 200, Loss: 0.3671, Train: 85.22%, Valid: 85.06%, Test: 85.30%
Epoch: 225, Loss: 0.3499, Train: 85.24%, Valid: 85.07%, Test: 85.32%
Epoch: 250, Loss: 0.3464, Train: 85.18%, Valid: 85.02%, Test: 85.27%
Epoch: 275, Loss: 0.3442, Train: 85.18%, Valid: 85.01%, Test: 85.26%
Epoch: 300, Loss: 0.3423, Train: 85.20%, Valid: 85.04%, Test: 85.29%
Epoch: 325, Loss: 0.9861, Train: 85.36%, Valid: 85.21%, Test: 85.43%
Epoch: 350, Loss: 1.1975, Train: 85.61%, Valid: 85.45%, Test: 85.66%
Epoch: 375, Loss: 1.4085, Train: 86.78%, Valid: 86.59%, Test: 86.74%
Epoch: 400, Loss: 0.8844, Train: 86.86%, Valid: 86.69%, Test: 86.82%
Epoch: 425, Loss: 0.5076, Train: 85.43%, Valid: 85.27%, Test: 85.48%
Epoch: 450, Loss: 0.3518, Train: 85.51%, Valid: 85.35%, Test: 85.58%
Epoch: 475, Loss: 0.3484, Train: 85.37%, Valid: 85.20%, Test: 85.45%
Run 01:
Highest Train: 87.10
Highest Valid: 86.88
  Final Train: 87.10
   Final Test: 87.08
All runs:
Highest Train: 87.10, nan
Highest Valid: 86.88, nan
  Final Train: 87.10, nan
   Final Test: 87.08, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.3764, Train: 15.77%, Valid: 15.91%, Test: 15.68%
Epoch: 25, Loss: 0.9253, Train: 86.73%, Valid: 86.79%, Test: 86.79%
Epoch: 50, Loss: 0.7405, Train: 86.28%, Valid: 86.31%, Test: 86.36%
Epoch: 75, Loss: 0.3745, Train: 85.84%, Valid: 85.88%, Test: 85.91%
Epoch: 100, Loss: 0.3484, Train: 85.80%, Valid: 85.77%, Test: 85.88%
Epoch: 125, Loss: 0.3360, Train: 85.67%, Valid: 85.66%, Test: 85.80%
Epoch: 150, Loss: 1.0590, Train: 85.71%, Valid: 85.72%, Test: 85.82%
Epoch: 175, Loss: 0.9343, Train: 85.71%, Valid: 85.73%, Test: 85.81%
Epoch: 200, Loss: 0.3520, Train: 85.69%, Valid: 85.70%, Test: 85.80%
Epoch: 225, Loss: 0.3444, Train: 86.12%, Valid: 85.93%, Test: 86.17%
Epoch: 250, Loss: 0.3367, Train: 86.07%, Valid: 86.04%, Test: 86.17%
Epoch: 275, Loss: 0.3311, Train: 85.88%, Valid: 85.85%, Test: 85.98%
Epoch: 300, Loss: 0.9003, Train: 85.63%, Valid: 85.66%, Test: 85.73%
Epoch: 325, Loss: 0.9228, Train: 85.51%, Valid: 85.55%, Test: 85.60%
Epoch: 350, Loss: 2.1206, Train: 85.73%, Valid: 85.78%, Test: 85.82%
Epoch: 375, Loss: 1.7329, Train: 85.83%, Valid: 85.88%, Test: 85.91%
Epoch: 400, Loss: 1.2312, Train: 85.81%, Valid: 85.87%, Test: 85.88%
Epoch: 425, Loss: 0.9950, Train: 85.61%, Valid: 85.65%, Test: 85.70%
Epoch: 450, Loss: 0.6681, Train: 85.54%, Valid: 85.54%, Test: 85.63%
Epoch: 475, Loss: 0.3378, Train: 86.04%, Valid: 86.05%, Test: 86.13%
Run 01:
Highest Train: 87.88
Highest Valid: 87.90
  Final Train: 87.88
   Final Test: 87.96
All runs:
Highest Train: 87.88, nan
Highest Valid: 87.90, nan
  Final Train: 87.88, nan
   Final Test: 87.96, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.5628, Train: 86.18%, Valid: 86.13%, Test: 86.23%
Epoch: 25, Loss: 0.4209, Train: 86.45%, Valid: 86.29%, Test: 86.63%
Epoch: 50, Loss: 0.3608, Train: 85.99%, Valid: 85.83%, Test: 86.15%
Epoch: 75, Loss: 0.3530, Train: 85.74%, Valid: 85.54%, Test: 85.85%
Epoch: 100, Loss: 0.3539, Train: 85.73%, Valid: 85.53%, Test: 85.83%
Epoch: 125, Loss: 0.9569, Train: 85.75%, Valid: 85.61%, Test: 85.86%
Epoch: 150, Loss: 0.8224, Train: 85.77%, Valid: 85.63%, Test: 85.88%
Epoch: 175, Loss: 0.4951, Train: 85.67%, Valid: 85.53%, Test: 85.78%
Epoch: 200, Loss: 0.3374, Train: 85.94%, Valid: 85.77%, Test: 86.06%
Epoch: 225, Loss: 0.3307, Train: 85.79%, Valid: 85.62%, Test: 85.91%
Epoch: 250, Loss: 0.7433, Train: 85.81%, Valid: 85.68%, Test: 85.93%
Epoch: 275, Loss: 1.3336, Train: 86.85%, Valid: 86.78%, Test: 87.01%
Epoch: 300, Loss: 0.6578, Train: 86.95%, Valid: 86.88%, Test: 87.03%
Epoch: 325, Loss: 0.7341, Train: 86.74%, Valid: 86.65%, Test: 86.80%
Epoch: 350, Loss: 0.6128, Train: 86.21%, Valid: 86.14%, Test: 86.27%
Epoch: 375, Loss: 0.3750, Train: 85.94%, Valid: 85.87%, Test: 86.04%
Epoch: 400, Loss: 0.3400, Train: 86.08%, Valid: 85.88%, Test: 86.20%
Epoch: 425, Loss: 0.3338, Train: 86.07%, Valid: 85.89%, Test: 86.20%
Epoch: 450, Loss: 0.4062, Train: 85.96%, Valid: 85.89%, Test: 86.06%
Epoch: 475, Loss: 0.4223, Train: 85.65%, Valid: 85.56%, Test: 85.77%
Run 01:
Highest Train: 87.07
Highest Valid: 86.97
  Final Train: 87.07
   Final Test: 87.12
All runs:
Highest Train: 87.07, nan
Highest Valid: 86.97, nan
  Final Train: 87.07, nan
   Final Test: 87.12, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.9414, Train: 83.74%, Valid: 83.71%, Test: 83.83%
Epoch: 25, Loss: 0.9022, Train: 85.80%, Valid: 85.66%, Test: 85.86%
Epoch: 50, Loss: 0.6735, Train: 85.44%, Valid: 85.28%, Test: 85.47%
Epoch: 75, Loss: 0.7221, Train: 85.41%, Valid: 85.25%, Test: 85.46%
Epoch: 100, Loss: 0.5885, Train: 85.26%, Valid: 85.09%, Test: 85.31%
Epoch: 125, Loss: 0.9512, Train: 85.26%, Valid: 85.06%, Test: 85.31%
Epoch: 150, Loss: 0.6733, Train: 85.22%, Valid: 85.05%, Test: 85.28%
Epoch: 175, Loss: 0.6811, Train: 84.97%, Valid: 84.80%, Test: 85.04%
Epoch: 200, Loss: 0.5286, Train: 84.98%, Valid: 84.81%, Test: 85.06%
Epoch: 225, Loss: 0.7874, Train: 85.38%, Valid: 85.20%, Test: 85.43%
Epoch: 250, Loss: 0.4686, Train: 85.30%, Valid: 85.13%, Test: 85.35%
Epoch: 275, Loss: 0.4161, Train: 85.36%, Valid: 85.19%, Test: 85.43%
Epoch: 300, Loss: 0.3708, Train: 85.49%, Valid: 85.31%, Test: 85.57%
Epoch: 325, Loss: 0.3538, Train: 85.61%, Valid: 85.42%, Test: 85.68%
Epoch: 350, Loss: 0.3501, Train: 85.66%, Valid: 85.47%, Test: 85.74%
Epoch: 375, Loss: 0.3599, Train: 85.68%, Valid: 85.48%, Test: 85.76%
Epoch: 400, Loss: 0.5011, Train: 86.80%, Valid: 86.75%, Test: 86.91%
Epoch: 425, Loss: 0.3591, Train: 87.20%, Valid: 87.18%, Test: 87.26%
Epoch: 450, Loss: 0.3442, Train: 87.35%, Valid: 87.36%, Test: 87.44%
Epoch: 475, Loss: 0.3401, Train: 87.14%, Valid: 87.17%, Test: 87.25%
Run 01:
Highest Train: 87.38
Highest Valid: 87.39
  Final Train: 87.38
   Final Test: 87.48
All runs:
Highest Train: 87.38, nan
Highest Valid: 87.39, nan
  Final Train: 87.38, nan
   Final Test: 87.48, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.3842, Train: 18.38%, Valid: 18.47%, Test: 18.37%
Epoch: 25, Loss: 0.6620, Train: 88.10%, Valid: 88.14%, Test: 88.18%
Epoch: 50, Loss: 0.3596, Train: 86.85%, Valid: 86.89%, Test: 86.90%
Epoch: 75, Loss: 0.3518, Train: 86.14%, Valid: 86.17%, Test: 86.15%
Epoch: 100, Loss: 0.3392, Train: 86.33%, Valid: 86.17%, Test: 86.31%
Epoch: 125, Loss: 1.3549, Train: 85.84%, Valid: 85.89%, Test: 85.94%
Epoch: 150, Loss: 1.8090, Train: 85.75%, Valid: 85.78%, Test: 85.84%
Epoch: 175, Loss: 1.2724, Train: 85.72%, Valid: 85.75%, Test: 85.80%
Epoch: 200, Loss: 0.4136, Train: 85.72%, Valid: 85.75%, Test: 85.79%
Epoch: 225, Loss: 0.7456, Train: 85.71%, Valid: 85.76%, Test: 85.80%
Epoch: 250, Loss: 0.6225, Train: 85.74%, Valid: 85.78%, Test: 85.83%
Epoch: 275, Loss: 0.4704, Train: 85.73%, Valid: 85.79%, Test: 85.82%
Epoch: 300, Loss: 0.3579, Train: 86.14%, Valid: 86.11%, Test: 86.17%
Epoch: 325, Loss: 0.3331, Train: 85.99%, Valid: 86.00%, Test: 86.08%
Epoch: 350, Loss: 0.3293, Train: 86.00%, Valid: 86.00%, Test: 86.09%
Epoch: 375, Loss: 0.5175, Train: 85.61%, Valid: 85.66%, Test: 85.69%
Epoch: 400, Loss: 0.3585, Train: 85.72%, Valid: 85.74%, Test: 85.85%
Epoch: 425, Loss: 0.3334, Train: 86.08%, Valid: 86.08%, Test: 86.18%
Epoch: 450, Loss: 0.3289, Train: 86.02%, Valid: 86.02%, Test: 86.12%
Epoch: 475, Loss: 1.1198, Train: 85.61%, Valid: 85.65%, Test: 85.70%
Run 01:
Highest Train: 88.51
Highest Valid: 88.56
  Final Train: 88.51
   Final Test: 88.54
All runs:
Highest Train: 88.51, nan
Highest Valid: 88.56, nan
  Final Train: 88.51, nan
   Final Test: 88.54, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 6.6280, Train: 85.04%, Valid: 84.90%, Test: 85.14%
Epoch: 25, Loss: 0.7507, Train: 86.67%, Valid: 86.66%, Test: 86.79%
Epoch: 50, Loss: 0.3844, Train: 86.24%, Valid: 86.18%, Test: 86.38%
Epoch: 75, Loss: 0.3602, Train: 85.51%, Valid: 85.40%, Test: 85.65%
Epoch: 100, Loss: 0.3546, Train: 85.68%, Valid: 85.53%, Test: 85.79%
Epoch: 125, Loss: 0.3480, Train: 85.62%, Valid: 85.47%, Test: 85.73%
Epoch: 150, Loss: 0.3394, Train: 85.61%, Valid: 85.45%, Test: 85.73%
Epoch: 175, Loss: 0.3343, Train: 85.97%, Valid: 85.82%, Test: 86.08%
Epoch: 200, Loss: 1.9707, Train: 85.80%, Valid: 85.68%, Test: 85.93%
Epoch: 225, Loss: 5.0237, Train: 85.91%, Valid: 85.81%, Test: 86.06%
Epoch: 250, Loss: 2.5320, Train: 85.88%, Valid: 85.75%, Test: 85.99%
Epoch: 275, Loss: 2.6086, Train: 85.87%, Valid: 85.74%, Test: 85.96%
Epoch: 300, Loss: 2.5592, Train: 85.89%, Valid: 85.77%, Test: 85.99%
Epoch: 325, Loss: 2.4866, Train: 85.91%, Valid: 85.79%, Test: 86.00%
Epoch: 350, Loss: 2.3995, Train: 85.91%, Valid: 85.79%, Test: 86.01%
Epoch: 375, Loss: 2.2717, Train: 85.91%, Valid: 85.80%, Test: 86.01%
Epoch: 400, Loss: 2.0425, Train: 85.92%, Valid: 85.80%, Test: 86.02%
Epoch: 425, Loss: 1.5861, Train: 85.91%, Valid: 85.78%, Test: 86.00%
Epoch: 450, Loss: 0.3978, Train: 87.53%, Valid: 87.53%, Test: 87.68%
Epoch: 475, Loss: 0.6711, Train: 86.18%, Valid: 86.11%, Test: 86.27%
Run 01:
Highest Train: 87.53
Highest Valid: 87.53
  Final Train: 87.53
   Final Test: 87.68
All runs:
Highest Train: 87.53, nan
Highest Valid: 87.53, nan
  Final Train: 87.53, nan
   Final Test: 87.68, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=100.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.7596, Train: 84.82%, Valid: 84.63%, Test: 84.87%
Epoch: 25, Loss: 0.9001, Train: 85.15%, Valid: 84.97%, Test: 85.22%
Epoch: 50, Loss: 0.8457, Train: 85.36%, Valid: 85.20%, Test: 85.42%
Epoch: 75, Loss: 0.7503, Train: 85.60%, Valid: 85.43%, Test: 85.67%
Epoch: 100, Loss: 0.5779, Train: 85.43%, Valid: 85.25%, Test: 85.49%
Epoch: 125, Loss: 0.6231, Train: 85.64%, Valid: 85.47%, Test: 85.70%
Epoch: 150, Loss: 0.5543, Train: 85.31%, Valid: 85.14%, Test: 85.35%
Epoch: 175, Loss: 0.5383, Train: 85.26%, Valid: 85.09%, Test: 85.30%
Epoch: 200, Loss: 0.5253, Train: 85.24%, Valid: 85.06%, Test: 85.29%
Epoch: 225, Loss: 0.5423, Train: 85.29%, Valid: 85.10%, Test: 85.34%
Epoch: 250, Loss: 0.5687, Train: 85.31%, Valid: 85.13%, Test: 85.37%
Epoch: 275, Loss: 0.5637, Train: 85.36%, Valid: 85.17%, Test: 85.40%
Epoch: 300, Loss: 0.4281, Train: 85.50%, Valid: 85.29%, Test: 85.55%
Epoch: 325, Loss: 0.3586, Train: 85.53%, Valid: 85.33%, Test: 85.60%
Epoch: 350, Loss: 0.3557, Train: 85.52%, Valid: 85.33%, Test: 85.59%
Epoch: 375, Loss: 0.3535, Train: 85.52%, Valid: 85.32%, Test: 85.58%
Epoch: 400, Loss: 0.3514, Train: 85.48%, Valid: 85.29%, Test: 85.55%
Epoch: 425, Loss: 0.3492, Train: 85.42%, Valid: 85.22%, Test: 85.49%
Epoch: 450, Loss: 0.3462, Train: 85.35%, Valid: 85.15%, Test: 85.42%
Epoch: 475, Loss: 0.3438, Train: 85.28%, Valid: 85.08%, Test: 85.35%
Run 01:
Highest Train: 85.64
Highest Valid: 85.48
  Final Train: 85.64
   Final Test: 85.70
All runs:
Highest Train: 85.64, nan
Highest Valid: 85.48, nan
  Final Train: 85.64, nan
   Final Test: 85.70, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.7816, Train: 86.58%, Valid: 86.52%, Test: 86.67%
Epoch: 25, Loss: 0.6652, Train: 86.48%, Valid: 86.44%, Test: 86.56%
Epoch: 50, Loss: 23.8009, Train: 85.26%, Valid: 85.31%, Test: 85.37%
Epoch: 75, Loss: 3.9115, Train: 86.23%, Valid: 86.26%, Test: 86.30%
Epoch: 100, Loss: 5.0054, Train: 86.27%, Valid: 86.29%, Test: 86.34%
Epoch: 125, Loss: 4.7407, Train: 86.28%, Valid: 86.29%, Test: 86.35%
Epoch: 150, Loss: 4.6713, Train: 86.29%, Valid: 86.31%, Test: 86.36%
Epoch: 175, Loss: 4.6160, Train: 86.32%, Valid: 86.35%, Test: 86.40%
Epoch: 200, Loss: 4.1285, Train: 86.32%, Valid: 86.36%, Test: 86.40%
Epoch: 225, Loss: 3.3729, Train: 86.32%, Valid: 86.36%, Test: 86.39%
Epoch: 250, Loss: 3.4248, Train: 86.33%, Valid: 86.37%, Test: 86.40%
Epoch: 275, Loss: 3.5870, Train: 86.35%, Valid: 86.38%, Test: 86.42%
Epoch: 300, Loss: 2.7676, Train: 86.21%, Valid: 86.26%, Test: 86.28%
Epoch: 325, Loss: 2.1231, Train: 86.22%, Valid: 86.28%, Test: 86.29%
Epoch: 350, Loss: 2.6418, Train: 86.25%, Valid: 86.27%, Test: 86.32%
Epoch: 375, Loss: 3.0661, Train: 86.38%, Valid: 86.41%, Test: 86.44%
Epoch: 400, Loss: 3.1175, Train: 86.38%, Valid: 86.41%, Test: 86.44%
Epoch: 425, Loss: 3.1645, Train: 86.38%, Valid: 86.42%, Test: 86.44%
Epoch: 450, Loss: 2.7670, Train: 86.38%, Valid: 86.41%, Test: 86.44%
Epoch: 475, Loss: 2.4882, Train: 86.38%, Valid: 86.41%, Test: 86.44%
Run 01:
Highest Train: 86.81
Highest Valid: 86.86
  Final Train: 86.81
   Final Test: 86.89
All runs:
Highest Train: 86.81, nan
Highest Valid: 86.86, nan
  Final Train: 86.81, nan
   Final Test: 86.89, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.7949, Train: 86.02%, Valid: 85.90%, Test: 86.04%
Epoch: 25, Loss: 0.5292, Train: 86.44%, Valid: 86.34%, Test: 86.54%
Epoch: 50, Loss: 1.4532, Train: 85.72%, Valid: 85.61%, Test: 85.81%
Epoch: 75, Loss: 0.6580, Train: 85.25%, Valid: 85.08%, Test: 85.38%
Epoch: 100, Loss: 0.6355, Train: 85.21%, Valid: 85.05%, Test: 85.33%
Epoch: 125, Loss: 0.5908, Train: 85.18%, Valid: 85.03%, Test: 85.30%
Epoch: 150, Loss: 1.0053, Train: 85.31%, Valid: 85.15%, Test: 85.43%
Epoch: 175, Loss: 0.5363, Train: 85.33%, Valid: 85.18%, Test: 85.46%
Epoch: 200, Loss: 0.4518, Train: 86.68%, Valid: 86.57%, Test: 86.77%
Epoch: 225, Loss: 2.1267, Train: 86.21%, Valid: 86.05%, Test: 86.32%
Epoch: 250, Loss: 0.6095, Train: 85.71%, Valid: 85.51%, Test: 85.82%
Epoch: 275, Loss: 48.5958, Train: 13.47%, Valid: 13.42%, Test: 13.45%
Epoch: 300, Loss: 18.6597, Train: 13.95%, Valid: 14.07%, Test: 13.86%
Epoch: 325, Loss: 1.5220, Train: 87.36%, Valid: 87.21%, Test: 87.45%
Epoch: 350, Loss: 2.1691, Train: 87.19%, Valid: 87.08%, Test: 87.30%
Epoch: 375, Loss: 2.8956, Train: 87.20%, Valid: 87.08%, Test: 87.30%
Epoch: 400, Loss: 3.1552, Train: 87.18%, Valid: 87.06%, Test: 87.27%
Epoch: 425, Loss: 2.5285, Train: 87.16%, Valid: 87.04%, Test: 87.26%
Epoch: 450, Loss: 2.5615, Train: 87.15%, Valid: 87.03%, Test: 87.25%
Epoch: 475, Loss: 3.1546, Train: 87.12%, Valid: 87.00%, Test: 87.23%
Run 01:
Highest Train: 87.48
Highest Valid: 87.41
  Final Train: 87.48
   Final Test: 87.54
All runs:
Highest Train: 87.48, nan
Highest Valid: 87.41, nan
  Final Train: 87.48, nan
   Final Test: 87.54, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.9149, Train: 84.11%, Valid: 83.95%, Test: 84.22%
Epoch: 25, Loss: 0.4596, Train: 85.43%, Valid: 85.29%, Test: 85.46%
Epoch: 50, Loss: 0.4191, Train: 85.58%, Valid: 85.44%, Test: 85.56%
Epoch: 75, Loss: 0.3917, Train: 85.95%, Valid: 85.87%, Test: 85.96%
Epoch: 100, Loss: 0.3767, Train: 85.32%, Valid: 85.18%, Test: 85.38%
Epoch: 125, Loss: 0.3741, Train: 85.39%, Valid: 85.28%, Test: 85.45%
Epoch: 150, Loss: 0.3712, Train: 85.40%, Valid: 85.28%, Test: 85.47%
Epoch: 175, Loss: 0.3682, Train: 85.41%, Valid: 85.28%, Test: 85.48%
Epoch: 200, Loss: 0.3651, Train: 85.41%, Valid: 85.28%, Test: 85.48%
Epoch: 225, Loss: 0.3614, Train: 85.41%, Valid: 85.27%, Test: 85.48%
Epoch: 250, Loss: 0.3574, Train: 85.41%, Valid: 85.27%, Test: 85.48%
Epoch: 275, Loss: 0.3522, Train: 85.34%, Valid: 85.19%, Test: 85.42%
Epoch: 300, Loss: 0.3477, Train: 85.23%, Valid: 85.08%, Test: 85.33%
Epoch: 325, Loss: 0.3437, Train: 85.16%, Valid: 85.00%, Test: 85.26%
Epoch: 350, Loss: 0.3450, Train: 85.18%, Valid: 85.01%, Test: 85.27%
Epoch: 375, Loss: 0.3408, Train: 85.17%, Valid: 85.00%, Test: 85.25%
Epoch: 400, Loss: 0.3391, Train: 85.18%, Valid: 85.02%, Test: 85.27%
Epoch: 425, Loss: 0.3380, Train: 85.23%, Valid: 85.07%, Test: 85.32%
Epoch: 450, Loss: 0.3370, Train: 85.34%, Valid: 85.18%, Test: 85.42%
Epoch: 475, Loss: 0.3366, Train: 85.32%, Valid: 85.15%, Test: 85.40%
Run 01:
Highest Train: 86.24
Highest Valid: 86.20
  Final Train: 86.24
   Final Test: 86.36
All runs:
Highest Train: 86.24, nan
Highest Valid: 86.20, nan
  Final Train: 86.24, nan
   Final Test: 86.36, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.5162, Train: 84.77%, Valid: 84.68%, Test: 84.90%
Epoch: 25, Loss: 0.4641, Train: 86.88%, Valid: 86.91%, Test: 86.91%
Epoch: 50, Loss: 2.4312, Train: 87.45%, Valid: 87.54%, Test: 87.44%
Epoch: 75, Loss: 2.6304, Train: 87.26%, Valid: 87.34%, Test: 87.25%
Epoch: 100, Loss: 2.7883, Train: 87.24%, Valid: 87.33%, Test: 87.24%
Epoch: 125, Loss: 3.4921, Train: 87.15%, Valid: 87.24%, Test: 87.17%
Epoch: 150, Loss: 4.9422, Train: 86.87%, Valid: 86.92%, Test: 86.88%
Epoch: 175, Loss: 6.4377, Train: 86.86%, Valid: 86.88%, Test: 86.83%
Epoch: 200, Loss: 6.2392, Train: 86.92%, Valid: 86.95%, Test: 86.90%
Epoch: 225, Loss: 6.5646, Train: 86.92%, Valid: 86.94%, Test: 86.89%
Epoch: 250, Loss: 6.2631, Train: 86.92%, Valid: 86.94%, Test: 86.89%
Epoch: 275, Loss: 6.7839, Train: 86.88%, Valid: 86.89%, Test: 86.84%
Epoch: 300, Loss: 5.5000, Train: 86.93%, Valid: 86.95%, Test: 86.90%
Epoch: 325, Loss: 5.7855, Train: 86.92%, Valid: 86.92%, Test: 86.89%
Epoch: 350, Loss: 5.1739, Train: 86.94%, Valid: 86.97%, Test: 86.92%
Epoch: 375, Loss: 5.4029, Train: 86.86%, Valid: 86.91%, Test: 86.85%
Epoch: 400, Loss: 5.3681, Train: 86.91%, Valid: 86.94%, Test: 86.89%
Epoch: 425, Loss: 5.8814, Train: 86.93%, Valid: 86.95%, Test: 86.90%
Epoch: 450, Loss: 5.1450, Train: 86.94%, Valid: 86.96%, Test: 86.92%
Epoch: 475, Loss: 5.2079, Train: 86.93%, Valid: 86.95%, Test: 86.91%
Run 01:
Highest Train: 87.93
Highest Valid: 87.99
  Final Train: 87.93
   Final Test: 87.91
All runs:
Highest Train: 87.93, nan
Highest Valid: 87.99, nan
  Final Train: 87.93, nan
   Final Test: 87.91, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.4176, Train: 24.88%, Valid: 24.75%, Test: 24.57%
Epoch: 25, Loss: 0.3958, Train: 85.36%, Valid: 85.23%, Test: 85.47%
Epoch: 50, Loss: 0.3863, Train: 85.38%, Valid: 85.23%, Test: 85.49%
Epoch: 75, Loss: 0.3723, Train: 85.34%, Valid: 85.19%, Test: 85.45%
Epoch: 100, Loss: 0.3668, Train: 85.07%, Valid: 84.99%, Test: 85.20%
Epoch: 125, Loss: 0.3661, Train: 85.15%, Valid: 85.08%, Test: 85.29%
Epoch: 150, Loss: 0.3655, Train: 85.43%, Valid: 85.34%, Test: 85.55%
Epoch: 175, Loss: 0.3649, Train: 85.62%, Valid: 85.53%, Test: 85.73%
Epoch: 200, Loss: 0.3638, Train: 85.67%, Valid: 85.59%, Test: 85.79%
Epoch: 225, Loss: 0.3624, Train: 85.79%, Valid: 85.70%, Test: 85.90%
Epoch: 250, Loss: 0.3605, Train: 85.90%, Valid: 85.80%, Test: 86.01%
Epoch: 275, Loss: 0.3579, Train: 85.86%, Valid: 85.75%, Test: 85.96%
Epoch: 300, Loss: 0.3549, Train: 85.86%, Valid: 85.73%, Test: 85.96%
Epoch: 325, Loss: 0.3513, Train: 85.76%, Valid: 85.63%, Test: 85.85%
Epoch: 350, Loss: 0.3468, Train: 85.64%, Valid: 85.50%, Test: 85.73%
Epoch: 375, Loss: 0.3409, Train: 85.55%, Valid: 85.39%, Test: 85.67%
Epoch: 400, Loss: 0.3355, Train: 85.47%, Valid: 85.32%, Test: 85.59%
Epoch: 425, Loss: 0.3318, Train: 85.53%, Valid: 85.41%, Test: 85.66%
Epoch: 450, Loss: 0.3311, Train: 85.74%, Valid: 85.60%, Test: 85.85%
Epoch: 475, Loss: 0.3289, Train: 85.91%, Valid: 85.77%, Test: 86.03%
Run 01:
Highest Train: 86.33
Highest Valid: 86.18
  Final Train: 86.33
   Final Test: 86.43
All runs:
Highest Train: 86.33, nan
Highest Valid: 86.18, nan
  Final Train: 86.33, nan
   Final Test: 86.43, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.2996, Train: 85.21%, Valid: 85.23%, Test: 85.26%
Epoch: 25, Loss: 0.6159, Train: 85.13%, Valid: 84.99%, Test: 85.19%
Epoch: 50, Loss: 0.5898, Train: 85.06%, Valid: 84.91%, Test: 85.11%
Epoch: 75, Loss: 0.5742, Train: 84.95%, Valid: 84.79%, Test: 85.00%
Epoch: 100, Loss: 0.5280, Train: 80.00%, Valid: 80.02%, Test: 80.35%
Epoch: 125, Loss: 0.6995, Train: 85.01%, Valid: 84.86%, Test: 85.06%
Epoch: 150, Loss: 0.6403, Train: 85.00%, Valid: 84.85%, Test: 85.06%
Epoch: 175, Loss: 0.6267, Train: 84.99%, Valid: 84.84%, Test: 85.04%
Epoch: 200, Loss: 0.6097, Train: 84.99%, Valid: 84.85%, Test: 85.05%
Epoch: 225, Loss: 0.5742, Train: 85.04%, Valid: 84.87%, Test: 85.08%
Epoch: 250, Loss: 0.5365, Train: 85.01%, Valid: 84.86%, Test: 85.06%
Epoch: 275, Loss: 0.5177, Train: 84.76%, Valid: 84.63%, Test: 84.84%
Epoch: 300, Loss: 0.6361, Train: 85.02%, Valid: 84.87%, Test: 85.10%
Epoch: 325, Loss: 0.6968, Train: 85.17%, Valid: 85.04%, Test: 85.21%
Epoch: 350, Loss: 16.4511, Train: 85.33%, Valid: 85.22%, Test: 85.38%
Epoch: 375, Loss: 11.4091, Train: 85.39%, Valid: 85.26%, Test: 85.40%
Epoch: 400, Loss: 8.7481, Train: 85.36%, Valid: 85.23%, Test: 85.38%
Epoch: 425, Loss: 8.9422, Train: 85.34%, Valid: 85.21%, Test: 85.36%
Epoch: 450, Loss: 8.4336, Train: 85.31%, Valid: 85.18%, Test: 85.35%
Epoch: 475, Loss: 8.0457, Train: 85.29%, Valid: 85.16%, Test: 85.34%
Run 01:
Highest Train: 85.41
Highest Valid: 85.30
  Final Train: 85.41
   Final Test: 85.43
All runs:
Highest Train: 85.41, nan
Highest Valid: 85.30, nan
  Final Train: 85.41, nan
   Final Test: 85.43, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.6464, Train: 87.54%, Valid: 87.56%, Test: 87.56%
Epoch: 25, Loss: 0.4017, Train: 87.67%, Valid: 87.70%, Test: 87.69%
Epoch: 50, Loss: 0.4926, Train: 87.82%, Valid: 87.83%, Test: 87.81%
Epoch: 75, Loss: 0.5366, Train: 87.80%, Valid: 87.83%, Test: 87.80%
Epoch: 100, Loss: 0.5102, Train: 87.86%, Valid: 87.91%, Test: 87.85%
Epoch: 125, Loss: 0.4856, Train: 87.82%, Valid: 87.85%, Test: 87.83%
Epoch: 150, Loss: 0.4820, Train: 87.78%, Valid: 87.82%, Test: 87.76%
Epoch: 175, Loss: 0.4559, Train: 87.64%, Valid: 87.67%, Test: 87.63%
Epoch: 200, Loss: 0.4513, Train: 87.66%, Valid: 87.70%, Test: 87.65%
Epoch: 225, Loss: 0.4645, Train: 87.52%, Valid: 87.52%, Test: 87.47%
Epoch: 250, Loss: 0.4627, Train: 86.49%, Valid: 86.46%, Test: 86.53%
Epoch: 275, Loss: 0.4584, Train: 86.21%, Valid: 86.21%, Test: 86.27%
Epoch: 300, Loss: 0.4541, Train: 85.89%, Valid: 85.93%, Test: 85.97%
Epoch: 325, Loss: 0.4430, Train: 85.76%, Valid: 85.78%, Test: 85.85%
Epoch: 350, Loss: 0.4424, Train: 85.92%, Valid: 85.94%, Test: 86.02%
Epoch: 375, Loss: 0.4402, Train: 85.76%, Valid: 85.78%, Test: 85.85%
Epoch: 400, Loss: 0.4349, Train: 85.75%, Valid: 85.79%, Test: 85.82%
Epoch: 425, Loss: 0.4278, Train: 85.75%, Valid: 85.79%, Test: 85.84%
Epoch: 450, Loss: 0.4128, Train: 85.75%, Valid: 85.79%, Test: 85.86%
Epoch: 475, Loss: 0.3746, Train: 85.78%, Valid: 85.79%, Test: 85.88%
Run 01:
Highest Train: 87.91
Highest Valid: 87.98
  Final Train: 87.91
   Final Test: 87.94
All runs:
Highest Train: 87.91, nan
Highest Valid: 87.98, nan
  Final Train: 87.91, nan
   Final Test: 87.94, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.6203, Train: 85.72%, Valid: 85.64%, Test: 85.75%
Epoch: 25, Loss: 1.9893, Train: 85.11%, Valid: 84.97%, Test: 85.22%
Epoch: 50, Loss: 0.5509, Train: 85.15%, Valid: 85.02%, Test: 85.28%
Epoch: 75, Loss: 0.4225, Train: 84.65%, Valid: 84.53%, Test: 84.75%
Epoch: 100, Loss: 0.6104, Train: 85.86%, Valid: 85.84%, Test: 85.92%
Epoch: 125, Loss: 0.5301, Train: 86.15%, Valid: 86.13%, Test: 86.24%
Epoch: 150, Loss: 0.5645, Train: 85.90%, Valid: 85.88%, Test: 85.99%
Epoch: 175, Loss: 0.5700, Train: 85.84%, Valid: 85.76%, Test: 85.90%
Epoch: 200, Loss: 0.7024, Train: 87.25%, Valid: 87.13%, Test: 87.25%
Epoch: 225, Loss: 29.8156, Train: 13.71%, Valid: 13.83%, Test: 13.65%
Epoch: 250, Loss: 14424106016700366848.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 275, Loss: 350240480.0000, Train: 85.92%, Valid: 85.84%, Test: 85.96%
Epoch: 300, Loss: 222223572424377572863770624.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 325, Loss: 143552832.0000, Train: 86.23%, Valid: 86.10%, Test: 86.22%
Epoch: 350, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 375, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 400, Loss: 585286588446200572769443250176.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 425, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 450, Loss: inf, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 475, Loss: nan, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 87.34
Highest Valid: 87.25
  Final Train: 87.34
   Final Test: 87.34
All runs:
Highest Train: 87.34, nan
Highest Valid: 87.25, nan
  Final Train: 87.34, nan
   Final Test: 87.34, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.9729, Train: 84.39%, Valid: 84.32%, Test: 84.44%
Epoch: 25, Loss: 0.6668, Train: 85.05%, Valid: 84.91%, Test: 85.13%
Epoch: 50, Loss: 0.5987, Train: 85.24%, Valid: 85.09%, Test: 85.31%
Epoch: 75, Loss: 0.5805, Train: 85.33%, Valid: 85.18%, Test: 85.40%
Epoch: 100, Loss: 0.5690, Train: 85.18%, Valid: 85.03%, Test: 85.24%
Epoch: 125, Loss: 0.5598, Train: 85.12%, Valid: 84.97%, Test: 85.18%
Epoch: 150, Loss: 0.5503, Train: 85.10%, Valid: 84.94%, Test: 85.16%
Epoch: 175, Loss: 0.5389, Train: 85.06%, Valid: 84.89%, Test: 85.12%
Epoch: 200, Loss: 0.5257, Train: 85.03%, Valid: 84.86%, Test: 85.09%
Epoch: 225, Loss: 0.5113, Train: 85.00%, Valid: 84.83%, Test: 85.07%
Epoch: 250, Loss: 0.4899, Train: 84.91%, Valid: 84.73%, Test: 84.98%
Epoch: 275, Loss: 0.4790, Train: 84.93%, Valid: 84.75%, Test: 85.00%
Epoch: 300, Loss: 0.4637, Train: 84.88%, Valid: 84.70%, Test: 84.95%
Epoch: 325, Loss: 0.4448, Train: 84.77%, Valid: 84.59%, Test: 84.85%
Epoch: 350, Loss: 0.4288, Train: 84.68%, Valid: 84.50%, Test: 84.76%
Epoch: 375, Loss: 0.4159, Train: 84.58%, Valid: 84.40%, Test: 84.67%
Epoch: 400, Loss: 0.4269, Train: 84.84%, Valid: 84.66%, Test: 84.93%
Epoch: 425, Loss: 0.4009, Train: 84.75%, Valid: 84.57%, Test: 84.84%
Epoch: 450, Loss: 0.4223, Train: 84.81%, Valid: 84.63%, Test: 84.92%
Epoch: 475, Loss: 0.4035, Train: 84.81%, Valid: 84.64%, Test: 84.92%
Run 01:
Highest Train: 85.35
Highest Valid: 85.19
  Final Train: 85.35
   Final Test: 85.42
All runs:
Highest Train: 85.35, nan
Highest Valid: 85.19, nan
  Final Train: 85.35, nan
   Final Test: 85.42, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.5347, Train: 87.21%, Valid: 87.07%, Test: 87.23%
Epoch: 25, Loss: 0.4637, Train: 87.29%, Valid: 87.37%, Test: 87.32%
Epoch: 50, Loss: 0.3622, Train: 86.68%, Valid: 86.68%, Test: 86.72%
Epoch: 75, Loss: 0.3568, Train: 86.30%, Valid: 86.28%, Test: 86.40%
Epoch: 100, Loss: 0.3485, Train: 86.68%, Valid: 86.69%, Test: 86.72%
Epoch: 125, Loss: 0.3430, Train: 86.18%, Valid: 86.15%, Test: 86.24%
Epoch: 150, Loss: 0.3447, Train: 85.43%, Valid: 85.45%, Test: 85.47%
Epoch: 175, Loss: 0.3344, Train: 85.48%, Valid: 85.50%, Test: 85.58%
Epoch: 200, Loss: 0.3298, Train: 85.47%, Valid: 85.46%, Test: 85.58%
Epoch: 225, Loss: 1.1795, Train: 85.38%, Valid: 85.39%, Test: 85.46%
Epoch: 250, Loss: 1.1342, Train: 85.36%, Valid: 85.38%, Test: 85.44%
Epoch: 275, Loss: 0.5749, Train: 85.37%, Valid: 85.38%, Test: 85.45%
Epoch: 300, Loss: 0.5604, Train: 85.37%, Valid: 85.40%, Test: 85.45%
Epoch: 325, Loss: 0.3831, Train: 85.48%, Valid: 85.51%, Test: 85.57%
Epoch: 350, Loss: 0.3399, Train: 85.85%, Valid: 85.65%, Test: 85.92%
Epoch: 375, Loss: 0.3313, Train: 85.38%, Valid: 85.40%, Test: 85.46%
Epoch: 400, Loss: 0.3272, Train: 85.92%, Valid: 85.71%, Test: 86.00%
Epoch: 425, Loss: 0.3285, Train: 86.50%, Valid: 86.38%, Test: 86.54%
Epoch: 450, Loss: 0.3259, Train: 85.72%, Valid: 85.71%, Test: 85.78%
Epoch: 475, Loss: 0.3593, Train: 85.61%, Valid: 85.64%, Test: 85.69%
Run 01:
Highest Train: 88.30
Highest Valid: 88.36
  Final Train: 88.30
   Final Test: 88.32
All runs:
Highest Train: 88.30, nan
Highest Valid: 88.36, nan
  Final Train: 88.30, nan
   Final Test: 88.32, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 6.1629, Train: 85.34%, Valid: 85.18%, Test: 85.49%
Epoch: 25, Loss: 0.6444, Train: 86.80%, Valid: 86.78%, Test: 86.79%
Epoch: 50, Loss: 0.4500, Train: 86.90%, Valid: 86.93%, Test: 86.99%
Epoch: 75, Loss: 0.3609, Train: 85.80%, Valid: 85.70%, Test: 85.89%
Epoch: 100, Loss: 0.3546, Train: 85.68%, Valid: 85.51%, Test: 85.78%
Epoch: 125, Loss: 0.3460, Train: 85.51%, Valid: 85.33%, Test: 85.63%
Epoch: 150, Loss: 0.3381, Train: 85.44%, Valid: 85.29%, Test: 85.56%
Epoch: 175, Loss: 0.3393, Train: 85.85%, Valid: 85.71%, Test: 85.96%
Epoch: 200, Loss: 0.3325, Train: 85.50%, Valid: 85.35%, Test: 85.63%
Epoch: 225, Loss: 0.3286, Train: 85.72%, Valid: 85.55%, Test: 85.84%
Epoch: 250, Loss: 1.0070, Train: 85.86%, Valid: 85.72%, Test: 85.99%
Epoch: 275, Loss: 2.2284, Train: 85.90%, Valid: 85.77%, Test: 86.01%
Epoch: 300, Loss: 2.1375, Train: 85.89%, Valid: 85.75%, Test: 86.01%
Epoch: 325, Loss: 1.8129, Train: 85.92%, Valid: 85.78%, Test: 86.04%
Epoch: 350, Loss: 1.2211, Train: 85.95%, Valid: 85.81%, Test: 86.06%
Epoch: 375, Loss: 0.4079, Train: 85.87%, Valid: 85.74%, Test: 85.98%
Epoch: 400, Loss: 0.8238, Train: 85.99%, Valid: 85.86%, Test: 86.10%
Epoch: 425, Loss: 0.7068, Train: 85.99%, Valid: 85.86%, Test: 86.11%
Epoch: 450, Loss: 0.3965, Train: 85.97%, Valid: 85.82%, Test: 86.07%
Epoch: 475, Loss: 0.3406, Train: 86.16%, Valid: 86.00%, Test: 86.29%
Run 01:
Highest Train: 87.24
Highest Valid: 87.25
  Final Train: 87.22
   Final Test: 87.32
All runs:
Highest Train: 87.24, nan
Highest Valid: 87.25, nan
  Final Train: 87.22, nan
   Final Test: 87.32, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 17.3401, Train: 83.77%, Valid: 83.67%, Test: 83.85%
Epoch: 25, Loss: 1.0806, Train: 85.92%, Valid: 85.82%, Test: 85.94%
Epoch: 50, Loss: 0.7891, Train: 85.88%, Valid: 85.78%, Test: 85.92%
Epoch: 75, Loss: 0.6099, Train: 85.71%, Valid: 85.59%, Test: 85.74%
Epoch: 100, Loss: 0.4868, Train: 85.30%, Valid: 85.18%, Test: 85.35%
Epoch: 125, Loss: 0.4646, Train: 85.19%, Valid: 85.06%, Test: 85.25%
Epoch: 150, Loss: 0.4444, Train: 85.12%, Valid: 85.00%, Test: 85.20%
Epoch: 175, Loss: 0.4227, Train: 85.16%, Valid: 85.02%, Test: 85.24%
Epoch: 200, Loss: 0.3936, Train: 85.22%, Valid: 85.07%, Test: 85.30%
Epoch: 225, Loss: 0.3707, Train: 85.34%, Valid: 85.19%, Test: 85.43%
Epoch: 250, Loss: 0.3631, Train: 85.32%, Valid: 85.17%, Test: 85.40%
Epoch: 275, Loss: 0.3587, Train: 85.28%, Valid: 85.13%, Test: 85.36%
Epoch: 300, Loss: 0.3552, Train: 85.16%, Valid: 85.01%, Test: 85.25%
Epoch: 325, Loss: 0.3522, Train: 84.98%, Valid: 84.82%, Test: 85.08%
Epoch: 350, Loss: 0.7089, Train: 85.33%, Valid: 85.18%, Test: 85.42%
Epoch: 375, Loss: 0.4873, Train: 85.32%, Valid: 85.16%, Test: 85.38%
Epoch: 400, Loss: 0.3596, Train: 85.26%, Valid: 85.10%, Test: 85.34%
Epoch: 425, Loss: 0.3533, Train: 85.11%, Valid: 84.95%, Test: 85.20%
Epoch: 450, Loss: 0.3507, Train: 85.02%, Valid: 84.86%, Test: 85.12%
Epoch: 475, Loss: 0.3489, Train: 85.00%, Valid: 84.83%, Test: 85.09%
Run 01:
Highest Train: 86.33
Highest Valid: 86.23
  Final Train: 86.30
   Final Test: 86.40
All runs:
Highest Train: 86.33, nan
Highest Valid: 86.23, nan
  Final Train: 86.30, nan
   Final Test: 86.40, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 3.2073, Train: 84.84%, Valid: 84.76%, Test: 84.95%
Epoch: 25, Loss: 0.5449, Train: 86.48%, Valid: 86.51%, Test: 86.53%
Epoch: 50, Loss: 0.3900, Train: 85.82%, Valid: 85.88%, Test: 85.90%
Epoch: 75, Loss: 0.3550, Train: 85.83%, Valid: 85.85%, Test: 85.92%
Epoch: 100, Loss: 0.3461, Train: 85.52%, Valid: 85.51%, Test: 85.64%
Epoch: 125, Loss: 0.3343, Train: 85.53%, Valid: 85.56%, Test: 85.67%
Epoch: 150, Loss: 0.7198, Train: 85.68%, Valid: 85.71%, Test: 85.80%
Epoch: 175, Loss: 0.4806, Train: 85.67%, Valid: 85.72%, Test: 85.75%
Epoch: 200, Loss: 0.3413, Train: 86.39%, Valid: 86.19%, Test: 86.51%
Epoch: 225, Loss: 0.3361, Train: 87.53%, Valid: 87.33%, Test: 87.52%
Epoch: 250, Loss: 0.3308, Train: 85.78%, Valid: 85.74%, Test: 85.88%
Epoch: 275, Loss: 0.3275, Train: 86.79%, Valid: 86.60%, Test: 86.78%
Epoch: 300, Loss: 0.4232, Train: 85.65%, Valid: 85.69%, Test: 85.74%
Epoch: 325, Loss: 0.9965, Train: 85.67%, Valid: 85.71%, Test: 85.77%
Epoch: 350, Loss: 0.7419, Train: 85.68%, Valid: 85.71%, Test: 85.77%
Epoch: 375, Loss: 0.5248, Train: 85.69%, Valid: 85.73%, Test: 85.78%
Epoch: 400, Loss: 0.3565, Train: 86.01%, Valid: 86.03%, Test: 86.10%
Epoch: 425, Loss: 0.3345, Train: 86.06%, Valid: 86.06%, Test: 86.15%
Epoch: 450, Loss: 0.3940, Train: 87.53%, Valid: 87.37%, Test: 87.55%
Epoch: 475, Loss: 0.3400, Train: 85.83%, Valid: 85.85%, Test: 85.90%
Run 01:
Highest Train: 88.60
Highest Valid: 88.65
  Final Train: 88.60
   Final Test: 88.67
All runs:
Highest Train: 88.60, nan
Highest Valid: 88.65, nan
  Final Train: 88.60, nan
   Final Test: 88.67, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 3.5051, Train: 85.78%, Valid: 85.63%, Test: 85.84%
Epoch: 25, Loss: 0.6728, Train: 85.62%, Valid: 85.49%, Test: 85.72%
Epoch: 50, Loss: 0.6831, Train: 86.35%, Valid: 86.31%, Test: 86.41%
Epoch: 75, Loss: 0.5812, Train: 86.19%, Valid: 86.14%, Test: 86.26%
Epoch: 100, Loss: 0.3831, Train: 85.79%, Valid: 85.68%, Test: 85.88%
Epoch: 125, Loss: 0.3583, Train: 85.82%, Valid: 85.68%, Test: 85.91%
Epoch: 150, Loss: 0.3527, Train: 85.75%, Valid: 85.60%, Test: 85.85%
Epoch: 175, Loss: 0.3452, Train: 85.62%, Valid: 85.43%, Test: 85.72%
Epoch: 200, Loss: 0.3387, Train: 85.46%, Valid: 85.29%, Test: 85.58%
Epoch: 225, Loss: 0.4084, Train: 85.74%, Valid: 85.58%, Test: 85.83%
Epoch: 250, Loss: 0.3457, Train: 85.91%, Valid: 85.76%, Test: 86.03%
Epoch: 275, Loss: 0.3392, Train: 85.96%, Valid: 85.78%, Test: 86.08%
Epoch: 300, Loss: 0.3330, Train: 85.96%, Valid: 85.77%, Test: 86.07%
Epoch: 325, Loss: 0.3291, Train: 85.97%, Valid: 85.77%, Test: 86.09%
Epoch: 350, Loss: 0.5118, Train: 85.91%, Valid: 85.79%, Test: 86.06%
Epoch: 375, Loss: 0.5177, Train: 86.51%, Valid: 86.39%, Test: 86.61%
Epoch: 400, Loss: 0.4816, Train: 87.04%, Valid: 86.92%, Test: 87.13%
Epoch: 425, Loss: 0.3475, Train: 87.37%, Valid: 87.21%, Test: 87.45%
Epoch: 450, Loss: 0.3386, Train: 86.25%, Valid: 86.08%, Test: 86.37%
Epoch: 475, Loss: 0.3370, Train: 86.18%, Valid: 86.00%, Test: 86.31%
Run 01:
Highest Train: 87.58
Highest Valid: 87.42
  Final Train: 87.58
   Final Test: 87.66
All runs:
Highest Train: 87.58, nan
Highest Valid: 87.42, nan
  Final Train: 87.58, nan
   Final Test: 87.66, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 12.0773, Train: 81.48%, Valid: 81.55%, Test: 81.79%
Epoch: 25, Loss: 0.7928, Train: 85.27%, Valid: 85.12%, Test: 85.32%
Epoch: 50, Loss: 0.7368, Train: 85.21%, Valid: 85.05%, Test: 85.25%
Epoch: 75, Loss: 0.6623, Train: 85.11%, Valid: 84.95%, Test: 85.16%
Epoch: 100, Loss: 0.7220, Train: 85.06%, Valid: 84.89%, Test: 85.11%
Epoch: 125, Loss: 0.7606, Train: 85.17%, Valid: 85.00%, Test: 85.24%
Epoch: 150, Loss: 0.6432, Train: 85.09%, Valid: 84.91%, Test: 85.14%
Epoch: 175, Loss: 0.8468, Train: 85.13%, Valid: 84.96%, Test: 85.19%
Epoch: 200, Loss: 0.6233, Train: 85.15%, Valid: 84.97%, Test: 85.20%
Epoch: 225, Loss: 0.5489, Train: 84.98%, Valid: 84.80%, Test: 85.03%
Epoch: 250, Loss: 0.5966, Train: 85.03%, Valid: 84.86%, Test: 85.09%
Epoch: 275, Loss: 0.5588, Train: 85.06%, Valid: 84.88%, Test: 85.11%
Epoch: 300, Loss: 0.5016, Train: 84.93%, Valid: 84.75%, Test: 85.00%
Epoch: 325, Loss: 0.7103, Train: 85.03%, Valid: 84.86%, Test: 85.11%
Epoch: 350, Loss: 0.5900, Train: 85.11%, Valid: 84.92%, Test: 85.17%
Epoch: 375, Loss: 0.5034, Train: 84.98%, Valid: 84.81%, Test: 85.05%
Epoch: 400, Loss: 0.4721, Train: 84.90%, Valid: 84.72%, Test: 84.96%
Epoch: 425, Loss: 0.4959, Train: 85.02%, Valid: 84.84%, Test: 85.09%
Epoch: 450, Loss: 0.4714, Train: 84.96%, Valid: 84.78%, Test: 85.04%
Epoch: 475, Loss: 0.4730, Train: 84.88%, Valid: 84.70%, Test: 84.96%
Run 01:
Highest Train: 85.56
Highest Valid: 85.46
  Final Train: 85.56
   Final Test: 85.63
All runs:
Highest Train: 85.56, nan
Highest Valid: 85.46, nan
  Final Train: 85.56, nan
   Final Test: 85.63, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.7555, Train: 84.86%, Valid: 84.70%, Test: 84.97%
Epoch: 25, Loss: 0.6524, Train: 86.67%, Valid: 86.70%, Test: 86.70%
Epoch: 50, Loss: 0.6613, Train: 86.49%, Valid: 86.53%, Test: 86.55%
Epoch: 75, Loss: 0.5336, Train: 86.41%, Valid: 86.42%, Test: 86.42%
Epoch: 100, Loss: 0.3675, Train: 87.22%, Valid: 87.08%, Test: 87.25%
Epoch: 125, Loss: 0.3581, Train: 85.88%, Valid: 85.91%, Test: 85.90%
Epoch: 150, Loss: 0.3510, Train: 86.91%, Valid: 86.95%, Test: 87.06%
Epoch: 175, Loss: 0.3423, Train: 86.35%, Valid: 86.41%, Test: 86.50%
Epoch: 200, Loss: 0.3350, Train: 85.11%, Valid: 85.08%, Test: 85.19%
Epoch: 225, Loss: 0.3452, Train: 85.69%, Valid: 85.73%, Test: 85.79%
Epoch: 250, Loss: 0.3299, Train: 85.65%, Valid: 85.67%, Test: 85.77%
Epoch: 275, Loss: 0.3268, Train: 85.68%, Valid: 85.70%, Test: 85.83%
Epoch: 300, Loss: 0.3251, Train: 85.90%, Valid: 85.88%, Test: 85.98%
Epoch: 325, Loss: 0.3369, Train: 83.95%, Valid: 83.95%, Test: 84.10%
Epoch: 350, Loss: 2.6939, Train: 85.72%, Valid: 85.81%, Test: 85.82%
Epoch: 375, Loss: 115.9736, Train: 85.44%, Valid: 85.49%, Test: 85.54%
Epoch: 400, Loss: 124.5213, Train: 85.57%, Valid: 85.63%, Test: 85.71%
Epoch: 425, Loss: 100.2591, Train: 85.57%, Valid: 85.64%, Test: 85.69%
Epoch: 450, Loss: 1.5976, Train: 85.31%, Valid: 85.31%, Test: 85.43%
Epoch: 475, Loss: 2.0511, Train: 85.62%, Valid: 85.65%, Test: 85.69%
Run 01:
Highest Train: 88.21
Highest Valid: 88.27
  Final Train: 88.21
   Final Test: 88.26
All runs:
Highest Train: 88.21, nan
Highest Valid: 88.27, nan
  Final Train: 88.21, nan
   Final Test: 88.26, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 3.4397, Train: 85.48%, Valid: 85.33%, Test: 85.51%
Epoch: 25, Loss: 0.6324, Train: 85.66%, Valid: 85.68%, Test: 85.80%
Epoch: 50, Loss: 0.5488, Train: 87.66%, Valid: 87.56%, Test: 87.69%
Epoch: 75, Loss: 0.4481, Train: 87.57%, Valid: 87.50%, Test: 87.58%
Epoch: 100, Loss: 0.3810, Train: 86.58%, Valid: 86.47%, Test: 86.64%
Epoch: 125, Loss: 0.3537, Train: 85.70%, Valid: 85.51%, Test: 85.84%
Epoch: 150, Loss: 0.3464, Train: 85.87%, Valid: 85.68%, Test: 85.99%
Epoch: 175, Loss: 0.3354, Train: 85.80%, Valid: 85.61%, Test: 85.92%
Epoch: 200, Loss: 0.3313, Train: 85.94%, Valid: 85.78%, Test: 86.07%
Epoch: 225, Loss: 0.3404, Train: 86.09%, Valid: 85.92%, Test: 86.21%
Epoch: 250, Loss: 0.3650, Train: 85.75%, Valid: 85.59%, Test: 85.87%
Epoch: 275, Loss: 0.4653, Train: 85.84%, Valid: 85.72%, Test: 85.93%
Epoch: 300, Loss: 0.3490, Train: 86.17%, Valid: 86.01%, Test: 86.27%
Epoch: 325, Loss: 0.3388, Train: 86.13%, Valid: 85.93%, Test: 86.25%
Epoch: 350, Loss: 0.3336, Train: 86.17%, Valid: 85.99%, Test: 86.29%
Epoch: 375, Loss: 0.3284, Train: 86.16%, Valid: 85.99%, Test: 86.28%
Epoch: 400, Loss: 0.3257, Train: 86.14%, Valid: 85.97%, Test: 86.25%
Epoch: 425, Loss: 0.8511, Train: 85.91%, Valid: 85.78%, Test: 86.04%
Epoch: 450, Loss: 0.7643, Train: 85.92%, Valid: 85.78%, Test: 86.03%
Epoch: 475, Loss: 0.5577, Train: 85.93%, Valid: 85.78%, Test: 86.04%
Run 01:
Highest Train: 87.76
Highest Valid: 87.66
  Final Train: 87.76
   Final Test: 87.77
All runs:
Highest Train: 87.76, nan
Highest Valid: 87.66, nan
  Final Train: 87.76, nan
   Final Test: 87.77, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.5, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 4.3146, Train: 84.21%, Valid: 84.05%, Test: 84.23%
Epoch: 25, Loss: 0.7178, Train: 85.43%, Valid: 85.26%, Test: 85.50%
Epoch: 50, Loss: 0.6746, Train: 85.27%, Valid: 85.10%, Test: 85.33%
Epoch: 75, Loss: 0.6280, Train: 85.14%, Valid: 84.98%, Test: 85.19%
Epoch: 100, Loss: 0.6134, Train: 85.13%, Valid: 84.96%, Test: 85.18%
Epoch: 125, Loss: 0.6628, Train: 85.11%, Valid: 84.94%, Test: 85.17%
Epoch: 150, Loss: 0.5907, Train: 85.15%, Valid: 84.98%, Test: 85.21%
Epoch: 175, Loss: 0.7008, Train: 85.19%, Valid: 85.00%, Test: 85.25%
Epoch: 200, Loss: 0.5926, Train: 85.20%, Valid: 85.02%, Test: 85.26%
Epoch: 225, Loss: 0.5171, Train: 85.03%, Valid: 84.85%, Test: 85.11%
Epoch: 250, Loss: 0.8270, Train: 85.25%, Valid: 85.07%, Test: 85.33%
Epoch: 275, Loss: 0.8194, Train: 85.33%, Valid: 85.15%, Test: 85.39%
Epoch: 300, Loss: 0.5464, Train: 85.21%, Valid: 85.04%, Test: 85.27%
Epoch: 325, Loss: 0.4827, Train: 85.12%, Valid: 84.94%, Test: 85.18%
Epoch: 350, Loss: 0.4784, Train: 85.18%, Valid: 84.99%, Test: 85.24%
Epoch: 375, Loss: 0.3956, Train: 85.32%, Valid: 85.13%, Test: 85.38%
Epoch: 400, Loss: 0.3636, Train: 85.47%, Valid: 85.28%, Test: 85.54%
Epoch: 425, Loss: 0.3555, Train: 85.54%, Valid: 85.35%, Test: 85.61%
Epoch: 450, Loss: 0.3512, Train: 85.57%, Valid: 85.37%, Test: 85.63%
Epoch: 475, Loss: 0.3480, Train: 85.56%, Valid: 85.37%, Test: 85.64%
Run 01:
Highest Train: 85.58
Highest Valid: 85.38
  Final Train: 85.57
   Final Test: 85.64
All runs:
Highest Train: 85.58, nan
Highest Valid: 85.38, nan
  Final Train: 85.57, nan
   Final Test: 85.64, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 14.0543, Train: 84.07%, Valid: 83.87%, Test: 84.17%
Epoch: 25, Loss: 0.9070, Train: 86.03%, Valid: 86.03%, Test: 86.12%
Epoch: 50, Loss: 0.7193, Train: 85.92%, Valid: 85.96%, Test: 86.01%
Epoch: 75, Loss: 0.3604, Train: 86.56%, Valid: 86.63%, Test: 86.67%
Epoch: 100, Loss: 0.3463, Train: 85.58%, Valid: 85.56%, Test: 85.66%
Epoch: 125, Loss: 0.3409, Train: 85.37%, Valid: 85.40%, Test: 85.48%
Epoch: 150, Loss: 0.3325, Train: 85.75%, Valid: 85.71%, Test: 85.83%
Epoch: 175, Loss: 1.2290, Train: 85.38%, Valid: 85.40%, Test: 85.47%
Epoch: 200, Loss: 1.9208, Train: 85.41%, Valid: 85.44%, Test: 85.49%
Epoch: 225, Loss: 1.5880, Train: 85.37%, Valid: 85.39%, Test: 85.46%
Epoch: 250, Loss: 0.9863, Train: 85.35%, Valid: 85.37%, Test: 85.43%
Epoch: 275, Loss: 0.7438, Train: 85.35%, Valid: 85.36%, Test: 85.43%
Epoch: 300, Loss: 0.3443, Train: 85.60%, Valid: 85.57%, Test: 85.68%
Epoch: 325, Loss: 0.3344, Train: 85.78%, Valid: 85.74%, Test: 85.85%
Epoch: 350, Loss: 0.8062, Train: 85.60%, Valid: 85.58%, Test: 85.68%
Epoch: 375, Loss: 2.4740, Train: 85.25%, Valid: 85.29%, Test: 85.33%
Epoch: 400, Loss: 1.8266, Train: 85.36%, Valid: 85.39%, Test: 85.44%
Epoch: 425, Loss: 0.7588, Train: 85.41%, Valid: 85.41%, Test: 85.51%
Epoch: 450, Loss: 1.5384, Train: 85.38%, Valid: 85.41%, Test: 85.47%
Epoch: 475, Loss: 1.3639, Train: 85.39%, Valid: 85.41%, Test: 85.47%
Run 01:
Highest Train: 87.41
Highest Valid: 87.28
  Final Train: 87.41
   Final Test: 87.39
All runs:
Highest Train: 87.41, nan
Highest Valid: 87.28, nan
  Final Train: 87.41, nan
   Final Test: 87.39, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.4807, Train: 87.09%, Valid: 87.05%, Test: 87.09%
Epoch: 25, Loss: 0.3908, Train: 85.51%, Valid: 85.38%, Test: 85.63%
Epoch: 50, Loss: 0.3685, Train: 85.69%, Valid: 85.54%, Test: 85.79%
Epoch: 75, Loss: 0.3597, Train: 85.72%, Valid: 85.55%, Test: 85.84%
Epoch: 100, Loss: 0.3501, Train: 85.70%, Valid: 85.53%, Test: 85.83%
Epoch: 125, Loss: 0.3424, Train: 85.42%, Valid: 85.26%, Test: 85.54%
Epoch: 150, Loss: 0.3366, Train: 85.36%, Valid: 85.20%, Test: 85.49%
Epoch: 175, Loss: 1.5176, Train: 85.73%, Valid: 85.62%, Test: 85.84%
Epoch: 200, Loss: 1.6397, Train: 85.98%, Valid: 85.94%, Test: 86.00%
Epoch: 225, Loss: 0.9681, Train: 87.38%, Valid: 87.38%, Test: 87.38%
Epoch: 250, Loss: 0.3569, Train: 85.87%, Valid: 85.76%, Test: 85.94%
Epoch: 275, Loss: 0.3484, Train: 85.96%, Valid: 85.81%, Test: 86.09%
Epoch: 300, Loss: 0.3384, Train: 85.99%, Valid: 85.79%, Test: 86.10%
Epoch: 325, Loss: 0.3318, Train: 85.73%, Valid: 85.54%, Test: 85.86%
Epoch: 350, Loss: 0.5275, Train: 86.66%, Valid: 86.57%, Test: 86.74%
Epoch: 375, Loss: 1.1231, Train: 87.24%, Valid: 87.14%, Test: 87.32%
Epoch: 400, Loss: 0.4083, Train: 87.01%, Valid: 86.90%, Test: 87.11%
Epoch: 425, Loss: 0.9556, Train: 87.00%, Valid: 86.90%, Test: 87.09%
Epoch: 450, Loss: 0.7894, Train: 87.25%, Valid: 87.14%, Test: 87.33%
Epoch: 475, Loss: 0.5679, Train: 86.98%, Valid: 86.93%, Test: 87.07%
Run 01:
Highest Train: 87.46
Highest Valid: 87.40
  Final Train: 87.41
   Final Test: 87.43
All runs:
Highest Train: 87.46, nan
Highest Valid: 87.40, nan
  Final Train: 87.41, nan
   Final Test: 87.43, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 110.9186, Train: 78.82%, Valid: 78.76%, Test: 79.03%
Epoch: 25, Loss: 0.7944, Train: 86.97%, Valid: 86.77%, Test: 86.89%
Epoch: 50, Loss: 0.6582, Train: 86.54%, Valid: 86.37%, Test: 86.50%
Epoch: 75, Loss: 0.5936, Train: 85.43%, Valid: 85.33%, Test: 85.44%
Epoch: 100, Loss: 0.5026, Train: 84.90%, Valid: 84.81%, Test: 84.93%
Epoch: 125, Loss: 0.4502, Train: 84.79%, Valid: 84.67%, Test: 84.83%
Epoch: 150, Loss: 0.4323, Train: 85.04%, Valid: 84.90%, Test: 85.08%
Epoch: 175, Loss: 0.4264, Train: 85.20%, Valid: 85.05%, Test: 85.26%
Epoch: 200, Loss: 0.4239, Train: 85.21%, Valid: 85.07%, Test: 85.28%
Epoch: 225, Loss: 0.4223, Train: 85.22%, Valid: 85.07%, Test: 85.28%
Epoch: 250, Loss: 0.4211, Train: 85.21%, Valid: 85.06%, Test: 85.27%
Epoch: 275, Loss: 0.4199, Train: 85.21%, Valid: 85.07%, Test: 85.28%
Epoch: 300, Loss: 0.4188, Train: 85.21%, Valid: 85.06%, Test: 85.27%
Epoch: 325, Loss: 0.4175, Train: 85.22%, Valid: 85.07%, Test: 85.28%
Epoch: 350, Loss: 0.4163, Train: 85.20%, Valid: 85.06%, Test: 85.27%
Epoch: 375, Loss: 0.4149, Train: 85.20%, Valid: 85.06%, Test: 85.27%
Epoch: 400, Loss: 0.4134, Train: 85.19%, Valid: 85.04%, Test: 85.25%
Epoch: 425, Loss: 0.4118, Train: 85.19%, Valid: 85.04%, Test: 85.25%
Epoch: 450, Loss: 0.4100, Train: 85.19%, Valid: 85.04%, Test: 85.25%
Epoch: 475, Loss: 0.4079, Train: 85.19%, Valid: 85.03%, Test: 85.25%
Run 01:
Highest Train: 86.99
Highest Valid: 86.80
  Final Train: 86.99
   Final Test: 86.94
All runs:
Highest Train: 86.99, nan
Highest Valid: 86.80, nan
  Final Train: 86.99, nan
   Final Test: 86.94, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.4496, Train: 86.74%, Valid: 86.66%, Test: 86.74%
Epoch: 25, Loss: 0.5089, Train: 88.16%, Valid: 88.25%, Test: 88.20%
Epoch: 50, Loss: 0.3617, Train: 86.72%, Valid: 86.75%, Test: 86.72%
Epoch: 75, Loss: 0.3549, Train: 85.24%, Valid: 85.25%, Test: 85.33%
Epoch: 100, Loss: 0.3477, Train: 85.55%, Valid: 85.48%, Test: 85.59%
Epoch: 125, Loss: 0.3382, Train: 85.32%, Valid: 85.36%, Test: 85.46%
Epoch: 150, Loss: 1.1446, Train: 85.78%, Valid: 85.78%, Test: 85.89%
Epoch: 175, Loss: 0.5631, Train: 85.56%, Valid: 85.58%, Test: 85.68%
Epoch: 200, Loss: 0.4214, Train: 85.79%, Valid: 85.81%, Test: 85.88%
Epoch: 225, Loss: 0.3392, Train: 86.01%, Valid: 86.00%, Test: 86.12%
Epoch: 250, Loss: 0.3332, Train: 85.97%, Valid: 85.96%, Test: 86.10%
Epoch: 275, Loss: 0.3296, Train: 85.99%, Valid: 85.97%, Test: 86.11%
Epoch: 300, Loss: 1.5280, Train: 85.40%, Valid: 85.41%, Test: 85.53%
Epoch: 325, Loss: 2.2334, Train: 85.73%, Valid: 85.77%, Test: 85.83%
Epoch: 350, Loss: 0.8554, Train: 85.48%, Valid: 85.53%, Test: 85.62%
Epoch: 375, Loss: 0.4065, Train: 85.80%, Valid: 85.83%, Test: 85.81%
Epoch: 400, Loss: 0.3380, Train: 86.13%, Valid: 86.12%, Test: 86.21%
Epoch: 425, Loss: 0.3315, Train: 85.99%, Valid: 85.97%, Test: 86.11%
Epoch: 450, Loss: 1.6446, Train: 85.62%, Valid: 85.64%, Test: 85.74%
Epoch: 475, Loss: 3.2367, Train: 85.60%, Valid: 85.66%, Test: 85.70%
Run 01:
Highest Train: 88.16
Highest Valid: 88.25
  Final Train: 88.16
   Final Test: 88.20
All runs:
Highest Train: 88.16, nan
Highest Valid: 88.25, nan
  Final Train: 88.16, nan
   Final Test: 88.20, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.5439, Train: 85.35%, Valid: 85.30%, Test: 85.38%
Epoch: 25, Loss: 0.8104, Train: 86.16%, Valid: 86.05%, Test: 86.27%
Epoch: 50, Loss: 0.4455, Train: 86.02%, Valid: 85.96%, Test: 86.10%
Epoch: 75, Loss: 0.3718, Train: 85.68%, Valid: 85.52%, Test: 85.78%
Epoch: 100, Loss: 0.3590, Train: 85.83%, Valid: 85.64%, Test: 85.93%
Epoch: 125, Loss: 0.3513, Train: 85.78%, Valid: 85.64%, Test: 85.88%
Epoch: 150, Loss: 0.3427, Train: 85.62%, Valid: 85.48%, Test: 85.74%
Epoch: 175, Loss: 0.3435, Train: 85.73%, Valid: 85.56%, Test: 85.84%
Epoch: 200, Loss: 0.3343, Train: 85.67%, Valid: 85.51%, Test: 85.78%
Epoch: 225, Loss: 1.1716, Train: 85.82%, Valid: 85.67%, Test: 85.95%
Epoch: 250, Loss: 0.4504, Train: 85.61%, Valid: 85.47%, Test: 85.75%
Epoch: 275, Loss: 0.9881, Train: 85.91%, Valid: 85.76%, Test: 86.04%
Epoch: 300, Loss: 0.4195, Train: 87.00%, Valid: 86.85%, Test: 87.11%
Epoch: 325, Loss: 0.7480, Train: 86.95%, Valid: 86.83%, Test: 87.06%
Epoch: 350, Loss: 0.3458, Train: 85.51%, Valid: 85.36%, Test: 85.64%
Epoch: 375, Loss: 0.4093, Train: 86.25%, Valid: 86.13%, Test: 86.39%
Epoch: 400, Loss: 0.3412, Train: 85.96%, Valid: 85.80%, Test: 86.11%
Epoch: 425, Loss: 0.3346, Train: 86.08%, Valid: 85.92%, Test: 86.21%
Epoch: 450, Loss: 1.4418, Train: 85.76%, Valid: 85.65%, Test: 85.88%
Epoch: 475, Loss: 1.3438, Train: 85.80%, Valid: 85.69%, Test: 85.90%
Run 01:
Highest Train: 87.32
Highest Valid: 87.21
  Final Train: 87.32
   Final Test: 87.42
All runs:
Highest Train: 87.32, nan
Highest Valid: 87.21, nan
  Final Train: 87.32, nan
   Final Test: 87.42, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 3.8756, Train: 85.13%, Valid: 85.02%, Test: 85.20%
Epoch: 25, Loss: 1.2237, Train: 85.39%, Valid: 85.24%, Test: 85.45%
Epoch: 50, Loss: 1.0937, Train: 85.35%, Valid: 85.23%, Test: 85.40%
Epoch: 75, Loss: 0.8975, Train: 85.16%, Valid: 85.02%, Test: 85.20%
Epoch: 100, Loss: 0.9171, Train: 85.09%, Valid: 84.94%, Test: 85.14%
Epoch: 125, Loss: 0.8378, Train: 85.04%, Valid: 84.88%, Test: 85.09%
Epoch: 150, Loss: 0.8312, Train: 85.12%, Valid: 84.94%, Test: 85.18%
Epoch: 175, Loss: 0.8052, Train: 85.09%, Valid: 84.90%, Test: 85.15%
Epoch: 200, Loss: 0.7133, Train: 84.95%, Valid: 84.77%, Test: 85.03%
Epoch: 225, Loss: 0.9267, Train: 85.19%, Valid: 85.02%, Test: 85.25%
Epoch: 250, Loss: 0.7955, Train: 85.09%, Valid: 84.91%, Test: 85.15%
Epoch: 275, Loss: 0.7900, Train: 85.10%, Valid: 84.91%, Test: 85.16%
Epoch: 300, Loss: 1.0179, Train: 85.08%, Valid: 84.89%, Test: 85.15%
Epoch: 325, Loss: 0.6925, Train: 85.07%, Valid: 84.88%, Test: 85.14%
Epoch: 350, Loss: 0.6437, Train: 84.99%, Valid: 84.80%, Test: 85.06%
Epoch: 375, Loss: 0.7551, Train: 85.14%, Valid: 84.96%, Test: 85.23%
Epoch: 400, Loss: 1.0014, Train: 85.18%, Valid: 84.99%, Test: 85.26%
Epoch: 425, Loss: 0.7209, Train: 85.10%, Valid: 84.91%, Test: 85.17%
Epoch: 450, Loss: 0.5882, Train: 85.02%, Valid: 84.82%, Test: 85.10%
Epoch: 475, Loss: 0.6780, Train: 85.12%, Valid: 84.93%, Test: 85.20%
Run 01:
Highest Train: 86.35
Highest Valid: 86.25
  Final Train: 86.35
   Final Test: 86.40
All runs:
Highest Train: 86.35, nan
Highest Valid: 86.25, nan
  Final Train: 86.35, nan
   Final Test: 86.40, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 5.7897, Train: 86.78%, Valid: 86.79%, Test: 86.78%
Epoch: 25, Loss: 0.7552, Train: 86.70%, Valid: 86.72%, Test: 86.75%
Epoch: 50, Loss: 0.4135, Train: 87.85%, Valid: 87.91%, Test: 87.89%
Epoch: 75, Loss: 0.3566, Train: 85.69%, Valid: 85.67%, Test: 85.81%
Epoch: 100, Loss: 0.3467, Train: 85.87%, Valid: 85.83%, Test: 85.94%
Epoch: 125, Loss: 0.3416, Train: 87.32%, Valid: 87.26%, Test: 87.28%
Epoch: 150, Loss: 0.3348, Train: 85.83%, Valid: 85.86%, Test: 85.95%
Epoch: 175, Loss: 1.5780, Train: 85.55%, Valid: 85.55%, Test: 85.68%
Epoch: 200, Loss: 1.5494, Train: 85.02%, Valid: 84.99%, Test: 85.12%
Epoch: 225, Loss: 0.5354, Train: 85.83%, Valid: 85.85%, Test: 85.93%
Epoch: 250, Loss: 0.3455, Train: 85.85%, Valid: 85.88%, Test: 85.98%
Epoch: 275, Loss: 0.3396, Train: 85.86%, Valid: 85.85%, Test: 86.00%
Epoch: 300, Loss: 0.3364, Train: 85.85%, Valid: 85.84%, Test: 85.98%
Epoch: 325, Loss: 0.3310, Train: 85.85%, Valid: 85.85%, Test: 85.98%
Epoch: 350, Loss: 0.3296, Train: 85.73%, Valid: 85.73%, Test: 85.86%
Epoch: 375, Loss: 0.3288, Train: 85.49%, Valid: 85.49%, Test: 85.62%
Epoch: 400, Loss: 0.3304, Train: 85.92%, Valid: 85.84%, Test: 86.01%
Epoch: 425, Loss: 1.1318, Train: 85.28%, Valid: 85.27%, Test: 85.39%
Epoch: 450, Loss: 1.5495, Train: 85.53%, Valid: 85.60%, Test: 85.67%
Epoch: 475, Loss: 1.0102, Train: 85.71%, Valid: 85.75%, Test: 85.80%
Run 01:
Highest Train: 88.04
Highest Valid: 88.12
  Final Train: 88.03
   Final Test: 88.05
All runs:
Highest Train: 88.04, nan
Highest Valid: 88.12, nan
  Final Train: 88.03, nan
   Final Test: 88.05, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 4.4526, Train: 84.32%, Valid: 84.33%, Test: 84.41%
Epoch: 25, Loss: 0.7935, Train: 86.11%, Valid: 86.08%, Test: 86.18%
Epoch: 50, Loss: 0.6686, Train: 86.19%, Valid: 86.16%, Test: 86.24%
Epoch: 75, Loss: 0.4429, Train: 86.28%, Valid: 86.25%, Test: 86.31%
Epoch: 100, Loss: 0.3621, Train: 85.57%, Valid: 85.40%, Test: 85.72%
Epoch: 125, Loss: 0.3550, Train: 85.69%, Valid: 85.51%, Test: 85.81%
Epoch: 150, Loss: 0.3472, Train: 85.56%, Valid: 85.37%, Test: 85.68%
Epoch: 175, Loss: 0.3399, Train: 85.42%, Valid: 85.25%, Test: 85.55%
Epoch: 200, Loss: 0.3344, Train: 85.50%, Valid: 85.35%, Test: 85.62%
Epoch: 225, Loss: 0.3348, Train: 85.60%, Valid: 85.46%, Test: 85.74%
Epoch: 250, Loss: 0.3322, Train: 85.70%, Valid: 85.53%, Test: 85.83%
Epoch: 275, Loss: 0.3368, Train: 85.92%, Valid: 85.74%, Test: 86.04%
Epoch: 300, Loss: 0.3614, Train: 85.86%, Valid: 85.73%, Test: 85.98%
Epoch: 325, Loss: 2.0131, Train: 86.25%, Valid: 86.13%, Test: 86.36%
Epoch: 350, Loss: 1.5042, Train: 86.14%, Valid: 86.12%, Test: 86.22%
Epoch: 375, Loss: 0.9117, Train: 86.57%, Valid: 86.55%, Test: 86.65%
Epoch: 400, Loss: 0.5154, Train: 86.01%, Valid: 86.02%, Test: 86.04%
Epoch: 425, Loss: 0.3477, Train: 85.90%, Valid: 85.73%, Test: 86.03%
Epoch: 450, Loss: 0.3428, Train: 86.10%, Valid: 85.91%, Test: 86.22%
Epoch: 475, Loss: 0.3383, Train: 86.07%, Valid: 85.89%, Test: 86.20%
Run 01:
Highest Train: 86.74
Highest Valid: 86.70
  Final Train: 86.74
   Final Test: 86.72
All runs:
Highest Train: 86.74, nan
Highest Valid: 86.70, nan
  Final Train: 86.74, nan
   Final Test: 86.72, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=500, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 13.5075, Train: 25.83%, Valid: 25.72%, Test: 25.51%
Epoch: 25, Loss: 8.8879, Train: 82.08%, Valid: 82.16%, Test: 82.40%
Epoch: 50, Loss: 4.5533, Train: 85.66%, Valid: 85.49%, Test: 85.74%
Epoch: 75, Loss: 1.4575, Train: 85.58%, Valid: 85.43%, Test: 85.65%
Epoch: 100, Loss: 1.2559, Train: 85.59%, Valid: 85.44%, Test: 85.66%
Epoch: 125, Loss: 1.1948, Train: 85.57%, Valid: 85.41%, Test: 85.64%
Epoch: 150, Loss: 1.1256, Train: 85.53%, Valid: 85.37%, Test: 85.60%
Epoch: 175, Loss: 1.0569, Train: 85.17%, Valid: 85.01%, Test: 85.22%
Epoch: 200, Loss: 1.0011, Train: 85.13%, Valid: 84.96%, Test: 85.17%
Epoch: 225, Loss: 0.9318, Train: 85.04%, Valid: 84.88%, Test: 85.08%
Epoch: 250, Loss: 0.8971, Train: 84.96%, Valid: 84.80%, Test: 85.00%
Epoch: 275, Loss: 0.8789, Train: 84.93%, Valid: 84.77%, Test: 84.97%
Epoch: 300, Loss: 0.8558, Train: 84.86%, Valid: 84.70%, Test: 84.92%
Epoch: 325, Loss: 0.8276, Train: 84.72%, Valid: 84.57%, Test: 84.81%
Epoch: 350, Loss: 1.2156, Train: 84.56%, Valid: 84.39%, Test: 84.59%
Epoch: 375, Loss: 2.3010, Train: 85.07%, Valid: 84.91%, Test: 85.16%
Epoch: 400, Loss: 1.5880, Train: 85.16%, Valid: 85.01%, Test: 85.24%
Epoch: 425, Loss: 1.2735, Train: 85.20%, Valid: 85.04%, Test: 85.27%
Epoch: 450, Loss: 1.0784, Train: 85.19%, Valid: 85.03%, Test: 85.25%
Epoch: 475, Loss: 0.9888, Train: 85.15%, Valid: 84.98%, Test: 85.20%
Run 01:
Highest Train: 85.78
Highest Valid: 85.64
  Final Train: 85.78
   Final Test: 85.80
All runs:
Highest Train: 85.78, nan
Highest Valid: 85.64, nan
  Final Train: 85.78, nan
   Final Test: 85.80, nan
Saving results to results/genius.csv
20211122-23:24 ---> 20211123-12:41 Totl:47808 seconds
