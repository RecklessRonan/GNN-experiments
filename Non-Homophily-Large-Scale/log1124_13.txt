nohup: ignoring input
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip
  0%|          | 0/81 [00:00<?, ?it/s]Downloaded 0.00 GB:   0%|          | 0/81 [00:01<?, ?it/s]Downloaded 0.00 GB:   1%|1         | 1/81 [00:01<01:58,  1.49s/it]Downloaded 0.00 GB:   1%|1         | 1/81 [00:02<01:58,  1.49s/it]Downloaded 0.00 GB:   2%|2         | 2/81 [00:02<01:34,  1.20s/it]Downloaded 0.00 GB:   2%|2         | 2/81 [00:03<01:34,  1.20s/it]Downloaded 0.00 GB:   4%|3         | 3/81 [00:03<01:27,  1.12s/it]Downloaded 0.00 GB:   4%|3         | 3/81 [00:04<01:27,  1.12s/it]Downloaded 0.00 GB:   5%|4         | 4/81 [00:04<01:17,  1.01s/it]Downloaded 0.00 GB:   5%|4         | 4/81 [00:05<01:17,  1.01s/it]Downloaded 0.00 GB:   6%|6         | 5/81 [00:05<01:12,  1.05it/s]Downloaded 0.01 GB:   6%|6         | 5/81 [00:06<01:12,  1.05it/s]Downloaded 0.01 GB:   7%|7         | 6/81 [00:06<01:08,  1.10it/s]Downloaded 0.01 GB:   7%|7         | 6/81 [00:07<01:08,  1.10it/s]Downloaded 0.01 GB:   9%|8         | 7/81 [00:07<01:09,  1.07it/s]Downloaded 0.01 GB:   9%|8         | 7/81 [00:07<01:09,  1.07it/s]Downloaded 0.01 GB:  10%|9         | 8/81 [00:07<01:05,  1.11it/s]Downloaded 0.01 GB:  10%|9         | 8/81 [00:08<01:05,  1.11it/s]Downloaded 0.01 GB:  11%|#1        | 9/81 [00:08<01:03,  1.14it/s]Downloaded 0.01 GB:  11%|#1        | 9/81 [00:09<01:03,  1.14it/s]Downloaded 0.01 GB:  12%|#2        | 10/81 [00:09<01:01,  1.15it/s]Downloaded 0.01 GB:  12%|#2        | 10/81 [00:10<01:01,  1.15it/s]Downloaded 0.01 GB:  14%|#3        | 11/81 [00:10<00:59,  1.18it/s]Downloaded 0.01 GB:  14%|#3        | 11/81 [00:10<00:59,  1.18it/s]Downloaded 0.01 GB:  15%|#4        | 12/81 [00:10<00:54,  1.27it/s]Downloaded 0.01 GB:  15%|#4        | 12/81 [00:11<00:54,  1.27it/s]Downloaded 0.01 GB:  16%|#6        | 13/81 [00:11<00:53,  1.27it/s]Downloaded 0.01 GB:  16%|#6        | 13/81 [00:12<00:53,  1.27it/s]Downloaded 0.01 GB:  17%|#7        | 14/81 [00:12<00:49,  1.35it/s]Downloaded 0.01 GB:  17%|#7        | 14/81 [00:12<00:49,  1.35it/s]Downloaded 0.01 GB:  19%|#8        | 15/81 [00:12<00:43,  1.53it/s]Downloaded 0.02 GB:  19%|#8        | 15/81 [00:13<00:43,  1.53it/s]Downloaded 0.02 GB:  20%|#9        | 16/81 [00:13<00:41,  1.58it/s]Downloaded 0.02 GB:  20%|#9        | 16/81 [00:13<00:41,  1.58it/s]Downloaded 0.02 GB:  21%|##        | 17/81 [00:13<00:36,  1.75it/s]Downloaded 0.02 GB:  21%|##        | 17/81 [00:14<00:36,  1.75it/s]Downloaded 0.02 GB:  22%|##2       | 18/81 [00:14<00:33,  1.90it/s]Downloaded 0.02 GB:  22%|##2       | 18/81 [00:14<00:33,  1.90it/s]Downloaded 0.02 GB:  23%|##3       | 19/81 [00:14<00:30,  2.04it/s]Downloaded 0.02 GB:  23%|##3       | 19/81 [00:15<00:30,  2.04it/s]Downloaded 0.02 GB:  25%|##4       | 20/81 [00:15<00:28,  2.16it/s]Downloaded 0.02 GB:  25%|##4       | 20/81 [00:15<00:28,  2.16it/s]Downloaded 0.02 GB:  26%|##5       | 21/81 [00:15<00:26,  2.24it/s]Downloaded 0.02 GB:  26%|##5       | 21/81 [00:15<00:26,  2.24it/s]Downloaded 0.02 GB:  27%|##7       | 22/81 [00:15<00:25,  2.30it/s]Downloaded 0.02 GB:  27%|##7       | 22/81 [00:16<00:25,  2.30it/s]Downloaded 0.02 GB:  28%|##8       | 23/81 [00:16<00:25,  2.30it/s]Downloaded 0.02 GB:  28%|##8       | 23/81 [00:16<00:25,  2.30it/s]Downloaded 0.02 GB:  30%|##9       | 24/81 [00:16<00:27,  2.09it/s]Downloaded 0.02 GB:  30%|##9       | 24/81 [00:17<00:27,  2.09it/s]Downloaded 0.02 GB:  31%|###       | 25/81 [00:17<00:26,  2.11it/s]Downloaded 0.03 GB:  31%|###       | 25/81 [00:17<00:26,  2.11it/s]Downloaded 0.03 GB:  32%|###2      | 26/81 [00:17<00:28,  1.96it/s]Downloaded 0.03 GB:  32%|###2      | 26/81 [00:18<00:28,  1.96it/s]Downloaded 0.03 GB:  33%|###3      | 27/81 [00:18<00:26,  2.05it/s]Downloaded 0.03 GB:  33%|###3      | 27/81 [00:18<00:26,  2.05it/s]Downloaded 0.03 GB:  35%|###4      | 28/81 [00:18<00:27,  1.95it/s]Downloaded 0.03 GB:  35%|###4      | 28/81 [00:19<00:27,  1.95it/s]Downloaded 0.03 GB:  36%|###5      | 29/81 [00:19<00:25,  2.03it/s]Downloaded 0.03 GB:  36%|###5      | 29/81 [00:19<00:25,  2.03it/s]Downloaded 0.03 GB:  37%|###7      | 30/81 [00:19<00:24,  2.12it/s]Downloaded 0.03 GB:  37%|###7      | 30/81 [00:20<00:24,  2.12it/s]Downloaded 0.03 GB:  38%|###8      | 31/81 [00:20<00:22,  2.19it/s]Downloaded 0.03 GB:  38%|###8      | 31/81 [00:20<00:22,  2.19it/s]Downloaded 0.03 GB:  40%|###9      | 32/81 [00:20<00:24,  2.04it/s]Downloaded 0.03 GB:  40%|###9      | 32/81 [00:21<00:24,  2.04it/s]Downloaded 0.03 GB:  41%|####      | 33/81 [00:21<00:23,  2.07it/s]Downloaded 0.03 GB:  41%|####      | 33/81 [00:21<00:23,  2.07it/s]Downloaded 0.03 GB:  42%|####1     | 34/81 [00:21<00:24,  1.92it/s]Downloaded 0.03 GB:  42%|####1     | 34/81 [00:22<00:24,  1.92it/s]Downloaded 0.03 GB:  43%|####3     | 35/81 [00:22<00:25,  1.84it/s]Downloaded 0.04 GB:  43%|####3     | 35/81 [00:23<00:25,  1.84it/s]Downloaded 0.04 GB:  44%|####4     | 36/81 [00:23<00:24,  1.81it/s]Downloaded 0.04 GB:  44%|####4     | 36/81 [00:23<00:24,  1.81it/s]Downloaded 0.04 GB:  46%|####5     | 37/81 [00:23<00:23,  1.89it/s]Downloaded 0.04 GB:  46%|####5     | 37/81 [00:24<00:23,  1.89it/s]Downloaded 0.04 GB:  47%|####6     | 38/81 [00:24<00:23,  1.85it/s]Downloaded 0.04 GB:  47%|####6     | 38/81 [00:24<00:23,  1.85it/s]Downloaded 0.04 GB:  48%|####8     | 39/81 [00:24<00:21,  1.92it/s]Downloaded 0.04 GB:  48%|####8     | 39/81 [00:25<00:21,  1.92it/s]Downloaded 0.04 GB:  49%|####9     | 40/81 [00:25<00:21,  1.87it/s]Downloaded 0.04 GB:  49%|####9     | 40/81 [00:25<00:21,  1.87it/s]Downloaded 0.04 GB:  51%|#####     | 41/81 [00:25<00:20,  1.94it/s]Downloaded 0.04 GB:  51%|#####     | 41/81 [00:26<00:20,  1.94it/s]Downloaded 0.04 GB:  52%|#####1    | 42/81 [00:26<00:20,  1.89it/s]Downloaded 0.04 GB:  52%|#####1    | 42/81 [00:26<00:20,  1.89it/s]Downloaded 0.04 GB:  53%|#####3    | 43/81 [00:26<00:19,  1.96it/s]Downloaded 0.04 GB:  53%|#####3    | 43/81 [00:27<00:19,  1.96it/s]Downloaded 0.04 GB:  54%|#####4    | 44/81 [00:27<00:19,  1.89it/s]Downloaded 0.04 GB:  54%|#####4    | 44/81 [00:27<00:19,  1.89it/s]Downloaded 0.04 GB:  56%|#####5    | 45/81 [00:27<00:18,  1.97it/s]Downloaded 0.04 GB:  56%|#####5    | 45/81 [00:28<00:18,  1.97it/s]Downloaded 0.04 GB:  57%|#####6    | 46/81 [00:28<00:18,  1.91it/s]Downloaded 0.05 GB:  57%|#####6    | 46/81 [00:28<00:18,  1.91it/s]Downloaded 0.05 GB:  58%|#####8    | 47/81 [00:28<00:17,  1.97it/s]Downloaded 0.05 GB:  58%|#####8    | 47/81 [00:29<00:17,  1.97it/s]Downloaded 0.05 GB:  59%|#####9    | 48/81 [00:29<00:17,  1.92it/s]Downloaded 0.05 GB:  59%|#####9    | 48/81 [00:29<00:17,  1.92it/s]Downloaded 0.05 GB:  60%|######    | 49/81 [00:29<00:16,  1.98it/s]Downloaded 0.05 GB:  60%|######    | 49/81 [00:30<00:16,  1.98it/s]Downloaded 0.05 GB:  62%|######1   | 50/81 [00:30<00:15,  2.06it/s]Downloaded 0.05 GB:  62%|######1   | 50/81 [00:30<00:15,  2.06it/s]Downloaded 0.05 GB:  63%|######2   | 51/81 [00:30<00:15,  2.00it/s]Downloaded 0.05 GB:  63%|######2   | 51/81 [00:31<00:15,  2.00it/s]Downloaded 0.05 GB:  64%|######4   | 52/81 [00:31<00:13,  2.08it/s]Downloaded 0.05 GB:  64%|######4   | 52/81 [00:31<00:13,  2.08it/s]Downloaded 0.05 GB:  65%|######5   | 53/81 [00:31<00:12,  2.16it/s]Downloaded 0.05 GB:  65%|######5   | 53/81 [00:32<00:12,  2.16it/s]Downloaded 0.05 GB:  67%|######6   | 54/81 [00:32<00:12,  2.23it/s]Downloaded 0.05 GB:  67%|######6   | 54/81 [00:32<00:12,  2.23it/s]Downloaded 0.05 GB:  68%|######7   | 55/81 [00:32<00:11,  2.30it/s]Downloaded 0.05 GB:  68%|######7   | 55/81 [00:32<00:11,  2.30it/s]Downloaded 0.05 GB:  69%|######9   | 56/81 [00:32<00:10,  2.28it/s]Downloaded 0.06 GB:  69%|######9   | 56/81 [00:33<00:10,  2.28it/s]Downloaded 0.06 GB:  70%|#######   | 57/81 [00:33<00:10,  2.35it/s]Downloaded 0.06 GB:  70%|#######   | 57/81 [00:33<00:10,  2.35it/s]Downloaded 0.06 GB:  72%|#######1  | 58/81 [00:33<00:09,  2.31it/s]Downloaded 0.06 GB:  72%|#######1  | 58/81 [00:34<00:09,  2.31it/s]Downloaded 0.06 GB:  73%|#######2  | 59/81 [00:34<00:10,  2.18it/s]Downloaded 0.06 GB:  73%|#######2  | 59/81 [00:34<00:10,  2.18it/s]Downloaded 0.06 GB:  74%|#######4  | 60/81 [00:34<00:09,  2.22it/s]Downloaded 0.06 GB:  74%|#######4  | 60/81 [00:35<00:09,  2.22it/s]Downloaded 0.06 GB:  75%|#######5  | 61/81 [00:35<00:08,  2.27it/s]Downloaded 0.06 GB:  75%|#######5  | 61/81 [00:35<00:08,  2.27it/s]Downloaded 0.06 GB:  77%|#######6  | 62/81 [00:35<00:08,  2.32it/s]Downloaded 0.06 GB:  77%|#######6  | 62/81 [00:35<00:08,  2.32it/s]Downloaded 0.06 GB:  78%|#######7  | 63/81 [00:35<00:07,  2.39it/s]Downloaded 0.06 GB:  78%|#######7  | 63/81 [00:36<00:07,  2.39it/s]Downloaded 0.06 GB:  79%|#######9  | 64/81 [00:36<00:06,  2.57it/s]Downloaded 0.06 GB:  79%|#######9  | 64/81 [00:36<00:06,  2.57it/s]Downloaded 0.06 GB:  80%|########  | 65/81 [00:36<00:06,  2.55it/s]Downloaded 0.06 GB:  80%|########  | 65/81 [00:36<00:06,  2.55it/s]Downloaded 0.06 GB:  81%|########1 | 66/81 [00:36<00:05,  2.53it/s]Downloaded 0.07 GB:  81%|########1 | 66/81 [00:37<00:05,  2.53it/s]Downloaded 0.07 GB:  83%|########2 | 67/81 [00:37<00:05,  2.56it/s]Downloaded 0.07 GB:  83%|########2 | 67/81 [00:37<00:05,  2.56it/s]Downloaded 0.07 GB:  84%|########3 | 68/81 [00:37<00:05,  2.56it/s]Downloaded 0.07 GB:  84%|########3 | 68/81 [00:38<00:05,  2.56it/s]Downloaded 0.07 GB:  85%|########5 | 69/81 [00:38<00:04,  2.60it/s]Downloaded 0.07 GB:  85%|########5 | 69/81 [00:38<00:04,  2.60it/s]Downloaded 0.07 GB:  86%|########6 | 70/81 [00:38<00:04,  2.75it/s]Downloaded 0.07 GB:  86%|########6 | 70/81 [00:38<00:04,  2.75it/s]Downloaded 0.07 GB:  88%|########7 | 71/81 [00:38<00:03,  2.67it/s]Downloaded 0.07 GB:  88%|########7 | 71/81 [00:39<00:03,  2.67it/s]Downloaded 0.07 GB:  89%|########8 | 72/81 [00:39<00:03,  2.66it/s]Downloaded 0.07 GB:  89%|########8 | 72/81 [00:39<00:03,  2.66it/s]Downloaded 0.07 GB:  90%|######### | 73/81 [00:39<00:03,  2.63it/s]Downloaded 0.07 GB:  90%|######### | 73/81 [00:39<00:03,  2.63it/s]Downloaded 0.07 GB:  91%|#########1| 74/81 [00:39<00:02,  2.75it/s]Downloaded 0.07 GB:  91%|#########1| 74/81 [00:40<00:02,  2.75it/s]Downloaded 0.07 GB:  93%|#########2| 75/81 [00:40<00:02,  2.75it/s]Downloaded 0.07 GB:  93%|#########2| 75/81 [00:40<00:02,  2.75it/s]Downloaded 0.07 GB:  94%|#########3| 76/81 [00:40<00:01,  2.68it/s]Downloaded 0.08 GB:  94%|#########3| 76/81 [00:41<00:01,  2.68it/s]Downloaded 0.08 GB:  95%|#########5| 77/81 [00:41<00:01,  2.67it/s]Downloaded 0.08 GB:  95%|#########5| 77/81 [00:41<00:01,  2.67it/s]Downloaded 0.08 GB:  96%|#########6| 78/81 [00:41<00:01,  2.63it/s]Downloaded 0.08 GB:  96%|#########6| 78/81 [00:41<00:01,  2.63it/s]Downloaded 0.08 GB:  98%|#########7| 79/81 [00:41<00:00,  2.81it/s]Downloaded 0.08 GB:  98%|#########7| 79/81 [00:41<00:00,  2.81it/s]Downloaded 0.08 GB:  99%|#########8| 80/81 [00:41<00:00,  3.50it/s]Downloaded 0.08 GB:  99%|#########8| 80/81 [00:41<00:00,  3.50it/s]Downloaded 0.08 GB: 100%|##########| 81/81 [00:41<00:00,  1.93it/s]
Extracting dataset/arxiv.zip
Loading necessary files...
This might take a while.
Processing graphs...
  0%|          | 0/1 [00:00<?, ?it/s]100%|##########| 1/1 [00:00<00:00, 19972.88it/s]
Saving...
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6081, Train: 22.17%, Valid: 22.18%, Test: 22.75%
Epoch: 25, Loss: 1.5092, Train: 33.87%, Valid: 33.56%, Test: 33.83%
Epoch: 50, Loss: 1.4892, Train: 34.75%, Valid: 34.50%, Test: 34.65%
Epoch: 75, Loss: 1.4762, Train: 35.53%, Valid: 34.78%, Test: 35.32%
Epoch: 100, Loss: 1.4656, Train: 36.12%, Valid: 35.33%, Test: 35.63%
Epoch: 125, Loss: 1.4573, Train: 36.57%, Valid: 35.48%, Test: 35.64%
Epoch: 150, Loss: 1.4502, Train: 36.91%, Valid: 35.84%, Test: 36.04%
Epoch: 175, Loss: 1.4437, Train: 37.13%, Valid: 35.71%, Test: 36.03%
Epoch: 200, Loss: 1.4394, Train: 37.60%, Valid: 35.83%, Test: 36.11%
Epoch: 225, Loss: 1.4342, Train: 37.86%, Valid: 36.00%, Test: 36.21%
Epoch: 250, Loss: 1.4285, Train: 38.04%, Valid: 35.97%, Test: 36.24%
Epoch: 275, Loss: 1.4249, Train: 38.22%, Valid: 35.59%, Test: 36.00%
Epoch: 300, Loss: 1.4268, Train: 38.43%, Valid: 35.83%, Test: 36.11%
Epoch: 325, Loss: 1.4175, Train: 38.79%, Valid: 35.79%, Test: 36.06%
Epoch: 350, Loss: 1.4150, Train: 39.01%, Valid: 35.71%, Test: 36.16%
Epoch: 375, Loss: 1.4138, Train: 39.13%, Valid: 35.78%, Test: 36.09%
Epoch: 400, Loss: 1.4257, Train: 38.57%, Valid: 35.20%, Test: 35.43%
Epoch: 425, Loss: 1.4047, Train: 39.54%, Valid: 35.65%, Test: 36.02%
Epoch: 450, Loss: 1.4040, Train: 39.61%, Valid: 35.52%, Test: 35.84%
Epoch: 475, Loss: 1.4008, Train: 39.73%, Valid: 35.26%, Test: 35.56%
Epoch: 500, Loss: 1.3980, Train: 39.77%, Valid: 35.65%, Test: 35.83%
Epoch: 525, Loss: 1.3976, Train: 40.04%, Valid: 35.45%, Test: 35.45%
Epoch: 550, Loss: 1.4058, Train: 39.58%, Valid: 35.21%, Test: 35.29%
Epoch: 575, Loss: 1.3924, Train: 40.33%, Valid: 35.53%, Test: 35.63%
Epoch: 600, Loss: 1.3889, Train: 40.53%, Valid: 35.39%, Test: 35.57%
Epoch: 625, Loss: 1.3896, Train: 40.52%, Valid: 35.61%, Test: 35.76%
Epoch: 650, Loss: 1.3880, Train: 40.59%, Valid: 35.42%, Test: 35.67%
Epoch: 675, Loss: 1.3857, Train: 40.68%, Valid: 35.33%, Test: 35.53%
Epoch: 700, Loss: 1.3873, Train: 40.36%, Valid: 35.33%, Test: 35.58%
Epoch: 725, Loss: 1.3816, Train: 40.81%, Valid: 35.46%, Test: 35.67%
Epoch: 750, Loss: 1.3839, Train: 40.28%, Valid: 34.78%, Test: 35.39%
Epoch: 775, Loss: 1.3792, Train: 40.92%, Valid: 35.45%, Test: 35.82%
Epoch: 800, Loss: 1.3796, Train: 40.71%, Valid: 35.12%, Test: 35.58%
Epoch: 825, Loss: 1.3766, Train: 41.08%, Valid: 35.30%, Test: 35.57%
Epoch: 850, Loss: 1.3777, Train: 41.01%, Valid: 35.40%, Test: 35.84%
Epoch: 875, Loss: 1.3776, Train: 40.94%, Valid: 35.42%, Test: 35.70%
Epoch: 900, Loss: 1.3761, Train: 40.94%, Valid: 34.95%, Test: 35.40%
Epoch: 925, Loss: 1.3753, Train: 40.99%, Valid: 35.16%, Test: 35.55%
Epoch: 950, Loss: 1.3738, Train: 41.15%, Valid: 35.23%, Test: 35.52%
Epoch: 975, Loss: 1.3812, Train: 40.85%, Valid: 35.11%, Test: 35.54%
Run 01:
Highest Train: 41.36
Highest Valid: 36.18
  Final Train: 38.27
   Final Test: 36.47
All runs:
Highest Train: 41.36, nan
Highest Valid: 36.18, nan
  Final Train: 38.27, nan
   Final Test: 36.47, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6093, Train: 28.70%, Valid: 28.50%, Test: 28.80%
Epoch: 25, Loss: 1.5101, Train: 33.68%, Valid: 33.47%, Test: 33.74%
Epoch: 50, Loss: 1.4552, Train: 35.47%, Valid: 35.05%, Test: 35.38%
Epoch: 75, Loss: 1.4398, Train: 36.99%, Valid: 36.88%, Test: 37.22%
Epoch: 100, Loss: 1.4297, Train: 39.53%, Valid: 39.19%, Test: 39.57%
Epoch: 125, Loss: 1.4231, Train: 39.44%, Valid: 38.89%, Test: 39.41%
Epoch: 150, Loss: 1.4093, Train: 40.38%, Valid: 39.50%, Test: 40.03%
Epoch: 175, Loss: 1.3978, Train: 40.59%, Valid: 39.63%, Test: 40.24%
Epoch: 200, Loss: 1.3997, Train: 40.11%, Valid: 39.05%, Test: 39.65%
Epoch: 225, Loss: 1.3804, Train: 41.12%, Valid: 40.04%, Test: 40.37%
Epoch: 250, Loss: 1.3689, Train: 41.52%, Valid: 40.35%, Test: 40.67%
Epoch: 275, Loss: 1.3836, Train: 38.87%, Valid: 37.97%, Test: 38.28%
Epoch: 300, Loss: 1.3636, Train: 41.45%, Valid: 40.39%, Test: 40.73%
Epoch: 325, Loss: 1.3531, Train: 41.66%, Valid: 40.56%, Test: 40.68%
Epoch: 350, Loss: 1.3465, Train: 41.94%, Valid: 40.75%, Test: 40.86%
Epoch: 375, Loss: 1.3616, Train: 41.40%, Valid: 40.29%, Test: 40.41%
Epoch: 400, Loss: 1.3422, Train: 41.96%, Valid: 40.86%, Test: 40.91%
Epoch: 425, Loss: 1.3332, Train: 42.25%, Valid: 40.95%, Test: 41.06%
Epoch: 450, Loss: 1.3542, Train: 41.51%, Valid: 40.30%, Test: 40.60%
Epoch: 475, Loss: 1.3343, Train: 42.37%, Valid: 40.99%, Test: 41.09%
Epoch: 500, Loss: 1.3273, Train: 42.52%, Valid: 41.27%, Test: 41.18%
Epoch: 525, Loss: 1.3358, Train: 41.98%, Valid: 40.60%, Test: 40.72%
Epoch: 550, Loss: 1.3235, Train: 42.67%, Valid: 41.18%, Test: 41.38%
Epoch: 575, Loss: 1.3222, Train: 42.73%, Valid: 41.28%, Test: 41.45%
Epoch: 600, Loss: 1.3656, Train: 41.11%, Valid: 40.54%, Test: 40.85%
Epoch: 625, Loss: 1.3492, Train: 41.85%, Valid: 41.40%, Test: 41.48%
Epoch: 650, Loss: 1.3382, Train: 41.96%, Valid: 41.48%, Test: 41.53%
Epoch: 675, Loss: 1.3324, Train: 42.50%, Valid: 41.88%, Test: 42.00%
Epoch: 700, Loss: 1.3329, Train: 42.19%, Valid: 41.66%, Test: 41.72%
Epoch: 725, Loss: 1.3513, Train: 41.57%, Valid: 40.96%, Test: 41.09%
Epoch: 750, Loss: 1.3205, Train: 42.73%, Valid: 42.10%, Test: 42.17%
Epoch: 775, Loss: 1.3219, Train: 42.55%, Valid: 41.85%, Test: 41.85%
Epoch: 800, Loss: 1.3203, Train: 42.01%, Valid: 41.46%, Test: 41.38%
Epoch: 825, Loss: 1.3078, Train: 42.65%, Valid: 42.01%, Test: 42.20%
Epoch: 850, Loss: 1.3289, Train: 42.72%, Valid: 41.93%, Test: 42.24%
Epoch: 875, Loss: 1.2999, Train: 42.96%, Valid: 42.27%, Test: 42.45%
Epoch: 900, Loss: 1.2989, Train: 43.03%, Valid: 42.20%, Test: 42.43%
Epoch: 925, Loss: 1.2952, Train: 43.16%, Valid: 42.39%, Test: 42.58%
Epoch: 950, Loss: 1.2914, Train: 43.07%, Valid: 42.32%, Test: 42.52%
Epoch: 975, Loss: 1.3641, Train: 40.74%, Valid: 40.04%, Test: 40.21%
Run 01:
Highest Train: 43.46
Highest Valid: 42.57
  Final Train: 43.33
   Final Test: 42.65
All runs:
Highest Train: 43.46, nan
Highest Valid: 42.57, nan
  Final Train: 43.33, nan
   Final Test: 42.65, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6487, Train: 28.26%, Valid: 27.89%, Test: 28.31%
Epoch: 25, Loss: 1.4406, Train: 34.01%, Valid: 33.74%, Test: 34.55%
Epoch: 50, Loss: 1.4066, Train: 36.79%, Valid: 36.37%, Test: 37.03%
Epoch: 75, Loss: 1.3834, Train: 38.85%, Valid: 38.66%, Test: 38.96%
Epoch: 100, Loss: 1.3945, Train: 38.39%, Valid: 38.05%, Test: 38.36%
Epoch: 125, Loss: 1.3692, Train: 39.69%, Valid: 39.31%, Test: 39.54%
Epoch: 150, Loss: 1.4012, Train: 38.25%, Valid: 38.02%, Test: 38.04%
Epoch: 175, Loss: 1.3791, Train: 39.18%, Valid: 38.89%, Test: 39.11%
Epoch: 200, Loss: 1.3653, Train: 40.02%, Valid: 39.80%, Test: 39.95%
Epoch: 225, Loss: 1.3838, Train: 38.63%, Valid: 38.35%, Test: 38.59%
Epoch: 250, Loss: 1.3592, Train: 40.51%, Valid: 40.35%, Test: 40.59%
Epoch: 275, Loss: 1.3654, Train: 40.48%, Valid: 40.27%, Test: 40.75%
Epoch: 300, Loss: 1.4042, Train: 39.53%, Valid: 39.39%, Test: 39.67%
Epoch: 325, Loss: 1.4189, Train: 39.97%, Valid: 39.60%, Test: 39.95%
Epoch: 350, Loss: 1.3582, Train: 40.79%, Valid: 40.42%, Test: 40.75%
Epoch: 375, Loss: 1.3495, Train: 41.25%, Valid: 40.85%, Test: 41.31%
Epoch: 400, Loss: 1.3400, Train: 41.96%, Valid: 41.58%, Test: 41.83%
Epoch: 425, Loss: 1.3338, Train: 42.41%, Valid: 42.08%, Test: 42.23%
Epoch: 450, Loss: 1.3490, Train: 41.16%, Valid: 40.99%, Test: 41.39%
Epoch: 475, Loss: 1.3270, Train: 42.63%, Valid: 41.98%, Test: 42.49%
Epoch: 500, Loss: 1.3208, Train: 42.34%, Valid: 41.82%, Test: 42.42%
Epoch: 525, Loss: 1.3197, Train: 42.67%, Valid: 42.14%, Test: 42.54%
Epoch: 550, Loss: 1.3133, Train: 42.09%, Valid: 41.80%, Test: 42.38%
Epoch: 575, Loss: 1.3141, Train: 42.54%, Valid: 42.14%, Test: 42.65%
Epoch: 600, Loss: 1.3187, Train: 42.69%, Valid: 42.38%, Test: 42.91%
Epoch: 625, Loss: 1.3056, Train: 42.68%, Valid: 42.50%, Test: 43.01%
Epoch: 650, Loss: 1.3082, Train: 42.93%, Valid: 42.73%, Test: 43.04%
Epoch: 675, Loss: 1.3042, Train: 43.06%, Valid: 42.77%, Test: 43.26%
Epoch: 700, Loss: 1.4847, Train: 32.67%, Valid: 32.72%, Test: 32.62%
Epoch: 725, Loss: 1.4133, Train: 37.87%, Valid: 37.73%, Test: 37.70%
Epoch: 750, Loss: 1.3921, Train: 39.31%, Valid: 39.06%, Test: 39.05%
Epoch: 775, Loss: 1.3818, Train: 40.31%, Valid: 39.95%, Test: 40.35%
Epoch: 800, Loss: 1.3604, Train: 40.70%, Valid: 40.46%, Test: 40.74%
Epoch: 825, Loss: 1.3531, Train: 40.85%, Valid: 40.66%, Test: 40.89%
Epoch: 850, Loss: 1.3410, Train: 41.51%, Valid: 41.13%, Test: 41.55%
Epoch: 875, Loss: 1.3412, Train: 41.73%, Valid: 41.40%, Test: 41.82%
Epoch: 900, Loss: 1.3448, Train: 41.78%, Valid: 41.28%, Test: 41.40%
Epoch: 925, Loss: 1.3294, Train: 42.36%, Valid: 41.96%, Test: 42.32%
Epoch: 950, Loss: 1.3305, Train: 42.61%, Valid: 42.18%, Test: 42.54%
Epoch: 975, Loss: 1.3217, Train: 42.67%, Valid: 42.34%, Test: 42.80%
Run 01:
Highest Train: 43.26
Highest Valid: 43.01
  Final Train: 43.26
   Final Test: 43.34
All runs:
Highest Train: 43.26, nan
Highest Valid: 43.01, nan
  Final Train: 43.26, nan
   Final Test: 43.34, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6098, Train: 22.39%, Valid: 22.39%, Test: 22.92%
Epoch: 25, Loss: 1.5164, Train: 33.75%, Valid: 33.68%, Test: 33.79%
Epoch: 50, Loss: 1.4918, Train: 34.69%, Valid: 34.43%, Test: 34.57%
Epoch: 75, Loss: 1.4771, Train: 35.65%, Valid: 34.93%, Test: 35.50%
Epoch: 100, Loss: 1.4664, Train: 36.16%, Valid: 35.19%, Test: 35.71%
Epoch: 125, Loss: 1.4568, Train: 36.66%, Valid: 35.43%, Test: 36.03%
Epoch: 150, Loss: 1.4482, Train: 36.95%, Valid: 35.80%, Test: 35.93%
Epoch: 175, Loss: 1.4406, Train: 37.54%, Valid: 35.93%, Test: 36.30%
Epoch: 200, Loss: 1.4368, Train: 37.77%, Valid: 35.85%, Test: 36.25%
Epoch: 225, Loss: 1.4307, Train: 37.94%, Valid: 35.68%, Test: 36.22%
Epoch: 250, Loss: 1.4269, Train: 38.16%, Valid: 35.79%, Test: 36.36%
Epoch: 275, Loss: 1.4224, Train: 38.13%, Valid: 35.76%, Test: 35.98%
Epoch: 300, Loss: 1.4196, Train: 38.45%, Valid: 35.61%, Test: 36.21%
Epoch: 325, Loss: 1.4153, Train: 38.74%, Valid: 35.66%, Test: 36.01%
Epoch: 350, Loss: 1.4124, Train: 38.78%, Valid: 35.57%, Test: 36.01%
Epoch: 375, Loss: 1.4144, Train: 38.39%, Valid: 34.88%, Test: 35.43%
Epoch: 400, Loss: 1.4074, Train: 39.13%, Valid: 35.57%, Test: 35.90%
Epoch: 425, Loss: 1.4045, Train: 39.16%, Valid: 35.50%, Test: 35.89%
Epoch: 450, Loss: 1.4044, Train: 39.18%, Valid: 35.58%, Test: 36.00%
Epoch: 475, Loss: 1.4050, Train: 39.22%, Valid: 35.40%, Test: 35.71%
Epoch: 500, Loss: 1.3988, Train: 39.54%, Valid: 35.58%, Test: 35.97%
Epoch: 525, Loss: 1.3985, Train: 39.55%, Valid: 35.62%, Test: 35.95%
Epoch: 550, Loss: 1.3973, Train: 39.64%, Valid: 35.66%, Test: 35.99%
Epoch: 575, Loss: 1.3947, Train: 39.80%, Valid: 35.42%, Test: 35.90%
Epoch: 600, Loss: 1.3931, Train: 39.75%, Valid: 35.56%, Test: 35.82%
Epoch: 625, Loss: 1.3920, Train: 39.95%, Valid: 35.51%, Test: 35.85%
Epoch: 650, Loss: 1.3942, Train: 39.92%, Valid: 35.31%, Test: 35.90%
Epoch: 675, Loss: 1.3911, Train: 39.91%, Valid: 35.59%, Test: 35.81%
Epoch: 700, Loss: 1.3898, Train: 40.09%, Valid: 35.43%, Test: 35.82%
Epoch: 725, Loss: 1.3903, Train: 40.08%, Valid: 35.47%, Test: 35.99%
Epoch: 750, Loss: 1.3915, Train: 39.99%, Valid: 35.63%, Test: 35.77%
Epoch: 775, Loss: 1.3874, Train: 40.24%, Valid: 35.28%, Test: 35.65%
Epoch: 800, Loss: 1.3875, Train: 40.12%, Valid: 35.01%, Test: 35.47%
Epoch: 825, Loss: 1.3854, Train: 40.11%, Valid: 35.19%, Test: 35.43%
Epoch: 850, Loss: 1.3855, Train: 40.19%, Valid: 35.03%, Test: 35.56%
Epoch: 875, Loss: 1.3943, Train: 39.47%, Valid: 34.25%, Test: 34.76%
Epoch: 900, Loss: 1.3823, Train: 40.40%, Valid: 35.34%, Test: 35.74%
Epoch: 925, Loss: 1.3817, Train: 40.32%, Valid: 34.76%, Test: 35.23%
Epoch: 950, Loss: 1.3819, Train: 40.29%, Valid: 35.11%, Test: 35.50%
Epoch: 975, Loss: 1.3814, Train: 40.24%, Valid: 34.69%, Test: 35.18%
Run 01:
Highest Train: 40.63
Highest Valid: 36.05
  Final Train: 37.82
   Final Test: 36.37
All runs:
Highest Train: 40.63, nan
Highest Valid: 36.05, nan
  Final Train: 37.82, nan
   Final Test: 36.37, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6106, Train: 26.93%, Valid: 27.00%, Test: 27.70%
Epoch: 25, Loss: 1.5203, Train: 32.37%, Valid: 32.14%, Test: 32.66%
Epoch: 50, Loss: 1.4708, Train: 36.36%, Valid: 36.12%, Test: 36.77%
Epoch: 75, Loss: 1.4490, Train: 38.03%, Valid: 37.77%, Test: 38.27%
Epoch: 100, Loss: 1.4296, Train: 39.65%, Valid: 39.34%, Test: 39.68%
Epoch: 125, Loss: 1.4486, Train: 38.74%, Valid: 38.20%, Test: 38.79%
Epoch: 150, Loss: 1.4171, Train: 39.93%, Valid: 39.37%, Test: 39.99%
Epoch: 175, Loss: 1.4284, Train: 37.86%, Valid: 37.36%, Test: 37.86%
Epoch: 200, Loss: 1.4001, Train: 40.62%, Valid: 40.26%, Test: 40.55%
Epoch: 225, Loss: 1.4483, Train: 36.21%, Valid: 35.77%, Test: 35.94%
Epoch: 250, Loss: 1.3960, Train: 40.39%, Valid: 39.72%, Test: 40.23%
Epoch: 275, Loss: 1.3836, Train: 40.94%, Valid: 40.14%, Test: 40.63%
Epoch: 300, Loss: 1.3804, Train: 40.76%, Valid: 40.07%, Test: 40.64%
Epoch: 325, Loss: 1.3717, Train: 41.04%, Valid: 40.50%, Test: 40.79%
Epoch: 350, Loss: 1.4031, Train: 40.42%, Valid: 39.57%, Test: 40.33%
Epoch: 375, Loss: 1.3738, Train: 41.36%, Valid: 40.71%, Test: 41.19%
Epoch: 400, Loss: 1.3648, Train: 41.62%, Valid: 40.82%, Test: 41.08%
Epoch: 425, Loss: 1.3684, Train: 40.82%, Valid: 40.10%, Test: 40.38%
Epoch: 450, Loss: 1.3582, Train: 41.63%, Valid: 40.77%, Test: 41.19%
Epoch: 475, Loss: 1.3687, Train: 41.85%, Valid: 40.66%, Test: 41.27%
Epoch: 500, Loss: 1.3622, Train: 41.05%, Valid: 40.19%, Test: 40.55%
Epoch: 525, Loss: 1.3566, Train: 42.18%, Valid: 41.16%, Test: 41.60%
Epoch: 550, Loss: 1.3493, Train: 42.08%, Valid: 41.08%, Test: 41.32%
Epoch: 575, Loss: 1.3491, Train: 42.17%, Valid: 41.10%, Test: 41.63%
Epoch: 600, Loss: 1.3463, Train: 41.96%, Valid: 40.86%, Test: 41.36%
Epoch: 625, Loss: 1.3555, Train: 42.17%, Valid: 41.18%, Test: 41.60%
Epoch: 650, Loss: 1.3496, Train: 41.75%, Valid: 40.72%, Test: 41.15%
Epoch: 675, Loss: 1.3442, Train: 42.03%, Valid: 40.93%, Test: 41.21%
Epoch: 700, Loss: 1.3550, Train: 42.10%, Valid: 41.38%, Test: 41.55%
Epoch: 725, Loss: 1.3449, Train: 41.81%, Valid: 41.03%, Test: 41.23%
Epoch: 750, Loss: 1.3475, Train: 41.78%, Valid: 40.89%, Test: 41.24%
Epoch: 775, Loss: 1.3422, Train: 42.18%, Valid: 41.46%, Test: 41.84%
Epoch: 800, Loss: 1.3476, Train: 42.01%, Valid: 41.28%, Test: 41.58%
Epoch: 825, Loss: 1.4191, Train: 38.88%, Valid: 38.45%, Test: 38.89%
Epoch: 850, Loss: 1.3749, Train: 41.15%, Valid: 40.24%, Test: 41.04%
Epoch: 875, Loss: 1.3738, Train: 41.22%, Valid: 40.40%, Test: 40.84%
Epoch: 900, Loss: 1.3633, Train: 41.94%, Valid: 41.08%, Test: 41.53%
Epoch: 925, Loss: 1.3645, Train: 41.97%, Valid: 41.13%, Test: 41.58%
Epoch: 950, Loss: 1.3555, Train: 42.23%, Valid: 41.39%, Test: 41.81%
Epoch: 975, Loss: 1.3513, Train: 42.38%, Valid: 41.40%, Test: 41.93%
Run 01:
Highest Train: 42.60
Highest Valid: 41.80
  Final Train: 42.53
   Final Test: 42.10
All runs:
Highest Train: 42.60, nan
Highest Valid: 41.80, nan
  Final Train: 42.53, nan
   Final Test: 42.10, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.7354, Train: 35.07%, Valid: 34.62%, Test: 34.63%
Epoch: 25, Loss: 1.4390, Train: 37.20%, Valid: 36.94%, Test: 37.02%
Epoch: 50, Loss: 1.4193, Train: 37.81%, Valid: 37.46%, Test: 37.60%
Epoch: 75, Loss: 1.4252, Train: 37.08%, Valid: 36.69%, Test: 36.95%
Epoch: 100, Loss: 1.3894, Train: 38.24%, Valid: 37.88%, Test: 38.12%
Epoch: 125, Loss: 1.3750, Train: 38.57%, Valid: 38.20%, Test: 38.47%
Epoch: 150, Loss: 1.3703, Train: 38.72%, Valid: 38.48%, Test: 38.64%
Epoch: 175, Loss: 1.3605, Train: 37.51%, Valid: 37.42%, Test: 37.69%
Epoch: 200, Loss: 1.3649, Train: 39.39%, Valid: 39.23%, Test: 39.33%
Epoch: 225, Loss: 1.3486, Train: 40.53%, Valid: 40.44%, Test: 40.60%
Epoch: 250, Loss: 1.3825, Train: 38.92%, Valid: 38.67%, Test: 38.82%
Epoch: 275, Loss: 1.3460, Train: 40.55%, Valid: 40.48%, Test: 40.63%
Epoch: 300, Loss: 1.3397, Train: 41.42%, Valid: 41.32%, Test: 41.50%
Epoch: 325, Loss: 1.3258, Train: 42.02%, Valid: 41.97%, Test: 42.08%
Epoch: 350, Loss: 1.3267, Train: 42.52%, Valid: 42.34%, Test: 42.54%
Epoch: 375, Loss: 1.3156, Train: 42.88%, Valid: 42.61%, Test: 42.99%
Epoch: 400, Loss: 1.3558, Train: 40.72%, Valid: 40.35%, Test: 40.48%
Epoch: 425, Loss: 1.3472, Train: 41.10%, Valid: 40.78%, Test: 41.13%
Epoch: 450, Loss: 1.3257, Train: 42.28%, Valid: 42.18%, Test: 42.51%
Epoch: 475, Loss: 1.3289, Train: 41.75%, Valid: 41.53%, Test: 41.98%
Epoch: 500, Loss: 1.3156, Train: 42.80%, Valid: 42.57%, Test: 42.98%
Epoch: 525, Loss: 1.3225, Train: 42.41%, Valid: 42.21%, Test: 42.72%
Epoch: 550, Loss: 1.3159, Train: 42.70%, Valid: 42.36%, Test: 42.67%
Epoch: 575, Loss: 1.3087, Train: 43.07%, Valid: 42.84%, Test: 43.17%
Epoch: 600, Loss: 1.3110, Train: 43.10%, Valid: 42.89%, Test: 42.98%
Epoch: 625, Loss: 1.3042, Train: 43.23%, Valid: 42.92%, Test: 43.50%
Epoch: 650, Loss: 1.3126, Train: 43.46%, Valid: 43.44%, Test: 43.56%
Epoch: 675, Loss: 1.3009, Train: 43.53%, Valid: 43.26%, Test: 43.69%
Epoch: 700, Loss: 1.2996, Train: 43.81%, Valid: 43.58%, Test: 44.12%
Epoch: 725, Loss: 1.5824, Train: 28.73%, Valid: 28.50%, Test: 29.30%
Epoch: 750, Loss: 1.4135, Train: 37.51%, Valid: 37.43%, Test: 37.44%
Epoch: 775, Loss: 1.3629, Train: 39.43%, Valid: 39.30%, Test: 39.37%
Epoch: 800, Loss: 1.3756, Train: 39.35%, Valid: 39.19%, Test: 39.35%
Epoch: 825, Loss: 1.3396, Train: 41.44%, Valid: 41.23%, Test: 41.41%
Epoch: 850, Loss: 1.3370, Train: 41.20%, Valid: 41.11%, Test: 41.39%
Epoch: 875, Loss: 1.3354, Train: 41.28%, Valid: 41.15%, Test: 41.38%
Epoch: 900, Loss: 1.3314, Train: 41.62%, Valid: 41.34%, Test: 41.51%
Epoch: 925, Loss: 1.3266, Train: 41.71%, Valid: 41.50%, Test: 41.76%
Epoch: 950, Loss: 1.3375, Train: 40.07%, Valid: 39.75%, Test: 40.31%
Epoch: 975, Loss: 1.3224, Train: 41.65%, Valid: 41.36%, Test: 41.79%
Run 01:
Highest Train: 43.90
Highest Valid: 43.68
  Final Train: 43.90
   Final Test: 44.11
All runs:
Highest Train: 43.90, nan
Highest Valid: 43.68, nan
  Final Train: 43.90, nan
   Final Test: 44.11, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6101, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.5202, Train: 33.91%, Valid: 33.69%, Test: 33.90%
Epoch: 50, Loss: 1.4980, Train: 34.74%, Valid: 34.42%, Test: 34.65%
Epoch: 75, Loss: 1.4863, Train: 35.72%, Valid: 35.14%, Test: 35.34%
Epoch: 100, Loss: 1.4791, Train: 36.20%, Valid: 35.49%, Test: 35.66%
Epoch: 125, Loss: 1.4750, Train: 36.68%, Valid: 35.63%, Test: 35.99%
Epoch: 150, Loss: 1.4725, Train: 37.03%, Valid: 35.77%, Test: 36.09%
Epoch: 175, Loss: 1.4690, Train: 37.32%, Valid: 35.85%, Test: 36.07%
Epoch: 200, Loss: 1.4652, Train: 37.54%, Valid: 35.93%, Test: 36.05%
Epoch: 225, Loss: 1.4648, Train: 37.64%, Valid: 35.80%, Test: 36.18%
Epoch: 250, Loss: 1.4633, Train: 37.81%, Valid: 35.92%, Test: 36.28%
Epoch: 275, Loss: 1.4616, Train: 37.95%, Valid: 35.82%, Test: 36.26%
Epoch: 300, Loss: 1.4604, Train: 37.95%, Valid: 36.01%, Test: 36.30%
Epoch: 325, Loss: 1.4582, Train: 38.07%, Valid: 36.03%, Test: 36.21%
Epoch: 350, Loss: 1.4586, Train: 38.10%, Valid: 35.95%, Test: 36.20%
Epoch: 375, Loss: 1.4574, Train: 38.17%, Valid: 35.94%, Test: 36.12%
Epoch: 400, Loss: 1.4579, Train: 38.20%, Valid: 35.94%, Test: 36.19%
Epoch: 425, Loss: 1.4571, Train: 38.21%, Valid: 35.85%, Test: 36.24%
Epoch: 450, Loss: 1.4552, Train: 38.28%, Valid: 35.85%, Test: 36.22%
Epoch: 475, Loss: 1.4560, Train: 38.27%, Valid: 35.91%, Test: 36.20%
Epoch: 500, Loss: 1.4548, Train: 38.38%, Valid: 35.89%, Test: 36.10%
Epoch: 525, Loss: 1.4541, Train: 38.38%, Valid: 35.84%, Test: 36.10%
Epoch: 550, Loss: 1.4538, Train: 38.28%, Valid: 35.76%, Test: 36.13%
Epoch: 575, Loss: 1.4546, Train: 38.49%, Valid: 35.86%, Test: 36.04%
Epoch: 600, Loss: 1.4538, Train: 38.38%, Valid: 35.70%, Test: 36.05%
Epoch: 625, Loss: 1.4538, Train: 38.48%, Valid: 35.78%, Test: 36.10%
Epoch: 650, Loss: 1.4524, Train: 38.49%, Valid: 35.85%, Test: 36.14%
Epoch: 675, Loss: 1.4521, Train: 38.56%, Valid: 35.88%, Test: 36.05%
Epoch: 700, Loss: 1.4531, Train: 38.55%, Valid: 35.85%, Test: 36.20%
Epoch: 725, Loss: 1.4531, Train: 38.47%, Valid: 35.84%, Test: 36.07%
Epoch: 750, Loss: 1.4515, Train: 38.44%, Valid: 35.80%, Test: 36.13%
Epoch: 775, Loss: 1.4505, Train: 38.51%, Valid: 35.71%, Test: 36.11%
Epoch: 800, Loss: 1.4492, Train: 38.68%, Valid: 35.71%, Test: 36.07%
Epoch: 825, Loss: 1.4503, Train: 38.56%, Valid: 35.75%, Test: 36.11%
Epoch: 850, Loss: 1.4484, Train: 38.61%, Valid: 35.71%, Test: 36.05%
Epoch: 875, Loss: 1.4502, Train: 38.62%, Valid: 35.66%, Test: 35.97%
Epoch: 900, Loss: 1.4512, Train: 38.64%, Valid: 35.65%, Test: 36.10%
Epoch: 925, Loss: 1.4504, Train: 38.68%, Valid: 35.58%, Test: 36.06%
Epoch: 950, Loss: 1.4496, Train: 38.65%, Valid: 35.61%, Test: 35.99%
Epoch: 975, Loss: 1.4493, Train: 38.67%, Valid: 35.52%, Test: 35.99%
Run 01:
Highest Train: 38.78
Highest Valid: 36.06
  Final Train: 38.10
   Final Test: 36.21
All runs:
Highest Train: 38.78, nan
Highest Valid: 36.06, nan
  Final Train: 38.10, nan
   Final Test: 36.21, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6096, Train: 28.67%, Valid: 28.48%, Test: 28.77%
Epoch: 25, Loss: 1.5153, Train: 34.08%, Valid: 33.95%, Test: 34.10%
Epoch: 50, Loss: 1.4951, Train: 34.90%, Valid: 34.59%, Test: 34.89%
Epoch: 75, Loss: 1.4836, Train: 35.63%, Valid: 35.07%, Test: 35.49%
Epoch: 100, Loss: 1.4758, Train: 36.42%, Valid: 35.52%, Test: 35.93%
Epoch: 125, Loss: 1.4698, Train: 37.07%, Valid: 35.86%, Test: 36.20%
Epoch: 150, Loss: 1.4662, Train: 37.45%, Valid: 36.03%, Test: 36.43%
Epoch: 175, Loss: 1.4622, Train: 37.68%, Valid: 36.13%, Test: 36.47%
Epoch: 200, Loss: 1.4593, Train: 37.99%, Valid: 36.27%, Test: 36.65%
Epoch: 225, Loss: 1.4568, Train: 38.14%, Valid: 36.28%, Test: 36.59%
Epoch: 250, Loss: 1.4465, Train: 38.34%, Valid: 36.67%, Test: 36.99%
Epoch: 275, Loss: 1.4541, Train: 38.26%, Valid: 36.14%, Test: 36.48%
Epoch: 300, Loss: 1.4667, Train: 37.65%, Valid: 35.76%, Test: 36.12%
Epoch: 325, Loss: 1.4573, Train: 38.08%, Valid: 36.01%, Test: 36.32%
Epoch: 350, Loss: 1.4521, Train: 38.51%, Valid: 36.22%, Test: 36.70%
Epoch: 375, Loss: 1.4400, Train: 39.16%, Valid: 37.04%, Test: 37.35%
Epoch: 400, Loss: 1.4374, Train: 39.16%, Valid: 37.00%, Test: 37.36%
Epoch: 425, Loss: 1.4354, Train: 38.88%, Valid: 36.80%, Test: 37.11%
Epoch: 450, Loss: 1.4357, Train: 39.03%, Valid: 36.96%, Test: 37.27%
Epoch: 475, Loss: 1.4379, Train: 38.84%, Valid: 36.83%, Test: 37.21%
Epoch: 500, Loss: 1.4441, Train: 37.96%, Valid: 36.51%, Test: 36.71%
Epoch: 525, Loss: 1.4395, Train: 38.60%, Valid: 36.79%, Test: 37.02%
Epoch: 550, Loss: 1.4439, Train: 38.39%, Valid: 36.61%, Test: 36.86%
Epoch: 575, Loss: 1.4317, Train: 39.35%, Valid: 37.39%, Test: 37.65%
Epoch: 600, Loss: 1.4784, Train: 36.50%, Valid: 35.62%, Test: 35.94%
Epoch: 625, Loss: 1.4382, Train: 38.77%, Valid: 37.19%, Test: 37.31%
Epoch: 650, Loss: 1.4638, Train: 38.39%, Valid: 36.87%, Test: 37.11%
Epoch: 675, Loss: 1.4599, Train: 38.86%, Valid: 37.22%, Test: 37.51%
Epoch: 700, Loss: 1.4464, Train: 38.41%, Valid: 37.18%, Test: 37.40%
Epoch: 725, Loss: 1.4374, Train: 39.27%, Valid: 37.89%, Test: 38.00%
Epoch: 750, Loss: 1.4364, Train: 39.48%, Valid: 38.07%, Test: 38.17%
Epoch: 775, Loss: 1.4313, Train: 40.69%, Valid: 39.21%, Test: 39.05%
Epoch: 800, Loss: 1.4192, Train: 40.53%, Valid: 39.27%, Test: 39.36%
Epoch: 825, Loss: 1.4259, Train: 41.06%, Valid: 39.65%, Test: 39.70%
Epoch: 850, Loss: 1.4303, Train: 40.93%, Valid: 39.58%, Test: 39.75%
Epoch: 875, Loss: 1.4049, Train: 39.84%, Valid: 39.02%, Test: 39.14%
Epoch: 900, Loss: 1.5009, Train: 32.37%, Valid: 31.42%, Test: 31.57%
Epoch: 925, Loss: 1.4303, Train: 30.85%, Valid: 30.75%, Test: 31.12%
Epoch: 950, Loss: 1.4655, Train: 36.13%, Valid: 35.64%, Test: 35.62%
Epoch: 975, Loss: 1.4409, Train: 36.59%, Valid: 36.04%, Test: 36.24%
Run 01:
Highest Train: 41.62
Highest Valid: 40.25
  Final Train: 41.45
   Final Test: 40.28
All runs:
Highest Train: 41.62, nan
Highest Valid: 40.25, nan
  Final Train: 41.45, nan
   Final Test: 40.28, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6199, Train: 21.24%, Valid: 21.39%, Test: 21.87%
Epoch: 25, Loss: 1.5152, Train: 28.73%, Valid: 28.43%, Test: 28.80%
Epoch: 50, Loss: 1.4710, Train: 36.28%, Valid: 36.25%, Test: 36.61%
Epoch: 75, Loss: 1.4300, Train: 35.91%, Valid: 35.57%, Test: 35.53%
Epoch: 100, Loss: 1.4145, Train: 39.56%, Valid: 39.14%, Test: 39.31%
Epoch: 125, Loss: 1.4178, Train: 39.04%, Valid: 38.81%, Test: 38.80%
Epoch: 150, Loss: 1.3977, Train: 39.84%, Valid: 39.67%, Test: 39.60%
Epoch: 175, Loss: 1.3831, Train: 39.82%, Valid: 39.79%, Test: 39.69%
Epoch: 200, Loss: 1.3775, Train: 40.78%, Valid: 40.69%, Test: 40.57%
Epoch: 225, Loss: 1.3823, Train: 40.78%, Valid: 40.39%, Test: 40.48%
Epoch: 250, Loss: 1.3654, Train: 41.69%, Valid: 41.34%, Test: 41.38%
Epoch: 275, Loss: 1.3654, Train: 41.45%, Valid: 41.16%, Test: 41.03%
Epoch: 300, Loss: 1.3734, Train: 40.63%, Valid: 40.26%, Test: 40.30%
Epoch: 325, Loss: 1.3672, Train: 41.02%, Valid: 40.53%, Test: 40.60%
Epoch: 350, Loss: 1.3595, Train: 40.21%, Valid: 39.75%, Test: 39.85%
Epoch: 375, Loss: 1.3674, Train: 40.31%, Valid: 40.05%, Test: 39.99%
Epoch: 400, Loss: 1.3706, Train: 41.60%, Valid: 41.24%, Test: 41.23%
Epoch: 425, Loss: 1.3802, Train: 39.68%, Valid: 39.43%, Test: 39.59%
Epoch: 450, Loss: 1.3737, Train: 39.53%, Valid: 39.28%, Test: 39.10%
Epoch: 475, Loss: 1.3672, Train: 37.69%, Valid: 37.35%, Test: 37.57%
Epoch: 500, Loss: 1.4109, Train: 39.92%, Valid: 39.74%, Test: 39.91%
Epoch: 525, Loss: 1.4201, Train: 38.94%, Valid: 38.63%, Test: 38.77%
Epoch: 550, Loss: 1.3799, Train: 36.62%, Valid: 36.32%, Test: 36.24%
Epoch: 575, Loss: 1.3701, Train: 38.79%, Valid: 38.40%, Test: 38.45%
Epoch: 600, Loss: 1.3616, Train: 40.06%, Valid: 40.02%, Test: 40.01%
Epoch: 625, Loss: 1.3544, Train: 41.49%, Valid: 41.16%, Test: 41.27%
Epoch: 650, Loss: 1.3619, Train: 41.64%, Valid: 41.33%, Test: 41.53%
Epoch: 675, Loss: 1.5177, Train: 24.59%, Valid: 24.72%, Test: 24.86%
Epoch: 700, Loss: 1.4185, Train: 30.80%, Valid: 30.41%, Test: 30.65%
Epoch: 725, Loss: 1.3974, Train: 42.31%, Valid: 42.12%, Test: 42.16%
Epoch: 750, Loss: 1.3841, Train: 41.79%, Valid: 41.45%, Test: 41.68%
Epoch: 775, Loss: 1.3789, Train: 40.75%, Valid: 40.49%, Test: 40.59%
Epoch: 800, Loss: 1.4044, Train: 41.54%, Valid: 41.18%, Test: 41.37%
Epoch: 825, Loss: 1.3796, Train: 41.39%, Valid: 41.04%, Test: 41.16%
Epoch: 850, Loss: 1.3764, Train: 40.26%, Valid: 39.92%, Test: 40.02%
Epoch: 875, Loss: 1.3702, Train: 40.55%, Valid: 40.25%, Test: 40.43%
Epoch: 900, Loss: 1.3866, Train: 40.15%, Valid: 39.70%, Test: 39.90%
Epoch: 925, Loss: 1.3829, Train: 38.63%, Valid: 38.21%, Test: 38.34%
Epoch: 950, Loss: 1.3698, Train: 40.71%, Valid: 40.39%, Test: 40.55%
Epoch: 975, Loss: 1.3601, Train: 39.82%, Valid: 39.45%, Test: 39.50%
Run 01:
Highest Train: 42.52
Highest Valid: 42.41
  Final Train: 42.50
   Final Test: 42.56
All runs:
Highest Train: 42.52, nan
Highest Valid: 42.41, nan
  Final Train: 42.50, nan
   Final Test: 42.56, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6108, Train: 28.60%, Valid: 28.42%, Test: 28.63%
Epoch: 25, Loss: 1.5325, Train: 33.88%, Valid: 33.71%, Test: 33.95%
Epoch: 50, Loss: 1.5005, Train: 34.63%, Valid: 34.32%, Test: 34.53%
Epoch: 75, Loss: 1.4895, Train: 35.22%, Valid: 34.77%, Test: 34.89%
Epoch: 100, Loss: 1.4815, Train: 36.01%, Valid: 35.23%, Test: 35.62%
Epoch: 125, Loss: 1.4787, Train: 36.43%, Valid: 35.40%, Test: 35.89%
Epoch: 150, Loss: 1.4748, Train: 36.71%, Valid: 35.49%, Test: 35.90%
Epoch: 175, Loss: 1.4721, Train: 36.99%, Valid: 35.49%, Test: 36.01%
Epoch: 200, Loss: 1.4697, Train: 37.20%, Valid: 35.65%, Test: 36.04%
Epoch: 225, Loss: 1.4666, Train: 37.41%, Valid: 35.55%, Test: 36.16%
Epoch: 250, Loss: 1.4685, Train: 37.49%, Valid: 35.62%, Test: 36.19%
Epoch: 275, Loss: 1.4642, Train: 37.69%, Valid: 35.68%, Test: 36.23%
Epoch: 300, Loss: 1.4632, Train: 37.74%, Valid: 35.71%, Test: 36.24%
Epoch: 325, Loss: 1.4620, Train: 37.93%, Valid: 35.72%, Test: 36.26%
Epoch: 350, Loss: 1.4597, Train: 38.02%, Valid: 35.85%, Test: 36.20%
Epoch: 375, Loss: 1.4609, Train: 38.09%, Valid: 35.81%, Test: 36.27%
Epoch: 400, Loss: 1.4604, Train: 38.20%, Valid: 35.91%, Test: 36.26%
Epoch: 425, Loss: 1.4590, Train: 38.14%, Valid: 35.71%, Test: 36.09%
Epoch: 450, Loss: 1.4576, Train: 38.15%, Valid: 35.67%, Test: 36.25%
Epoch: 475, Loss: 1.4576, Train: 38.24%, Valid: 35.69%, Test: 36.17%
Epoch: 500, Loss: 1.4585, Train: 38.26%, Valid: 35.59%, Test: 36.17%
Epoch: 525, Loss: 1.4557, Train: 38.29%, Valid: 35.65%, Test: 36.24%
Epoch: 550, Loss: 1.4567, Train: 38.25%, Valid: 35.62%, Test: 36.17%
Epoch: 575, Loss: 1.4570, Train: 38.21%, Valid: 35.61%, Test: 36.19%
Epoch: 600, Loss: 1.4569, Train: 38.31%, Valid: 35.61%, Test: 36.18%
Epoch: 625, Loss: 1.4544, Train: 38.35%, Valid: 35.61%, Test: 36.16%
Epoch: 650, Loss: 1.4563, Train: 38.33%, Valid: 35.52%, Test: 36.24%
Epoch: 675, Loss: 1.4569, Train: 38.39%, Valid: 35.59%, Test: 36.23%
Epoch: 700, Loss: 1.4530, Train: 38.32%, Valid: 35.51%, Test: 36.26%
Epoch: 725, Loss: 1.4550, Train: 38.39%, Valid: 35.54%, Test: 36.11%
Epoch: 750, Loss: 1.4545, Train: 38.47%, Valid: 35.48%, Test: 36.14%
Epoch: 775, Loss: 1.4539, Train: 38.31%, Valid: 35.51%, Test: 36.13%
Epoch: 800, Loss: 1.4538, Train: 38.34%, Valid: 35.53%, Test: 36.04%
Epoch: 825, Loss: 1.4556, Train: 38.35%, Valid: 35.48%, Test: 36.02%
Epoch: 850, Loss: 1.4556, Train: 38.40%, Valid: 35.53%, Test: 36.07%
Epoch: 875, Loss: 1.4532, Train: 38.42%, Valid: 35.52%, Test: 36.04%
Epoch: 900, Loss: 1.4526, Train: 38.44%, Valid: 35.46%, Test: 36.04%
Epoch: 925, Loss: 1.4519, Train: 38.45%, Valid: 35.42%, Test: 35.96%
Epoch: 950, Loss: 1.4533, Train: 38.45%, Valid: 35.41%, Test: 36.01%
Epoch: 975, Loss: 1.4546, Train: 38.39%, Valid: 35.54%, Test: 36.10%
Run 01:
Highest Train: 38.61
Highest Valid: 35.91
  Final Train: 38.20
   Final Test: 36.26
All runs:
Highest Train: 38.61, nan
Highest Valid: 35.91, nan
  Final Train: 38.20, nan
   Final Test: 36.26, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6104, Train: 30.20%, Valid: 29.92%, Test: 30.31%
Epoch: 25, Loss: 1.5175, Train: 34.07%, Valid: 33.84%, Test: 34.04%
Epoch: 50, Loss: 1.4971, Train: 34.77%, Valid: 34.50%, Test: 34.64%
Epoch: 75, Loss: 1.4883, Train: 35.41%, Valid: 34.92%, Test: 35.15%
Epoch: 100, Loss: 1.4797, Train: 36.31%, Valid: 35.37%, Test: 35.83%
Epoch: 125, Loss: 1.4734, Train: 36.76%, Valid: 35.54%, Test: 36.02%
Epoch: 150, Loss: 1.4691, Train: 37.06%, Valid: 35.63%, Test: 36.13%
Epoch: 175, Loss: 1.4644, Train: 37.43%, Valid: 35.80%, Test: 36.33%
Epoch: 200, Loss: 1.4617, Train: 37.72%, Valid: 36.05%, Test: 36.49%
Epoch: 225, Loss: 1.4559, Train: 38.04%, Valid: 36.28%, Test: 36.59%
Epoch: 250, Loss: 1.4558, Train: 38.13%, Valid: 36.33%, Test: 36.71%
Epoch: 275, Loss: 1.4597, Train: 37.80%, Valid: 35.91%, Test: 36.19%
Epoch: 300, Loss: 1.4476, Train: 38.50%, Valid: 36.64%, Test: 36.91%
Epoch: 325, Loss: 1.4373, Train: 39.03%, Valid: 37.16%, Test: 37.57%
Epoch: 350, Loss: 1.4424, Train: 38.53%, Valid: 36.62%, Test: 36.89%
Epoch: 375, Loss: 1.4600, Train: 38.24%, Valid: 36.19%, Test: 36.43%
Epoch: 400, Loss: 1.4376, Train: 39.01%, Valid: 36.93%, Test: 37.19%
Epoch: 425, Loss: 1.4314, Train: 39.23%, Valid: 36.98%, Test: 37.30%
Epoch: 450, Loss: 1.4393, Train: 38.76%, Valid: 36.60%, Test: 36.94%
Epoch: 475, Loss: 1.4306, Train: 39.52%, Valid: 37.56%, Test: 37.89%
Epoch: 500, Loss: 1.4348, Train: 38.87%, Valid: 37.13%, Test: 37.53%
Epoch: 525, Loss: 1.4445, Train: 37.81%, Valid: 36.37%, Test: 36.40%
Epoch: 550, Loss: 1.4426, Train: 37.98%, Valid: 36.34%, Test: 36.50%
Epoch: 575, Loss: 1.4364, Train: 39.52%, Valid: 37.81%, Test: 37.82%
Epoch: 600, Loss: 1.4357, Train: 38.89%, Valid: 37.08%, Test: 37.54%
Epoch: 625, Loss: 1.4431, Train: 38.01%, Valid: 36.55%, Test: 36.68%
Epoch: 650, Loss: 1.4498, Train: 38.99%, Valid: 37.45%, Test: 37.53%
Epoch: 675, Loss: 1.4308, Train: 39.37%, Valid: 37.88%, Test: 37.91%
Epoch: 700, Loss: 1.4361, Train: 39.80%, Valid: 38.08%, Test: 38.29%
Epoch: 725, Loss: 1.4279, Train: 39.61%, Valid: 38.06%, Test: 38.23%
Epoch: 750, Loss: 1.4382, Train: 39.01%, Valid: 37.51%, Test: 37.72%
Epoch: 775, Loss: 1.4349, Train: 38.86%, Valid: 37.15%, Test: 37.55%
Epoch: 800, Loss: 1.4432, Train: 38.48%, Valid: 36.95%, Test: 37.23%
Epoch: 825, Loss: 1.4377, Train: 38.91%, Valid: 37.30%, Test: 37.44%
Epoch: 850, Loss: 1.4368, Train: 39.37%, Valid: 38.10%, Test: 38.37%
Epoch: 875, Loss: 1.4368, Train: 39.64%, Valid: 38.29%, Test: 38.47%
Epoch: 900, Loss: 1.4283, Train: 41.28%, Valid: 39.85%, Test: 39.80%
Epoch: 925, Loss: 1.4277, Train: 38.46%, Valid: 37.10%, Test: 37.33%
Epoch: 950, Loss: 1.4157, Train: 41.89%, Valid: 40.61%, Test: 40.67%
Epoch: 975, Loss: 1.4255, Train: 39.09%, Valid: 37.91%, Test: 38.12%
Run 01:
Highest Train: 41.89
Highest Valid: 40.63
  Final Train: 41.89
   Final Test: 40.63
All runs:
Highest Train: 41.89, nan
Highest Valid: 40.63, nan
  Final Train: 41.89, nan
   Final Test: 40.63, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6239, Train: 21.01%, Valid: 21.25%, Test: 21.59%
Epoch: 25, Loss: 1.5032, Train: 28.29%, Valid: 28.63%, Test: 28.96%
Epoch: 50, Loss: 1.4434, Train: 32.27%, Valid: 32.16%, Test: 32.85%
Epoch: 75, Loss: 1.4294, Train: 38.45%, Valid: 38.13%, Test: 38.30%
Epoch: 100, Loss: 1.4413, Train: 38.18%, Valid: 37.74%, Test: 37.98%
Epoch: 125, Loss: 1.4267, Train: 35.36%, Valid: 35.11%, Test: 35.37%
Epoch: 150, Loss: 1.4093, Train: 35.76%, Valid: 35.12%, Test: 35.50%
Epoch: 175, Loss: 1.5662, Train: 20.55%, Valid: 20.49%, Test: 20.43%
Epoch: 200, Loss: 1.4505, Train: 36.69%, Valid: 36.33%, Test: 36.92%
Epoch: 225, Loss: 1.4210, Train: 35.48%, Valid: 35.34%, Test: 35.82%
Epoch: 250, Loss: 1.4443, Train: 35.71%, Valid: 35.34%, Test: 35.86%
Epoch: 275, Loss: 1.4341, Train: 38.69%, Valid: 38.40%, Test: 38.73%
Epoch: 300, Loss: 1.4328, Train: 41.29%, Valid: 40.98%, Test: 41.41%
Epoch: 325, Loss: 1.4292, Train: 39.65%, Valid: 39.39%, Test: 39.65%
Epoch: 350, Loss: 1.4064, Train: 38.83%, Valid: 38.59%, Test: 39.14%
Epoch: 375, Loss: 1.4053, Train: 40.11%, Valid: 39.74%, Test: 40.21%
Epoch: 400, Loss: 1.4210, Train: 39.34%, Valid: 38.91%, Test: 39.32%
Epoch: 425, Loss: 1.4049, Train: 40.16%, Valid: 39.82%, Test: 40.31%
Epoch: 450, Loss: 1.4022, Train: 35.05%, Valid: 34.81%, Test: 35.21%
Epoch: 475, Loss: 1.4035, Train: 41.37%, Valid: 41.08%, Test: 41.47%
Epoch: 500, Loss: 1.5579, Train: 20.22%, Valid: 20.42%, Test: 20.36%
Epoch: 525, Loss: 1.5131, Train: 30.49%, Valid: 30.63%, Test: 31.03%
Epoch: 550, Loss: 1.4729, Train: 36.02%, Valid: 36.07%, Test: 36.47%
Epoch: 575, Loss: 1.4633, Train: 35.52%, Valid: 35.29%, Test: 35.67%
Epoch: 600, Loss: 1.4455, Train: 34.58%, Valid: 34.44%, Test: 35.03%
Epoch: 625, Loss: 1.4933, Train: 29.31%, Valid: 29.75%, Test: 29.96%
Epoch: 650, Loss: 1.4944, Train: 28.25%, Valid: 28.34%, Test: 29.04%
Epoch: 675, Loss: 1.4664, Train: 37.59%, Valid: 37.37%, Test: 38.02%
Epoch: 700, Loss: 1.4567, Train: 38.33%, Valid: 38.23%, Test: 38.78%
Epoch: 725, Loss: 1.4509, Train: 38.87%, Valid: 38.76%, Test: 39.18%
Epoch: 750, Loss: 1.4585, Train: 38.78%, Valid: 38.95%, Test: 39.03%
Epoch: 775, Loss: 1.4317, Train: 36.14%, Valid: 36.19%, Test: 36.06%
Epoch: 800, Loss: 1.4387, Train: 36.76%, Valid: 36.57%, Test: 36.63%
Epoch: 825, Loss: 1.4193, Train: 37.23%, Valid: 37.20%, Test: 37.34%
Epoch: 850, Loss: 1.4548, Train: 37.79%, Valid: 37.70%, Test: 37.68%
Epoch: 875, Loss: 1.4482, Train: 33.00%, Valid: 33.26%, Test: 33.33%
Epoch: 900, Loss: 1.4232, Train: 30.37%, Valid: 30.41%, Test: 29.99%
Epoch: 925, Loss: 1.4246, Train: 36.33%, Valid: 36.19%, Test: 36.00%
Epoch: 950, Loss: 1.4076, Train: 37.85%, Valid: 37.66%, Test: 37.72%
Epoch: 975, Loss: 1.4543, Train: 37.76%, Valid: 37.43%, Test: 37.55%
Run 01:
Highest Train: 41.37
Highest Valid: 41.08
  Final Train: 41.37
   Final Test: 41.47
All runs:
Highest Train: 41.37, nan
Highest Valid: 41.08, nan
  Final Train: 41.37, nan
   Final Test: 41.47, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6105, Train: 17.64%, Valid: 17.78%, Test: 17.33%
Epoch: 25, Loss: 1.5083, Train: 34.04%, Valid: 33.79%, Test: 34.02%
Epoch: 50, Loss: 1.4887, Train: 34.87%, Valid: 34.54%, Test: 34.71%
Epoch: 75, Loss: 1.4745, Train: 35.57%, Valid: 35.01%, Test: 35.42%
Epoch: 100, Loss: 1.4612, Train: 36.37%, Valid: 35.58%, Test: 36.17%
Epoch: 125, Loss: 1.4512, Train: 36.93%, Valid: 35.99%, Test: 36.15%
Epoch: 150, Loss: 1.4427, Train: 37.33%, Valid: 35.97%, Test: 36.35%
Epoch: 175, Loss: 1.4380, Train: 37.61%, Valid: 36.09%, Test: 36.47%
Epoch: 200, Loss: 1.4388, Train: 37.75%, Valid: 36.15%, Test: 36.23%
Epoch: 225, Loss: 1.4267, Train: 38.07%, Valid: 36.23%, Test: 36.45%
Epoch: 250, Loss: 1.4224, Train: 38.28%, Valid: 36.20%, Test: 36.25%
Epoch: 275, Loss: 1.4181, Train: 38.65%, Valid: 36.01%, Test: 36.27%
Epoch: 300, Loss: 1.4155, Train: 38.64%, Valid: 36.17%, Test: 36.26%
Epoch: 325, Loss: 1.4088, Train: 39.05%, Valid: 35.77%, Test: 36.11%
Epoch: 350, Loss: 1.4066, Train: 39.14%, Valid: 36.02%, Test: 36.26%
Epoch: 375, Loss: 1.4040, Train: 39.18%, Valid: 35.67%, Test: 36.04%
Epoch: 400, Loss: 1.4004, Train: 39.46%, Valid: 35.39%, Test: 36.07%
Epoch: 425, Loss: 1.4009, Train: 39.52%, Valid: 35.15%, Test: 35.64%
Epoch: 450, Loss: 1.3959, Train: 39.84%, Valid: 35.42%, Test: 35.84%
Epoch: 475, Loss: 1.3927, Train: 39.96%, Valid: 35.39%, Test: 35.90%
Epoch: 500, Loss: 1.3930, Train: 39.99%, Valid: 35.47%, Test: 35.98%
Epoch: 525, Loss: 1.3962, Train: 40.09%, Valid: 35.10%, Test: 35.56%
Epoch: 550, Loss: 1.3870, Train: 40.32%, Valid: 35.46%, Test: 35.87%
Epoch: 575, Loss: 1.3854, Train: 40.40%, Valid: 35.36%, Test: 35.83%
Epoch: 600, Loss: 1.3820, Train: 40.49%, Valid: 35.25%, Test: 35.69%
Epoch: 625, Loss: 1.3824, Train: 40.54%, Valid: 35.50%, Test: 35.84%
Epoch: 650, Loss: 1.3845, Train: 40.66%, Valid: 35.36%, Test: 35.79%
Epoch: 675, Loss: 1.3812, Train: 40.76%, Valid: 35.08%, Test: 35.56%
Epoch: 700, Loss: 1.3844, Train: 40.33%, Valid: 34.29%, Test: 34.92%
Epoch: 725, Loss: 1.3743, Train: 40.63%, Valid: 35.65%, Test: 35.95%
Epoch: 750, Loss: 1.3757, Train: 40.88%, Valid: 34.80%, Test: 35.10%
Epoch: 775, Loss: 1.3710, Train: 41.00%, Valid: 35.39%, Test: 35.64%
Epoch: 800, Loss: 1.3749, Train: 41.16%, Valid: 35.07%, Test: 35.30%
Epoch: 825, Loss: 1.3684, Train: 41.05%, Valid: 34.95%, Test: 35.20%
Epoch: 850, Loss: 1.3728, Train: 40.88%, Valid: 35.22%, Test: 35.45%
Epoch: 875, Loss: 1.3767, Train: 41.13%, Valid: 34.78%, Test: 34.83%
Epoch: 900, Loss: 1.3670, Train: 41.10%, Valid: 34.86%, Test: 35.08%
Epoch: 925, Loss: 1.3640, Train: 41.42%, Valid: 35.12%, Test: 35.40%
Epoch: 950, Loss: 1.3649, Train: 41.33%, Valid: 35.29%, Test: 35.39%
Epoch: 975, Loss: 1.3726, Train: 41.10%, Valid: 35.37%, Test: 35.53%
Run 01:
Highest Train: 41.57
Highest Valid: 36.36
  Final Train: 37.77
   Final Test: 36.60
All runs:
Highest Train: 41.57, nan
Highest Valid: 36.36, nan
  Final Train: 37.77, nan
   Final Test: 36.60, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6111, Train: 26.80%, Valid: 26.70%, Test: 26.57%
Epoch: 25, Loss: 1.5533, Train: 28.71%, Valid: 28.52%, Test: 28.80%
Epoch: 50, Loss: 1.5226, Train: 28.70%, Valid: 28.52%, Test: 28.80%
Epoch: 75, Loss: 1.4927, Train: 29.52%, Valid: 29.48%, Test: 29.56%
Epoch: 100, Loss: 1.4712, Train: 30.21%, Valid: 30.31%, Test: 30.55%
Epoch: 125, Loss: 1.4791, Train: 28.71%, Valid: 28.54%, Test: 28.66%
Epoch: 150, Loss: 1.4660, Train: 30.39%, Valid: 30.20%, Test: 30.74%
Epoch: 175, Loss: 1.4588, Train: 32.05%, Valid: 31.84%, Test: 32.33%
Epoch: 200, Loss: 1.4800, Train: 28.74%, Valid: 28.53%, Test: 28.83%
Epoch: 225, Loss: 1.4529, Train: 32.74%, Valid: 32.26%, Test: 33.25%
Epoch: 250, Loss: 1.4389, Train: 34.19%, Valid: 33.88%, Test: 34.74%
Epoch: 275, Loss: 1.4329, Train: 34.80%, Valid: 34.56%, Test: 35.46%
Epoch: 300, Loss: 1.4226, Train: 36.70%, Valid: 36.46%, Test: 37.06%
Epoch: 325, Loss: 1.4068, Train: 37.88%, Valid: 37.90%, Test: 38.35%
Epoch: 350, Loss: 1.4179, Train: 37.20%, Valid: 37.11%, Test: 37.62%
Epoch: 375, Loss: 1.4023, Train: 38.53%, Valid: 38.37%, Test: 38.70%
Epoch: 400, Loss: 1.4090, Train: 38.99%, Valid: 38.85%, Test: 39.25%
Epoch: 425, Loss: 1.3986, Train: 39.41%, Valid: 39.24%, Test: 39.67%
Epoch: 450, Loss: 1.4160, Train: 37.93%, Valid: 37.96%, Test: 38.40%
Epoch: 475, Loss: 1.3933, Train: 39.52%, Valid: 39.40%, Test: 39.75%
Epoch: 500, Loss: 1.3883, Train: 40.18%, Valid: 40.23%, Test: 40.52%
Epoch: 525, Loss: 1.4037, Train: 38.63%, Valid: 38.31%, Test: 38.70%
Epoch: 550, Loss: 1.3907, Train: 40.81%, Valid: 40.68%, Test: 41.12%
Epoch: 575, Loss: 1.3813, Train: 39.67%, Valid: 39.08%, Test: 39.55%
Epoch: 600, Loss: 1.3926, Train: 39.21%, Valid: 38.87%, Test: 39.07%
Epoch: 625, Loss: 1.3807, Train: 40.26%, Valid: 39.75%, Test: 40.17%
Epoch: 650, Loss: 1.4200, Train: 38.59%, Valid: 38.42%, Test: 38.73%
Epoch: 675, Loss: 1.3779, Train: 40.91%, Valid: 40.57%, Test: 41.06%
Epoch: 700, Loss: 1.3666, Train: 40.90%, Valid: 40.40%, Test: 40.99%
Epoch: 725, Loss: 1.3957, Train: 39.90%, Valid: 39.49%, Test: 39.55%
Epoch: 750, Loss: 1.3723, Train: 41.02%, Valid: 40.75%, Test: 41.31%
Epoch: 775, Loss: 1.3649, Train: 41.16%, Valid: 40.82%, Test: 41.24%
Epoch: 800, Loss: 1.3595, Train: 41.10%, Valid: 40.83%, Test: 41.29%
Epoch: 825, Loss: 1.3609, Train: 41.24%, Valid: 40.94%, Test: 41.42%
Epoch: 850, Loss: 1.3555, Train: 41.12%, Valid: 40.87%, Test: 41.29%
Epoch: 875, Loss: 1.3503, Train: 41.82%, Valid: 41.48%, Test: 41.81%
Epoch: 900, Loss: 1.3479, Train: 41.73%, Valid: 41.43%, Test: 41.81%
Epoch: 925, Loss: 1.3465, Train: 41.14%, Valid: 40.76%, Test: 41.11%
Epoch: 950, Loss: 1.4156, Train: 38.86%, Valid: 38.59%, Test: 39.02%
Epoch: 975, Loss: 1.3665, Train: 41.03%, Valid: 40.93%, Test: 41.07%
Run 01:
Highest Train: 41.97
Highest Valid: 41.67
  Final Train: 41.93
   Final Test: 41.86
All runs:
Highest Train: 41.97, nan
Highest Valid: 41.67, nan
  Final Train: 41.93, nan
   Final Test: 41.86, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6171, Train: 24.97%, Valid: 24.87%, Test: 24.95%
Epoch: 25, Loss: 1.4804, Train: 29.78%, Valid: 29.82%, Test: 29.95%
Epoch: 50, Loss: 1.4525, Train: 28.62%, Valid: 28.43%, Test: 28.74%
Epoch: 75, Loss: 1.4422, Train: 28.70%, Valid: 28.51%, Test: 28.77%
Epoch: 100, Loss: 1.4520, Train: 31.23%, Valid: 31.17%, Test: 31.26%
Epoch: 125, Loss: 1.4240, Train: 37.89%, Valid: 37.60%, Test: 37.60%
Epoch: 150, Loss: 1.4528, Train: 35.21%, Valid: 34.95%, Test: 35.25%
Epoch: 175, Loss: 1.4226, Train: 36.67%, Valid: 36.43%, Test: 36.55%
Epoch: 200, Loss: 1.4042, Train: 38.11%, Valid: 37.82%, Test: 37.89%
Epoch: 225, Loss: 1.4560, Train: 33.83%, Valid: 33.52%, Test: 34.12%
Epoch: 250, Loss: 1.4126, Train: 37.43%, Valid: 37.13%, Test: 37.32%
Epoch: 275, Loss: 1.4054, Train: 38.39%, Valid: 38.02%, Test: 38.36%
Epoch: 300, Loss: 1.3856, Train: 39.08%, Valid: 38.71%, Test: 39.04%
Epoch: 325, Loss: 1.4394, Train: 34.50%, Valid: 34.37%, Test: 34.51%
Epoch: 350, Loss: 1.4524, Train: 33.79%, Valid: 33.49%, Test: 33.95%
Epoch: 375, Loss: 1.3993, Train: 38.38%, Valid: 38.12%, Test: 38.34%
Epoch: 400, Loss: 1.3806, Train: 38.68%, Valid: 38.43%, Test: 38.74%
Epoch: 425, Loss: 1.3710, Train: 38.90%, Valid: 38.66%, Test: 38.96%
Epoch: 450, Loss: 1.3675, Train: 38.79%, Valid: 38.48%, Test: 38.69%
Epoch: 475, Loss: 1.3700, Train: 38.84%, Valid: 38.59%, Test: 38.92%
Epoch: 500, Loss: 1.3593, Train: 38.89%, Valid: 38.58%, Test: 38.80%
Epoch: 525, Loss: 1.3551, Train: 39.02%, Valid: 38.85%, Test: 39.11%
Epoch: 550, Loss: 1.3506, Train: 39.30%, Valid: 39.12%, Test: 39.37%
Epoch: 575, Loss: 1.3581, Train: 36.05%, Valid: 35.85%, Test: 36.03%
Epoch: 600, Loss: 1.3779, Train: 38.49%, Valid: 38.16%, Test: 38.43%
Epoch: 625, Loss: 1.3711, Train: 39.54%, Valid: 39.36%, Test: 39.60%
Epoch: 650, Loss: 1.4335, Train: 36.95%, Valid: 36.89%, Test: 37.23%
Epoch: 675, Loss: 1.4768, Train: 35.63%, Valid: 35.54%, Test: 35.58%
Epoch: 700, Loss: 1.4217, Train: 37.70%, Valid: 37.35%, Test: 37.55%
Epoch: 725, Loss: 1.3958, Train: 38.61%, Valid: 38.45%, Test: 38.66%
Epoch: 750, Loss: 1.3831, Train: 38.94%, Valid: 38.80%, Test: 38.96%
Epoch: 775, Loss: 1.3752, Train: 39.27%, Valid: 39.13%, Test: 39.10%
Epoch: 800, Loss: 1.3685, Train: 39.45%, Valid: 39.21%, Test: 39.25%
Epoch: 825, Loss: 1.4062, Train: 37.65%, Valid: 37.46%, Test: 37.45%
Epoch: 850, Loss: 1.3854, Train: 39.25%, Valid: 38.79%, Test: 39.06%
Epoch: 875, Loss: 1.3744, Train: 39.66%, Valid: 39.36%, Test: 39.60%
Epoch: 900, Loss: 1.3674, Train: 39.99%, Valid: 39.54%, Test: 39.86%
Epoch: 925, Loss: 1.3632, Train: 39.54%, Valid: 39.26%, Test: 39.35%
Epoch: 950, Loss: 1.3641, Train: 40.16%, Valid: 39.73%, Test: 40.09%
Epoch: 975, Loss: 1.3604, Train: 39.96%, Valid: 39.46%, Test: 39.74%
Run 01:
Highest Train: 40.38
Highest Valid: 40.03
  Final Train: 40.31
   Final Test: 40.26
All runs:
Highest Train: 40.38, nan
Highest Valid: 40.03, nan
  Final Train: 40.31, nan
   Final Test: 40.26, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6102, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.5082, Train: 33.91%, Valid: 33.70%, Test: 33.84%
Epoch: 50, Loss: 1.4902, Train: 34.71%, Valid: 34.45%, Test: 34.66%
Epoch: 75, Loss: 1.4775, Train: 35.56%, Valid: 34.92%, Test: 35.30%
Epoch: 100, Loss: 1.4663, Train: 36.13%, Valid: 35.29%, Test: 35.75%
Epoch: 125, Loss: 1.4574, Train: 36.58%, Valid: 35.67%, Test: 35.96%
Epoch: 150, Loss: 1.4503, Train: 36.93%, Valid: 35.82%, Test: 36.03%
Epoch: 175, Loss: 1.4435, Train: 36.97%, Valid: 35.65%, Test: 35.91%
Epoch: 200, Loss: 1.4377, Train: 37.50%, Valid: 36.13%, Test: 36.32%
Epoch: 225, Loss: 1.4339, Train: 37.71%, Valid: 36.10%, Test: 36.24%
Epoch: 250, Loss: 1.4360, Train: 37.92%, Valid: 35.85%, Test: 36.23%
Epoch: 275, Loss: 1.4249, Train: 38.16%, Valid: 35.78%, Test: 36.19%
Epoch: 300, Loss: 1.4210, Train: 38.24%, Valid: 35.86%, Test: 36.19%
Epoch: 325, Loss: 1.4189, Train: 38.47%, Valid: 35.70%, Test: 36.27%
Epoch: 350, Loss: 1.4284, Train: 38.64%, Valid: 35.64%, Test: 36.16%
Epoch: 375, Loss: 1.4107, Train: 38.98%, Valid: 35.43%, Test: 35.98%
Epoch: 400, Loss: 1.4061, Train: 39.22%, Valid: 35.52%, Test: 36.16%
Epoch: 425, Loss: 1.4040, Train: 39.43%, Valid: 35.52%, Test: 36.15%
Epoch: 450, Loss: 1.4010, Train: 39.52%, Valid: 35.46%, Test: 35.94%
Epoch: 475, Loss: 1.3995, Train: 39.61%, Valid: 35.24%, Test: 35.58%
Epoch: 500, Loss: 1.3959, Train: 39.80%, Valid: 35.50%, Test: 35.92%
Epoch: 525, Loss: 1.3936, Train: 39.73%, Valid: 35.51%, Test: 36.06%
Epoch: 550, Loss: 1.3899, Train: 40.04%, Valid: 35.20%, Test: 35.73%
Epoch: 575, Loss: 1.3887, Train: 40.05%, Valid: 35.39%, Test: 35.83%
Epoch: 600, Loss: 1.3863, Train: 40.04%, Valid: 34.88%, Test: 35.19%
Epoch: 625, Loss: 1.3845, Train: 40.21%, Valid: 35.25%, Test: 35.75%
Epoch: 650, Loss: 1.3835, Train: 40.35%, Valid: 35.24%, Test: 35.47%
Epoch: 675, Loss: 1.3943, Train: 40.29%, Valid: 35.15%, Test: 35.57%
Epoch: 700, Loss: 1.3791, Train: 40.57%, Valid: 34.87%, Test: 35.26%
Epoch: 725, Loss: 1.3826, Train: 40.31%, Valid: 35.48%, Test: 35.90%
Epoch: 750, Loss: 1.3783, Train: 40.62%, Valid: 34.72%, Test: 35.04%
Epoch: 775, Loss: 1.3813, Train: 40.43%, Valid: 34.73%, Test: 35.12%
Epoch: 800, Loss: 1.3736, Train: 40.75%, Valid: 34.92%, Test: 35.20%
Epoch: 825, Loss: 1.3744, Train: 40.81%, Valid: 34.63%, Test: 35.06%
Epoch: 850, Loss: 1.3729, Train: 40.83%, Valid: 34.58%, Test: 35.01%
Epoch: 875, Loss: 1.3723, Train: 40.83%, Valid: 34.77%, Test: 35.29%
Epoch: 900, Loss: 1.3696, Train: 41.03%, Valid: 34.67%, Test: 35.19%
Epoch: 925, Loss: 1.3723, Train: 40.80%, Valid: 34.84%, Test: 35.11%
Epoch: 950, Loss: 1.3734, Train: 40.97%, Valid: 34.71%, Test: 35.35%
Epoch: 975, Loss: 1.3716, Train: 40.94%, Valid: 34.24%, Test: 34.75%
Run 01:
Highest Train: 41.35
Highest Valid: 36.16
  Final Train: 37.02
   Final Test: 36.23
All runs:
Highest Train: 41.35, nan
Highest Valid: 36.16, nan
  Final Train: 37.02, nan
   Final Test: 36.23, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6103, Train: 23.59%, Valid: 23.22%, Test: 23.71%
Epoch: 25, Loss: 1.5570, Train: 28.71%, Valid: 28.52%, Test: 28.81%
Epoch: 50, Loss: 1.5353, Train: 28.71%, Valid: 28.53%, Test: 28.82%
Epoch: 75, Loss: 1.4922, Train: 28.72%, Valid: 28.54%, Test: 28.83%
Epoch: 100, Loss: 1.4599, Train: 36.69%, Valid: 36.48%, Test: 37.28%
Epoch: 125, Loss: 1.4268, Train: 38.04%, Valid: 37.76%, Test: 38.38%
Epoch: 150, Loss: 1.4106, Train: 39.41%, Valid: 39.19%, Test: 39.67%
Epoch: 175, Loss: 1.3937, Train: 37.37%, Valid: 37.32%, Test: 37.62%
Epoch: 200, Loss: 1.4003, Train: 39.44%, Valid: 39.22%, Test: 39.67%
Epoch: 225, Loss: 1.3450, Train: 42.31%, Valid: 41.88%, Test: 42.16%
Epoch: 250, Loss: 1.3388, Train: 42.89%, Valid: 42.64%, Test: 42.74%
Epoch: 275, Loss: 1.3240, Train: 43.29%, Valid: 42.91%, Test: 42.97%
Epoch: 300, Loss: 1.3406, Train: 42.27%, Valid: 42.07%, Test: 42.12%
Epoch: 325, Loss: 1.3173, Train: 44.32%, Valid: 43.76%, Test: 43.90%
Epoch: 350, Loss: 1.3244, Train: 44.02%, Valid: 43.69%, Test: 43.65%
Epoch: 375, Loss: 1.3251, Train: 44.55%, Valid: 43.95%, Test: 44.03%
Epoch: 400, Loss: 1.3001, Train: 45.17%, Valid: 44.50%, Test: 44.64%
Epoch: 425, Loss: 1.3010, Train: 45.31%, Valid: 44.49%, Test: 44.56%
Epoch: 450, Loss: 1.2894, Train: 45.66%, Valid: 44.96%, Test: 44.96%
Epoch: 475, Loss: 1.2869, Train: 45.62%, Valid: 44.94%, Test: 45.17%
Epoch: 500, Loss: 1.3048, Train: 45.31%, Valid: 44.53%, Test: 44.54%
Epoch: 525, Loss: 1.2921, Train: 45.58%, Valid: 44.81%, Test: 45.02%
Epoch: 550, Loss: 1.3065, Train: 45.33%, Valid: 44.57%, Test: 44.72%
Epoch: 575, Loss: 1.2794, Train: 46.13%, Valid: 45.35%, Test: 45.68%
Epoch: 600, Loss: 1.2943, Train: 45.19%, Valid: 44.43%, Test: 44.67%
Epoch: 625, Loss: 1.2762, Train: 46.38%, Valid: 45.58%, Test: 45.81%
Epoch: 650, Loss: 1.2688, Train: 46.46%, Valid: 45.77%, Test: 45.91%
Epoch: 675, Loss: 1.2681, Train: 46.35%, Valid: 45.54%, Test: 45.78%
Epoch: 700, Loss: 1.2728, Train: 46.19%, Valid: 45.28%, Test: 45.57%
Epoch: 725, Loss: 1.2856, Train: 46.15%, Valid: 45.43%, Test: 45.34%
Epoch: 750, Loss: 1.2733, Train: 46.82%, Valid: 45.88%, Test: 46.08%
Epoch: 775, Loss: 1.2829, Train: 44.70%, Valid: 44.12%, Test: 44.08%
Epoch: 800, Loss: 1.2845, Train: 46.29%, Valid: 45.46%, Test: 45.53%
Epoch: 825, Loss: 1.2585, Train: 47.19%, Valid: 46.38%, Test: 46.38%
Epoch: 850, Loss: 1.2581, Train: 46.85%, Valid: 46.16%, Test: 46.12%
Epoch: 875, Loss: 1.2859, Train: 46.25%, Valid: 45.48%, Test: 45.72%
Epoch: 900, Loss: 1.2771, Train: 46.92%, Valid: 46.03%, Test: 46.28%
Epoch: 925, Loss: 1.2524, Train: 47.45%, Valid: 46.60%, Test: 46.62%
Epoch: 950, Loss: 1.2543, Train: 47.24%, Valid: 46.41%, Test: 46.40%
Epoch: 975, Loss: 1.2524, Train: 47.04%, Valid: 46.32%, Test: 46.41%
Run 01:
Highest Train: 47.63
Highest Valid: 46.78
  Final Train: 47.57
   Final Test: 46.65
All runs:
Highest Train: 47.63, nan
Highest Valid: 46.78, nan
  Final Train: 47.57, nan
   Final Test: 46.65, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.5839, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4715, Train: 28.59%, Valid: 28.17%, Test: 28.33%
Epoch: 50, Loss: 1.4596, Train: 36.87%, Valid: 36.64%, Test: 36.68%
Epoch: 75, Loss: 1.4388, Train: 37.76%, Valid: 37.49%, Test: 37.52%
Epoch: 100, Loss: 1.4533, Train: 37.05%, Valid: 36.78%, Test: 36.85%
Epoch: 125, Loss: 1.4348, Train: 37.77%, Valid: 37.42%, Test: 37.51%
Epoch: 150, Loss: 1.4122, Train: 38.11%, Valid: 37.76%, Test: 37.81%
Epoch: 175, Loss: 1.4053, Train: 38.38%, Valid: 38.11%, Test: 38.14%
Epoch: 200, Loss: 1.4766, Train: 34.71%, Valid: 34.48%, Test: 34.72%
Epoch: 225, Loss: 1.4410, Train: 36.12%, Valid: 35.93%, Test: 36.09%
Epoch: 250, Loss: 1.4176, Train: 37.90%, Valid: 37.58%, Test: 37.78%
Epoch: 275, Loss: 1.4153, Train: 38.15%, Valid: 37.90%, Test: 38.04%
Epoch: 300, Loss: 1.4016, Train: 38.66%, Valid: 38.42%, Test: 38.44%
Epoch: 325, Loss: 1.4235, Train: 38.96%, Valid: 38.60%, Test: 38.64%
Epoch: 350, Loss: 1.4020, Train: 39.07%, Valid: 38.75%, Test: 38.73%
Epoch: 375, Loss: 1.4297, Train: 39.13%, Valid: 38.89%, Test: 38.89%
Epoch: 400, Loss: 1.3913, Train: 39.50%, Valid: 39.21%, Test: 39.18%
Epoch: 425, Loss: 1.3819, Train: 39.46%, Valid: 39.09%, Test: 39.18%
Epoch: 450, Loss: 1.4033, Train: 38.80%, Valid: 38.63%, Test: 38.57%
Epoch: 475, Loss: 1.3929, Train: 39.09%, Valid: 38.92%, Test: 38.77%
Epoch: 500, Loss: 1.3792, Train: 39.60%, Valid: 39.32%, Test: 39.23%
Epoch: 525, Loss: 1.3720, Train: 39.77%, Valid: 39.47%, Test: 39.46%
Epoch: 550, Loss: 1.3830, Train: 39.83%, Valid: 39.43%, Test: 39.54%
Epoch: 575, Loss: 1.3698, Train: 40.66%, Valid: 40.26%, Test: 40.38%
Epoch: 600, Loss: 1.3976, Train: 39.50%, Valid: 39.25%, Test: 39.60%
Epoch: 625, Loss: 1.4015, Train: 39.54%, Valid: 39.14%, Test: 39.29%
Epoch: 650, Loss: 1.3784, Train: 40.32%, Valid: 40.17%, Test: 40.32%
Epoch: 675, Loss: 1.3655, Train: 41.36%, Valid: 41.08%, Test: 41.09%
Epoch: 700, Loss: 1.4314, Train: 37.93%, Valid: 37.43%, Test: 37.79%
Epoch: 725, Loss: 1.4115, Train: 39.52%, Valid: 39.27%, Test: 39.26%
Epoch: 750, Loss: 1.3950, Train: 40.88%, Valid: 40.50%, Test: 40.76%
Epoch: 775, Loss: 1.4364, Train: 37.28%, Valid: 37.04%, Test: 37.19%
Epoch: 800, Loss: 1.4018, Train: 39.44%, Valid: 39.18%, Test: 39.72%
Epoch: 825, Loss: 1.3847, Train: 40.33%, Valid: 40.05%, Test: 40.64%
Epoch: 850, Loss: 1.4135, Train: 37.76%, Valid: 37.58%, Test: 38.17%
Epoch: 875, Loss: 1.4057, Train: 39.01%, Valid: 38.77%, Test: 39.37%
Epoch: 900, Loss: 1.3916, Train: 39.49%, Valid: 39.30%, Test: 39.79%
Epoch: 925, Loss: 1.3822, Train: 39.78%, Valid: 39.63%, Test: 39.97%
Epoch: 950, Loss: 1.3927, Train: 39.58%, Valid: 39.50%, Test: 39.79%
Epoch: 975, Loss: 1.3841, Train: 39.73%, Valid: 39.61%, Test: 39.89%
Run 01:
Highest Train: 41.38
Highest Valid: 41.08
  Final Train: 41.36
   Final Test: 41.09
All runs:
Highest Train: 41.38, nan
Highest Valid: 41.08, nan
  Final Train: 41.36, nan
   Final Test: 41.09, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6109, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.5283, Train: 33.91%, Valid: 33.72%, Test: 33.87%
Epoch: 50, Loss: 1.5011, Train: 34.56%, Valid: 34.31%, Test: 34.50%
Epoch: 75, Loss: 1.4924, Train: 35.10%, Valid: 34.67%, Test: 34.93%
Epoch: 100, Loss: 1.4860, Train: 35.58%, Valid: 35.00%, Test: 35.20%
Epoch: 125, Loss: 1.4827, Train: 36.18%, Valid: 35.43%, Test: 35.71%
Epoch: 150, Loss: 1.4779, Train: 36.59%, Valid: 35.62%, Test: 35.97%
Epoch: 175, Loss: 1.4748, Train: 36.82%, Valid: 35.64%, Test: 36.07%
Epoch: 200, Loss: 1.4727, Train: 37.07%, Valid: 35.81%, Test: 36.22%
Epoch: 225, Loss: 1.4687, Train: 37.38%, Valid: 36.03%, Test: 36.42%
Epoch: 250, Loss: 1.4661, Train: 37.64%, Valid: 36.05%, Test: 36.41%
Epoch: 275, Loss: 1.4637, Train: 37.74%, Valid: 36.13%, Test: 36.44%
Epoch: 300, Loss: 1.4635, Train: 38.00%, Valid: 36.20%, Test: 36.63%
Epoch: 325, Loss: 1.4615, Train: 37.95%, Valid: 35.96%, Test: 36.53%
Epoch: 350, Loss: 1.4595, Train: 38.00%, Valid: 36.00%, Test: 36.47%
Epoch: 375, Loss: 1.4597, Train: 38.12%, Valid: 36.06%, Test: 36.50%
Epoch: 400, Loss: 1.4591, Train: 38.20%, Valid: 36.04%, Test: 36.50%
Epoch: 425, Loss: 1.4581, Train: 38.24%, Valid: 35.99%, Test: 36.51%
Epoch: 450, Loss: 1.4566, Train: 38.25%, Valid: 35.96%, Test: 36.46%
Epoch: 475, Loss: 1.4565, Train: 38.28%, Valid: 35.97%, Test: 36.36%
Epoch: 500, Loss: 1.4550, Train: 38.46%, Valid: 36.00%, Test: 36.37%
Epoch: 525, Loss: 1.4546, Train: 38.46%, Valid: 35.92%, Test: 36.35%
Epoch: 550, Loss: 1.4538, Train: 38.52%, Valid: 35.95%, Test: 36.46%
Epoch: 575, Loss: 1.4552, Train: 38.43%, Valid: 35.93%, Test: 36.33%
Epoch: 600, Loss: 1.4528, Train: 38.34%, Valid: 35.85%, Test: 36.29%
Epoch: 625, Loss: 1.4545, Train: 38.57%, Valid: 35.94%, Test: 36.34%
Epoch: 650, Loss: 1.4547, Train: 38.53%, Valid: 36.01%, Test: 36.34%
Epoch: 675, Loss: 1.4529, Train: 38.51%, Valid: 35.94%, Test: 36.30%
Epoch: 700, Loss: 1.4490, Train: 38.58%, Valid: 35.92%, Test: 36.22%
Epoch: 725, Loss: 1.4508, Train: 38.60%, Valid: 35.85%, Test: 36.25%
Epoch: 750, Loss: 1.4518, Train: 38.58%, Valid: 35.96%, Test: 36.29%
Epoch: 775, Loss: 1.4506, Train: 38.60%, Valid: 35.82%, Test: 36.22%
Epoch: 800, Loss: 1.4506, Train: 38.57%, Valid: 35.83%, Test: 36.25%
Epoch: 825, Loss: 1.4510, Train: 38.61%, Valid: 35.86%, Test: 36.25%
Epoch: 850, Loss: 1.4496, Train: 38.67%, Valid: 35.81%, Test: 36.16%
Epoch: 875, Loss: 1.4501, Train: 38.68%, Valid: 35.86%, Test: 36.30%
Epoch: 900, Loss: 1.4507, Train: 38.74%, Valid: 35.81%, Test: 36.27%
Epoch: 925, Loss: 1.4502, Train: 38.86%, Valid: 35.85%, Test: 36.15%
Epoch: 950, Loss: 1.4483, Train: 38.81%, Valid: 35.80%, Test: 36.21%
Epoch: 975, Loss: 1.4480, Train: 38.75%, Valid: 35.76%, Test: 36.29%
Run 01:
Highest Train: 38.86
Highest Valid: 36.22
  Final Train: 37.91
   Final Test: 36.65
All runs:
Highest Train: 38.86, nan
Highest Valid: 36.22, nan
  Final Train: 37.91, nan
   Final Test: 36.65, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6086, Train: 28.51%, Valid: 28.27%, Test: 28.59%
Epoch: 25, Loss: 1.5178, Train: 34.04%, Valid: 33.81%, Test: 33.98%
Epoch: 50, Loss: 1.4991, Train: 34.74%, Valid: 34.53%, Test: 34.61%
Epoch: 75, Loss: 1.4896, Train: 35.26%, Valid: 34.78%, Test: 35.03%
Epoch: 100, Loss: 1.4810, Train: 36.14%, Valid: 35.32%, Test: 35.63%
Epoch: 125, Loss: 1.4739, Train: 36.70%, Valid: 35.75%, Test: 35.97%
Epoch: 150, Loss: 1.4683, Train: 37.20%, Valid: 35.91%, Test: 36.18%
Epoch: 175, Loss: 1.4630, Train: 37.62%, Valid: 36.15%, Test: 36.37%
Epoch: 200, Loss: 1.4572, Train: 37.90%, Valid: 36.10%, Test: 36.54%
Epoch: 225, Loss: 1.4408, Train: 38.78%, Valid: 36.96%, Test: 37.42%
Epoch: 250, Loss: 1.4387, Train: 38.60%, Valid: 37.01%, Test: 37.32%
Epoch: 275, Loss: 1.4389, Train: 38.74%, Valid: 37.16%, Test: 37.50%
Epoch: 300, Loss: 1.4376, Train: 38.44%, Valid: 37.01%, Test: 37.25%
Epoch: 325, Loss: 1.4378, Train: 39.44%, Valid: 37.75%, Test: 37.88%
Epoch: 350, Loss: 1.4591, Train: 38.01%, Valid: 36.71%, Test: 37.00%
Epoch: 375, Loss: 1.4470, Train: 39.12%, Valid: 37.52%, Test: 37.95%
Epoch: 400, Loss: 1.4400, Train: 39.29%, Valid: 37.65%, Test: 38.26%
Epoch: 425, Loss: 1.4461, Train: 39.09%, Valid: 37.65%, Test: 38.02%
Epoch: 450, Loss: 1.4561, Train: 37.67%, Valid: 36.54%, Test: 36.97%
Epoch: 475, Loss: 1.4431, Train: 39.35%, Valid: 37.90%, Test: 38.32%
Epoch: 500, Loss: 1.4346, Train: 39.81%, Valid: 38.14%, Test: 38.58%
Epoch: 525, Loss: 1.4293, Train: 40.02%, Valid: 38.49%, Test: 38.79%
Epoch: 550, Loss: 1.4275, Train: 40.20%, Valid: 38.75%, Test: 39.15%
Epoch: 575, Loss: 1.4695, Train: 35.90%, Valid: 35.43%, Test: 35.71%
Epoch: 600, Loss: 1.4396, Train: 40.58%, Valid: 38.96%, Test: 39.32%
Epoch: 625, Loss: 1.4396, Train: 39.15%, Valid: 38.04%, Test: 38.47%
Epoch: 650, Loss: 1.4289, Train: 40.69%, Valid: 39.26%, Test: 39.54%
Epoch: 675, Loss: 1.4388, Train: 39.57%, Valid: 38.32%, Test: 38.68%
Epoch: 700, Loss: 1.4293, Train: 40.26%, Valid: 38.94%, Test: 39.28%
Epoch: 725, Loss: 1.4290, Train: 40.52%, Valid: 39.49%, Test: 39.69%
Epoch: 750, Loss: 1.4302, Train: 40.55%, Valid: 39.44%, Test: 39.55%
Epoch: 775, Loss: 1.4175, Train: 40.01%, Valid: 38.82%, Test: 39.43%
Epoch: 800, Loss: 1.4114, Train: 41.71%, Valid: 40.82%, Test: 41.02%
Epoch: 825, Loss: 1.4343, Train: 40.57%, Valid: 39.78%, Test: 39.95%
Epoch: 850, Loss: 1.5477, Train: 30.68%, Valid: 30.43%, Test: 30.89%
Epoch: 875, Loss: 1.5241, Train: 34.01%, Valid: 33.86%, Test: 34.06%
Epoch: 900, Loss: 1.4911, Train: 33.80%, Valid: 33.46%, Test: 33.85%
Epoch: 925, Loss: 1.4655, Train: 36.51%, Valid: 35.97%, Test: 36.27%
Epoch: 950, Loss: 1.4455, Train: 36.06%, Valid: 35.42%, Test: 36.04%
Epoch: 975, Loss: 1.4364, Train: 37.41%, Valid: 36.51%, Test: 36.78%
Run 01:
Highest Train: 42.70
Highest Valid: 41.28
  Final Train: 42.28
   Final Test: 41.39
All runs:
Highest Train: 42.70, nan
Highest Valid: 41.28, nan
  Final Train: 42.28, nan
   Final Test: 41.39, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6495, Train: 28.74%, Valid: 28.58%, Test: 28.85%
Epoch: 25, Loss: 1.5345, Train: 28.50%, Valid: 28.31%, Test: 28.63%
Epoch: 50, Loss: 1.4926, Train: 25.34%, Valid: 25.09%, Test: 25.36%
Epoch: 75, Loss: 1.4735, Train: 26.33%, Valid: 26.10%, Test: 26.35%
Epoch: 100, Loss: 1.4678, Train: 18.53%, Valid: 18.55%, Test: 18.66%
Epoch: 125, Loss: 1.4707, Train: 18.25%, Valid: 18.26%, Test: 18.33%
Epoch: 150, Loss: 1.4597, Train: 17.94%, Valid: 17.92%, Test: 17.98%
Epoch: 175, Loss: 1.4484, Train: 17.77%, Valid: 17.77%, Test: 17.81%
Epoch: 200, Loss: 1.4493, Train: 17.00%, Valid: 17.05%, Test: 16.85%
Epoch: 225, Loss: 1.4382, Train: 21.96%, Valid: 21.87%, Test: 21.76%
Epoch: 250, Loss: 1.4414, Train: 18.56%, Valid: 18.52%, Test: 18.61%
Epoch: 275, Loss: 1.4513, Train: 12.98%, Valid: 13.01%, Test: 12.79%
Epoch: 300, Loss: 1.4424, Train: 12.39%, Valid: 12.25%, Test: 12.30%
Epoch: 325, Loss: 1.4336, Train: 11.96%, Valid: 11.99%, Test: 11.98%
Epoch: 350, Loss: 1.4584, Train: 14.57%, Valid: 14.71%, Test: 14.44%
Epoch: 375, Loss: 1.4426, Train: 14.41%, Valid: 14.50%, Test: 14.42%
Epoch: 400, Loss: 1.4292, Train: 24.10%, Valid: 23.98%, Test: 24.39%
Epoch: 425, Loss: 1.4577, Train: 16.20%, Valid: 16.22%, Test: 16.40%
Epoch: 450, Loss: 1.4484, Train: 22.44%, Valid: 22.33%, Test: 22.49%
Epoch: 475, Loss: 1.4531, Train: 17.26%, Valid: 17.05%, Test: 17.16%
Epoch: 500, Loss: 1.4326, Train: 21.28%, Valid: 20.91%, Test: 21.57%
Epoch: 525, Loss: 1.4308, Train: 17.55%, Valid: 17.29%, Test: 17.62%
Epoch: 550, Loss: 1.4289, Train: 20.03%, Valid: 19.79%, Test: 20.19%
Epoch: 575, Loss: 1.4277, Train: 16.44%, Valid: 16.15%, Test: 16.42%
Epoch: 600, Loss: 1.4299, Train: 15.92%, Valid: 15.70%, Test: 15.91%
Epoch: 625, Loss: 1.4357, Train: 34.20%, Valid: 33.76%, Test: 34.10%
Epoch: 650, Loss: 1.4363, Train: 20.17%, Valid: 19.94%, Test: 20.19%
Epoch: 675, Loss: 1.4354, Train: 21.38%, Valid: 21.07%, Test: 21.17%
Epoch: 700, Loss: 1.4245, Train: 36.14%, Valid: 35.60%, Test: 35.95%
Epoch: 725, Loss: 1.4210, Train: 30.35%, Valid: 30.09%, Test: 30.15%
Epoch: 750, Loss: 1.4193, Train: 37.26%, Valid: 36.79%, Test: 37.13%
Epoch: 775, Loss: 1.4171, Train: 20.40%, Valid: 20.26%, Test: 20.13%
Epoch: 800, Loss: 1.4125, Train: 25.91%, Valid: 25.65%, Test: 25.73%
Epoch: 825, Loss: 1.4192, Train: 33.37%, Valid: 32.99%, Test: 33.17%
Epoch: 850, Loss: 1.4135, Train: 30.85%, Valid: 30.60%, Test: 30.52%
Epoch: 875, Loss: 1.4164, Train: 27.35%, Valid: 27.07%, Test: 27.19%
Epoch: 900, Loss: 1.4101, Train: 26.54%, Valid: 26.37%, Test: 26.49%
Epoch: 925, Loss: 1.4165, Train: 32.28%, Valid: 31.76%, Test: 32.12%
Epoch: 950, Loss: 1.4252, Train: 26.55%, Valid: 26.36%, Test: 26.39%
Epoch: 975, Loss: 1.4160, Train: 30.07%, Valid: 29.67%, Test: 29.46%
Run 01:
Highest Train: 37.38
Highest Valid: 37.16
  Final Train: 37.38
   Final Test: 37.20
All runs:
Highest Train: 37.38, nan
Highest Valid: 37.16, nan
  Final Train: 37.38, nan
   Final Test: 37.20, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6084, Train: 28.72%, Valid: 28.55%, Test: 28.83%
Epoch: 25, Loss: 1.5191, Train: 33.93%, Valid: 33.73%, Test: 33.93%
Epoch: 50, Loss: 1.4980, Train: 34.68%, Valid: 34.48%, Test: 34.62%
Epoch: 75, Loss: 1.4890, Train: 35.43%, Valid: 34.85%, Test: 35.28%
Epoch: 100, Loss: 1.4815, Train: 36.02%, Valid: 35.30%, Test: 35.62%
Epoch: 125, Loss: 1.4758, Train: 36.49%, Valid: 35.50%, Test: 35.83%
Epoch: 150, Loss: 1.4723, Train: 36.85%, Valid: 35.72%, Test: 36.16%
Epoch: 175, Loss: 1.4690, Train: 37.16%, Valid: 35.88%, Test: 36.33%
Epoch: 200, Loss: 1.4666, Train: 37.52%, Valid: 35.97%, Test: 36.40%
Epoch: 225, Loss: 1.4613, Train: 37.73%, Valid: 36.06%, Test: 36.47%
Epoch: 250, Loss: 1.4600, Train: 37.86%, Valid: 35.94%, Test: 36.45%
Epoch: 275, Loss: 1.4572, Train: 38.04%, Valid: 35.97%, Test: 36.43%
Epoch: 300, Loss: 1.4558, Train: 38.19%, Valid: 36.02%, Test: 36.58%
Epoch: 325, Loss: 1.4558, Train: 38.22%, Valid: 36.00%, Test: 36.46%
Epoch: 350, Loss: 1.4544, Train: 38.39%, Valid: 36.01%, Test: 36.46%
Epoch: 375, Loss: 1.4519, Train: 38.46%, Valid: 35.97%, Test: 36.57%
Epoch: 400, Loss: 1.4526, Train: 38.54%, Valid: 36.02%, Test: 36.49%
Epoch: 425, Loss: 1.4512, Train: 38.54%, Valid: 36.06%, Test: 36.45%
Epoch: 450, Loss: 1.4517, Train: 38.68%, Valid: 36.10%, Test: 36.49%
Epoch: 475, Loss: 1.4509, Train: 38.67%, Valid: 36.01%, Test: 36.44%
Epoch: 500, Loss: 1.4511, Train: 38.74%, Valid: 36.00%, Test: 36.45%
Epoch: 525, Loss: 1.4502, Train: 38.72%, Valid: 36.04%, Test: 36.46%
Epoch: 550, Loss: 1.4505, Train: 38.72%, Valid: 35.95%, Test: 36.40%
Epoch: 575, Loss: 1.4485, Train: 38.82%, Valid: 35.88%, Test: 36.41%
Epoch: 600, Loss: 1.4478, Train: 38.90%, Valid: 35.92%, Test: 36.41%
Epoch: 625, Loss: 1.4487, Train: 38.91%, Valid: 35.88%, Test: 36.33%
Epoch: 650, Loss: 1.4493, Train: 38.93%, Valid: 36.03%, Test: 36.41%
Epoch: 675, Loss: 1.4461, Train: 39.04%, Valid: 35.98%, Test: 36.41%
Epoch: 700, Loss: 1.4468, Train: 38.99%, Valid: 35.92%, Test: 36.37%
Epoch: 725, Loss: 1.4461, Train: 39.01%, Valid: 35.89%, Test: 36.41%
Epoch: 750, Loss: 1.4434, Train: 39.11%, Valid: 35.91%, Test: 36.40%
Epoch: 775, Loss: 1.4462, Train: 39.02%, Valid: 35.90%, Test: 36.35%
Epoch: 800, Loss: 1.4453, Train: 39.06%, Valid: 35.92%, Test: 36.28%
Epoch: 825, Loss: 1.4446, Train: 39.10%, Valid: 35.88%, Test: 36.26%
Epoch: 850, Loss: 1.4467, Train: 39.18%, Valid: 35.84%, Test: 36.35%
Epoch: 875, Loss: 1.4460, Train: 39.24%, Valid: 35.83%, Test: 36.32%
Epoch: 900, Loss: 1.4445, Train: 39.17%, Valid: 35.84%, Test: 36.27%
Epoch: 925, Loss: 1.4447, Train: 39.14%, Valid: 35.85%, Test: 36.28%
Epoch: 950, Loss: 1.4440, Train: 39.13%, Valid: 35.69%, Test: 36.35%
Epoch: 975, Loss: 1.4431, Train: 39.15%, Valid: 35.85%, Test: 36.22%
Run 01:
Highest Train: 39.27
Highest Valid: 36.14
  Final Train: 37.72
   Final Test: 36.48
All runs:
Highest Train: 39.27, nan
Highest Valid: 36.14, nan
  Final Train: 37.72, nan
   Final Test: 36.48, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6093, Train: 28.85%, Valid: 28.57%, Test: 28.96%
Epoch: 25, Loss: 1.5191, Train: 33.91%, Valid: 33.69%, Test: 33.80%
Epoch: 50, Loss: 1.4980, Train: 34.75%, Valid: 34.48%, Test: 34.77%
Epoch: 75, Loss: 1.4860, Train: 35.63%, Valid: 35.10%, Test: 35.55%
Epoch: 100, Loss: 1.4790, Train: 36.24%, Valid: 35.55%, Test: 35.93%
Epoch: 125, Loss: 1.4738, Train: 36.82%, Valid: 35.73%, Test: 35.99%
Epoch: 150, Loss: 1.4682, Train: 37.19%, Valid: 35.87%, Test: 36.12%
Epoch: 175, Loss: 1.4637, Train: 37.45%, Valid: 36.13%, Test: 36.29%
Epoch: 200, Loss: 1.4597, Train: 37.74%, Valid: 36.23%, Test: 36.47%
Epoch: 225, Loss: 1.4610, Train: 37.87%, Valid: 36.21%, Test: 36.47%
Epoch: 250, Loss: 1.4420, Train: 38.88%, Valid: 37.12%, Test: 37.39%
Epoch: 275, Loss: 1.4489, Train: 38.38%, Valid: 36.76%, Test: 37.01%
Epoch: 300, Loss: 1.4417, Train: 39.11%, Valid: 37.42%, Test: 37.68%
Epoch: 325, Loss: 1.4488, Train: 38.32%, Valid: 36.85%, Test: 37.04%
Epoch: 350, Loss: 1.4350, Train: 39.19%, Valid: 37.57%, Test: 37.78%
Epoch: 375, Loss: 1.4478, Train: 38.69%, Valid: 37.17%, Test: 37.32%
Epoch: 400, Loss: 1.4375, Train: 38.94%, Valid: 37.27%, Test: 37.70%
Epoch: 425, Loss: 1.4402, Train: 38.41%, Valid: 37.04%, Test: 37.28%
Epoch: 450, Loss: 1.4356, Train: 40.00%, Valid: 38.42%, Test: 38.65%
Epoch: 475, Loss: 1.4344, Train: 39.95%, Valid: 38.50%, Test: 38.82%
Epoch: 500, Loss: 1.4446, Train: 37.55%, Valid: 36.45%, Test: 36.65%
Epoch: 525, Loss: 1.4364, Train: 38.88%, Valid: 37.71%, Test: 37.93%
Epoch: 550, Loss: 1.4369, Train: 39.48%, Valid: 37.99%, Test: 38.45%
Epoch: 575, Loss: 1.4263, Train: 40.37%, Valid: 38.88%, Test: 39.12%
Epoch: 600, Loss: 1.4359, Train: 39.78%, Valid: 38.18%, Test: 38.74%
Epoch: 625, Loss: 1.4394, Train: 39.36%, Valid: 37.94%, Test: 38.26%
Epoch: 650, Loss: 1.4376, Train: 40.40%, Valid: 39.13%, Test: 39.40%
Epoch: 675, Loss: 1.4298, Train: 39.42%, Valid: 37.99%, Test: 38.25%
Epoch: 700, Loss: 1.4291, Train: 39.96%, Valid: 38.63%, Test: 38.91%
Epoch: 725, Loss: 1.4358, Train: 39.95%, Valid: 38.81%, Test: 39.04%
Epoch: 750, Loss: 1.4699, Train: 39.39%, Valid: 38.42%, Test: 38.47%
Epoch: 775, Loss: 1.4375, Train: 39.64%, Valid: 38.72%, Test: 39.02%
Epoch: 800, Loss: 1.4488, Train: 39.37%, Valid: 38.37%, Test: 38.66%
Epoch: 825, Loss: 1.4438, Train: 39.41%, Valid: 38.38%, Test: 38.81%
Epoch: 850, Loss: 1.4182, Train: 39.91%, Valid: 38.92%, Test: 39.08%
Epoch: 875, Loss: 1.4273, Train: 41.46%, Valid: 40.54%, Test: 40.56%
Epoch: 900, Loss: 1.4271, Train: 40.42%, Valid: 39.40%, Test: 39.69%
Epoch: 925, Loss: 1.4092, Train: 40.51%, Valid: 40.17%, Test: 40.17%
Epoch: 950, Loss: 1.8341, Train: 29.84%, Valid: 29.70%, Test: 30.06%
Epoch: 975, Loss: 1.5375, Train: 30.87%, Valid: 30.59%, Test: 30.86%
Run 01:
Highest Train: 42.92
Highest Valid: 42.15
  Final Train: 42.82
   Final Test: 42.28
All runs:
Highest Train: 42.92, nan
Highest Valid: 42.15, nan
  Final Train: 42.82, nan
   Final Test: 42.28, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.5778, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.5375, Train: 29.70%, Valid: 29.28%, Test: 29.73%
Epoch: 50, Loss: 1.5006, Train: 25.27%, Valid: 25.15%, Test: 25.17%
Epoch: 75, Loss: 1.5050, Train: 24.19%, Valid: 24.20%, Test: 24.06%
Epoch: 100, Loss: 1.4671, Train: 27.81%, Valid: 27.82%, Test: 27.77%
Epoch: 125, Loss: 1.4563, Train: 28.36%, Valid: 28.33%, Test: 28.20%
Epoch: 150, Loss: 1.4562, Train: 26.48%, Valid: 26.58%, Test: 26.44%
Epoch: 175, Loss: 1.4598, Train: 31.65%, Valid: 31.76%, Test: 31.55%
Epoch: 200, Loss: 1.4527, Train: 26.47%, Valid: 26.51%, Test: 26.45%
Epoch: 225, Loss: 1.4324, Train: 26.99%, Valid: 26.87%, Test: 26.78%
Epoch: 250, Loss: 1.4364, Train: 30.55%, Valid: 30.60%, Test: 30.22%
Epoch: 275, Loss: 1.4138, Train: 30.40%, Valid: 30.49%, Test: 30.07%
Epoch: 300, Loss: 1.5336, Train: 34.50%, Valid: 34.19%, Test: 34.31%
Epoch: 325, Loss: 1.4930, Train: 25.71%, Valid: 25.63%, Test: 25.46%
Epoch: 350, Loss: 1.4605, Train: 26.24%, Valid: 26.24%, Test: 26.05%
Epoch: 375, Loss: 1.4467, Train: 27.31%, Valid: 27.36%, Test: 27.16%
Epoch: 400, Loss: 1.4483, Train: 26.71%, Valid: 26.77%, Test: 26.52%
Epoch: 425, Loss: 1.4510, Train: 27.55%, Valid: 27.58%, Test: 27.56%
Epoch: 450, Loss: 1.4362, Train: 26.90%, Valid: 26.76%, Test: 26.61%
Epoch: 475, Loss: 1.4243, Train: 26.26%, Valid: 26.18%, Test: 26.21%
Epoch: 500, Loss: 1.4197, Train: 26.98%, Valid: 26.85%, Test: 26.63%
Epoch: 525, Loss: 1.4215, Train: 27.16%, Valid: 26.99%, Test: 26.92%
Epoch: 550, Loss: 1.4190, Train: 27.07%, Valid: 26.99%, Test: 26.80%
Epoch: 575, Loss: 1.4166, Train: 27.38%, Valid: 27.32%, Test: 27.21%
Epoch: 600, Loss: 1.4099, Train: 39.15%, Valid: 38.64%, Test: 38.75%
Epoch: 625, Loss: 1.4030, Train: 27.64%, Valid: 27.49%, Test: 27.54%
Epoch: 650, Loss: 1.4054, Train: 27.22%, Valid: 27.16%, Test: 27.24%
Epoch: 675, Loss: 1.4071, Train: 27.77%, Valid: 27.63%, Test: 27.57%
Epoch: 700, Loss: 1.6085, Train: 31.12%, Valid: 30.91%, Test: 31.06%
Epoch: 725, Loss: 1.4681, Train: 22.28%, Valid: 22.45%, Test: 22.27%
Epoch: 750, Loss: 1.4567, Train: 28.96%, Valid: 29.11%, Test: 28.63%
Epoch: 775, Loss: 1.4379, Train: 30.42%, Valid: 30.49%, Test: 30.09%
Epoch: 800, Loss: 1.4377, Train: 38.53%, Valid: 38.22%, Test: 38.42%
Epoch: 825, Loss: 1.4402, Train: 30.28%, Valid: 30.36%, Test: 30.34%
Epoch: 850, Loss: 1.4233, Train: 32.89%, Valid: 32.94%, Test: 32.95%
Epoch: 875, Loss: 1.4176, Train: 34.70%, Valid: 34.87%, Test: 35.00%
Epoch: 900, Loss: 1.4184, Train: 33.49%, Valid: 33.51%, Test: 33.50%
Epoch: 925, Loss: 1.4357, Train: 36.33%, Valid: 36.59%, Test: 36.24%
Epoch: 950, Loss: 1.4133, Train: 30.75%, Valid: 30.79%, Test: 30.52%
Epoch: 975, Loss: 1.4297, Train: 28.27%, Valid: 28.02%, Test: 28.01%
Run 01:
Highest Train: 39.93
Highest Valid: 39.51
  Final Train: 39.93
   Final Test: 39.86
All runs:
Highest Train: 39.93, nan
Highest Valid: 39.51, nan
  Final Train: 39.93, nan
   Final Test: 39.86, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6057, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4923, Train: 34.63%, Valid: 34.39%, Test: 34.66%
Epoch: 50, Loss: 1.4721, Train: 35.86%, Valid: 35.24%, Test: 35.61%
Epoch: 75, Loss: 1.4585, Train: 36.73%, Valid: 35.73%, Test: 35.99%
Epoch: 100, Loss: 1.4472, Train: 37.04%, Valid: 35.99%, Test: 36.38%
Epoch: 125, Loss: 1.4382, Train: 37.60%, Valid: 36.28%, Test: 36.35%
Epoch: 150, Loss: 1.4329, Train: 37.79%, Valid: 36.16%, Test: 36.28%
Epoch: 175, Loss: 1.4267, Train: 38.14%, Valid: 36.26%, Test: 36.40%
Epoch: 200, Loss: 1.4191, Train: 38.43%, Valid: 35.55%, Test: 36.10%
Epoch: 225, Loss: 1.4170, Train: 38.26%, Valid: 35.29%, Test: 35.84%
Epoch: 250, Loss: 1.4090, Train: 38.88%, Valid: 35.50%, Test: 36.07%
Epoch: 275, Loss: 1.4072, Train: 38.96%, Valid: 35.67%, Test: 35.92%
Epoch: 300, Loss: 1.4008, Train: 39.37%, Valid: 35.56%, Test: 36.11%
Epoch: 325, Loss: 1.4103, Train: 39.40%, Valid: 35.65%, Test: 35.98%
Epoch: 350, Loss: 1.3953, Train: 39.50%, Valid: 35.49%, Test: 35.99%
Epoch: 375, Loss: 1.3940, Train: 39.67%, Valid: 35.68%, Test: 35.93%
Epoch: 400, Loss: 1.3920, Train: 39.86%, Valid: 35.64%, Test: 35.94%
Epoch: 425, Loss: 1.3886, Train: 39.86%, Valid: 35.39%, Test: 35.83%
Epoch: 450, Loss: 1.3873, Train: 39.83%, Valid: 35.33%, Test: 35.75%
Epoch: 475, Loss: 1.3834, Train: 40.21%, Valid: 35.43%, Test: 35.65%
Epoch: 500, Loss: 1.3838, Train: 40.16%, Valid: 35.24%, Test: 35.62%
Epoch: 525, Loss: 1.3810, Train: 40.22%, Valid: 35.29%, Test: 35.45%
Epoch: 550, Loss: 1.3799, Train: 40.32%, Valid: 35.11%, Test: 35.30%
Epoch: 575, Loss: 1.3785, Train: 40.38%, Valid: 35.16%, Test: 35.30%
Epoch: 600, Loss: 1.3747, Train: 40.36%, Valid: 34.91%, Test: 35.10%
Epoch: 625, Loss: 1.3723, Train: 40.56%, Valid: 34.88%, Test: 35.08%
Epoch: 650, Loss: 1.3715, Train: 40.69%, Valid: 35.02%, Test: 35.14%
Epoch: 675, Loss: 1.3706, Train: 40.61%, Valid: 35.19%, Test: 35.23%
Epoch: 700, Loss: 1.3703, Train: 40.85%, Valid: 35.15%, Test: 35.20%
Epoch: 725, Loss: 1.3673, Train: 40.95%, Valid: 35.00%, Test: 34.97%
Epoch: 750, Loss: 1.3674, Train: 40.99%, Valid: 34.90%, Test: 34.90%
Epoch: 775, Loss: 1.3687, Train: 40.93%, Valid: 35.10%, Test: 35.13%
Epoch: 800, Loss: 1.3700, Train: 41.16%, Valid: 34.98%, Test: 35.09%
Epoch: 825, Loss: 1.3640, Train: 41.13%, Valid: 35.14%, Test: 35.06%
Epoch: 850, Loss: 1.3636, Train: 41.11%, Valid: 34.81%, Test: 34.80%
Epoch: 875, Loss: 1.3690, Train: 40.84%, Valid: 34.81%, Test: 34.87%
Epoch: 900, Loss: 1.3613, Train: 41.17%, Valid: 34.67%, Test: 34.69%
Epoch: 925, Loss: 1.3573, Train: 41.44%, Valid: 34.77%, Test: 34.99%
Epoch: 950, Loss: 1.3620, Train: 41.24%, Valid: 34.88%, Test: 34.89%
Epoch: 975, Loss: 1.3577, Train: 41.26%, Valid: 34.78%, Test: 34.83%
Run 01:
Highest Train: 41.52
Highest Valid: 36.32
  Final Train: 37.36
   Final Test: 36.16
All runs:
Highest Train: 41.52, nan
Highest Valid: 36.32, nan
  Final Train: 37.36, nan
   Final Test: 36.16, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6260, Train: 24.63%, Valid: 24.70%, Test: 25.38%
Epoch: 25, Loss: 1.4851, Train: 34.57%, Valid: 34.39%, Test: 34.59%
Epoch: 50, Loss: 1.4494, Train: 36.27%, Valid: 35.78%, Test: 36.04%
Epoch: 75, Loss: 1.4170, Train: 38.38%, Valid: 37.84%, Test: 37.91%
Epoch: 100, Loss: 1.4112, Train: 38.52%, Valid: 37.87%, Test: 38.21%
Epoch: 125, Loss: 1.4052, Train: 38.94%, Valid: 38.17%, Test: 38.46%
Epoch: 150, Loss: 1.3668, Train: 40.84%, Valid: 39.74%, Test: 40.04%
Epoch: 175, Loss: 1.3483, Train: 41.00%, Valid: 40.10%, Test: 40.34%
Epoch: 200, Loss: 1.3607, Train: 41.15%, Valid: 40.16%, Test: 40.50%
Epoch: 225, Loss: 1.3312, Train: 41.90%, Valid: 41.11%, Test: 41.46%
Epoch: 250, Loss: 1.3232, Train: 42.50%, Valid: 41.43%, Test: 42.00%
Epoch: 275, Loss: 1.3164, Train: 42.83%, Valid: 41.67%, Test: 42.06%
Epoch: 300, Loss: 1.3057, Train: 43.20%, Valid: 42.07%, Test: 42.46%
Epoch: 325, Loss: 1.3262, Train: 42.64%, Valid: 41.46%, Test: 41.90%
Epoch: 350, Loss: 1.3020, Train: 43.57%, Valid: 42.26%, Test: 42.58%
Epoch: 375, Loss: 1.2954, Train: 43.95%, Valid: 42.57%, Test: 42.81%
Epoch: 400, Loss: 1.2931, Train: 44.04%, Valid: 42.61%, Test: 42.74%
Epoch: 425, Loss: 1.2821, Train: 44.59%, Valid: 43.04%, Test: 43.19%
Epoch: 450, Loss: 1.2783, Train: 44.57%, Valid: 43.12%, Test: 43.00%
Epoch: 475, Loss: 1.3035, Train: 44.16%, Valid: 42.32%, Test: 42.73%
Epoch: 500, Loss: 1.2864, Train: 44.00%, Valid: 42.84%, Test: 43.01%
Epoch: 525, Loss: 1.2756, Train: 44.90%, Valid: 43.40%, Test: 43.57%
Epoch: 550, Loss: 1.2644, Train: 45.28%, Valid: 43.49%, Test: 43.78%
Epoch: 575, Loss: 1.2641, Train: 44.47%, Valid: 42.81%, Test: 42.79%
Epoch: 600, Loss: 1.2963, Train: 44.07%, Valid: 42.76%, Test: 43.16%
Epoch: 625, Loss: 1.2757, Train: 44.69%, Valid: 43.09%, Test: 43.39%
Epoch: 650, Loss: 1.2599, Train: 45.48%, Valid: 43.78%, Test: 43.99%
Epoch: 675, Loss: 1.2782, Train: 44.54%, Valid: 42.93%, Test: 43.32%
Epoch: 700, Loss: 1.2550, Train: 45.69%, Valid: 43.73%, Test: 44.11%
Epoch: 725, Loss: 1.2492, Train: 45.82%, Valid: 43.82%, Test: 44.07%
Epoch: 750, Loss: 1.2443, Train: 46.00%, Valid: 43.87%, Test: 44.15%
Epoch: 775, Loss: 1.2414, Train: 46.34%, Valid: 44.29%, Test: 44.46%
Epoch: 800, Loss: 1.2398, Train: 46.57%, Valid: 44.34%, Test: 44.48%
Epoch: 825, Loss: 1.2360, Train: 46.14%, Valid: 43.97%, Test: 44.13%
Epoch: 850, Loss: 1.2336, Train: 46.70%, Valid: 44.28%, Test: 44.55%
Epoch: 875, Loss: 1.2307, Train: 46.77%, Valid: 44.34%, Test: 44.76%
Epoch: 900, Loss: 1.2294, Train: 46.88%, Valid: 44.27%, Test: 44.70%
Epoch: 925, Loss: 1.2247, Train: 47.08%, Valid: 44.59%, Test: 44.79%
Epoch: 950, Loss: 1.3310, Train: 42.58%, Valid: 41.52%, Test: 41.82%
Epoch: 975, Loss: 1.2669, Train: 45.24%, Valid: 43.81%, Test: 44.04%
Run 01:
Highest Train: 47.18
Highest Valid: 44.65
  Final Train: 47.02
   Final Test: 44.90
All runs:
Highest Train: 47.18, nan
Highest Valid: 44.65, nan
  Final Train: 47.02, nan
   Final Test: 44.90, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6802, Train: 28.62%, Valid: 28.37%, Test: 28.72%
Epoch: 25, Loss: 1.4250, Train: 37.30%, Valid: 37.09%, Test: 37.15%
Epoch: 50, Loss: 1.4153, Train: 37.12%, Valid: 36.72%, Test: 37.02%
Epoch: 75, Loss: 1.3890, Train: 39.76%, Valid: 39.90%, Test: 40.11%
Epoch: 100, Loss: 1.3642, Train: 40.90%, Valid: 40.77%, Test: 41.22%
Epoch: 125, Loss: 1.3543, Train: 41.30%, Valid: 41.19%, Test: 41.39%
Epoch: 150, Loss: 1.3560, Train: 40.45%, Valid: 40.34%, Test: 40.82%
Epoch: 175, Loss: 1.3371, Train: 41.99%, Valid: 41.87%, Test: 42.07%
Epoch: 200, Loss: 1.3614, Train: 41.04%, Valid: 40.81%, Test: 41.28%
Epoch: 225, Loss: 1.3356, Train: 41.86%, Valid: 41.74%, Test: 41.95%
Epoch: 250, Loss: 1.3264, Train: 42.22%, Valid: 41.97%, Test: 42.17%
Epoch: 275, Loss: 1.3342, Train: 41.44%, Valid: 40.99%, Test: 41.46%
Epoch: 300, Loss: 1.3189, Train: 42.42%, Valid: 42.01%, Test: 42.33%
Epoch: 325, Loss: 1.3102, Train: 42.69%, Valid: 42.31%, Test: 42.60%
Epoch: 350, Loss: 1.3316, Train: 42.34%, Valid: 42.11%, Test: 42.16%
Epoch: 375, Loss: 1.3118, Train: 42.96%, Valid: 42.50%, Test: 42.77%
Epoch: 400, Loss: 1.3040, Train: 42.97%, Valid: 42.64%, Test: 42.92%
Epoch: 425, Loss: 1.3263, Train: 41.95%, Valid: 41.43%, Test: 41.56%
Epoch: 450, Loss: 1.3119, Train: 43.04%, Valid: 42.56%, Test: 42.89%
Epoch: 475, Loss: 1.2978, Train: 43.34%, Valid: 42.95%, Test: 43.13%
Epoch: 500, Loss: 1.2929, Train: 43.58%, Valid: 43.22%, Test: 43.36%
Epoch: 525, Loss: 1.3453, Train: 43.47%, Valid: 42.96%, Test: 43.34%
Epoch: 550, Loss: 1.2999, Train: 43.91%, Valid: 43.45%, Test: 43.67%
Epoch: 575, Loss: 1.3477, Train: 43.04%, Valid: 42.62%, Test: 42.65%
Epoch: 600, Loss: 1.2980, Train: 43.67%, Valid: 43.26%, Test: 43.46%
Epoch: 625, Loss: 1.2888, Train: 43.92%, Valid: 43.40%, Test: 43.56%
Epoch: 650, Loss: 1.2850, Train: 44.25%, Valid: 43.72%, Test: 43.82%
Epoch: 675, Loss: 1.2883, Train: 43.99%, Valid: 43.29%, Test: 43.78%
Epoch: 700, Loss: 1.2811, Train: 44.32%, Valid: 44.03%, Test: 44.07%
Epoch: 725, Loss: 1.2897, Train: 43.90%, Valid: 43.42%, Test: 43.68%
Epoch: 750, Loss: 1.2735, Train: 44.57%, Valid: 44.15%, Test: 44.26%
Epoch: 775, Loss: 1.4066, Train: 36.97%, Valid: 36.32%, Test: 36.86%
Epoch: 800, Loss: 1.3442, Train: 41.42%, Valid: 40.80%, Test: 41.18%
Epoch: 825, Loss: 1.3273, Train: 43.31%, Valid: 42.82%, Test: 43.06%
Epoch: 850, Loss: 1.2979, Train: 44.16%, Valid: 43.71%, Test: 43.75%
Epoch: 875, Loss: 1.3114, Train: 43.56%, Valid: 43.09%, Test: 43.15%
Epoch: 900, Loss: 1.2890, Train: 44.17%, Valid: 43.90%, Test: 43.83%
Epoch: 925, Loss: 1.2829, Train: 44.09%, Valid: 43.61%, Test: 43.78%
Epoch: 950, Loss: 1.2829, Train: 44.50%, Valid: 43.96%, Test: 44.15%
Epoch: 975, Loss: 1.2762, Train: 44.63%, Valid: 44.15%, Test: 44.23%
Run 01:
Highest Train: 44.77
Highest Valid: 44.25
  Final Train: 44.67
   Final Test: 44.24
All runs:
Highest Train: 44.77, nan
Highest Valid: 44.25, nan
  Final Train: 44.67, nan
   Final Test: 44.24, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.5973, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4909, Train: 34.66%, Valid: 34.33%, Test: 34.64%
Epoch: 50, Loss: 1.4692, Train: 35.96%, Valid: 35.29%, Test: 35.80%
Epoch: 75, Loss: 1.4535, Train: 36.66%, Valid: 35.90%, Test: 36.07%
Epoch: 100, Loss: 1.4441, Train: 37.12%, Valid: 36.13%, Test: 36.42%
Epoch: 125, Loss: 1.4338, Train: 37.53%, Valid: 36.11%, Test: 36.34%
Epoch: 150, Loss: 1.4284, Train: 37.90%, Valid: 35.95%, Test: 36.35%
Epoch: 175, Loss: 1.4198, Train: 38.42%, Valid: 35.89%, Test: 36.46%
Epoch: 200, Loss: 1.4133, Train: 38.74%, Valid: 35.90%, Test: 36.38%
Epoch: 225, Loss: 1.4097, Train: 38.86%, Valid: 36.01%, Test: 36.47%
Epoch: 250, Loss: 1.4010, Train: 39.20%, Valid: 35.86%, Test: 36.48%
Epoch: 275, Loss: 1.4049, Train: 39.14%, Valid: 35.51%, Test: 35.73%
Epoch: 300, Loss: 1.3917, Train: 39.64%, Valid: 35.72%, Test: 35.98%
Epoch: 325, Loss: 1.3885, Train: 39.73%, Valid: 35.49%, Test: 35.49%
Epoch: 350, Loss: 1.3859, Train: 40.10%, Valid: 35.44%, Test: 35.87%
Epoch: 375, Loss: 1.3921, Train: 39.01%, Valid: 35.26%, Test: 35.82%
Epoch: 400, Loss: 1.3782, Train: 40.49%, Valid: 35.43%, Test: 35.91%
Epoch: 425, Loss: 1.3748, Train: 40.50%, Valid: 35.18%, Test: 35.66%
Epoch: 450, Loss: 1.3739, Train: 40.80%, Valid: 35.33%, Test: 35.86%
Epoch: 475, Loss: 1.3691, Train: 40.71%, Valid: 34.39%, Test: 34.87%
Epoch: 500, Loss: 1.3665, Train: 41.04%, Valid: 34.64%, Test: 35.21%
Epoch: 525, Loss: 1.3658, Train: 41.24%, Valid: 34.89%, Test: 35.35%
Epoch: 550, Loss: 1.3639, Train: 41.34%, Valid: 34.79%, Test: 35.34%
Epoch: 575, Loss: 1.3617, Train: 41.45%, Valid: 34.81%, Test: 35.31%
Epoch: 600, Loss: 1.3575, Train: 41.48%, Valid: 34.77%, Test: 35.23%
Epoch: 625, Loss: 1.3639, Train: 41.09%, Valid: 34.55%, Test: 34.81%
Epoch: 650, Loss: 1.3620, Train: 40.72%, Valid: 35.05%, Test: 35.63%
Epoch: 675, Loss: 1.3502, Train: 41.85%, Valid: 34.49%, Test: 35.08%
Epoch: 700, Loss: 1.3520, Train: 41.73%, Valid: 34.08%, Test: 34.40%
Epoch: 725, Loss: 1.3553, Train: 41.34%, Valid: 35.03%, Test: 35.41%
Epoch: 750, Loss: 1.3496, Train: 41.79%, Valid: 34.37%, Test: 34.57%
Epoch: 775, Loss: 1.3482, Train: 41.88%, Valid: 34.33%, Test: 34.74%
Epoch: 800, Loss: 1.3550, Train: 41.63%, Valid: 34.14%, Test: 34.28%
Epoch: 825, Loss: 1.3436, Train: 42.13%, Valid: 34.18%, Test: 34.59%
Epoch: 850, Loss: 1.3410, Train: 42.15%, Valid: 34.11%, Test: 34.42%
Epoch: 875, Loss: 1.3410, Train: 42.21%, Valid: 34.32%, Test: 34.85%
Epoch: 900, Loss: 1.3429, Train: 41.94%, Valid: 34.43%, Test: 34.93%
Epoch: 925, Loss: 1.3415, Train: 41.96%, Valid: 34.67%, Test: 34.77%
Epoch: 950, Loss: 1.3339, Train: 42.59%, Valid: 34.40%, Test: 34.76%
Epoch: 975, Loss: 1.3362, Train: 42.59%, Valid: 34.23%, Test: 34.47%
Run 01:
Highest Train: 42.80
Highest Valid: 36.23
  Final Train: 37.37
   Final Test: 36.47
All runs:
Highest Train: 42.80, nan
Highest Valid: 36.23, nan
  Final Train: 37.37, nan
   Final Test: 36.47, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6116, Train: 29.17%, Valid: 28.92%, Test: 29.27%
Epoch: 25, Loss: 1.4851, Train: 34.94%, Valid: 34.75%, Test: 34.82%
Epoch: 50, Loss: 1.4463, Train: 36.87%, Valid: 36.16%, Test: 36.64%
Epoch: 75, Loss: 1.4077, Train: 39.05%, Valid: 37.97%, Test: 38.23%
Epoch: 100, Loss: 1.3898, Train: 39.99%, Valid: 39.19%, Test: 39.14%
Epoch: 125, Loss: 1.3859, Train: 37.65%, Valid: 36.69%, Test: 37.24%
Epoch: 150, Loss: 1.3515, Train: 41.65%, Valid: 40.25%, Test: 40.59%
Epoch: 175, Loss: 1.3354, Train: 42.51%, Valid: 41.00%, Test: 41.29%
Epoch: 200, Loss: 1.3283, Train: 42.88%, Valid: 41.15%, Test: 41.52%
Epoch: 225, Loss: 1.3198, Train: 43.16%, Valid: 41.61%, Test: 41.81%
Epoch: 250, Loss: 1.3188, Train: 43.54%, Valid: 41.76%, Test: 42.11%
Epoch: 275, Loss: 1.3014, Train: 43.56%, Valid: 41.83%, Test: 42.14%
Epoch: 300, Loss: 1.2982, Train: 43.00%, Valid: 41.04%, Test: 41.55%
Epoch: 325, Loss: 1.2969, Train: 44.15%, Valid: 42.16%, Test: 42.50%
Epoch: 350, Loss: 1.2863, Train: 44.06%, Valid: 42.22%, Test: 42.32%
Epoch: 375, Loss: 1.2815, Train: 44.62%, Valid: 42.49%, Test: 42.58%
Epoch: 400, Loss: 1.2909, Train: 44.30%, Valid: 42.36%, Test: 42.80%
Epoch: 425, Loss: 1.2778, Train: 44.12%, Valid: 41.99%, Test: 42.21%
Epoch: 450, Loss: 1.2693, Train: 44.74%, Valid: 42.44%, Test: 42.68%
Epoch: 475, Loss: 1.2639, Train: 45.41%, Valid: 42.97%, Test: 43.19%
Epoch: 500, Loss: 1.2594, Train: 45.47%, Valid: 42.85%, Test: 43.11%
Epoch: 525, Loss: 1.2966, Train: 44.09%, Valid: 42.13%, Test: 42.44%
Epoch: 550, Loss: 1.2660, Train: 45.16%, Valid: 42.83%, Test: 43.04%
Epoch: 575, Loss: 1.2558, Train: 45.58%, Valid: 43.25%, Test: 43.42%
Epoch: 600, Loss: 1.2507, Train: 45.72%, Valid: 43.26%, Test: 43.39%
Epoch: 625, Loss: 1.2918, Train: 42.20%, Valid: 40.11%, Test: 40.61%
Epoch: 650, Loss: 1.2608, Train: 45.54%, Valid: 43.16%, Test: 43.30%
Epoch: 675, Loss: 1.2452, Train: 46.00%, Valid: 43.37%, Test: 43.59%
Epoch: 700, Loss: 1.2460, Train: 46.07%, Valid: 43.25%, Test: 43.46%
Epoch: 725, Loss: 1.2361, Train: 46.65%, Valid: 43.55%, Test: 43.87%
Epoch: 750, Loss: 1.2326, Train: 46.78%, Valid: 43.77%, Test: 43.98%
Epoch: 775, Loss: 1.2336, Train: 46.60%, Valid: 43.66%, Test: 43.85%
Epoch: 800, Loss: 1.2254, Train: 46.86%, Valid: 43.65%, Test: 43.99%
Epoch: 825, Loss: 1.2240, Train: 47.20%, Valid: 44.04%, Test: 44.11%
Epoch: 850, Loss: 1.2462, Train: 45.97%, Valid: 42.93%, Test: 43.22%
Epoch: 875, Loss: 1.2196, Train: 47.29%, Valid: 44.16%, Test: 44.11%
Epoch: 900, Loss: 1.2140, Train: 47.50%, Valid: 44.21%, Test: 44.13%
Epoch: 925, Loss: 1.2237, Train: 47.10%, Valid: 43.80%, Test: 43.76%
Epoch: 950, Loss: 1.2313, Train: 47.40%, Valid: 43.86%, Test: 43.91%
Epoch: 975, Loss: 1.2100, Train: 47.81%, Valid: 44.38%, Test: 44.25%
Run 01:
Highest Train: 48.14
Highest Valid: 44.65
  Final Train: 48.12
   Final Test: 44.52
All runs:
Highest Train: 48.14, nan
Highest Valid: 44.65, nan
  Final Train: 48.12, nan
   Final Test: 44.52, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.7091, Train: 35.16%, Valid: 34.99%, Test: 35.03%
Epoch: 25, Loss: 1.4526, Train: 37.29%, Valid: 37.03%, Test: 37.07%
Epoch: 50, Loss: 1.4155, Train: 36.71%, Valid: 36.64%, Test: 37.22%
Epoch: 75, Loss: 1.3928, Train: 40.69%, Valid: 40.17%, Test: 40.67%
Epoch: 100, Loss: 1.4025, Train: 39.93%, Valid: 39.62%, Test: 39.94%
Epoch: 125, Loss: 1.3821, Train: 40.75%, Valid: 40.43%, Test: 40.71%
Epoch: 150, Loss: 1.3681, Train: 40.28%, Valid: 40.19%, Test: 40.38%
Epoch: 175, Loss: 1.3899, Train: 39.31%, Valid: 38.86%, Test: 39.37%
Epoch: 200, Loss: 1.3613, Train: 41.10%, Valid: 40.80%, Test: 41.18%
Epoch: 225, Loss: 1.3496, Train: 41.32%, Valid: 40.98%, Test: 41.52%
Epoch: 250, Loss: 1.4094, Train: 37.96%, Valid: 37.86%, Test: 37.82%
Epoch: 275, Loss: 1.3918, Train: 39.56%, Valid: 39.16%, Test: 39.45%
Epoch: 300, Loss: 1.3602, Train: 40.69%, Valid: 40.51%, Test: 40.55%
Epoch: 325, Loss: 1.3853, Train: 39.40%, Valid: 39.11%, Test: 39.48%
Epoch: 350, Loss: 1.3524, Train: 41.16%, Valid: 40.95%, Test: 41.05%
Epoch: 375, Loss: 1.3828, Train: 40.47%, Valid: 40.25%, Test: 40.47%
Epoch: 400, Loss: 1.3461, Train: 41.67%, Valid: 41.42%, Test: 41.83%
Epoch: 425, Loss: 1.3361, Train: 42.03%, Valid: 41.66%, Test: 42.23%
Epoch: 450, Loss: 1.4193, Train: 40.74%, Valid: 40.37%, Test: 40.74%
Epoch: 475, Loss: 1.3413, Train: 41.83%, Valid: 41.46%, Test: 41.92%
Epoch: 500, Loss: 1.3291, Train: 42.18%, Valid: 41.89%, Test: 42.30%
Epoch: 525, Loss: 1.3235, Train: 42.36%, Valid: 42.21%, Test: 42.55%
Epoch: 550, Loss: 1.4361, Train: 42.17%, Valid: 41.85%, Test: 42.32%
Epoch: 575, Loss: 1.3401, Train: 41.59%, Valid: 41.36%, Test: 41.58%
Epoch: 600, Loss: 1.3233, Train: 42.32%, Valid: 42.21%, Test: 42.66%
Epoch: 625, Loss: 1.3161, Train: 42.56%, Valid: 42.48%, Test: 42.76%
Epoch: 650, Loss: 1.3122, Train: 42.84%, Valid: 42.69%, Test: 42.95%
Epoch: 675, Loss: 1.3094, Train: 42.90%, Valid: 42.92%, Test: 43.01%
Epoch: 700, Loss: 1.3059, Train: 43.15%, Valid: 43.22%, Test: 43.18%
Epoch: 725, Loss: 1.3053, Train: 42.58%, Valid: 42.72%, Test: 42.77%
Epoch: 750, Loss: 1.3092, Train: 42.01%, Valid: 42.17%, Test: 42.30%
Epoch: 775, Loss: 1.2965, Train: 43.16%, Valid: 43.25%, Test: 43.26%
Epoch: 800, Loss: 1.3007, Train: 43.21%, Valid: 43.24%, Test: 43.30%
Epoch: 825, Loss: 1.2970, Train: 43.00%, Valid: 42.88%, Test: 43.07%
Epoch: 850, Loss: 1.2906, Train: 43.97%, Valid: 43.74%, Test: 43.81%
Epoch: 875, Loss: 1.2911, Train: 43.38%, Valid: 43.30%, Test: 43.49%
Epoch: 900, Loss: 1.2983, Train: 43.85%, Valid: 43.65%, Test: 43.61%
Epoch: 925, Loss: 1.2838, Train: 43.95%, Valid: 43.76%, Test: 43.88%
Epoch: 950, Loss: 1.2893, Train: 43.17%, Valid: 42.90%, Test: 43.13%
Epoch: 975, Loss: 1.7514, Train: 34.42%, Valid: 34.38%, Test: 34.54%
Run 01:
Highest Train: 44.32
Highest Valid: 44.20
  Final Train: 44.27
   Final Test: 44.10
All runs:
Highest Train: 44.32, nan
Highest Valid: 44.20, nan
  Final Train: 44.27, nan
   Final Test: 44.10, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6134, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.5043, Train: 34.39%, Valid: 34.24%, Test: 34.44%
Epoch: 50, Loss: 1.4890, Train: 35.37%, Valid: 34.93%, Test: 35.24%
Epoch: 75, Loss: 1.4785, Train: 36.34%, Valid: 35.47%, Test: 35.81%
Epoch: 100, Loss: 1.4720, Train: 36.83%, Valid: 35.79%, Test: 36.14%
Epoch: 125, Loss: 1.4686, Train: 37.02%, Valid: 35.90%, Test: 36.26%
Epoch: 150, Loss: 1.4661, Train: 37.46%, Valid: 36.06%, Test: 36.48%
Epoch: 175, Loss: 1.4641, Train: 37.68%, Valid: 36.04%, Test: 36.61%
Epoch: 200, Loss: 1.4636, Train: 37.75%, Valid: 36.04%, Test: 36.47%
Epoch: 225, Loss: 1.4598, Train: 37.95%, Valid: 36.10%, Test: 36.52%
Epoch: 250, Loss: 1.4585, Train: 38.03%, Valid: 36.12%, Test: 36.55%
Epoch: 275, Loss: 1.4586, Train: 38.17%, Valid: 35.97%, Test: 36.57%
Epoch: 300, Loss: 1.4577, Train: 38.23%, Valid: 36.01%, Test: 36.52%
Epoch: 325, Loss: 1.4569, Train: 38.30%, Valid: 36.08%, Test: 36.50%
Epoch: 350, Loss: 1.4563, Train: 38.26%, Valid: 36.01%, Test: 36.58%
Epoch: 375, Loss: 1.4565, Train: 38.34%, Valid: 36.04%, Test: 36.48%
Epoch: 400, Loss: 1.4543, Train: 38.35%, Valid: 35.95%, Test: 36.46%
Epoch: 425, Loss: 1.4536, Train: 38.45%, Valid: 35.98%, Test: 36.54%
Epoch: 450, Loss: 1.4551, Train: 38.44%, Valid: 35.89%, Test: 36.42%
Epoch: 475, Loss: 1.4534, Train: 38.43%, Valid: 35.85%, Test: 36.43%
Epoch: 500, Loss: 1.4527, Train: 38.59%, Valid: 35.84%, Test: 36.40%
Epoch: 525, Loss: 1.4530, Train: 38.50%, Valid: 35.95%, Test: 36.41%
Epoch: 550, Loss: 1.4506, Train: 38.56%, Valid: 35.74%, Test: 36.35%
Epoch: 575, Loss: 1.4536, Train: 38.58%, Valid: 35.91%, Test: 36.34%
Epoch: 600, Loss: 1.4516, Train: 38.55%, Valid: 35.83%, Test: 36.35%
Epoch: 625, Loss: 1.4507, Train: 38.54%, Valid: 35.76%, Test: 36.30%
Epoch: 650, Loss: 1.4498, Train: 38.64%, Valid: 35.83%, Test: 36.35%
Epoch: 675, Loss: 1.4478, Train: 38.59%, Valid: 35.83%, Test: 36.23%
Epoch: 700, Loss: 1.4479, Train: 38.55%, Valid: 36.00%, Test: 36.42%
Epoch: 725, Loss: 1.4457, Train: 38.71%, Valid: 35.85%, Test: 36.32%
Epoch: 750, Loss: 1.4451, Train: 38.70%, Valid: 35.99%, Test: 36.44%
Epoch: 775, Loss: 1.4428, Train: 38.77%, Valid: 35.94%, Test: 36.38%
Epoch: 800, Loss: 1.4416, Train: 38.80%, Valid: 35.97%, Test: 36.41%
Epoch: 825, Loss: 1.4395, Train: 38.70%, Valid: 35.96%, Test: 36.34%
Epoch: 850, Loss: 1.4406, Train: 38.71%, Valid: 36.02%, Test: 36.41%
Epoch: 875, Loss: 1.4387, Train: 38.79%, Valid: 36.02%, Test: 36.35%
Epoch: 900, Loss: 1.4362, Train: 38.97%, Valid: 36.08%, Test: 36.46%
Epoch: 925, Loss: 1.4380, Train: 38.84%, Valid: 35.89%, Test: 36.25%
Epoch: 950, Loss: 1.4424, Train: 38.49%, Valid: 35.92%, Test: 36.16%
Epoch: 975, Loss: 1.4411, Train: 38.90%, Valid: 36.37%, Test: 36.62%
Run 01:
Highest Train: 39.22
Highest Valid: 36.47
  Final Train: 39.19
   Final Test: 36.86
All runs:
Highest Train: 39.22, nan
Highest Valid: 36.47, nan
  Final Train: 39.19, nan
   Final Test: 36.86, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6163, Train: 28.73%, Valid: 28.54%, Test: 28.83%
Epoch: 25, Loss: 1.5032, Train: 34.59%, Valid: 34.37%, Test: 34.60%
Epoch: 50, Loss: 1.4855, Train: 35.53%, Valid: 35.13%, Test: 35.41%
Epoch: 75, Loss: 1.4532, Train: 37.29%, Valid: 36.46%, Test: 36.71%
Epoch: 100, Loss: 1.4283, Train: 38.07%, Valid: 36.88%, Test: 37.22%
Epoch: 125, Loss: 1.4236, Train: 38.17%, Valid: 37.07%, Test: 37.52%
Epoch: 150, Loss: 1.4771, Train: 36.99%, Valid: 36.33%, Test: 36.50%
Epoch: 175, Loss: 1.4345, Train: 38.98%, Valid: 37.92%, Test: 38.20%
Epoch: 200, Loss: 1.4195, Train: 39.38%, Valid: 38.28%, Test: 38.60%
Epoch: 225, Loss: 1.4007, Train: 39.84%, Valid: 38.52%, Test: 38.89%
Epoch: 250, Loss: 1.3978, Train: 39.84%, Valid: 38.55%, Test: 38.78%
Epoch: 275, Loss: 1.3857, Train: 40.10%, Valid: 38.79%, Test: 38.81%
Epoch: 300, Loss: 1.3940, Train: 41.19%, Valid: 40.03%, Test: 40.36%
Epoch: 325, Loss: 1.3800, Train: 40.93%, Valid: 39.70%, Test: 39.84%
Epoch: 350, Loss: 1.3914, Train: 39.88%, Valid: 38.94%, Test: 39.17%
Epoch: 375, Loss: 1.4252, Train: 40.51%, Valid: 39.42%, Test: 39.83%
Epoch: 400, Loss: 1.3931, Train: 40.09%, Valid: 38.94%, Test: 39.41%
Epoch: 425, Loss: 1.3680, Train: 42.12%, Valid: 40.91%, Test: 41.21%
Epoch: 450, Loss: 1.3967, Train: 41.66%, Valid: 40.68%, Test: 40.98%
Epoch: 475, Loss: 1.3662, Train: 38.16%, Valid: 37.36%, Test: 37.69%
Epoch: 500, Loss: 1.3743, Train: 42.37%, Valid: 41.30%, Test: 41.73%
Epoch: 525, Loss: 1.3603, Train: 42.39%, Valid: 41.55%, Test: 41.63%
Epoch: 550, Loss: 1.3620, Train: 42.56%, Valid: 41.57%, Test: 41.89%
Epoch: 575, Loss: 1.3635, Train: 42.05%, Valid: 41.16%, Test: 41.32%
Epoch: 600, Loss: 1.3582, Train: 43.05%, Valid: 42.23%, Test: 42.32%
Epoch: 625, Loss: 1.3526, Train: 41.14%, Valid: 40.50%, Test: 40.88%
Epoch: 650, Loss: 1.3594, Train: 42.07%, Valid: 41.31%, Test: 41.54%
Epoch: 675, Loss: 1.3765, Train: 41.32%, Valid: 40.40%, Test: 40.46%
Epoch: 700, Loss: 1.3649, Train: 42.41%, Valid: 41.46%, Test: 41.35%
Epoch: 725, Loss: 1.3526, Train: 42.80%, Valid: 41.80%, Test: 41.83%
Epoch: 750, Loss: 1.3494, Train: 42.70%, Valid: 41.72%, Test: 41.62%
Epoch: 775, Loss: 1.3362, Train: 43.10%, Valid: 42.13%, Test: 42.29%
Epoch: 800, Loss: 1.3430, Train: 42.81%, Valid: 41.95%, Test: 42.11%
Epoch: 825, Loss: 1.3647, Train: 43.50%, Valid: 42.70%, Test: 42.80%
Epoch: 850, Loss: 1.3332, Train: 42.81%, Valid: 41.85%, Test: 42.29%
Epoch: 875, Loss: 1.3366, Train: 42.74%, Valid: 41.84%, Test: 42.26%
Epoch: 900, Loss: 1.3396, Train: 43.72%, Valid: 42.77%, Test: 43.13%
Epoch: 925, Loss: 1.3428, Train: 42.66%, Valid: 41.91%, Test: 42.28%
Epoch: 950, Loss: 1.3525, Train: 42.57%, Valid: 41.77%, Test: 42.28%
Epoch: 975, Loss: 1.3647, Train: 42.61%, Valid: 42.16%, Test: 42.24%
Run 01:
Highest Train: 43.97
Highest Valid: 43.09
  Final Train: 43.97
   Final Test: 43.25
All runs:
Highest Train: 43.97, nan
Highest Valid: 43.09, nan
  Final Train: 43.97, nan
   Final Test: 43.25, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6243, Train: 32.44%, Valid: 32.19%, Test: 32.94%
Epoch: 25, Loss: 1.4436, Train: 25.98%, Valid: 25.86%, Test: 26.27%
Epoch: 50, Loss: 1.4193, Train: 35.70%, Valid: 35.25%, Test: 35.33%
Epoch: 75, Loss: 1.4140, Train: 29.74%, Valid: 29.34%, Test: 29.49%
Epoch: 100, Loss: 1.4070, Train: 36.48%, Valid: 35.94%, Test: 36.59%
Epoch: 125, Loss: 1.3827, Train: 35.26%, Valid: 35.02%, Test: 35.10%
Epoch: 150, Loss: 1.3757, Train: 32.71%, Valid: 32.67%, Test: 32.56%
Epoch: 175, Loss: 1.3945, Train: 32.73%, Valid: 32.75%, Test: 32.26%
Epoch: 200, Loss: 1.4037, Train: 34.75%, Valid: 34.62%, Test: 34.72%
Epoch: 225, Loss: 1.3715, Train: 37.96%, Valid: 37.69%, Test: 38.00%
Epoch: 250, Loss: 1.3707, Train: 37.42%, Valid: 37.15%, Test: 37.43%
Epoch: 275, Loss: 1.3683, Train: 37.68%, Valid: 37.52%, Test: 37.77%
Epoch: 300, Loss: 1.3725, Train: 38.86%, Valid: 38.55%, Test: 38.60%
Epoch: 325, Loss: 1.3720, Train: 38.90%, Valid: 38.65%, Test: 39.10%
Epoch: 350, Loss: 1.3793, Train: 40.26%, Valid: 40.07%, Test: 40.00%
Epoch: 375, Loss: 1.3632, Train: 41.61%, Valid: 41.09%, Test: 41.45%
Epoch: 400, Loss: 1.3509, Train: 42.16%, Valid: 41.83%, Test: 42.06%
Epoch: 425, Loss: 1.3576, Train: 38.86%, Valid: 38.35%, Test: 38.90%
Epoch: 450, Loss: 1.3552, Train: 42.08%, Valid: 41.67%, Test: 42.21%
Epoch: 475, Loss: 1.3571, Train: 42.03%, Valid: 41.62%, Test: 42.08%
Epoch: 500, Loss: 1.3555, Train: 41.16%, Valid: 40.85%, Test: 40.97%
Epoch: 525, Loss: 1.3549, Train: 40.28%, Valid: 39.91%, Test: 40.29%
Epoch: 550, Loss: 1.3466, Train: 41.20%, Valid: 40.75%, Test: 41.35%
Epoch: 575, Loss: 1.3572, Train: 42.53%, Valid: 42.03%, Test: 42.19%
Epoch: 600, Loss: 1.3534, Train: 42.29%, Valid: 42.06%, Test: 42.33%
Epoch: 625, Loss: 1.3592, Train: 41.76%, Valid: 41.43%, Test: 41.70%
Epoch: 650, Loss: 1.3478, Train: 42.49%, Valid: 42.06%, Test: 42.10%
Epoch: 675, Loss: 1.3516, Train: 42.68%, Valid: 42.40%, Test: 42.56%
Epoch: 700, Loss: 1.3431, Train: 43.10%, Valid: 42.77%, Test: 43.18%
Epoch: 725, Loss: 1.3464, Train: 42.79%, Valid: 42.27%, Test: 42.53%
Epoch: 750, Loss: 1.3331, Train: 40.96%, Valid: 40.74%, Test: 41.42%
Epoch: 775, Loss: 1.3385, Train: 42.11%, Valid: 41.76%, Test: 42.02%
Epoch: 800, Loss: 1.3582, Train: 42.70%, Valid: 42.44%, Test: 42.58%
Epoch: 825, Loss: 1.3360, Train: 42.63%, Valid: 42.45%, Test: 42.71%
Epoch: 850, Loss: 1.3468, Train: 42.83%, Valid: 42.61%, Test: 43.04%
Epoch: 875, Loss: 1.3473, Train: 43.14%, Valid: 42.87%, Test: 42.89%
Epoch: 900, Loss: 1.3396, Train: 42.08%, Valid: 41.67%, Test: 41.97%
Epoch: 925, Loss: 1.3348, Train: 42.23%, Valid: 42.12%, Test: 42.43%
Epoch: 950, Loss: 1.3639, Train: 42.75%, Valid: 42.34%, Test: 42.82%
Epoch: 975, Loss: 1.3639, Train: 42.68%, Valid: 42.41%, Test: 42.78%
Run 01:
Highest Train: 43.52
Highest Valid: 43.32
  Final Train: 43.51
   Final Test: 43.22
All runs:
Highest Train: 43.52, nan
Highest Valid: 43.32, nan
  Final Train: 43.51, nan
   Final Test: 43.22, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6353, Train: 22.16%, Valid: 22.17%, Test: 22.74%
Epoch: 25, Loss: 1.5086, Train: 34.31%, Valid: 34.18%, Test: 34.45%
Epoch: 50, Loss: 1.4897, Train: 35.58%, Valid: 35.12%, Test: 35.43%
Epoch: 75, Loss: 1.4811, Train: 36.18%, Valid: 35.38%, Test: 35.79%
Epoch: 100, Loss: 1.4754, Train: 36.68%, Valid: 35.62%, Test: 36.08%
Epoch: 125, Loss: 1.4711, Train: 36.89%, Valid: 35.67%, Test: 36.24%
Epoch: 150, Loss: 1.4695, Train: 37.15%, Valid: 35.89%, Test: 36.35%
Epoch: 175, Loss: 1.4684, Train: 37.29%, Valid: 35.95%, Test: 36.36%
Epoch: 200, Loss: 1.4665, Train: 37.45%, Valid: 35.90%, Test: 36.36%
Epoch: 225, Loss: 1.4651, Train: 37.58%, Valid: 36.02%, Test: 36.39%
Epoch: 250, Loss: 1.4632, Train: 37.69%, Valid: 36.01%, Test: 36.55%
Epoch: 275, Loss: 1.4628, Train: 37.79%, Valid: 36.00%, Test: 36.49%
Epoch: 300, Loss: 1.4618, Train: 37.88%, Valid: 35.98%, Test: 36.52%
Epoch: 325, Loss: 1.4626, Train: 37.85%, Valid: 35.96%, Test: 36.41%
Epoch: 350, Loss: 1.4598, Train: 37.89%, Valid: 35.98%, Test: 36.52%
Epoch: 375, Loss: 1.4599, Train: 37.97%, Valid: 36.02%, Test: 36.42%
Epoch: 400, Loss: 1.4595, Train: 38.10%, Valid: 35.96%, Test: 36.43%
Epoch: 425, Loss: 1.4560, Train: 38.15%, Valid: 35.97%, Test: 36.48%
Epoch: 450, Loss: 1.4579, Train: 38.13%, Valid: 35.94%, Test: 36.48%
Epoch: 475, Loss: 1.4566, Train: 38.27%, Valid: 36.01%, Test: 36.43%
Epoch: 500, Loss: 1.4546, Train: 38.18%, Valid: 35.96%, Test: 36.38%
Epoch: 525, Loss: 1.4540, Train: 38.36%, Valid: 36.07%, Test: 36.52%
Epoch: 550, Loss: 1.4555, Train: 38.28%, Valid: 36.02%, Test: 36.43%
Epoch: 575, Loss: 1.4545, Train: 38.42%, Valid: 36.07%, Test: 36.37%
Epoch: 600, Loss: 1.4525, Train: 38.51%, Valid: 36.09%, Test: 36.53%
Epoch: 625, Loss: 1.4520, Train: 38.35%, Valid: 36.08%, Test: 36.51%
Epoch: 650, Loss: 1.4514, Train: 38.57%, Valid: 36.06%, Test: 36.49%
Epoch: 675, Loss: 1.4498, Train: 38.45%, Valid: 36.07%, Test: 36.49%
Epoch: 700, Loss: 1.4479, Train: 38.58%, Valid: 36.28%, Test: 36.39%
Epoch: 725, Loss: 1.4496, Train: 38.58%, Valid: 36.12%, Test: 36.54%
Epoch: 750, Loss: 1.4489, Train: 38.66%, Valid: 36.18%, Test: 36.67%
Epoch: 775, Loss: 1.4487, Train: 38.58%, Valid: 36.03%, Test: 36.54%
Epoch: 800, Loss: 1.4458, Train: 38.67%, Valid: 36.03%, Test: 36.53%
Epoch: 825, Loss: 1.4474, Train: 38.61%, Valid: 36.06%, Test: 36.54%
Epoch: 850, Loss: 1.4446, Train: 38.88%, Valid: 36.28%, Test: 36.69%
Epoch: 875, Loss: 1.4425, Train: 38.97%, Valid: 36.34%, Test: 36.72%
Epoch: 900, Loss: 1.4436, Train: 39.03%, Valid: 36.29%, Test: 36.76%
Epoch: 925, Loss: 1.4445, Train: 38.73%, Valid: 36.14%, Test: 36.60%
Epoch: 950, Loss: 1.4408, Train: 39.05%, Valid: 36.52%, Test: 36.88%
Epoch: 975, Loss: 1.4404, Train: 39.10%, Valid: 36.56%, Test: 36.93%
Run 01:
Highest Train: 39.34
Highest Valid: 36.66
  Final Train: 39.31
   Final Test: 37.13
All runs:
Highest Train: 39.34, nan
Highest Valid: 36.66, nan
  Final Train: 39.31, nan
   Final Test: 37.13, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6151, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.5030, Train: 34.67%, Valid: 34.45%, Test: 34.61%
Epoch: 50, Loss: 1.4799, Train: 35.84%, Valid: 35.33%, Test: 35.62%
Epoch: 75, Loss: 1.4468, Train: 37.28%, Valid: 36.44%, Test: 36.85%
Epoch: 100, Loss: 1.4264, Train: 38.75%, Valid: 37.81%, Test: 38.13%
Epoch: 125, Loss: 1.4177, Train: 39.13%, Valid: 37.77%, Test: 38.18%
Epoch: 150, Loss: 1.4095, Train: 39.42%, Valid: 38.04%, Test: 38.39%
Epoch: 175, Loss: 1.4032, Train: 40.01%, Valid: 38.68%, Test: 39.23%
Epoch: 200, Loss: 1.4001, Train: 40.05%, Valid: 38.72%, Test: 39.00%
Epoch: 225, Loss: 1.3954, Train: 40.86%, Valid: 39.66%, Test: 39.62%
Epoch: 250, Loss: 1.3844, Train: 41.00%, Valid: 39.75%, Test: 40.14%
Epoch: 275, Loss: 1.3890, Train: 41.35%, Valid: 40.45%, Test: 40.49%
Epoch: 300, Loss: 1.3829, Train: 37.52%, Valid: 36.32%, Test: 36.86%
Epoch: 325, Loss: 1.3708, Train: 39.38%, Valid: 38.52%, Test: 38.78%
Epoch: 350, Loss: 1.3814, Train: 40.69%, Valid: 39.59%, Test: 39.89%
Epoch: 375, Loss: 1.3749, Train: 42.13%, Valid: 40.99%, Test: 41.06%
Epoch: 400, Loss: 1.3740, Train: 41.22%, Valid: 40.09%, Test: 40.31%
Epoch: 425, Loss: 1.3571, Train: 43.22%, Valid: 42.39%, Test: 42.17%
Epoch: 450, Loss: 1.3708, Train: 40.57%, Valid: 39.70%, Test: 39.79%
Epoch: 475, Loss: 1.3608, Train: 43.17%, Valid: 42.18%, Test: 42.43%
Epoch: 500, Loss: 1.3649, Train: 42.86%, Valid: 41.83%, Test: 41.81%
Epoch: 525, Loss: 1.3576, Train: 42.73%, Valid: 41.74%, Test: 41.89%
Epoch: 550, Loss: 1.3801, Train: 40.39%, Valid: 39.33%, Test: 39.58%
Epoch: 575, Loss: 1.3678, Train: 39.73%, Valid: 39.08%, Test: 39.31%
Epoch: 600, Loss: 1.3666, Train: 42.82%, Valid: 42.01%, Test: 42.16%
Epoch: 625, Loss: 1.3558, Train: 42.39%, Valid: 41.49%, Test: 41.83%
Epoch: 650, Loss: 1.3842, Train: 40.40%, Valid: 39.53%, Test: 39.69%
Epoch: 675, Loss: 1.3568, Train: 42.83%, Valid: 42.22%, Test: 42.27%
Epoch: 700, Loss: 1.3616, Train: 42.52%, Valid: 41.86%, Test: 41.85%
Epoch: 725, Loss: 1.3542, Train: 41.24%, Valid: 40.56%, Test: 40.84%
Epoch: 750, Loss: 1.3590, Train: 42.22%, Valid: 41.55%, Test: 41.72%
Epoch: 775, Loss: 1.3544, Train: 41.89%, Valid: 40.98%, Test: 41.50%
Epoch: 800, Loss: 1.3431, Train: 42.26%, Valid: 41.54%, Test: 41.83%
Epoch: 825, Loss: 1.3342, Train: 43.39%, Valid: 42.60%, Test: 42.83%
Epoch: 850, Loss: 1.3512, Train: 42.01%, Valid: 41.17%, Test: 41.27%
Epoch: 875, Loss: 1.3429, Train: 43.27%, Valid: 42.53%, Test: 42.67%
Epoch: 900, Loss: 1.3495, Train: 42.39%, Valid: 41.42%, Test: 41.68%
Epoch: 925, Loss: 1.3333, Train: 38.09%, Valid: 37.25%, Test: 37.97%
Epoch: 950, Loss: 1.3371, Train: 43.67%, Valid: 42.64%, Test: 43.06%
Epoch: 975, Loss: 1.3328, Train: 43.74%, Valid: 42.47%, Test: 42.92%
Run 01:
Highest Train: 44.21
Highest Valid: 43.23
  Final Train: 44.21
   Final Test: 43.38
All runs:
Highest Train: 44.21, nan
Highest Valid: 43.23, nan
  Final Train: 44.21, nan
   Final Test: 43.38, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.5637, Train: 28.60%, Valid: 28.38%, Test: 28.73%
Epoch: 25, Loss: 1.4402, Train: 30.31%, Valid: 30.28%, Test: 30.31%
Epoch: 50, Loss: 1.4138, Train: 36.44%, Valid: 35.99%, Test: 36.50%
Epoch: 75, Loss: 1.4063, Train: 35.86%, Valid: 35.47%, Test: 36.05%
Epoch: 100, Loss: 1.3969, Train: 34.23%, Valid: 33.75%, Test: 34.07%
Epoch: 125, Loss: 1.4071, Train: 31.21%, Valid: 31.02%, Test: 30.94%
Epoch: 150, Loss: 1.3767, Train: 33.34%, Valid: 32.92%, Test: 33.03%
Epoch: 175, Loss: 1.3672, Train: 36.45%, Valid: 36.07%, Test: 36.67%
Epoch: 200, Loss: 1.3748, Train: 34.29%, Valid: 34.24%, Test: 34.40%
Epoch: 225, Loss: 1.3675, Train: 36.25%, Valid: 36.14%, Test: 36.33%
Epoch: 250, Loss: 1.3777, Train: 37.37%, Valid: 37.21%, Test: 37.65%
Epoch: 275, Loss: 1.3583, Train: 38.08%, Valid: 37.85%, Test: 38.27%
Epoch: 300, Loss: 1.3547, Train: 41.36%, Valid: 40.99%, Test: 41.05%
Epoch: 325, Loss: 1.3540, Train: 38.79%, Valid: 38.53%, Test: 38.85%
Epoch: 350, Loss: 1.3552, Train: 41.64%, Valid: 41.39%, Test: 41.55%
Epoch: 375, Loss: 1.3531, Train: 38.84%, Valid: 38.66%, Test: 39.18%
Epoch: 400, Loss: 1.3622, Train: 41.63%, Valid: 41.27%, Test: 41.39%
Epoch: 425, Loss: 1.3705, Train: 41.84%, Valid: 41.56%, Test: 41.90%
Epoch: 450, Loss: 1.3592, Train: 42.62%, Valid: 42.31%, Test: 42.69%
Epoch: 475, Loss: 1.3836, Train: 41.20%, Valid: 40.89%, Test: 41.19%
Epoch: 500, Loss: 1.3607, Train: 41.89%, Valid: 41.58%, Test: 41.90%
Epoch: 525, Loss: 1.3642, Train: 38.50%, Valid: 38.46%, Test: 38.90%
Epoch: 550, Loss: 1.3728, Train: 41.81%, Valid: 41.50%, Test: 41.90%
Epoch: 575, Loss: 1.3569, Train: 41.88%, Valid: 41.52%, Test: 41.89%
Epoch: 600, Loss: 1.3574, Train: 42.36%, Valid: 42.04%, Test: 42.25%
Epoch: 625, Loss: 1.3593, Train: 41.56%, Valid: 41.28%, Test: 41.37%
Epoch: 650, Loss: 1.3446, Train: 41.51%, Valid: 41.08%, Test: 41.32%
Epoch: 675, Loss: 1.3509, Train: 41.49%, Valid: 41.20%, Test: 41.42%
Epoch: 700, Loss: 1.3698, Train: 40.01%, Valid: 39.95%, Test: 40.14%
Epoch: 725, Loss: 1.3539, Train: 40.53%, Valid: 40.34%, Test: 40.78%
Epoch: 750, Loss: 1.3734, Train: 40.56%, Valid: 40.22%, Test: 40.72%
Epoch: 775, Loss: 1.3419, Train: 39.98%, Valid: 39.80%, Test: 40.16%
Epoch: 800, Loss: 1.3442, Train: 40.88%, Valid: 40.60%, Test: 41.12%
Epoch: 825, Loss: 1.3442, Train: 41.55%, Valid: 41.39%, Test: 41.84%
Epoch: 850, Loss: 1.3493, Train: 41.60%, Valid: 41.34%, Test: 41.64%
Epoch: 875, Loss: 1.3613, Train: 41.74%, Valid: 41.36%, Test: 41.37%
Epoch: 900, Loss: 1.3581, Train: 40.96%, Valid: 40.70%, Test: 40.97%
Epoch: 925, Loss: 1.3483, Train: 42.34%, Valid: 42.06%, Test: 42.32%
Epoch: 950, Loss: 1.3408, Train: 41.97%, Valid: 41.67%, Test: 42.04%
Epoch: 975, Loss: 1.3623, Train: 39.69%, Valid: 39.46%, Test: 39.56%
Run 01:
Highest Train: 42.78
Highest Valid: 42.48
  Final Train: 42.78
   Final Test: 42.77
All runs:
Highest Train: 42.78, nan
Highest Valid: 42.48, nan
  Final Train: 42.78, nan
   Final Test: 42.77, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6022, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4898, Train: 34.84%, Valid: 34.53%, Test: 34.73%
Epoch: 50, Loss: 1.4706, Train: 35.84%, Valid: 35.12%, Test: 35.52%
Epoch: 75, Loss: 1.4544, Train: 36.68%, Valid: 35.88%, Test: 35.97%
Epoch: 100, Loss: 1.4421, Train: 37.39%, Valid: 36.19%, Test: 36.37%
Epoch: 125, Loss: 1.4346, Train: 37.64%, Valid: 36.25%, Test: 36.48%
Epoch: 150, Loss: 1.4294, Train: 38.04%, Valid: 36.07%, Test: 36.44%
Epoch: 175, Loss: 1.4242, Train: 38.16%, Valid: 35.75%, Test: 36.18%
Epoch: 200, Loss: 1.4295, Train: 36.53%, Valid: 34.42%, Test: 34.31%
Epoch: 225, Loss: 1.4118, Train: 38.77%, Valid: 35.95%, Test: 36.41%
Epoch: 250, Loss: 1.4054, Train: 39.27%, Valid: 35.88%, Test: 36.12%
Epoch: 275, Loss: 1.4033, Train: 39.26%, Valid: 35.91%, Test: 36.34%
Epoch: 300, Loss: 1.4000, Train: 39.60%, Valid: 35.78%, Test: 35.96%
Epoch: 325, Loss: 1.3943, Train: 39.67%, Valid: 35.80%, Test: 35.99%
Epoch: 350, Loss: 1.3971, Train: 39.34%, Valid: 35.68%, Test: 35.79%
Epoch: 375, Loss: 1.3884, Train: 40.06%, Valid: 35.33%, Test: 35.72%
Epoch: 400, Loss: 1.3847, Train: 40.13%, Valid: 35.84%, Test: 36.14%
Epoch: 425, Loss: 1.3835, Train: 40.29%, Valid: 35.57%, Test: 36.07%
Epoch: 450, Loss: 1.3811, Train: 40.30%, Valid: 35.29%, Test: 35.81%
Epoch: 475, Loss: 1.3812, Train: 40.49%, Valid: 35.47%, Test: 35.97%
Epoch: 500, Loss: 1.3757, Train: 40.65%, Valid: 35.52%, Test: 35.87%
Epoch: 525, Loss: 1.3770, Train: 40.81%, Valid: 35.64%, Test: 35.98%
Epoch: 550, Loss: 1.3719, Train: 41.02%, Valid: 35.58%, Test: 35.81%
Epoch: 575, Loss: 1.3724, Train: 40.20%, Valid: 35.45%, Test: 35.95%
Epoch: 600, Loss: 1.3625, Train: 41.25%, Valid: 35.63%, Test: 35.91%
Epoch: 625, Loss: 1.3710, Train: 40.66%, Valid: 35.76%, Test: 36.20%
Epoch: 650, Loss: 1.3562, Train: 41.69%, Valid: 35.90%, Test: 36.29%
Epoch: 675, Loss: 1.3664, Train: 39.88%, Valid: 34.26%, Test: 34.73%
Epoch: 700, Loss: 1.3526, Train: 41.76%, Valid: 35.69%, Test: 36.11%
Epoch: 725, Loss: 1.3387, Train: 42.28%, Valid: 36.33%, Test: 36.52%
Epoch: 750, Loss: 1.3389, Train: 42.42%, Valid: 35.89%, Test: 36.17%
Epoch: 775, Loss: 1.3498, Train: 41.02%, Valid: 35.52%, Test: 35.78%
Epoch: 800, Loss: 1.3299, Train: 42.69%, Valid: 36.18%, Test: 36.76%
Epoch: 825, Loss: 1.3518, Train: 41.33%, Valid: 35.39%, Test: 35.85%
Epoch: 850, Loss: 1.3317, Train: 42.51%, Valid: 36.59%, Test: 36.77%
Epoch: 875, Loss: 1.3074, Train: 43.66%, Valid: 36.87%, Test: 37.17%
Epoch: 900, Loss: 1.3179, Train: 43.27%, Valid: 37.32%, Test: 37.52%
Epoch: 925, Loss: 1.3769, Train: 37.38%, Valid: 32.44%, Test: 33.05%
Epoch: 950, Loss: 1.3128, Train: 43.59%, Valid: 37.47%, Test: 38.06%
Epoch: 975, Loss: 1.2887, Train: 44.62%, Valid: 37.74%, Test: 38.05%
Run 01:
Highest Train: 44.78
Highest Valid: 38.01
  Final Train: 44.78
   Final Test: 38.23
All runs:
Highest Train: 44.78, nan
Highest Valid: 38.01, nan
  Final Train: 44.78, nan
   Final Test: 38.23, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6115, Train: 28.72%, Valid: 28.53%, Test: 28.82%
Epoch: 25, Loss: 1.4892, Train: 34.87%, Valid: 34.53%, Test: 34.87%
Epoch: 50, Loss: 1.4449, Train: 36.94%, Valid: 36.41%, Test: 36.56%
Epoch: 75, Loss: 1.4118, Train: 38.74%, Valid: 37.89%, Test: 38.23%
Epoch: 100, Loss: 1.3769, Train: 40.09%, Valid: 38.90%, Test: 39.35%
Epoch: 125, Loss: 1.3620, Train: 40.96%, Valid: 39.92%, Test: 40.21%
Epoch: 150, Loss: 1.3308, Train: 42.00%, Valid: 40.96%, Test: 41.03%
Epoch: 175, Loss: 1.3929, Train: 36.85%, Valid: 35.95%, Test: 35.71%
Epoch: 200, Loss: 1.3028, Train: 43.12%, Valid: 42.05%, Test: 42.18%
Epoch: 225, Loss: 1.2753, Train: 44.70%, Valid: 43.35%, Test: 43.72%
Epoch: 250, Loss: 1.2611, Train: 45.16%, Valid: 43.64%, Test: 44.03%
Epoch: 275, Loss: 1.2475, Train: 45.92%, Valid: 44.28%, Test: 44.46%
Epoch: 300, Loss: 1.2383, Train: 46.39%, Valid: 44.62%, Test: 44.91%
Epoch: 325, Loss: 1.2311, Train: 46.74%, Valid: 44.90%, Test: 45.06%
Epoch: 350, Loss: 1.2190, Train: 47.01%, Valid: 45.26%, Test: 45.27%
Epoch: 375, Loss: 1.2345, Train: 46.65%, Valid: 45.24%, Test: 45.16%
Epoch: 400, Loss: 1.2103, Train: 47.74%, Valid: 45.84%, Test: 45.85%
Epoch: 425, Loss: 1.2125, Train: 47.71%, Valid: 45.70%, Test: 45.68%
Epoch: 450, Loss: 1.1954, Train: 48.31%, Valid: 46.25%, Test: 46.22%
Epoch: 475, Loss: 1.3217, Train: 43.70%, Valid: 42.43%, Test: 42.36%
Epoch: 500, Loss: 1.2424, Train: 46.28%, Valid: 44.83%, Test: 45.12%
Epoch: 525, Loss: 1.2210, Train: 47.16%, Valid: 45.55%, Test: 45.66%
Epoch: 550, Loss: 1.2200, Train: 47.03%, Valid: 45.44%, Test: 45.41%
Epoch: 575, Loss: 1.2033, Train: 47.71%, Valid: 46.03%, Test: 46.11%
Epoch: 600, Loss: 1.2307, Train: 47.33%, Valid: 45.38%, Test: 45.66%
Epoch: 625, Loss: 1.2075, Train: 47.71%, Valid: 46.08%, Test: 46.01%
Epoch: 650, Loss: 1.1914, Train: 48.34%, Valid: 46.48%, Test: 46.51%
Epoch: 675, Loss: 1.1902, Train: 48.80%, Valid: 46.65%, Test: 46.77%
Epoch: 700, Loss: 1.1843, Train: 48.73%, Valid: 46.67%, Test: 46.54%
Epoch: 725, Loss: 1.1928, Train: 48.17%, Valid: 46.17%, Test: 46.12%
Epoch: 750, Loss: 1.1796, Train: 49.14%, Valid: 46.74%, Test: 46.87%
Epoch: 775, Loss: 1.1951, Train: 48.38%, Valid: 46.59%, Test: 46.41%
Epoch: 800, Loss: 1.1879, Train: 48.73%, Valid: 46.72%, Test: 46.77%
Epoch: 825, Loss: 1.1807, Train: 49.19%, Valid: 46.88%, Test: 46.79%
Epoch: 850, Loss: 1.1745, Train: 49.43%, Valid: 46.84%, Test: 47.03%
Epoch: 875, Loss: 1.1670, Train: 49.60%, Valid: 47.26%, Test: 47.21%
Epoch: 900, Loss: 1.1592, Train: 50.01%, Valid: 47.43%, Test: 47.54%
Epoch: 925, Loss: 1.2019, Train: 48.74%, Valid: 46.62%, Test: 46.64%
Epoch: 950, Loss: 1.1669, Train: 49.62%, Valid: 47.29%, Test: 47.19%
Epoch: 975, Loss: 1.1687, Train: 49.80%, Valid: 47.26%, Test: 47.14%
Run 01:
Highest Train: 50.45
Highest Valid: 47.81
  Final Train: 50.35
   Final Test: 47.71
All runs:
Highest Train: 50.45, nan
Highest Valid: 47.81, nan
  Final Train: 50.35, nan
   Final Test: 47.71, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.7501, Train: 22.16%, Valid: 22.18%, Test: 22.74%
Epoch: 25, Loss: 1.5085, Train: 30.40%, Valid: 30.24%, Test: 30.70%
Epoch: 50, Loss: 1.4482, Train: 36.05%, Valid: 35.87%, Test: 36.15%
Epoch: 75, Loss: 1.4291, Train: 37.19%, Valid: 36.89%, Test: 37.33%
Epoch: 100, Loss: 1.4040, Train: 38.52%, Valid: 37.99%, Test: 38.51%
Epoch: 125, Loss: 1.3785, Train: 39.20%, Valid: 38.77%, Test: 39.23%
Epoch: 150, Loss: 1.3789, Train: 39.46%, Valid: 39.10%, Test: 39.59%
Epoch: 175, Loss: 1.3613, Train: 40.14%, Valid: 39.79%, Test: 39.95%
Epoch: 200, Loss: 1.3760, Train: 40.23%, Valid: 39.81%, Test: 39.89%
Epoch: 225, Loss: 1.3514, Train: 40.44%, Valid: 39.97%, Test: 40.05%
Epoch: 250, Loss: 1.3507, Train: 40.69%, Valid: 40.21%, Test: 40.64%
Epoch: 275, Loss: 1.3517, Train: 39.18%, Valid: 38.84%, Test: 39.18%
Epoch: 300, Loss: 1.3370, Train: 41.49%, Valid: 40.98%, Test: 41.26%
Epoch: 325, Loss: 1.3662, Train: 40.28%, Valid: 39.54%, Test: 40.04%
Epoch: 350, Loss: 1.3459, Train: 41.05%, Valid: 40.50%, Test: 40.77%
Epoch: 375, Loss: 1.3398, Train: 41.66%, Valid: 41.20%, Test: 41.27%
Epoch: 400, Loss: 1.3411, Train: 40.54%, Valid: 40.30%, Test: 40.79%
Epoch: 425, Loss: 1.3225, Train: 43.04%, Valid: 42.58%, Test: 42.66%
Epoch: 450, Loss: 1.3407, Train: 41.87%, Valid: 41.33%, Test: 41.72%
Epoch: 475, Loss: 1.3580, Train: 42.50%, Valid: 42.11%, Test: 42.44%
Epoch: 500, Loss: 1.3150, Train: 43.36%, Valid: 43.00%, Test: 43.34%
Epoch: 525, Loss: 1.3408, Train: 43.43%, Valid: 43.00%, Test: 43.44%
Epoch: 550, Loss: 1.3153, Train: 43.38%, Valid: 43.11%, Test: 43.45%
Epoch: 575, Loss: 1.3056, Train: 44.07%, Valid: 43.88%, Test: 44.03%
Epoch: 600, Loss: 1.3041, Train: 44.32%, Valid: 44.05%, Test: 44.28%
Epoch: 625, Loss: 1.3954, Train: 39.10%, Valid: 38.91%, Test: 39.29%
Epoch: 650, Loss: 1.3522, Train: 40.55%, Valid: 40.17%, Test: 40.37%
Epoch: 675, Loss: 1.3236, Train: 42.97%, Valid: 42.54%, Test: 42.73%
Epoch: 700, Loss: 1.3539, Train: 40.88%, Valid: 40.38%, Test: 40.89%
Epoch: 725, Loss: 1.3183, Train: 43.34%, Valid: 43.02%, Test: 43.32%
Epoch: 750, Loss: 1.3145, Train: 43.58%, Valid: 43.40%, Test: 43.61%
Epoch: 775, Loss: 1.3121, Train: 43.86%, Valid: 43.64%, Test: 43.81%
Epoch: 800, Loss: 1.3046, Train: 44.54%, Valid: 44.20%, Test: 44.40%
Epoch: 825, Loss: 1.2937, Train: 44.91%, Valid: 44.42%, Test: 44.48%
Epoch: 850, Loss: 1.3760, Train: 40.42%, Valid: 39.87%, Test: 40.23%
Epoch: 875, Loss: 1.3171, Train: 43.25%, Valid: 42.84%, Test: 43.13%
Epoch: 900, Loss: 1.3007, Train: 44.47%, Valid: 44.07%, Test: 44.28%
Epoch: 925, Loss: 1.3095, Train: 44.43%, Valid: 43.89%, Test: 44.08%
Epoch: 950, Loss: 1.2944, Train: 44.78%, Valid: 44.39%, Test: 44.51%
Epoch: 975, Loss: 1.3139, Train: 43.45%, Valid: 43.05%, Test: 43.24%
Run 01:
Highest Train: 45.09
Highest Valid: 44.73
  Final Train: 45.05
   Final Test: 44.77
All runs:
Highest Train: 45.09, nan
Highest Valid: 44.73, nan
  Final Train: 45.05, nan
   Final Test: 44.77, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6238, Train: 22.16%, Valid: 22.17%, Test: 22.74%
Epoch: 25, Loss: 1.4903, Train: 34.75%, Valid: 34.37%, Test: 34.72%
Epoch: 50, Loss: 1.4793, Train: 35.62%, Valid: 35.09%, Test: 35.59%
Epoch: 75, Loss: 1.4554, Train: 36.61%, Valid: 35.94%, Test: 36.14%
Epoch: 100, Loss: 1.4462, Train: 37.18%, Valid: 36.15%, Test: 36.20%
Epoch: 125, Loss: 1.4344, Train: 37.58%, Valid: 36.26%, Test: 36.14%
Epoch: 150, Loss: 1.4272, Train: 38.04%, Valid: 36.27%, Test: 36.51%
Epoch: 175, Loss: 1.4217, Train: 38.35%, Valid: 36.01%, Test: 36.42%
Epoch: 200, Loss: 1.4154, Train: 38.70%, Valid: 35.89%, Test: 36.36%
Epoch: 225, Loss: 1.4098, Train: 38.83%, Valid: 35.76%, Test: 36.13%
Epoch: 250, Loss: 1.4025, Train: 39.27%, Valid: 35.63%, Test: 36.11%
Epoch: 275, Loss: 1.4008, Train: 39.35%, Valid: 35.83%, Test: 36.02%
Epoch: 300, Loss: 1.3942, Train: 39.74%, Valid: 35.70%, Test: 35.86%
Epoch: 325, Loss: 1.3904, Train: 40.06%, Valid: 35.72%, Test: 36.17%
Epoch: 350, Loss: 1.3858, Train: 40.32%, Valid: 35.62%, Test: 36.05%
Epoch: 375, Loss: 1.3865, Train: 40.25%, Valid: 35.61%, Test: 35.93%
Epoch: 400, Loss: 1.3785, Train: 40.72%, Valid: 35.78%, Test: 36.11%
Epoch: 425, Loss: 1.3885, Train: 40.51%, Valid: 35.40%, Test: 35.83%
Epoch: 450, Loss: 1.3708, Train: 41.05%, Valid: 35.61%, Test: 35.97%
Epoch: 475, Loss: 1.4566, Train: 39.47%, Valid: 34.74%, Test: 34.92%
Epoch: 500, Loss: 1.3764, Train: 40.70%, Valid: 35.56%, Test: 35.95%
Epoch: 525, Loss: 1.3665, Train: 41.36%, Valid: 35.26%, Test: 35.58%
Epoch: 550, Loss: 1.3689, Train: 41.41%, Valid: 35.31%, Test: 35.67%
Epoch: 575, Loss: 1.3625, Train: 41.71%, Valid: 34.86%, Test: 35.36%
Epoch: 600, Loss: 1.3739, Train: 41.12%, Valid: 34.96%, Test: 35.21%
Epoch: 625, Loss: 1.3523, Train: 41.98%, Valid: 35.49%, Test: 35.93%
Epoch: 650, Loss: 1.3501, Train: 42.26%, Valid: 35.57%, Test: 35.95%
Epoch: 675, Loss: 1.3546, Train: 42.19%, Valid: 35.74%, Test: 36.05%
Epoch: 700, Loss: 1.3427, Train: 42.91%, Valid: 35.67%, Test: 36.01%
Epoch: 725, Loss: 1.3437, Train: 42.50%, Valid: 35.89%, Test: 36.27%
Epoch: 750, Loss: 1.3579, Train: 42.04%, Valid: 36.26%, Test: 36.71%
Epoch: 775, Loss: 1.3366, Train: 43.40%, Valid: 35.98%, Test: 36.39%
Epoch: 800, Loss: 1.3279, Train: 43.85%, Valid: 36.64%, Test: 37.07%
Epoch: 825, Loss: 1.3475, Train: 42.76%, Valid: 36.31%, Test: 36.57%
Epoch: 850, Loss: 1.3264, Train: 43.89%, Valid: 36.86%, Test: 37.54%
Epoch: 875, Loss: 1.3005, Train: 45.24%, Valid: 37.20%, Test: 37.83%
Epoch: 900, Loss: 1.4412, Train: 37.74%, Valid: 36.14%, Test: 36.58%
Epoch: 925, Loss: 1.3842, Train: 40.81%, Valid: 38.78%, Test: 39.10%
Epoch: 950, Loss: 1.3582, Train: 42.34%, Valid: 39.37%, Test: 39.77%
Epoch: 975, Loss: 1.3564, Train: 41.92%, Valid: 39.03%, Test: 39.29%
Run 01:
Highest Train: 45.35
Highest Valid: 39.89
  Final Train: 43.67
   Final Test: 40.32
All runs:
Highest Train: 45.35, nan
Highest Valid: 39.89, nan
  Final Train: 43.67, nan
   Final Test: 40.32, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6198, Train: 24.75%, Valid: 24.58%, Test: 25.36%
Epoch: 25, Loss: 1.4917, Train: 34.69%, Valid: 34.46%, Test: 34.75%
Epoch: 50, Loss: 1.4574, Train: 36.01%, Valid: 35.57%, Test: 35.94%
Epoch: 75, Loss: 1.4112, Train: 38.27%, Valid: 37.87%, Test: 37.87%
Epoch: 100, Loss: 1.3849, Train: 39.47%, Valid: 38.88%, Test: 38.93%
Epoch: 125, Loss: 1.3691, Train: 40.02%, Valid: 39.07%, Test: 39.37%
Epoch: 150, Loss: 1.3385, Train: 41.72%, Valid: 40.71%, Test: 40.82%
Epoch: 175, Loss: 1.3743, Train: 40.11%, Valid: 39.01%, Test: 39.27%
Epoch: 200, Loss: 1.3248, Train: 42.23%, Valid: 41.29%, Test: 41.38%
Epoch: 225, Loss: 1.3057, Train: 43.02%, Valid: 41.88%, Test: 42.15%
Epoch: 250, Loss: 1.2959, Train: 43.75%, Valid: 42.53%, Test: 42.61%
Epoch: 275, Loss: 1.3024, Train: 43.41%, Valid: 42.10%, Test: 42.24%
Epoch: 300, Loss: 1.2710, Train: 44.99%, Valid: 43.53%, Test: 43.68%
Epoch: 325, Loss: 1.2574, Train: 45.22%, Valid: 43.85%, Test: 44.04%
Epoch: 350, Loss: 1.2452, Train: 46.09%, Valid: 44.57%, Test: 44.64%
Epoch: 375, Loss: 1.2381, Train: 46.68%, Valid: 45.05%, Test: 45.19%
Epoch: 400, Loss: 1.2323, Train: 46.91%, Valid: 45.25%, Test: 45.33%
Epoch: 425, Loss: 1.2239, Train: 46.95%, Valid: 45.15%, Test: 45.26%
Epoch: 450, Loss: 1.2135, Train: 47.64%, Valid: 45.70%, Test: 45.88%
Epoch: 475, Loss: 1.2301, Train: 46.95%, Valid: 45.30%, Test: 45.25%
Epoch: 500, Loss: 1.2059, Train: 47.60%, Valid: 45.67%, Test: 45.78%
Epoch: 525, Loss: 1.2040, Train: 48.07%, Valid: 46.01%, Test: 46.10%
Epoch: 550, Loss: 1.1968, Train: 48.15%, Valid: 46.08%, Test: 46.21%
Epoch: 575, Loss: 1.1954, Train: 48.24%, Valid: 46.19%, Test: 46.37%
Epoch: 600, Loss: 1.2002, Train: 48.58%, Valid: 46.28%, Test: 46.31%
Epoch: 625, Loss: 1.1833, Train: 48.66%, Valid: 46.49%, Test: 46.47%
Epoch: 650, Loss: 1.1911, Train: 49.00%, Valid: 46.65%, Test: 46.76%
Epoch: 675, Loss: 1.1844, Train: 48.94%, Valid: 46.58%, Test: 46.73%
Epoch: 700, Loss: 1.1810, Train: 48.93%, Valid: 46.73%, Test: 46.89%
Epoch: 725, Loss: 1.1841, Train: 49.33%, Valid: 46.81%, Test: 47.02%
Epoch: 750, Loss: 1.1736, Train: 49.40%, Valid: 46.96%, Test: 46.98%
Epoch: 775, Loss: 1.1715, Train: 49.79%, Valid: 47.06%, Test: 47.13%
Epoch: 800, Loss: 1.1859, Train: 48.48%, Valid: 46.26%, Test: 46.48%
Epoch: 825, Loss: 1.2077, Train: 47.96%, Valid: 45.78%, Test: 45.94%
Epoch: 850, Loss: 1.1717, Train: 49.51%, Valid: 47.11%, Test: 47.07%
Epoch: 875, Loss: 1.1633, Train: 50.05%, Valid: 47.48%, Test: 47.34%
Epoch: 900, Loss: 1.1645, Train: 49.67%, Valid: 47.07%, Test: 47.04%
Epoch: 925, Loss: 1.1554, Train: 49.44%, Valid: 46.79%, Test: 46.67%
Epoch: 950, Loss: 1.1703, Train: 49.57%, Valid: 47.26%, Test: 47.56%
Epoch: 975, Loss: 1.1537, Train: 50.19%, Valid: 47.53%, Test: 47.61%
Run 01:
Highest Train: 50.34
Highest Valid: 47.71
  Final Train: 50.33
   Final Test: 47.73
All runs:
Highest Train: 50.34, nan
Highest Valid: 47.71, nan
  Final Train: 50.33, nan
   Final Test: 47.73, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.5606, Train: 28.36%, Valid: 28.29%, Test: 28.37%
Epoch: 25, Loss: 1.4561, Train: 35.21%, Valid: 34.85%, Test: 35.33%
Epoch: 50, Loss: 1.4185, Train: 35.06%, Valid: 34.50%, Test: 35.17%
Epoch: 75, Loss: 1.3923, Train: 37.87%, Valid: 37.33%, Test: 37.59%
Epoch: 100, Loss: 1.3716, Train: 38.60%, Valid: 38.27%, Test: 38.55%
Epoch: 125, Loss: 1.3646, Train: 39.11%, Valid: 38.67%, Test: 38.79%
Epoch: 150, Loss: 1.3605, Train: 39.96%, Valid: 39.36%, Test: 39.55%
Epoch: 175, Loss: 1.3440, Train: 40.60%, Valid: 40.20%, Test: 40.30%
Epoch: 200, Loss: 1.3796, Train: 39.61%, Valid: 39.23%, Test: 39.26%
Epoch: 225, Loss: 1.3585, Train: 39.30%, Valid: 38.92%, Test: 39.25%
Epoch: 250, Loss: 1.3377, Train: 40.70%, Valid: 40.21%, Test: 40.29%
Epoch: 275, Loss: 1.3334, Train: 41.08%, Valid: 40.76%, Test: 40.83%
Epoch: 300, Loss: 1.3204, Train: 41.46%, Valid: 40.98%, Test: 41.23%
Epoch: 325, Loss: 1.3350, Train: 40.86%, Valid: 40.15%, Test: 40.55%
Epoch: 350, Loss: 1.3177, Train: 42.08%, Valid: 41.65%, Test: 41.86%
Epoch: 375, Loss: 1.3118, Train: 42.37%, Valid: 41.93%, Test: 42.08%
Epoch: 400, Loss: 1.3056, Train: 42.49%, Valid: 41.99%, Test: 41.92%
Epoch: 425, Loss: 1.3066, Train: 42.39%, Valid: 41.87%, Test: 42.13%
Epoch: 450, Loss: 1.2996, Train: 42.70%, Valid: 42.14%, Test: 42.36%
Epoch: 475, Loss: 1.3059, Train: 42.59%, Valid: 41.88%, Test: 41.94%
Epoch: 500, Loss: 1.3004, Train: 42.99%, Valid: 42.36%, Test: 42.58%
Epoch: 525, Loss: 1.2897, Train: 43.13%, Valid: 42.39%, Test: 42.74%
Epoch: 550, Loss: 1.3158, Train: 42.11%, Valid: 41.46%, Test: 41.63%
Epoch: 575, Loss: 1.2947, Train: 42.99%, Valid: 42.42%, Test: 42.59%
Epoch: 600, Loss: 1.2838, Train: 43.69%, Valid: 42.99%, Test: 43.18%
Epoch: 625, Loss: 1.3099, Train: 42.17%, Valid: 41.62%, Test: 41.65%
Epoch: 650, Loss: 1.2929, Train: 42.91%, Valid: 42.37%, Test: 42.52%
Epoch: 675, Loss: 1.3035, Train: 43.05%, Valid: 42.44%, Test: 42.54%
Epoch: 700, Loss: 1.2855, Train: 43.38%, Valid: 42.66%, Test: 42.74%
Epoch: 725, Loss: 1.2842, Train: 43.58%, Valid: 42.90%, Test: 43.12%
Epoch: 750, Loss: 1.3207, Train: 41.18%, Valid: 40.42%, Test: 40.66%
Epoch: 775, Loss: 1.2865, Train: 43.78%, Valid: 43.03%, Test: 43.15%
Epoch: 800, Loss: 1.2752, Train: 44.05%, Valid: 43.29%, Test: 43.39%
Epoch: 825, Loss: 1.2992, Train: 42.67%, Valid: 41.88%, Test: 42.14%
Epoch: 850, Loss: 1.2803, Train: 43.81%, Valid: 43.07%, Test: 43.28%
Epoch: 875, Loss: 1.2706, Train: 44.38%, Valid: 43.39%, Test: 43.54%
Epoch: 900, Loss: 1.2891, Train: 42.92%, Valid: 41.93%, Test: 42.27%
Epoch: 925, Loss: 1.2731, Train: 44.12%, Valid: 43.30%, Test: 43.37%
Epoch: 950, Loss: 1.2669, Train: 44.49%, Valid: 43.64%, Test: 43.75%
Epoch: 975, Loss: 1.2797, Train: 43.79%, Valid: 43.13%, Test: 43.27%
Run 01:
Highest Train: 44.59
Highest Valid: 43.85
  Final Train: 44.59
   Final Test: 43.94
All runs:
Highest Train: 44.59, nan
Highest Valid: 43.85, nan
  Final Train: 44.59, nan
   Final Test: 43.94, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6119, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.5053, Train: 34.48%, Valid: 34.29%, Test: 34.49%
Epoch: 50, Loss: 1.4879, Train: 35.60%, Valid: 35.06%, Test: 35.30%
Epoch: 75, Loss: 1.4788, Train: 36.41%, Valid: 35.55%, Test: 35.91%
Epoch: 100, Loss: 1.4712, Train: 36.82%, Valid: 35.74%, Test: 36.23%
Epoch: 125, Loss: 1.4683, Train: 37.24%, Valid: 35.99%, Test: 36.33%
Epoch: 150, Loss: 1.4651, Train: 37.49%, Valid: 35.90%, Test: 36.25%
Epoch: 175, Loss: 1.4638, Train: 37.59%, Valid: 35.92%, Test: 36.34%
Epoch: 200, Loss: 1.4620, Train: 37.75%, Valid: 36.02%, Test: 36.37%
Epoch: 225, Loss: 1.4601, Train: 38.01%, Valid: 36.06%, Test: 36.52%
Epoch: 250, Loss: 1.4577, Train: 38.04%, Valid: 36.12%, Test: 36.50%
Epoch: 275, Loss: 1.4569, Train: 38.17%, Valid: 36.11%, Test: 36.38%
Epoch: 300, Loss: 1.4561, Train: 38.30%, Valid: 36.02%, Test: 36.56%
Epoch: 325, Loss: 1.4517, Train: 38.46%, Valid: 36.43%, Test: 36.62%
Epoch: 350, Loss: 1.4478, Train: 38.45%, Valid: 36.33%, Test: 36.59%
Epoch: 375, Loss: 1.4455, Train: 38.67%, Valid: 36.51%, Test: 36.95%
Epoch: 400, Loss: 1.4442, Train: 38.81%, Valid: 36.65%, Test: 36.99%
Epoch: 425, Loss: 1.4393, Train: 38.69%, Valid: 36.50%, Test: 36.85%
Epoch: 450, Loss: 1.4369, Train: 38.93%, Valid: 36.78%, Test: 37.15%
Epoch: 475, Loss: 1.4314, Train: 38.83%, Valid: 36.73%, Test: 37.05%
Epoch: 500, Loss: 1.4297, Train: 39.65%, Valid: 37.33%, Test: 37.89%
Epoch: 525, Loss: 1.4264, Train: 38.98%, Valid: 36.83%, Test: 37.22%
Epoch: 550, Loss: 1.4287, Train: 39.43%, Valid: 37.44%, Test: 37.86%
Epoch: 575, Loss: 1.4285, Train: 39.74%, Valid: 37.76%, Test: 38.13%
Epoch: 600, Loss: 1.4156, Train: 39.89%, Valid: 37.77%, Test: 38.13%
Epoch: 625, Loss: 1.4087, Train: 40.80%, Valid: 38.70%, Test: 39.00%
Epoch: 650, Loss: 1.4287, Train: 40.42%, Valid: 38.50%, Test: 38.81%
Epoch: 675, Loss: 1.4098, Train: 41.11%, Valid: 39.19%, Test: 39.25%
Epoch: 700, Loss: 1.4204, Train: 40.69%, Valid: 39.29%, Test: 39.40%
Epoch: 725, Loss: 1.3981, Train: 41.55%, Valid: 40.13%, Test: 40.17%
Epoch: 750, Loss: 1.3963, Train: 42.03%, Valid: 40.61%, Test: 40.83%
Epoch: 775, Loss: 1.3960, Train: 41.97%, Valid: 40.45%, Test: 40.55%
Epoch: 800, Loss: 1.4146, Train: 42.60%, Valid: 41.03%, Test: 41.28%
Epoch: 825, Loss: 1.3937, Train: 41.58%, Valid: 40.33%, Test: 40.60%
Epoch: 850, Loss: 1.3876, Train: 42.51%, Valid: 41.28%, Test: 41.30%
Epoch: 875, Loss: 1.3773, Train: 43.10%, Valid: 41.84%, Test: 42.10%
Epoch: 900, Loss: 1.3874, Train: 42.46%, Valid: 41.01%, Test: 41.27%
Epoch: 925, Loss: 1.3775, Train: 43.18%, Valid: 41.91%, Test: 42.22%
Epoch: 950, Loss: 1.3763, Train: 43.86%, Valid: 42.36%, Test: 42.75%
Epoch: 975, Loss: 1.3704, Train: 43.11%, Valid: 42.30%, Test: 42.36%
Run 01:
Highest Train: 44.28
Highest Valid: 42.93
  Final Train: 44.15
   Final Test: 43.16
All runs:
Highest Train: 44.28, nan
Highest Valid: 42.93, nan
  Final Train: 44.15, nan
   Final Test: 43.16, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6105, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.5040, Train: 34.59%, Valid: 34.46%, Test: 34.45%
Epoch: 50, Loss: 1.4870, Train: 35.65%, Valid: 35.16%, Test: 35.51%
Epoch: 75, Loss: 1.4520, Train: 37.46%, Valid: 36.42%, Test: 36.92%
Epoch: 100, Loss: 1.4204, Train: 38.32%, Valid: 37.40%, Test: 37.78%
Epoch: 125, Loss: 1.3958, Train: 39.66%, Valid: 38.76%, Test: 39.02%
Epoch: 150, Loss: 1.3819, Train: 39.54%, Valid: 38.99%, Test: 39.09%
Epoch: 175, Loss: 1.3795, Train: 39.33%, Valid: 38.65%, Test: 38.72%
Epoch: 200, Loss: 1.3699, Train: 39.94%, Valid: 39.07%, Test: 39.28%
Epoch: 225, Loss: 1.3762, Train: 38.64%, Valid: 38.02%, Test: 38.08%
Epoch: 250, Loss: 1.3557, Train: 40.35%, Valid: 39.72%, Test: 39.94%
Epoch: 275, Loss: 1.3813, Train: 41.46%, Valid: 40.75%, Test: 40.92%
Epoch: 300, Loss: 1.3533, Train: 41.77%, Valid: 41.11%, Test: 41.42%
Epoch: 325, Loss: 1.3364, Train: 42.10%, Valid: 41.30%, Test: 41.46%
Epoch: 350, Loss: 1.3489, Train: 42.69%, Valid: 42.18%, Test: 42.41%
Epoch: 375, Loss: 1.3505, Train: 40.54%, Valid: 39.98%, Test: 40.59%
Epoch: 400, Loss: 1.3678, Train: 33.72%, Valid: 33.39%, Test: 34.10%
Epoch: 425, Loss: 1.3380, Train: 42.70%, Valid: 42.26%, Test: 42.59%
Epoch: 450, Loss: 1.3666, Train: 40.81%, Valid: 40.30%, Test: 40.42%
Epoch: 475, Loss: 1.3355, Train: 41.99%, Valid: 41.59%, Test: 41.87%
Epoch: 500, Loss: 1.3823, Train: 42.47%, Valid: 42.03%, Test: 41.97%
Epoch: 525, Loss: 1.3404, Train: 43.38%, Valid: 42.78%, Test: 43.20%
Epoch: 550, Loss: 1.3156, Train: 42.51%, Valid: 42.08%, Test: 42.37%
Epoch: 575, Loss: 1.3277, Train: 37.29%, Valid: 36.95%, Test: 37.21%
Epoch: 600, Loss: 1.3224, Train: 43.89%, Valid: 43.19%, Test: 43.48%
Epoch: 625, Loss: 1.3306, Train: 43.10%, Valid: 42.84%, Test: 42.74%
Epoch: 650, Loss: 1.3234, Train: 43.68%, Valid: 43.16%, Test: 43.16%
Epoch: 675, Loss: 1.3277, Train: 42.72%, Valid: 42.42%, Test: 42.52%
Epoch: 700, Loss: 1.3384, Train: 43.83%, Valid: 43.38%, Test: 43.55%
Epoch: 725, Loss: 1.3264, Train: 42.58%, Valid: 42.07%, Test: 42.27%
Epoch: 750, Loss: 1.3227, Train: 44.21%, Valid: 43.48%, Test: 43.80%
Epoch: 775, Loss: 1.3435, Train: 44.22%, Valid: 43.43%, Test: 43.65%
Epoch: 800, Loss: 1.3183, Train: 40.94%, Valid: 40.89%, Test: 40.95%
Epoch: 825, Loss: 1.3049, Train: 40.73%, Valid: 40.19%, Test: 40.62%
Epoch: 850, Loss: 1.3092, Train: 41.79%, Valid: 41.20%, Test: 41.34%
Epoch: 875, Loss: 1.3191, Train: 36.71%, Valid: 36.64%, Test: 36.80%
Epoch: 900, Loss: 1.3487, Train: 43.44%, Valid: 42.88%, Test: 42.86%
Epoch: 925, Loss: 1.3474, Train: 43.10%, Valid: 42.67%, Test: 42.85%
Epoch: 950, Loss: 1.3101, Train: 40.51%, Valid: 40.10%, Test: 40.51%
Epoch: 975, Loss: 1.3205, Train: 40.05%, Valid: 39.90%, Test: 40.21%
Run 01:
Highest Train: 44.98
Highest Valid: 44.24
  Final Train: 44.78
   Final Test: 44.40
All runs:
Highest Train: 44.98, nan
Highest Valid: 44.24, nan
  Final Train: 44.78, nan
   Final Test: 44.40, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.7110, Train: 17.58%, Valid: 17.79%, Test: 17.32%
Epoch: 25, Loss: 1.4986, Train: 31.33%, Valid: 31.04%, Test: 31.83%
Epoch: 50, Loss: 1.4788, Train: 30.49%, Valid: 30.09%, Test: 30.78%
Epoch: 75, Loss: 1.4650, Train: 31.37%, Valid: 30.81%, Test: 31.92%
Epoch: 100, Loss: 1.4524, Train: 29.95%, Valid: 29.49%, Test: 30.40%
Epoch: 125, Loss: 1.4355, Train: 36.41%, Valid: 35.91%, Test: 36.39%
Epoch: 150, Loss: 1.4213, Train: 37.26%, Valid: 36.98%, Test: 37.15%
Epoch: 175, Loss: 1.3986, Train: 35.81%, Valid: 35.24%, Test: 35.52%
Epoch: 200, Loss: 1.3968, Train: 37.97%, Valid: 37.39%, Test: 37.68%
Epoch: 225, Loss: 1.3828, Train: 36.32%, Valid: 36.07%, Test: 36.17%
Epoch: 250, Loss: 1.4112, Train: 35.00%, Valid: 34.58%, Test: 34.79%
Epoch: 275, Loss: 1.3784, Train: 36.75%, Valid: 36.58%, Test: 36.57%
Epoch: 300, Loss: 1.3736, Train: 39.94%, Valid: 39.74%, Test: 39.79%
Epoch: 325, Loss: 1.3693, Train: 38.22%, Valid: 37.64%, Test: 37.86%
Epoch: 350, Loss: 1.3648, Train: 39.00%, Valid: 38.61%, Test: 38.70%
Epoch: 375, Loss: 1.3614, Train: 39.97%, Valid: 39.71%, Test: 39.86%
Epoch: 400, Loss: 1.3730, Train: 39.99%, Valid: 39.51%, Test: 39.65%
Epoch: 425, Loss: 1.3603, Train: 38.37%, Valid: 37.86%, Test: 38.16%
Epoch: 450, Loss: 1.3540, Train: 39.07%, Valid: 38.58%, Test: 38.64%
Epoch: 475, Loss: 1.3553, Train: 39.88%, Valid: 39.28%, Test: 39.56%
Epoch: 500, Loss: 1.3656, Train: 37.18%, Valid: 36.71%, Test: 37.10%
Epoch: 525, Loss: 1.3826, Train: 38.79%, Valid: 38.32%, Test: 38.47%
Epoch: 550, Loss: 1.3603, Train: 38.57%, Valid: 38.06%, Test: 38.26%
Epoch: 575, Loss: 1.3509, Train: 39.16%, Valid: 38.77%, Test: 38.92%
Epoch: 600, Loss: 1.3796, Train: 37.80%, Valid: 37.43%, Test: 37.45%
Epoch: 625, Loss: 1.3696, Train: 34.43%, Valid: 34.09%, Test: 34.01%
Epoch: 650, Loss: 1.3695, Train: 38.88%, Valid: 38.16%, Test: 38.52%
Epoch: 675, Loss: 1.3702, Train: 37.81%, Valid: 37.20%, Test: 37.38%
Epoch: 700, Loss: 1.3536, Train: 39.18%, Valid: 38.55%, Test: 38.64%
Epoch: 725, Loss: 1.3600, Train: 38.26%, Valid: 38.04%, Test: 38.13%
Epoch: 750, Loss: 1.3811, Train: 39.59%, Valid: 39.34%, Test: 39.41%
Epoch: 775, Loss: 1.3699, Train: 40.88%, Valid: 40.41%, Test: 40.75%
Epoch: 800, Loss: 1.3693, Train: 39.61%, Valid: 38.93%, Test: 39.39%
Epoch: 825, Loss: 1.3639, Train: 37.95%, Valid: 37.42%, Test: 37.72%
Epoch: 850, Loss: 1.3555, Train: 37.98%, Valid: 37.46%, Test: 37.85%
Epoch: 875, Loss: 1.3545, Train: 37.87%, Valid: 37.37%, Test: 37.63%
Epoch: 900, Loss: 1.3544, Train: 39.80%, Valid: 39.33%, Test: 39.64%
Epoch: 925, Loss: 1.3588, Train: 38.73%, Valid: 38.19%, Test: 38.42%
Epoch: 950, Loss: 1.3581, Train: 36.72%, Valid: 36.25%, Test: 36.62%
Epoch: 975, Loss: 1.3655, Train: 40.59%, Valid: 40.18%, Test: 40.43%
Run 01:
Highest Train: 41.59
Highest Valid: 41.11
  Final Train: 41.54
   Final Test: 41.07
All runs:
Highest Train: 41.59, nan
Highest Valid: 41.11, nan
  Final Train: 41.54, nan
   Final Test: 41.07, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6229, Train: 28.72%, Valid: 28.53%, Test: 28.82%
Epoch: 25, Loss: 1.5116, Train: 34.28%, Valid: 34.04%, Test: 34.26%
Epoch: 50, Loss: 1.4918, Train: 35.42%, Valid: 34.84%, Test: 35.19%
Epoch: 75, Loss: 1.4830, Train: 36.08%, Valid: 35.32%, Test: 35.52%
Epoch: 100, Loss: 1.4793, Train: 36.54%, Valid: 35.47%, Test: 35.86%
Epoch: 125, Loss: 1.4760, Train: 36.81%, Valid: 35.60%, Test: 35.91%
Epoch: 150, Loss: 1.4730, Train: 36.90%, Valid: 35.62%, Test: 35.89%
Epoch: 175, Loss: 1.4723, Train: 36.93%, Valid: 35.61%, Test: 36.01%
Epoch: 200, Loss: 1.4700, Train: 37.14%, Valid: 35.61%, Test: 36.09%
Epoch: 225, Loss: 1.4672, Train: 37.23%, Valid: 35.64%, Test: 36.07%
Epoch: 250, Loss: 1.4675, Train: 37.36%, Valid: 35.63%, Test: 36.20%
Epoch: 275, Loss: 1.4638, Train: 37.47%, Valid: 35.63%, Test: 36.23%
Epoch: 300, Loss: 1.4593, Train: 37.68%, Valid: 35.74%, Test: 36.33%
Epoch: 325, Loss: 1.4553, Train: 38.03%, Valid: 36.08%, Test: 36.58%
Epoch: 350, Loss: 1.4510, Train: 37.75%, Valid: 35.75%, Test: 36.23%
Epoch: 375, Loss: 1.4458, Train: 38.06%, Valid: 36.17%, Test: 36.79%
Epoch: 400, Loss: 1.4392, Train: 38.38%, Valid: 36.53%, Test: 36.89%
Epoch: 425, Loss: 1.4480, Train: 38.06%, Valid: 36.32%, Test: 36.84%
Epoch: 450, Loss: 1.4375, Train: 39.09%, Valid: 37.46%, Test: 37.86%
Epoch: 475, Loss: 1.4344, Train: 39.07%, Valid: 37.37%, Test: 37.91%
Epoch: 500, Loss: 1.4285, Train: 38.58%, Valid: 37.02%, Test: 37.35%
Epoch: 525, Loss: 1.4342, Train: 38.99%, Valid: 37.46%, Test: 37.71%
Epoch: 550, Loss: 1.4292, Train: 39.63%, Valid: 38.14%, Test: 38.32%
Epoch: 575, Loss: 1.4272, Train: 39.74%, Valid: 37.92%, Test: 38.34%
Epoch: 600, Loss: 1.4429, Train: 39.75%, Valid: 38.36%, Test: 38.69%
Epoch: 625, Loss: 1.4288, Train: 39.53%, Valid: 38.16%, Test: 38.42%
Epoch: 650, Loss: 1.4275, Train: 39.46%, Valid: 38.13%, Test: 38.31%
Epoch: 675, Loss: 1.4272, Train: 40.14%, Valid: 38.67%, Test: 38.86%
Epoch: 700, Loss: 1.4248, Train: 39.86%, Valid: 38.34%, Test: 38.76%
Epoch: 725, Loss: 1.4344, Train: 40.45%, Valid: 39.13%, Test: 39.42%
Epoch: 750, Loss: 1.4187, Train: 40.13%, Valid: 38.81%, Test: 38.95%
Epoch: 775, Loss: 1.4186, Train: 40.65%, Valid: 39.13%, Test: 39.43%
Epoch: 800, Loss: 1.4159, Train: 40.71%, Valid: 39.18%, Test: 39.43%
Epoch: 825, Loss: 1.4210, Train: 40.85%, Valid: 39.74%, Test: 40.06%
Epoch: 850, Loss: 1.4142, Train: 40.69%, Valid: 39.22%, Test: 39.62%
Epoch: 875, Loss: 1.4104, Train: 41.29%, Valid: 39.66%, Test: 40.25%
Epoch: 900, Loss: 1.4171, Train: 41.05%, Valid: 40.01%, Test: 40.31%
Epoch: 925, Loss: 1.4102, Train: 40.74%, Valid: 39.39%, Test: 39.58%
Epoch: 950, Loss: 1.4058, Train: 40.68%, Valid: 39.16%, Test: 39.41%
Epoch: 975, Loss: 1.4215, Train: 40.99%, Valid: 39.36%, Test: 40.12%
Run 01:
Highest Train: 41.54
Highest Valid: 40.26
  Final Train: 41.54
   Final Test: 40.27
All runs:
Highest Train: 41.54, nan
Highest Valid: 40.26, nan
  Final Train: 41.54, nan
   Final Test: 40.27, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6077, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4994, Train: 34.70%, Valid: 34.52%, Test: 34.71%
Epoch: 50, Loss: 1.4806, Train: 35.99%, Valid: 35.36%, Test: 35.71%
Epoch: 75, Loss: 1.4480, Train: 37.32%, Valid: 36.43%, Test: 36.79%
Epoch: 100, Loss: 1.4182, Train: 35.20%, Valid: 34.18%, Test: 34.54%
Epoch: 125, Loss: 1.4064, Train: 38.54%, Valid: 37.19%, Test: 37.76%
Epoch: 150, Loss: 1.3994, Train: 39.53%, Valid: 38.43%, Test: 38.76%
Epoch: 175, Loss: 1.3862, Train: 39.19%, Valid: 38.32%, Test: 38.74%
Epoch: 200, Loss: 1.3855, Train: 40.25%, Valid: 39.44%, Test: 39.75%
Epoch: 225, Loss: 1.3595, Train: 41.93%, Valid: 41.12%, Test: 41.30%
Epoch: 250, Loss: 1.3565, Train: 41.04%, Valid: 40.47%, Test: 40.60%
Epoch: 275, Loss: 1.3520, Train: 42.50%, Valid: 41.79%, Test: 42.14%
Epoch: 300, Loss: 1.3464, Train: 41.82%, Valid: 41.48%, Test: 41.50%
Epoch: 325, Loss: 1.3593, Train: 41.39%, Valid: 40.84%, Test: 41.19%
Epoch: 350, Loss: 1.4070, Train: 29.42%, Valid: 29.40%, Test: 29.68%
Epoch: 375, Loss: 1.3857, Train: 29.31%, Valid: 28.95%, Test: 29.59%
Epoch: 400, Loss: 1.3657, Train: 36.81%, Valid: 36.48%, Test: 36.69%
Epoch: 425, Loss: 1.3575, Train: 38.17%, Valid: 37.77%, Test: 38.09%
Epoch: 450, Loss: 1.3485, Train: 39.71%, Valid: 39.36%, Test: 39.37%
Epoch: 475, Loss: 1.3553, Train: 40.55%, Valid: 40.00%, Test: 40.06%
Epoch: 500, Loss: 1.3669, Train: 40.72%, Valid: 40.06%, Test: 40.09%
Epoch: 525, Loss: 1.3531, Train: 41.38%, Valid: 40.85%, Test: 40.68%
Epoch: 550, Loss: 1.3351, Train: 41.68%, Valid: 41.20%, Test: 41.23%
Epoch: 575, Loss: 1.3470, Train: 41.43%, Valid: 41.11%, Test: 41.15%
Epoch: 600, Loss: 1.3344, Train: 42.50%, Valid: 42.14%, Test: 42.10%
Epoch: 625, Loss: 1.3293, Train: 41.49%, Valid: 41.02%, Test: 40.98%
Epoch: 650, Loss: 1.3459, Train: 42.29%, Valid: 41.55%, Test: 41.68%
Epoch: 675, Loss: 1.3438, Train: 42.65%, Valid: 42.15%, Test: 42.46%
Epoch: 700, Loss: 1.3351, Train: 43.33%, Valid: 42.62%, Test: 42.75%
Epoch: 725, Loss: 1.3444, Train: 42.20%, Valid: 41.40%, Test: 41.53%
Epoch: 750, Loss: 1.3349, Train: 41.06%, Valid: 40.34%, Test: 40.50%
Epoch: 775, Loss: 1.3256, Train: 42.12%, Valid: 41.55%, Test: 41.72%
Epoch: 800, Loss: 1.3231, Train: 38.66%, Valid: 38.22%, Test: 38.62%
Epoch: 825, Loss: 1.3545, Train: 39.64%, Valid: 39.42%, Test: 39.34%
Epoch: 850, Loss: 1.3364, Train: 40.28%, Valid: 39.92%, Test: 40.16%
Epoch: 875, Loss: 1.3184, Train: 41.80%, Valid: 41.36%, Test: 41.34%
Epoch: 900, Loss: 1.3273, Train: 43.32%, Valid: 42.63%, Test: 42.84%
Epoch: 925, Loss: 1.3233, Train: 42.52%, Valid: 41.93%, Test: 42.02%
Epoch: 950, Loss: 1.3296, Train: 42.72%, Valid: 42.14%, Test: 42.25%
Epoch: 975, Loss: 1.3272, Train: 43.05%, Valid: 42.44%, Test: 42.44%
Run 01:
Highest Train: 43.74
Highest Valid: 42.94
  Final Train: 43.59
   Final Test: 43.17
All runs:
Highest Train: 43.74, nan
Highest Valid: 42.94, nan
  Final Train: 43.59, nan
   Final Test: 43.17, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.5809, Train: 28.53%, Valid: 28.35%, Test: 28.66%
Epoch: 25, Loss: 1.4935, Train: 28.84%, Valid: 28.69%, Test: 28.98%
Epoch: 50, Loss: 1.4712, Train: 31.85%, Valid: 31.67%, Test: 31.99%
Epoch: 75, Loss: 1.4503, Train: 35.31%, Valid: 35.15%, Test: 35.26%
Epoch: 100, Loss: 1.4198, Train: 35.27%, Valid: 34.83%, Test: 35.22%
Epoch: 125, Loss: 1.3981, Train: 38.02%, Valid: 37.78%, Test: 37.84%
Epoch: 150, Loss: 1.3879, Train: 38.26%, Valid: 38.12%, Test: 38.06%
Epoch: 175, Loss: 1.3770, Train: 38.96%, Valid: 38.74%, Test: 38.77%
Epoch: 200, Loss: 1.3685, Train: 39.34%, Valid: 39.10%, Test: 39.12%
Epoch: 225, Loss: 1.3697, Train: 39.69%, Valid: 39.36%, Test: 39.62%
Epoch: 250, Loss: 1.3683, Train: 40.13%, Valid: 39.83%, Test: 39.87%
Epoch: 275, Loss: 1.3723, Train: 39.81%, Valid: 39.55%, Test: 39.45%
Epoch: 300, Loss: 1.3695, Train: 39.93%, Valid: 39.74%, Test: 39.73%
Epoch: 325, Loss: 1.3678, Train: 40.64%, Valid: 40.39%, Test: 40.40%
Epoch: 350, Loss: 1.3642, Train: 40.61%, Valid: 40.43%, Test: 40.23%
Epoch: 375, Loss: 1.3697, Train: 40.47%, Valid: 40.08%, Test: 40.15%
Epoch: 400, Loss: 1.3643, Train: 40.66%, Valid: 40.38%, Test: 40.37%
Epoch: 425, Loss: 1.3607, Train: 40.87%, Valid: 40.59%, Test: 40.72%
Epoch: 450, Loss: 1.3503, Train: 41.27%, Valid: 40.86%, Test: 41.06%
Epoch: 475, Loss: 1.3564, Train: 41.16%, Valid: 40.57%, Test: 40.84%
Epoch: 500, Loss: 1.3513, Train: 41.05%, Valid: 40.47%, Test: 40.79%
Epoch: 525, Loss: 1.3517, Train: 41.35%, Valid: 40.94%, Test: 41.19%
Epoch: 550, Loss: 1.3548, Train: 41.65%, Valid: 41.13%, Test: 41.38%
Epoch: 575, Loss: 1.3627, Train: 41.04%, Valid: 40.60%, Test: 40.88%
Epoch: 600, Loss: 1.3743, Train: 40.13%, Valid: 39.67%, Test: 39.85%
Epoch: 625, Loss: 1.3721, Train: 40.46%, Valid: 40.19%, Test: 40.52%
Epoch: 650, Loss: 1.3747, Train: 41.03%, Valid: 40.80%, Test: 40.78%
Epoch: 675, Loss: 1.3613, Train: 40.86%, Valid: 40.46%, Test: 40.74%
Epoch: 700, Loss: 1.3544, Train: 40.87%, Valid: 40.46%, Test: 40.65%
Epoch: 725, Loss: 1.3462, Train: 41.21%, Valid: 40.79%, Test: 40.96%
Epoch: 750, Loss: 1.3488, Train: 41.40%, Valid: 40.97%, Test: 41.12%
Epoch: 775, Loss: 1.3519, Train: 41.04%, Valid: 40.53%, Test: 40.77%
Epoch: 800, Loss: 1.3535, Train: 40.80%, Valid: 40.40%, Test: 40.64%
Epoch: 825, Loss: 1.3466, Train: 40.53%, Valid: 40.06%, Test: 40.40%
Epoch: 850, Loss: 1.3420, Train: 40.52%, Valid: 40.06%, Test: 40.35%
Epoch: 875, Loss: 1.3652, Train: 39.86%, Valid: 39.60%, Test: 39.74%
Epoch: 900, Loss: 1.3565, Train: 39.96%, Valid: 39.51%, Test: 39.93%
Epoch: 925, Loss: 1.3471, Train: 39.92%, Valid: 39.47%, Test: 39.84%
Epoch: 950, Loss: 1.3470, Train: 40.89%, Valid: 40.54%, Test: 40.69%
Epoch: 975, Loss: 1.3392, Train: 39.76%, Valid: 39.25%, Test: 39.74%
Run 01:
Highest Train: 41.81
Highest Valid: 41.31
  Final Train: 41.79
   Final Test: 41.55
All runs:
Highest Train: 41.81, nan
Highest Valid: 41.31, nan
  Final Train: 41.79, nan
   Final Test: 41.55, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6031, Train: 28.71%, Valid: 28.53%, Test: 28.82%
Epoch: 25, Loss: 1.5111, Train: 37.84%, Valid: 37.52%, Test: 37.74%
Epoch: 50, Loss: 1.4380, Train: 40.28%, Valid: 39.72%, Test: 40.27%
Epoch: 75, Loss: 1.3946, Train: 41.29%, Valid: 40.58%, Test: 40.98%
Epoch: 100, Loss: 1.3725, Train: 41.91%, Valid: 41.28%, Test: 41.60%
Epoch: 125, Loss: 1.3436, Train: 42.77%, Valid: 41.74%, Test: 42.20%
Epoch: 150, Loss: 1.3200, Train: 43.66%, Valid: 42.39%, Test: 42.69%
Epoch: 175, Loss: 1.3045, Train: 44.15%, Valid: 42.65%, Test: 43.01%
Epoch: 200, Loss: 1.3392, Train: 42.77%, Valid: 41.28%, Test: 41.57%
Epoch: 225, Loss: 1.2825, Train: 45.14%, Valid: 43.07%, Test: 43.55%
Epoch: 250, Loss: 1.2757, Train: 45.82%, Valid: 43.72%, Test: 43.79%
Epoch: 275, Loss: 1.2547, Train: 46.36%, Valid: 43.76%, Test: 44.14%
Epoch: 300, Loss: 1.2593, Train: 46.24%, Valid: 43.81%, Test: 43.99%
Epoch: 325, Loss: 1.2378, Train: 47.13%, Valid: 44.16%, Test: 44.35%
Epoch: 350, Loss: 1.2309, Train: 47.58%, Valid: 44.20%, Test: 44.45%
Epoch: 375, Loss: 1.2385, Train: 47.02%, Valid: 44.00%, Test: 44.22%
Epoch: 400, Loss: 1.2122, Train: 48.41%, Valid: 44.63%, Test: 45.08%
Epoch: 425, Loss: 1.2142, Train: 48.69%, Valid: 44.64%, Test: 45.03%
Epoch: 450, Loss: 1.2070, Train: 48.88%, Valid: 44.88%, Test: 45.17%
Epoch: 475, Loss: 1.1911, Train: 49.48%, Valid: 45.01%, Test: 45.35%
Epoch: 500, Loss: 1.1908, Train: 49.46%, Valid: 45.11%, Test: 45.27%
Epoch: 525, Loss: 1.1950, Train: 49.28%, Valid: 44.71%, Test: 45.27%
Epoch: 550, Loss: 1.1737, Train: 50.23%, Valid: 45.39%, Test: 45.75%
Epoch: 575, Loss: 1.1872, Train: 49.70%, Valid: 44.95%, Test: 45.43%
Epoch: 600, Loss: 1.1658, Train: 50.55%, Valid: 45.52%, Test: 45.90%
Epoch: 625, Loss: 1.1667, Train: 50.62%, Valid: 45.40%, Test: 45.67%
Epoch: 650, Loss: 1.1636, Train: 50.75%, Valid: 45.20%, Test: 45.79%
Epoch: 675, Loss: 1.1597, Train: 50.72%, Valid: 45.32%, Test: 45.58%
Epoch: 700, Loss: 1.1528, Train: 51.05%, Valid: 45.69%, Test: 46.16%
Epoch: 725, Loss: 1.1531, Train: 50.97%, Valid: 45.53%, Test: 46.05%
Epoch: 750, Loss: 1.1586, Train: 50.50%, Valid: 44.96%, Test: 45.58%
Epoch: 775, Loss: 1.1640, Train: 50.58%, Valid: 45.45%, Test: 45.81%
Epoch: 800, Loss: 1.1438, Train: 51.55%, Valid: 45.89%, Test: 46.28%
Epoch: 825, Loss: 1.1404, Train: 51.63%, Valid: 45.97%, Test: 46.16%
Epoch: 850, Loss: 1.1403, Train: 51.75%, Valid: 45.93%, Test: 46.21%
Epoch: 875, Loss: 1.1406, Train: 51.42%, Valid: 45.86%, Test: 46.26%
Epoch: 900, Loss: 1.1357, Train: 51.87%, Valid: 46.09%, Test: 46.42%
Epoch: 925, Loss: 1.1385, Train: 51.87%, Valid: 45.88%, Test: 46.31%
Epoch: 950, Loss: 1.1316, Train: 51.81%, Valid: 45.75%, Test: 45.93%
Epoch: 975, Loss: 1.1313, Train: 52.21%, Valid: 46.31%, Test: 46.71%
Run 01:
Highest Train: 52.34
Highest Valid: 46.33
  Final Train: 52.24
   Final Test: 46.77
All runs:
Highest Train: 52.34, nan
Highest Valid: 46.33, nan
  Final Train: 52.24, nan
   Final Test: 46.77, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 3.5713, Train: 23.39%, Valid: 23.37%, Test: 23.05%
Epoch: 25, Loss: 1.4542, Train: 37.47%, Valid: 37.08%, Test: 37.43%
Epoch: 50, Loss: 1.4161, Train: 39.09%, Valid: 38.83%, Test: 38.93%
Epoch: 75, Loss: 1.3901, Train: 40.12%, Valid: 39.86%, Test: 40.05%
Epoch: 100, Loss: 1.3861, Train: 40.66%, Valid: 40.39%, Test: 40.69%
Epoch: 125, Loss: 1.3679, Train: 40.71%, Valid: 40.60%, Test: 40.67%
Epoch: 150, Loss: 1.3534, Train: 41.61%, Valid: 41.36%, Test: 41.23%
Epoch: 175, Loss: 1.3534, Train: 41.55%, Valid: 41.45%, Test: 41.47%
Epoch: 200, Loss: 1.3336, Train: 42.30%, Valid: 42.21%, Test: 42.17%
Epoch: 225, Loss: 1.3726, Train: 40.36%, Valid: 40.30%, Test: 40.48%
Epoch: 250, Loss: 1.3430, Train: 41.98%, Valid: 41.83%, Test: 41.86%
Epoch: 275, Loss: 1.3293, Train: 42.37%, Valid: 42.28%, Test: 42.22%
Epoch: 300, Loss: 1.3378, Train: 39.27%, Valid: 39.38%, Test: 39.43%
Epoch: 325, Loss: 1.3268, Train: 42.57%, Valid: 42.52%, Test: 42.42%
Epoch: 350, Loss: 1.3170, Train: 42.80%, Valid: 42.83%, Test: 42.59%
Epoch: 375, Loss: 1.3111, Train: 43.06%, Valid: 43.16%, Test: 42.92%
Epoch: 400, Loss: 1.4133, Train: 39.57%, Valid: 39.72%, Test: 39.61%
Epoch: 425, Loss: 1.4191, Train: 38.36%, Valid: 38.01%, Test: 38.15%
Epoch: 450, Loss: 1.3656, Train: 41.30%, Valid: 41.15%, Test: 41.22%
Epoch: 475, Loss: 1.3486, Train: 42.10%, Valid: 41.93%, Test: 42.05%
Epoch: 500, Loss: 1.3932, Train: 40.93%, Valid: 40.81%, Test: 40.84%
Epoch: 525, Loss: 1.3528, Train: 41.58%, Valid: 41.35%, Test: 41.39%
Epoch: 550, Loss: 1.3345, Train: 42.56%, Valid: 42.37%, Test: 42.52%
Epoch: 575, Loss: 1.3258, Train: 42.62%, Valid: 42.54%, Test: 42.57%
Epoch: 600, Loss: 1.3172, Train: 42.92%, Valid: 42.78%, Test: 42.82%
Epoch: 625, Loss: 1.3501, Train: 39.32%, Valid: 39.58%, Test: 39.67%
Epoch: 650, Loss: 1.3168, Train: 42.66%, Valid: 42.44%, Test: 42.49%
Epoch: 675, Loss: 1.3057, Train: 43.45%, Valid: 43.25%, Test: 43.18%
Epoch: 700, Loss: 1.2982, Train: 43.74%, Valid: 43.61%, Test: 43.38%
Epoch: 725, Loss: 1.2924, Train: 44.00%, Valid: 43.86%, Test: 43.62%
Epoch: 750, Loss: 1.3279, Train: 41.84%, Valid: 41.82%, Test: 42.01%
Epoch: 775, Loss: 1.2956, Train: 43.85%, Valid: 43.63%, Test: 43.49%
Epoch: 800, Loss: 1.2858, Train: 44.38%, Valid: 44.16%, Test: 43.95%
Epoch: 825, Loss: 1.2827, Train: 44.69%, Valid: 44.44%, Test: 44.14%
Epoch: 850, Loss: 1.2759, Train: 44.78%, Valid: 44.53%, Test: 44.48%
Epoch: 875, Loss: 1.2729, Train: 45.09%, Valid: 44.92%, Test: 44.72%
Epoch: 900, Loss: 1.2725, Train: 45.14%, Valid: 44.86%, Test: 44.69%
Epoch: 925, Loss: 1.2855, Train: 44.08%, Valid: 43.77%, Test: 43.83%
Epoch: 950, Loss: 1.2701, Train: 45.07%, Valid: 44.86%, Test: 44.79%
Epoch: 975, Loss: 1.2621, Train: 45.52%, Valid: 45.39%, Test: 45.25%
Run 01:
Highest Train: 45.67
Highest Valid: 45.49
  Final Train: 45.66
   Final Test: 45.42
All runs:
Highest Train: 45.67, nan
Highest Valid: 45.49, nan
  Final Train: 45.66, nan
   Final Test: 45.42, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 25.3518, Train: 28.68%, Valid: 28.83%, Test: 28.32%
Epoch: 25, Loss: 2.3207, Train: 28.11%, Valid: 27.75%, Test: 28.23%
Epoch: 50, Loss: 1.4534, Train: 27.42%, Valid: 27.15%, Test: 27.53%
Epoch: 75, Loss: 1.4319, Train: 28.38%, Valid: 28.23%, Test: 28.47%
Epoch: 100, Loss: 1.4258, Train: 28.39%, Valid: 28.23%, Test: 28.50%
Epoch: 125, Loss: 1.4223, Train: 28.39%, Valid: 28.23%, Test: 28.53%
Epoch: 150, Loss: 1.4193, Train: 28.45%, Valid: 28.27%, Test: 28.57%
Epoch: 175, Loss: 1.4164, Train: 28.53%, Valid: 28.33%, Test: 28.61%
Epoch: 200, Loss: 1.4136, Train: 28.60%, Valid: 28.41%, Test: 28.70%
Epoch: 225, Loss: 1.4109, Train: 28.74%, Valid: 28.51%, Test: 28.83%
Epoch: 250, Loss: 1.4083, Train: 28.87%, Valid: 28.65%, Test: 28.97%
Epoch: 275, Loss: 1.4058, Train: 29.01%, Valid: 28.75%, Test: 29.16%
Epoch: 300, Loss: 1.4032, Train: 29.16%, Valid: 28.96%, Test: 29.30%
Epoch: 325, Loss: 1.4006, Train: 29.31%, Valid: 29.14%, Test: 29.43%
Epoch: 350, Loss: 1.3979, Train: 29.50%, Valid: 29.40%, Test: 29.60%
Epoch: 375, Loss: 1.3953, Train: 29.72%, Valid: 29.68%, Test: 29.79%
Epoch: 400, Loss: 1.3927, Train: 29.88%, Valid: 29.86%, Test: 30.00%
Epoch: 425, Loss: 1.3902, Train: 30.15%, Valid: 30.11%, Test: 30.21%
Epoch: 450, Loss: 1.3877, Train: 30.37%, Valid: 30.28%, Test: 30.41%
Epoch: 475, Loss: 1.3854, Train: 30.55%, Valid: 30.42%, Test: 30.55%
Epoch: 500, Loss: 1.3832, Train: 30.70%, Valid: 30.49%, Test: 30.73%
Epoch: 525, Loss: 1.3811, Train: 30.86%, Valid: 30.65%, Test: 30.84%
Epoch: 550, Loss: 1.3792, Train: 30.99%, Valid: 30.75%, Test: 31.01%
Epoch: 575, Loss: 1.3773, Train: 31.12%, Valid: 30.88%, Test: 31.14%
Epoch: 600, Loss: 1.5579, Train: 29.21%, Valid: 28.85%, Test: 29.17%
Epoch: 625, Loss: 1.4099, Train: 29.51%, Valid: 29.51%, Test: 29.68%
Epoch: 650, Loss: 1.3996, Train: 29.86%, Valid: 29.83%, Test: 30.05%
Epoch: 675, Loss: 1.3931, Train: 29.98%, Valid: 29.99%, Test: 30.18%
Epoch: 700, Loss: 1.3893, Train: 30.19%, Valid: 30.26%, Test: 30.42%
Epoch: 725, Loss: 1.3863, Train: 30.51%, Valid: 30.58%, Test: 30.65%
Epoch: 750, Loss: 1.3837, Train: 30.68%, Valid: 30.69%, Test: 30.76%
Epoch: 775, Loss: 1.3814, Train: 30.86%, Valid: 30.89%, Test: 30.94%
Epoch: 800, Loss: 1.3792, Train: 31.00%, Valid: 30.93%, Test: 31.16%
Epoch: 825, Loss: 1.3773, Train: 31.13%, Valid: 31.05%, Test: 31.30%
Epoch: 850, Loss: 1.3755, Train: 31.27%, Valid: 31.14%, Test: 31.38%
Epoch: 875, Loss: 1.3737, Train: 31.34%, Valid: 31.23%, Test: 31.45%
Epoch: 900, Loss: 1.3721, Train: 31.45%, Valid: 31.33%, Test: 31.57%
Epoch: 925, Loss: 1.3706, Train: 31.59%, Valid: 31.46%, Test: 31.63%
Epoch: 950, Loss: 1.3775, Train: 31.39%, Valid: 31.19%, Test: 31.39%
Epoch: 975, Loss: 1.4002, Train: 29.86%, Valid: 29.50%, Test: 29.82%
Run 01:
Highest Train: 37.61
Highest Valid: 37.38
  Final Train: 37.61
   Final Test: 37.52
All runs:
Highest Train: 37.61, nan
Highest Valid: 37.38, nan
  Final Train: 37.61, nan
   Final Test: 37.52, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6015, Train: 28.70%, Valid: 28.51%, Test: 28.81%
Epoch: 25, Loss: 1.5198, Train: 37.35%, Valid: 36.96%, Test: 37.40%
Epoch: 50, Loss: 1.4547, Train: 39.39%, Valid: 39.14%, Test: 39.20%
Epoch: 75, Loss: 1.4080, Train: 40.68%, Valid: 40.26%, Test: 40.67%
Epoch: 100, Loss: 1.3805, Train: 41.20%, Valid: 40.47%, Test: 40.91%
Epoch: 125, Loss: 1.3609, Train: 41.97%, Valid: 41.36%, Test: 41.63%
Epoch: 150, Loss: 1.3627, Train: 41.34%, Valid: 40.73%, Test: 40.90%
Epoch: 175, Loss: 1.3286, Train: 43.22%, Valid: 42.22%, Test: 42.38%
Epoch: 200, Loss: 1.3140, Train: 43.48%, Valid: 42.42%, Test: 42.61%
Epoch: 225, Loss: 1.3415, Train: 42.75%, Valid: 41.91%, Test: 42.04%
Epoch: 250, Loss: 1.3075, Train: 43.97%, Valid: 42.78%, Test: 42.94%
Epoch: 275, Loss: 1.2903, Train: 44.72%, Valid: 43.17%, Test: 43.14%
Epoch: 300, Loss: 1.2754, Train: 45.41%, Valid: 43.61%, Test: 43.60%
Epoch: 325, Loss: 1.2658, Train: 45.13%, Valid: 43.35%, Test: 43.18%
Epoch: 350, Loss: 1.2539, Train: 46.12%, Valid: 44.07%, Test: 44.03%
Epoch: 375, Loss: 1.2469, Train: 46.54%, Valid: 43.96%, Test: 43.94%
Epoch: 400, Loss: 1.2475, Train: 46.65%, Valid: 44.04%, Test: 44.20%
Epoch: 425, Loss: 1.2352, Train: 47.01%, Valid: 44.33%, Test: 44.45%
Epoch: 450, Loss: 1.2254, Train: 47.34%, Valid: 44.38%, Test: 44.36%
Epoch: 475, Loss: 1.2246, Train: 47.45%, Valid: 44.71%, Test: 44.74%
Epoch: 500, Loss: 1.2151, Train: 47.75%, Valid: 44.69%, Test: 44.64%
Epoch: 525, Loss: 1.2139, Train: 47.84%, Valid: 44.61%, Test: 45.00%
Epoch: 550, Loss: 1.2037, Train: 48.53%, Valid: 44.85%, Test: 44.85%
Epoch: 575, Loss: 1.2053, Train: 48.13%, Valid: 44.32%, Test: 44.46%
Epoch: 600, Loss: 1.1969, Train: 48.90%, Valid: 45.07%, Test: 45.27%
Epoch: 625, Loss: 1.1938, Train: 47.87%, Valid: 44.50%, Test: 44.78%
Epoch: 650, Loss: 1.1905, Train: 49.11%, Valid: 45.01%, Test: 45.01%
Epoch: 675, Loss: 1.1813, Train: 49.02%, Valid: 45.15%, Test: 45.37%
Epoch: 700, Loss: 1.1881, Train: 49.39%, Valid: 45.28%, Test: 45.21%
Epoch: 725, Loss: 1.2010, Train: 49.50%, Valid: 45.19%, Test: 45.32%
Epoch: 750, Loss: 1.1718, Train: 49.84%, Valid: 45.57%, Test: 45.47%
Epoch: 775, Loss: 1.1733, Train: 49.85%, Valid: 45.61%, Test: 45.40%
Epoch: 800, Loss: 1.1679, Train: 50.07%, Valid: 45.60%, Test: 45.62%
Epoch: 825, Loss: 1.1795, Train: 49.38%, Valid: 45.29%, Test: 45.15%
Epoch: 850, Loss: 1.1675, Train: 50.08%, Valid: 45.23%, Test: 45.13%
Epoch: 875, Loss: 1.1603, Train: 50.35%, Valid: 45.60%, Test: 45.54%
Epoch: 900, Loss: 1.1799, Train: 50.08%, Valid: 45.27%, Test: 45.06%
Epoch: 925, Loss: 1.1584, Train: 50.63%, Valid: 45.71%, Test: 45.55%
Epoch: 950, Loss: 1.1644, Train: 50.19%, Valid: 45.60%, Test: 45.47%
Epoch: 975, Loss: 1.1556, Train: 50.79%, Valid: 45.74%, Test: 45.56%
Run 01:
Highest Train: 50.91
Highest Valid: 45.86
  Final Train: 50.43
   Final Test: 45.65
All runs:
Highest Train: 50.91, nan
Highest Valid: 45.86, nan
  Final Train: 50.43, nan
   Final Test: 45.65, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 3.1283, Train: 30.54%, Valid: 30.37%, Test: 30.42%
Epoch: 25, Loss: 1.4593, Train: 28.09%, Valid: 27.89%, Test: 28.10%
Epoch: 50, Loss: 1.4116, Train: 40.09%, Valid: 39.79%, Test: 39.73%
Epoch: 75, Loss: 1.3832, Train: 41.11%, Valid: 40.79%, Test: 40.86%
Epoch: 100, Loss: 1.3695, Train: 40.50%, Valid: 40.48%, Test: 40.43%
Epoch: 125, Loss: 1.3593, Train: 40.17%, Valid: 40.21%, Test: 40.18%
Epoch: 150, Loss: 1.3502, Train: 42.27%, Valid: 42.18%, Test: 42.03%
Epoch: 175, Loss: 1.3390, Train: 42.57%, Valid: 42.38%, Test: 42.34%
Epoch: 200, Loss: 1.4408, Train: 42.26%, Valid: 42.05%, Test: 41.95%
Epoch: 225, Loss: 1.3434, Train: 42.50%, Valid: 42.54%, Test: 42.40%
Epoch: 250, Loss: 1.3270, Train: 43.02%, Valid: 42.88%, Test: 42.87%
Epoch: 275, Loss: 1.3152, Train: 43.42%, Valid: 43.43%, Test: 43.18%
Epoch: 300, Loss: 1.3212, Train: 43.02%, Valid: 43.03%, Test: 42.78%
Epoch: 325, Loss: 1.3038, Train: 43.69%, Valid: 43.70%, Test: 43.48%
Epoch: 350, Loss: 1.2952, Train: 44.24%, Valid: 44.18%, Test: 43.99%
Epoch: 375, Loss: 1.3031, Train: 44.13%, Valid: 44.13%, Test: 43.72%
Epoch: 400, Loss: 1.3003, Train: 44.02%, Valid: 44.02%, Test: 43.76%
Epoch: 425, Loss: 1.3443, Train: 43.62%, Valid: 43.16%, Test: 43.15%
Epoch: 450, Loss: 1.2974, Train: 44.13%, Valid: 44.21%, Test: 43.93%
Epoch: 475, Loss: 1.2856, Train: 44.52%, Valid: 44.54%, Test: 44.26%
Epoch: 500, Loss: 1.2831, Train: 44.39%, Valid: 44.40%, Test: 44.25%
Epoch: 525, Loss: 1.2966, Train: 42.60%, Valid: 42.72%, Test: 42.64%
Epoch: 550, Loss: 1.3597, Train: 41.94%, Valid: 41.81%, Test: 41.65%
Epoch: 575, Loss: 1.3331, Train: 42.61%, Valid: 42.31%, Test: 42.48%
Epoch: 600, Loss: 1.3143, Train: 42.83%, Valid: 42.78%, Test: 42.80%
Epoch: 625, Loss: 1.3040, Train: 43.47%, Valid: 43.34%, Test: 43.33%
Epoch: 650, Loss: 1.3065, Train: 43.54%, Valid: 43.24%, Test: 43.12%
Epoch: 675, Loss: 1.3071, Train: 44.02%, Valid: 43.98%, Test: 43.62%
Epoch: 700, Loss: 1.2859, Train: 44.27%, Valid: 44.15%, Test: 43.96%
Epoch: 725, Loss: 1.2820, Train: 44.57%, Valid: 44.38%, Test: 44.17%
Epoch: 750, Loss: 1.2780, Train: 44.49%, Valid: 44.35%, Test: 44.11%
Epoch: 775, Loss: 1.3294, Train: 41.50%, Valid: 41.62%, Test: 41.36%
Epoch: 800, Loss: 1.2809, Train: 44.74%, Valid: 44.50%, Test: 44.21%
Epoch: 825, Loss: 1.2719, Train: 44.91%, Valid: 44.77%, Test: 44.47%
Epoch: 850, Loss: 1.2780, Train: 41.63%, Valid: 41.76%, Test: 41.46%
Epoch: 875, Loss: 1.2694, Train: 45.11%, Valid: 44.93%, Test: 44.55%
Epoch: 900, Loss: 1.2625, Train: 45.32%, Valid: 45.24%, Test: 44.92%
Epoch: 925, Loss: 1.2609, Train: 44.48%, Valid: 44.44%, Test: 44.08%
Epoch: 950, Loss: 1.2630, Train: 45.70%, Valid: 45.55%, Test: 45.17%
Epoch: 975, Loss: 1.2546, Train: 45.87%, Valid: 45.62%, Test: 45.35%
Run 01:
Highest Train: 45.96
Highest Valid: 45.71
  Final Train: 45.95
   Final Test: 45.42
All runs:
Highest Train: 45.96, nan
Highest Valid: 45.71, nan
  Final Train: 45.95, nan
   Final Test: 45.42, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 61.0740, Train: 27.19%, Valid: 26.99%, Test: 27.13%
Epoch: 25, Loss: 1.6901, Train: 28.41%, Valid: 28.11%, Test: 28.49%
Epoch: 50, Loss: 1.4642, Train: 28.59%, Valid: 28.31%, Test: 28.61%
Epoch: 75, Loss: 1.4397, Train: 28.72%, Valid: 28.44%, Test: 28.73%
Epoch: 100, Loss: 1.4357, Train: 28.83%, Valid: 28.65%, Test: 28.82%
Epoch: 125, Loss: 1.4321, Train: 28.92%, Valid: 28.73%, Test: 28.90%
Epoch: 150, Loss: 1.4288, Train: 29.04%, Valid: 28.81%, Test: 29.03%
Epoch: 175, Loss: 1.4255, Train: 29.21%, Valid: 28.97%, Test: 29.25%
Epoch: 200, Loss: 1.4224, Train: 29.44%, Valid: 29.21%, Test: 29.50%
Epoch: 225, Loss: 1.4195, Train: 29.69%, Valid: 29.58%, Test: 29.73%
Epoch: 250, Loss: 1.4168, Train: 29.95%, Valid: 29.91%, Test: 30.04%
Epoch: 275, Loss: 1.4143, Train: 30.14%, Valid: 30.11%, Test: 30.25%
Epoch: 300, Loss: 1.4120, Train: 30.31%, Valid: 30.28%, Test: 30.41%
Epoch: 325, Loss: 1.4099, Train: 30.43%, Valid: 30.41%, Test: 30.53%
Epoch: 350, Loss: 1.4079, Train: 30.64%, Valid: 30.57%, Test: 30.72%
Epoch: 375, Loss: 1.4060, Train: 30.77%, Valid: 30.66%, Test: 30.85%
Epoch: 400, Loss: 1.4041, Train: 30.85%, Valid: 30.75%, Test: 30.98%
Epoch: 425, Loss: 1.4023, Train: 30.98%, Valid: 30.86%, Test: 31.08%
Epoch: 450, Loss: 1.4004, Train: 31.09%, Valid: 30.90%, Test: 31.21%
Epoch: 475, Loss: 1.4017, Train: 30.87%, Valid: 30.58%, Test: 30.83%
Epoch: 500, Loss: 1.6329, Train: 28.95%, Valid: 28.64%, Test: 28.99%
Epoch: 525, Loss: 1.4107, Train: 30.65%, Valid: 30.55%, Test: 30.80%
Epoch: 550, Loss: 1.4043, Train: 30.30%, Valid: 30.24%, Test: 30.48%
Epoch: 575, Loss: 1.3988, Train: 30.88%, Valid: 30.83%, Test: 31.09%
Epoch: 600, Loss: 1.3954, Train: 31.06%, Valid: 30.95%, Test: 31.19%
Epoch: 625, Loss: 1.3927, Train: 31.16%, Valid: 31.05%, Test: 31.31%
Epoch: 650, Loss: 1.3903, Train: 31.25%, Valid: 31.13%, Test: 31.46%
Epoch: 675, Loss: 1.3878, Train: 31.36%, Valid: 31.23%, Test: 31.60%
Epoch: 700, Loss: 1.3853, Train: 31.48%, Valid: 31.35%, Test: 31.78%
Epoch: 725, Loss: 1.3826, Train: 31.56%, Valid: 31.44%, Test: 31.90%
Epoch: 750, Loss: 1.3989, Train: 31.22%, Valid: 31.02%, Test: 31.57%
Epoch: 775, Loss: 1.7530, Train: 28.74%, Valid: 28.52%, Test: 28.88%
Epoch: 800, Loss: 1.4166, Train: 29.18%, Valid: 28.92%, Test: 29.30%
Epoch: 825, Loss: 1.3990, Train: 29.58%, Valid: 29.53%, Test: 29.78%
Epoch: 850, Loss: 1.3869, Train: 31.08%, Valid: 30.99%, Test: 31.21%
Epoch: 875, Loss: 1.3805, Train: 31.43%, Valid: 31.36%, Test: 31.66%
Epoch: 900, Loss: 1.3761, Train: 31.55%, Valid: 31.40%, Test: 31.85%
Epoch: 925, Loss: 1.3729, Train: 31.60%, Valid: 31.60%, Test: 31.88%
Epoch: 950, Loss: 1.3702, Train: 31.76%, Valid: 31.80%, Test: 32.12%
Epoch: 975, Loss: 1.3678, Train: 31.87%, Valid: 31.96%, Test: 32.31%
Run 01:
Highest Train: 32.00
Highest Valid: 32.14
  Final Train: 32.00
   Final Test: 32.53
All runs:
Highest Train: 32.00, nan
Highest Valid: 32.14, nan
  Final Train: 32.00, nan
   Final Test: 32.53, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6049, Train: 28.81%, Valid: 28.63%, Test: 28.81%
Epoch: 25, Loss: 1.5117, Train: 34.19%, Valid: 33.93%, Test: 34.32%
Epoch: 50, Loss: 1.4851, Train: 35.93%, Valid: 35.30%, Test: 35.63%
Epoch: 75, Loss: 1.4585, Train: 37.84%, Valid: 37.26%, Test: 37.43%
Epoch: 100, Loss: 1.4346, Train: 39.49%, Valid: 38.84%, Test: 39.08%
Epoch: 125, Loss: 1.4234, Train: 40.85%, Valid: 40.06%, Test: 40.54%
Epoch: 150, Loss: 1.4032, Train: 42.03%, Valid: 41.20%, Test: 41.28%
Epoch: 175, Loss: 1.3846, Train: 42.15%, Valid: 41.21%, Test: 41.20%
Epoch: 200, Loss: 1.3780, Train: 42.83%, Valid: 41.75%, Test: 41.81%
Epoch: 225, Loss: 1.3603, Train: 43.28%, Valid: 42.06%, Test: 41.91%
Epoch: 250, Loss: 1.3463, Train: 43.29%, Valid: 41.91%, Test: 41.88%
Epoch: 275, Loss: 1.3459, Train: 43.76%, Valid: 42.18%, Test: 42.29%
Epoch: 300, Loss: 1.3404, Train: 44.30%, Valid: 42.63%, Test: 42.83%
Epoch: 325, Loss: 1.3344, Train: 43.60%, Valid: 42.05%, Test: 42.36%
Epoch: 350, Loss: 1.3369, Train: 44.04%, Valid: 42.18%, Test: 42.56%
Epoch: 375, Loss: 1.3242, Train: 44.35%, Valid: 42.59%, Test: 42.82%
Epoch: 400, Loss: 1.3218, Train: 45.31%, Valid: 43.58%, Test: 43.95%
Epoch: 425, Loss: 1.3136, Train: 44.88%, Valid: 43.11%, Test: 43.32%
Epoch: 450, Loss: 1.3227, Train: 45.22%, Valid: 43.35%, Test: 43.64%
Epoch: 475, Loss: 1.3128, Train: 44.15%, Valid: 42.48%, Test: 42.87%
Epoch: 500, Loss: 1.3144, Train: 43.35%, Valid: 41.81%, Test: 42.15%
Epoch: 525, Loss: 1.3094, Train: 45.65%, Valid: 44.10%, Test: 44.40%
Epoch: 550, Loss: 1.3055, Train: 45.43%, Valid: 43.79%, Test: 43.95%
Epoch: 575, Loss: 1.3079, Train: 44.16%, Valid: 42.57%, Test: 42.68%
Epoch: 600, Loss: 1.3024, Train: 45.92%, Valid: 44.28%, Test: 44.46%
Epoch: 625, Loss: 1.2903, Train: 45.51%, Valid: 43.82%, Test: 44.20%
Epoch: 650, Loss: 1.2999, Train: 45.57%, Valid: 43.94%, Test: 44.08%
Epoch: 675, Loss: 1.2889, Train: 44.64%, Valid: 42.82%, Test: 43.30%
Epoch: 700, Loss: 1.2942, Train: 45.85%, Valid: 44.21%, Test: 44.30%
Epoch: 725, Loss: 1.2956, Train: 45.75%, Valid: 43.89%, Test: 44.23%
Epoch: 750, Loss: 1.2937, Train: 46.26%, Valid: 44.46%, Test: 44.60%
Epoch: 775, Loss: 1.2914, Train: 46.15%, Valid: 44.51%, Test: 44.74%
Epoch: 800, Loss: 1.3031, Train: 45.89%, Valid: 44.14%, Test: 44.45%
Epoch: 825, Loss: 1.2990, Train: 46.21%, Valid: 44.67%, Test: 44.80%
Epoch: 850, Loss: 1.2873, Train: 46.11%, Valid: 44.33%, Test: 44.51%
Epoch: 875, Loss: 1.2846, Train: 44.31%, Valid: 42.93%, Test: 42.94%
Epoch: 900, Loss: 1.2849, Train: 46.60%, Valid: 44.83%, Test: 44.99%
Epoch: 925, Loss: 1.2835, Train: 45.89%, Valid: 44.20%, Test: 44.16%
Epoch: 950, Loss: 1.2829, Train: 46.26%, Valid: 44.52%, Test: 44.65%
Epoch: 975, Loss: 1.2830, Train: 44.61%, Valid: 43.11%, Test: 43.35%
Run 01:
Highest Train: 46.81
Highest Valid: 45.02
  Final Train: 46.59
   Final Test: 45.05
All runs:
Highest Train: 46.81, nan
Highest Valid: 45.02, nan
  Final Train: 46.59, nan
   Final Test: 45.05, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.9892, Train: 27.04%, Valid: 26.89%, Test: 27.03%
Epoch: 25, Loss: 1.4374, Train: 30.00%, Valid: 29.72%, Test: 29.61%
Epoch: 50, Loss: 1.4151, Train: 29.44%, Valid: 29.56%, Test: 29.13%
Epoch: 75, Loss: 1.3947, Train: 30.81%, Valid: 30.93%, Test: 30.63%
Epoch: 100, Loss: 1.3934, Train: 32.15%, Valid: 32.37%, Test: 32.07%
Epoch: 125, Loss: 1.3831, Train: 32.16%, Valid: 32.45%, Test: 31.96%
Epoch: 150, Loss: 1.3767, Train: 33.41%, Valid: 33.59%, Test: 33.29%
Epoch: 175, Loss: 1.3751, Train: 33.99%, Valid: 34.15%, Test: 34.26%
Epoch: 200, Loss: 1.3599, Train: 33.58%, Valid: 33.68%, Test: 33.53%
Epoch: 225, Loss: 1.3642, Train: 34.50%, Valid: 34.69%, Test: 34.54%
Epoch: 250, Loss: 1.3512, Train: 35.51%, Valid: 35.68%, Test: 35.57%
Epoch: 275, Loss: 1.3467, Train: 35.35%, Valid: 35.42%, Test: 35.53%
Epoch: 300, Loss: 1.3575, Train: 34.98%, Valid: 35.14%, Test: 35.19%
Epoch: 325, Loss: 1.3347, Train: 35.98%, Valid: 35.94%, Test: 35.85%
Epoch: 350, Loss: 1.3383, Train: 39.18%, Valid: 38.85%, Test: 39.02%
Epoch: 375, Loss: 1.3221, Train: 35.11%, Valid: 34.88%, Test: 34.98%
Epoch: 400, Loss: 1.3652, Train: 34.55%, Valid: 34.56%, Test: 34.38%
Epoch: 425, Loss: 1.3274, Train: 37.02%, Valid: 36.74%, Test: 36.82%
Epoch: 450, Loss: 1.3189, Train: 38.98%, Valid: 38.93%, Test: 38.83%
Epoch: 475, Loss: 1.3471, Train: 40.13%, Valid: 39.79%, Test: 39.89%
Epoch: 500, Loss: 1.3301, Train: 35.09%, Valid: 35.16%, Test: 34.90%
Epoch: 525, Loss: 1.3279, Train: 37.61%, Valid: 37.67%, Test: 37.61%
Epoch: 550, Loss: 1.3157, Train: 39.27%, Valid: 39.02%, Test: 39.03%
Epoch: 575, Loss: 1.3511, Train: 38.45%, Valid: 38.45%, Test: 38.25%
Epoch: 600, Loss: 1.3130, Train: 39.25%, Valid: 39.18%, Test: 39.15%
Epoch: 625, Loss: 1.3184, Train: 39.60%, Valid: 39.41%, Test: 39.51%
Epoch: 650, Loss: 1.3164, Train: 38.63%, Valid: 38.21%, Test: 38.23%
Epoch: 675, Loss: 1.3160, Train: 38.44%, Valid: 38.32%, Test: 38.22%
Epoch: 700, Loss: 1.3193, Train: 39.43%, Valid: 39.27%, Test: 39.13%
Epoch: 725, Loss: 1.3108, Train: 37.73%, Valid: 37.59%, Test: 37.49%
Epoch: 750, Loss: 1.3065, Train: 41.07%, Valid: 40.98%, Test: 40.94%
Epoch: 775, Loss: 1.3615, Train: 38.16%, Valid: 37.98%, Test: 38.11%
Epoch: 800, Loss: 1.3283, Train: 39.60%, Valid: 39.72%, Test: 39.31%
Epoch: 825, Loss: 1.3664, Train: 39.14%, Valid: 39.11%, Test: 38.75%
Epoch: 850, Loss: 1.3224, Train: 40.03%, Valid: 39.94%, Test: 39.79%
Epoch: 875, Loss: 1.3106, Train: 41.31%, Valid: 41.23%, Test: 41.23%
Epoch: 900, Loss: 1.3108, Train: 38.39%, Valid: 38.40%, Test: 38.13%
Epoch: 925, Loss: 1.3128, Train: 42.26%, Valid: 41.86%, Test: 42.18%
Epoch: 950, Loss: 1.2979, Train: 42.51%, Valid: 42.22%, Test: 42.30%
Epoch: 975, Loss: 1.3022, Train: 40.97%, Valid: 40.64%, Test: 40.76%
Run 01:
Highest Train: 43.11
Highest Valid: 43.02
  Final Train: 43.11
   Final Test: 42.79
All runs:
Highest Train: 43.11, nan
Highest Valid: 43.02, nan
  Final Train: 43.11, nan
   Final Test: 42.79, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 121.1483, Train: 22.78%, Valid: 22.99%, Test: 22.92%
Epoch: 25, Loss: 1.7277, Train: 32.19%, Valid: 32.11%, Test: 32.80%
Epoch: 50, Loss: 1.6010, Train: 32.52%, Valid: 32.38%, Test: 33.11%
Epoch: 75, Loss: 1.7727, Train: 32.73%, Valid: 32.53%, Test: 33.33%
Epoch: 100, Loss: 1.5746, Train: 32.79%, Valid: 32.51%, Test: 33.42%
Epoch: 125, Loss: 1.5423, Train: 32.78%, Valid: 32.53%, Test: 33.34%
Epoch: 150, Loss: 1.5354, Train: 32.69%, Valid: 32.47%, Test: 33.28%
Epoch: 175, Loss: 1.5251, Train: 32.56%, Valid: 32.37%, Test: 33.14%
Epoch: 200, Loss: 1.5591, Train: 32.53%, Valid: 32.38%, Test: 33.11%
Epoch: 225, Loss: 1.5218, Train: 32.46%, Valid: 32.21%, Test: 33.07%
Epoch: 250, Loss: 1.5179, Train: 32.39%, Valid: 32.18%, Test: 32.98%
Epoch: 275, Loss: 1.5291, Train: 32.24%, Valid: 32.07%, Test: 32.85%
Epoch: 300, Loss: 1.4937, Train: 32.20%, Valid: 32.07%, Test: 32.83%
Epoch: 325, Loss: 1.4831, Train: 32.07%, Valid: 31.96%, Test: 32.66%
Epoch: 350, Loss: 1.4877, Train: 32.00%, Valid: 31.90%, Test: 32.58%
Epoch: 375, Loss: 1.4761, Train: 31.87%, Valid: 31.72%, Test: 32.44%
Epoch: 400, Loss: 1.4742, Train: 31.62%, Valid: 31.39%, Test: 32.13%
Epoch: 425, Loss: 1.4709, Train: 31.68%, Valid: 31.48%, Test: 32.12%
Epoch: 450, Loss: 1.4529, Train: 31.64%, Valid: 31.41%, Test: 32.14%
Epoch: 475, Loss: 1.4443, Train: 31.67%, Valid: 31.49%, Test: 32.15%
Epoch: 500, Loss: 1.4649, Train: 31.76%, Valid: 31.65%, Test: 32.30%
Epoch: 525, Loss: 1.4479, Train: 31.92%, Valid: 31.91%, Test: 32.36%
Epoch: 550, Loss: 1.4402, Train: 34.32%, Valid: 34.23%, Test: 34.33%
Epoch: 575, Loss: 1.4367, Train: 37.62%, Valid: 37.51%, Test: 37.61%
Epoch: 600, Loss: 1.4423, Train: 38.20%, Valid: 38.00%, Test: 38.00%
Epoch: 625, Loss: 1.4249, Train: 38.21%, Valid: 37.91%, Test: 37.96%
Epoch: 650, Loss: 1.4285, Train: 38.16%, Valid: 37.92%, Test: 37.92%
Epoch: 675, Loss: 1.4253, Train: 38.13%, Valid: 37.87%, Test: 37.91%
Epoch: 700, Loss: 1.4244, Train: 38.08%, Valid: 37.78%, Test: 37.76%
Epoch: 725, Loss: 1.4201, Train: 38.02%, Valid: 37.71%, Test: 37.69%
Epoch: 750, Loss: 1.4139, Train: 37.59%, Valid: 37.28%, Test: 37.24%
Epoch: 775, Loss: 1.4182, Train: 37.33%, Valid: 37.01%, Test: 37.04%
Epoch: 800, Loss: 1.4158, Train: 30.96%, Valid: 30.67%, Test: 30.85%
Epoch: 825, Loss: 1.4146, Train: 32.40%, Valid: 32.08%, Test: 32.22%
Epoch: 850, Loss: 1.4150, Train: 27.68%, Valid: 27.55%, Test: 27.46%
Epoch: 875, Loss: 1.4088, Train: 25.91%, Valid: 25.88%, Test: 25.77%
Epoch: 900, Loss: 1.4077, Train: 24.51%, Valid: 24.37%, Test: 24.38%
Epoch: 925, Loss: 1.4106, Train: 28.52%, Valid: 28.38%, Test: 28.38%
Epoch: 950, Loss: 1.4045, Train: 23.44%, Valid: 23.33%, Test: 23.15%
Epoch: 975, Loss: 1.4034, Train: 34.37%, Valid: 34.02%, Test: 34.08%
Run 01:
Highest Train: 38.30
Highest Valid: 38.14
  Final Train: 38.30
   Final Test: 38.09
All runs:
Highest Train: 38.30, nan
Highest Valid: 38.14, nan
  Final Train: 38.30, nan
   Final Test: 38.09, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6025, Train: 28.74%, Valid: 28.55%, Test: 28.83%
Epoch: 25, Loss: 1.5228, Train: 37.57%, Valid: 37.27%, Test: 37.63%
Epoch: 50, Loss: 1.4678, Train: 40.17%, Valid: 39.83%, Test: 39.93%
Epoch: 75, Loss: 1.4460, Train: 40.11%, Valid: 39.51%, Test: 39.68%
Epoch: 100, Loss: 1.4162, Train: 40.24%, Valid: 39.61%, Test: 40.05%
Epoch: 125, Loss: 1.3988, Train: 40.96%, Valid: 40.39%, Test: 40.56%
Epoch: 150, Loss: 1.3985, Train: 39.73%, Valid: 39.33%, Test: 39.58%
Epoch: 175, Loss: 1.3791, Train: 40.26%, Valid: 39.71%, Test: 39.71%
Epoch: 200, Loss: 1.3780, Train: 41.84%, Valid: 41.27%, Test: 41.22%
Epoch: 225, Loss: 1.3750, Train: 41.59%, Valid: 40.85%, Test: 40.97%
Epoch: 250, Loss: 1.3658, Train: 42.56%, Valid: 41.68%, Test: 41.80%
Epoch: 275, Loss: 1.3581, Train: 42.95%, Valid: 42.30%, Test: 42.25%
Epoch: 300, Loss: 1.3570, Train: 43.52%, Valid: 42.81%, Test: 42.99%
Epoch: 325, Loss: 1.3509, Train: 43.27%, Valid: 42.45%, Test: 42.71%
Epoch: 350, Loss: 1.3555, Train: 44.02%, Valid: 43.23%, Test: 43.30%
Epoch: 375, Loss: 1.3442, Train: 44.08%, Valid: 43.49%, Test: 43.48%
Epoch: 400, Loss: 1.3390, Train: 44.14%, Valid: 43.18%, Test: 43.39%
Epoch: 425, Loss: 1.3325, Train: 44.29%, Valid: 43.51%, Test: 43.69%
Epoch: 450, Loss: 1.3322, Train: 43.80%, Valid: 42.78%, Test: 42.87%
Epoch: 475, Loss: 1.3315, Train: 44.41%, Valid: 43.56%, Test: 43.67%
Epoch: 500, Loss: 1.3395, Train: 44.43%, Valid: 43.48%, Test: 43.67%
Epoch: 525, Loss: 1.3297, Train: 44.42%, Valid: 43.55%, Test: 43.44%
Epoch: 550, Loss: 1.3226, Train: 44.88%, Valid: 43.94%, Test: 43.99%
Epoch: 575, Loss: 1.3301, Train: 44.69%, Valid: 43.63%, Test: 43.73%
Epoch: 600, Loss: 1.3298, Train: 44.54%, Valid: 43.51%, Test: 43.65%
Epoch: 625, Loss: 1.3202, Train: 44.96%, Valid: 43.97%, Test: 44.12%
Epoch: 650, Loss: 1.3300, Train: 44.92%, Valid: 43.93%, Test: 43.78%
Epoch: 675, Loss: 1.3253, Train: 44.85%, Valid: 44.08%, Test: 43.95%
Epoch: 700, Loss: 1.3163, Train: 44.82%, Valid: 43.94%, Test: 43.96%
Epoch: 725, Loss: 1.3249, Train: 44.45%, Valid: 43.58%, Test: 43.62%
Epoch: 750, Loss: 1.3245, Train: 44.86%, Valid: 43.82%, Test: 44.09%
Epoch: 775, Loss: 1.3199, Train: 44.49%, Valid: 43.77%, Test: 43.72%
Epoch: 800, Loss: 1.3141, Train: 45.00%, Valid: 43.94%, Test: 43.93%
Epoch: 825, Loss: 1.3203, Train: 44.93%, Valid: 44.07%, Test: 44.12%
Epoch: 850, Loss: 1.3054, Train: 44.31%, Valid: 43.30%, Test: 43.41%
Epoch: 875, Loss: 1.3134, Train: 44.33%, Valid: 43.45%, Test: 43.40%
Epoch: 900, Loss: 1.3142, Train: 44.73%, Valid: 43.80%, Test: 43.88%
Epoch: 925, Loss: 1.3099, Train: 44.64%, Valid: 43.73%, Test: 43.79%
Epoch: 950, Loss: 1.3056, Train: 44.59%, Valid: 43.74%, Test: 43.65%
Epoch: 975, Loss: 1.3098, Train: 44.56%, Valid: 43.85%, Test: 43.62%
Run 01:
Highest Train: 45.72
Highest Valid: 44.54
  Final Train: 45.72
   Final Test: 44.57
All runs:
Highest Train: 45.72, nan
Highest Valid: 44.54, nan
  Final Train: 45.72, nan
   Final Test: 44.57, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 2.5906, Train: 36.11%, Valid: 35.87%, Test: 35.99%
Epoch: 25, Loss: 1.4686, Train: 31.77%, Valid: 31.63%, Test: 32.28%
Epoch: 50, Loss: 1.4335, Train: 21.64%, Valid: 21.54%, Test: 21.58%
Epoch: 75, Loss: 1.4212, Train: 25.16%, Valid: 25.10%, Test: 24.91%
Epoch: 100, Loss: 1.4123, Train: 28.72%, Valid: 28.50%, Test: 28.46%
Epoch: 125, Loss: 1.3960, Train: 29.84%, Valid: 29.83%, Test: 29.51%
Epoch: 150, Loss: 1.3872, Train: 32.47%, Valid: 32.49%, Test: 32.24%
Epoch: 175, Loss: 1.3901, Train: 26.88%, Valid: 26.76%, Test: 26.70%
Epoch: 200, Loss: 1.4095, Train: 25.36%, Valid: 25.45%, Test: 24.93%
Epoch: 225, Loss: 1.3782, Train: 27.35%, Valid: 27.51%, Test: 27.22%
Epoch: 250, Loss: 1.3597, Train: 26.19%, Valid: 26.21%, Test: 26.02%
Epoch: 275, Loss: 1.4193, Train: 31.32%, Valid: 31.52%, Test: 31.00%
Epoch: 300, Loss: 1.3731, Train: 34.51%, Valid: 34.70%, Test: 34.38%
Epoch: 325, Loss: 1.3500, Train: 33.59%, Valid: 33.77%, Test: 33.46%
Epoch: 350, Loss: 1.3543, Train: 32.65%, Valid: 32.67%, Test: 32.35%
Epoch: 375, Loss: 1.3625, Train: 39.37%, Valid: 39.39%, Test: 39.04%
Epoch: 400, Loss: 1.3700, Train: 34.57%, Valid: 34.47%, Test: 34.14%
Epoch: 425, Loss: 1.3904, Train: 32.17%, Valid: 32.18%, Test: 31.62%
Epoch: 450, Loss: 1.3536, Train: 33.94%, Valid: 34.11%, Test: 33.58%
Epoch: 475, Loss: 1.3566, Train: 36.94%, Valid: 36.93%, Test: 36.56%
Epoch: 500, Loss: 1.3481, Train: 36.37%, Valid: 36.16%, Test: 35.93%
Epoch: 525, Loss: 1.3510, Train: 36.35%, Valid: 36.52%, Test: 36.08%
Epoch: 550, Loss: 1.3326, Train: 32.43%, Valid: 32.51%, Test: 32.18%
Epoch: 575, Loss: 1.3621, Train: 35.31%, Valid: 34.99%, Test: 35.09%
Epoch: 600, Loss: 1.3309, Train: 33.36%, Valid: 33.19%, Test: 33.18%
Epoch: 625, Loss: 1.3487, Train: 32.77%, Valid: 32.68%, Test: 32.48%
Epoch: 650, Loss: 1.3715, Train: 33.39%, Valid: 33.06%, Test: 33.15%
Epoch: 675, Loss: 1.3358, Train: 38.60%, Valid: 38.48%, Test: 38.16%
Epoch: 700, Loss: 1.3334, Train: 34.06%, Valid: 34.00%, Test: 33.88%
Epoch: 725, Loss: 1.3382, Train: 32.60%, Valid: 32.79%, Test: 32.35%
Epoch: 750, Loss: 1.3232, Train: 36.19%, Valid: 36.13%, Test: 36.03%
Epoch: 775, Loss: 1.3436, Train: 36.02%, Valid: 36.10%, Test: 35.89%
Epoch: 800, Loss: 1.3157, Train: 35.89%, Valid: 35.82%, Test: 35.64%
Epoch: 825, Loss: 1.3292, Train: 35.39%, Valid: 35.25%, Test: 35.10%
Epoch: 850, Loss: 1.3176, Train: 33.62%, Valid: 33.65%, Test: 33.46%
Epoch: 875, Loss: 1.3253, Train: 32.39%, Valid: 32.65%, Test: 32.25%
Epoch: 900, Loss: 1.3470, Train: 35.10%, Valid: 35.17%, Test: 35.03%
Epoch: 925, Loss: 1.3120, Train: 33.35%, Valid: 33.48%, Test: 33.08%
Epoch: 950, Loss: 1.3131, Train: 35.60%, Valid: 35.57%, Test: 35.53%
Epoch: 975, Loss: 1.3583, Train: 33.49%, Valid: 33.38%, Test: 33.30%
Run 01:
Highest Train: 41.71
Highest Valid: 41.26
  Final Train: 41.31
   Final Test: 40.84
All runs:
Highest Train: 41.71, nan
Highest Valid: 41.26, nan
  Final Train: 41.31, nan
   Final Test: 40.84, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 88.7767, Train: 30.27%, Valid: 30.26%, Test: 30.37%
Epoch: 25, Loss: 1.7146, Train: 28.75%, Valid: 28.47%, Test: 28.92%
Epoch: 50, Loss: 1.9486, Train: 23.42%, Valid: 23.61%, Test: 23.53%
Epoch: 75, Loss: 1.6007, Train: 26.45%, Valid: 26.39%, Test: 26.70%
Epoch: 100, Loss: 1.5554, Train: 23.56%, Valid: 23.71%, Test: 23.61%
Epoch: 125, Loss: 1.5369, Train: 25.15%, Valid: 25.26%, Test: 25.54%
Epoch: 150, Loss: 1.5299, Train: 24.73%, Valid: 24.86%, Test: 24.94%
Epoch: 175, Loss: 1.5516, Train: 23.83%, Valid: 23.92%, Test: 23.87%
Epoch: 200, Loss: 1.4997, Train: 24.94%, Valid: 25.09%, Test: 25.16%
Epoch: 225, Loss: 1.4954, Train: 26.25%, Valid: 26.20%, Test: 26.57%
Epoch: 250, Loss: 1.4875, Train: 30.00%, Valid: 30.11%, Test: 30.16%
Epoch: 275, Loss: 1.5310, Train: 38.13%, Valid: 38.07%, Test: 38.15%
Epoch: 300, Loss: 1.4800, Train: 39.25%, Valid: 39.07%, Test: 39.07%
Epoch: 325, Loss: 1.4767, Train: 39.70%, Valid: 39.64%, Test: 39.70%
Epoch: 350, Loss: 1.4890, Train: 38.89%, Valid: 38.60%, Test: 38.88%
Epoch: 375, Loss: 1.5081, Train: 38.26%, Valid: 38.08%, Test: 38.26%
Epoch: 400, Loss: 1.4726, Train: 39.43%, Valid: 39.26%, Test: 39.35%
Epoch: 425, Loss: 1.4664, Train: 36.66%, Valid: 36.71%, Test: 36.82%
Epoch: 450, Loss: 1.4625, Train: 39.22%, Valid: 38.90%, Test: 39.04%
Epoch: 475, Loss: 1.4595, Train: 39.36%, Valid: 39.18%, Test: 39.17%
Epoch: 500, Loss: 1.4905, Train: 39.35%, Valid: 39.19%, Test: 39.23%
Epoch: 525, Loss: 1.4507, Train: 39.10%, Valid: 38.89%, Test: 38.96%
Epoch: 550, Loss: 1.4528, Train: 38.83%, Valid: 38.62%, Test: 38.82%
Epoch: 575, Loss: 1.4587, Train: 38.62%, Valid: 38.34%, Test: 38.48%
Epoch: 600, Loss: 1.4797, Train: 35.41%, Valid: 35.57%, Test: 35.46%
Epoch: 625, Loss: 1.4425, Train: 33.57%, Valid: 33.78%, Test: 33.35%
Epoch: 650, Loss: 1.4376, Train: 35.33%, Valid: 35.38%, Test: 35.33%
Epoch: 675, Loss: 1.4449, Train: 32.89%, Valid: 33.05%, Test: 32.54%
Epoch: 700, Loss: 1.4600, Train: 37.13%, Valid: 36.97%, Test: 37.24%
Epoch: 725, Loss: 1.4466, Train: 33.37%, Valid: 33.53%, Test: 33.15%
Epoch: 750, Loss: 1.4542, Train: 32.54%, Valid: 32.80%, Test: 32.34%
Epoch: 775, Loss: 1.4341, Train: 30.60%, Valid: 30.74%, Test: 30.57%
Epoch: 800, Loss: 1.4560, Train: 32.89%, Valid: 33.06%, Test: 33.06%
Epoch: 825, Loss: 1.4465, Train: 34.02%, Valid: 34.09%, Test: 34.14%
Epoch: 850, Loss: 1.4322, Train: 34.70%, Valid: 34.87%, Test: 34.76%
Epoch: 875, Loss: 1.4199, Train: 33.14%, Valid: 33.32%, Test: 33.33%
Epoch: 900, Loss: 1.4214, Train: 30.67%, Valid: 30.76%, Test: 30.62%
Epoch: 925, Loss: 1.4266, Train: 31.73%, Valid: 31.82%, Test: 31.73%
Epoch: 950, Loss: 1.4339, Train: 29.31%, Valid: 29.59%, Test: 29.31%
Epoch: 975, Loss: 1.4126, Train: 29.92%, Valid: 30.11%, Test: 29.83%
Run 01:
Highest Train: 39.82
Highest Valid: 39.77
  Final Train: 39.79
   Final Test: 39.78
All runs:
Highest Train: 39.82, nan
Highest Valid: 39.77, nan
  Final Train: 39.79, nan
   Final Test: 39.78, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.5993, Train: 29.08%, Valid: 28.81%, Test: 29.10%
Epoch: 25, Loss: 1.5147, Train: 29.91%, Valid: 29.83%, Test: 30.25%
Epoch: 50, Loss: 1.4495, Train: 37.33%, Valid: 36.98%, Test: 37.22%
Epoch: 75, Loss: 1.4090, Train: 39.11%, Valid: 38.67%, Test: 38.97%
Epoch: 100, Loss: 1.3854, Train: 40.52%, Valid: 40.30%, Test: 40.12%
Epoch: 125, Loss: 1.3935, Train: 39.44%, Valid: 38.89%, Test: 39.03%
Epoch: 150, Loss: 1.3433, Train: 42.64%, Valid: 42.06%, Test: 42.02%
Epoch: 175, Loss: 1.3228, Train: 43.45%, Valid: 42.72%, Test: 42.86%
Epoch: 200, Loss: 1.3071, Train: 44.58%, Valid: 43.86%, Test: 43.77%
Epoch: 225, Loss: 1.2803, Train: 45.54%, Valid: 44.59%, Test: 44.60%
Epoch: 250, Loss: 1.3101, Train: 44.63%, Valid: 44.16%, Test: 44.17%
Epoch: 275, Loss: 1.2720, Train: 46.13%, Valid: 45.20%, Test: 45.29%
Epoch: 300, Loss: 1.2563, Train: 46.77%, Valid: 45.46%, Test: 45.71%
Epoch: 325, Loss: 1.2511, Train: 46.92%, Valid: 45.39%, Test: 45.81%
Epoch: 350, Loss: 1.2582, Train: 46.95%, Valid: 45.52%, Test: 45.62%
Epoch: 375, Loss: 1.2354, Train: 47.58%, Valid: 45.83%, Test: 46.07%
Epoch: 400, Loss: 1.2135, Train: 48.28%, Valid: 46.18%, Test: 46.40%
Epoch: 425, Loss: 1.2128, Train: 48.30%, Valid: 45.94%, Test: 46.13%
Epoch: 450, Loss: 1.2169, Train: 48.11%, Valid: 46.17%, Test: 46.09%
Epoch: 475, Loss: 1.2028, Train: 47.74%, Valid: 45.32%, Test: 45.48%
Epoch: 500, Loss: 1.1895, Train: 48.39%, Valid: 45.75%, Test: 45.74%
Epoch: 525, Loss: 1.1989, Train: 48.95%, Valid: 46.41%, Test: 46.30%
Epoch: 550, Loss: 1.1790, Train: 49.81%, Valid: 46.87%, Test: 46.81%
Epoch: 575, Loss: 1.1744, Train: 49.75%, Valid: 46.40%, Test: 46.58%
Epoch: 600, Loss: 1.1700, Train: 50.18%, Valid: 46.70%, Test: 46.83%
Epoch: 625, Loss: 1.1623, Train: 50.67%, Valid: 47.07%, Test: 47.00%
Epoch: 650, Loss: 1.1599, Train: 50.57%, Valid: 47.04%, Test: 46.98%
Epoch: 675, Loss: 1.1567, Train: 50.56%, Valid: 47.01%, Test: 46.88%
Epoch: 700, Loss: 1.1506, Train: 50.71%, Valid: 47.01%, Test: 46.95%
Epoch: 725, Loss: 1.1441, Train: 51.01%, Valid: 47.26%, Test: 47.11%
Epoch: 750, Loss: 1.1528, Train: 50.86%, Valid: 47.01%, Test: 47.14%
Epoch: 775, Loss: 1.1618, Train: 50.62%, Valid: 46.75%, Test: 46.96%
Epoch: 800, Loss: 1.1315, Train: 51.67%, Valid: 47.39%, Test: 47.60%
Epoch: 825, Loss: 1.1298, Train: 51.71%, Valid: 47.53%, Test: 47.64%
Epoch: 850, Loss: 1.1304, Train: 51.96%, Valid: 47.33%, Test: 47.49%
Epoch: 875, Loss: 1.1224, Train: 52.05%, Valid: 47.56%, Test: 47.65%
Epoch: 900, Loss: 1.1274, Train: 52.21%, Valid: 47.70%, Test: 47.71%
Epoch: 925, Loss: 1.1168, Train: 52.33%, Valid: 47.75%, Test: 47.91%
Epoch: 950, Loss: 1.1253, Train: 51.84%, Valid: 47.49%, Test: 47.57%
Epoch: 975, Loss: 1.1170, Train: 52.10%, Valid: 47.72%, Test: 47.74%
Run 01:
Highest Train: 52.72
Highest Valid: 47.91
  Final Train: 52.49
   Final Test: 47.80
All runs:
Highest Train: 52.72, nan
Highest Valid: 47.91, nan
  Final Train: 52.49, nan
   Final Test: 47.80, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 2.0542, Train: 28.52%, Valid: 28.55%, Test: 28.34%
Epoch: 25, Loss: 1.4682, Train: 28.71%, Valid: 28.31%, Test: 28.64%
Epoch: 50, Loss: 1.4438, Train: 29.45%, Valid: 29.12%, Test: 29.48%
Epoch: 75, Loss: 1.4611, Train: 28.94%, Valid: 28.78%, Test: 29.06%
Epoch: 100, Loss: 1.4361, Train: 32.05%, Valid: 31.57%, Test: 32.37%
Epoch: 125, Loss: 1.4465, Train: 32.26%, Valid: 31.78%, Test: 32.63%
Epoch: 150, Loss: 1.4228, Train: 34.43%, Valid: 34.16%, Test: 34.96%
Epoch: 175, Loss: 1.4180, Train: 34.26%, Valid: 34.26%, Test: 34.90%
Epoch: 200, Loss: 1.4027, Train: 35.00%, Valid: 34.95%, Test: 35.58%
Epoch: 225, Loss: 1.3885, Train: 34.71%, Valid: 34.57%, Test: 35.17%
Epoch: 250, Loss: 1.3911, Train: 36.53%, Valid: 36.67%, Test: 36.82%
Epoch: 275, Loss: 1.3564, Train: 41.51%, Valid: 41.51%, Test: 41.58%
Epoch: 300, Loss: 1.3594, Train: 41.77%, Valid: 42.01%, Test: 42.06%
Epoch: 325, Loss: 1.3262, Train: 43.00%, Valid: 42.93%, Test: 43.01%
Epoch: 350, Loss: 1.3161, Train: 43.24%, Valid: 43.43%, Test: 43.37%
Epoch: 375, Loss: 1.3155, Train: 43.57%, Valid: 43.50%, Test: 43.54%
Epoch: 400, Loss: 1.3030, Train: 43.15%, Valid: 43.02%, Test: 43.01%
Epoch: 425, Loss: 1.3385, Train: 42.92%, Valid: 42.84%, Test: 43.03%
Epoch: 450, Loss: 1.3144, Train: 43.78%, Valid: 43.65%, Test: 43.73%
Epoch: 475, Loss: 1.3040, Train: 44.28%, Valid: 44.07%, Test: 44.24%
Epoch: 500, Loss: 1.2993, Train: 44.28%, Valid: 44.07%, Test: 44.22%
Epoch: 525, Loss: 1.3530, Train: 44.46%, Valid: 44.07%, Test: 44.17%
Epoch: 550, Loss: 1.3444, Train: 43.16%, Valid: 42.88%, Test: 43.21%
Epoch: 575, Loss: 1.3166, Train: 44.12%, Valid: 43.82%, Test: 44.06%
Epoch: 600, Loss: 1.3081, Train: 44.46%, Valid: 44.20%, Test: 44.30%
Epoch: 625, Loss: 1.2945, Train: 44.86%, Valid: 44.61%, Test: 44.62%
Epoch: 650, Loss: 1.3279, Train: 43.78%, Valid: 43.65%, Test: 43.81%
Epoch: 675, Loss: 1.3000, Train: 44.49%, Valid: 44.20%, Test: 44.43%
Epoch: 700, Loss: 1.2873, Train: 45.00%, Valid: 44.85%, Test: 44.86%
Epoch: 725, Loss: 1.2788, Train: 45.13%, Valid: 44.96%, Test: 44.91%
Epoch: 750, Loss: 1.2764, Train: 45.46%, Valid: 45.27%, Test: 45.21%
Epoch: 775, Loss: 1.2632, Train: 45.82%, Valid: 45.49%, Test: 45.47%
Epoch: 800, Loss: 1.2811, Train: 45.14%, Valid: 45.08%, Test: 44.80%
Epoch: 825, Loss: 1.2584, Train: 45.53%, Valid: 45.17%, Test: 45.27%
Epoch: 850, Loss: 1.2609, Train: 45.67%, Valid: 45.40%, Test: 45.42%
Epoch: 875, Loss: 1.2487, Train: 46.51%, Valid: 45.97%, Test: 46.13%
Epoch: 900, Loss: 1.2667, Train: 45.88%, Valid: 45.74%, Test: 45.86%
Epoch: 925, Loss: 1.2543, Train: 46.53%, Valid: 46.01%, Test: 46.21%
Epoch: 950, Loss: 1.2451, Train: 45.42%, Valid: 45.16%, Test: 45.35%
Epoch: 975, Loss: 1.2637, Train: 45.78%, Valid: 45.45%, Test: 45.70%
Run 01:
Highest Train: 46.91
Highest Valid: 46.32
  Final Train: 46.70
   Final Test: 46.35
All runs:
Highest Train: 46.91, nan
Highest Valid: 46.32, nan
  Final Train: 46.70, nan
   Final Test: 46.35, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 4.3532, Train: 28.77%, Valid: 28.44%, Test: 28.78%
Epoch: 25, Loss: 1.4956, Train: 28.67%, Valid: 28.38%, Test: 28.65%
Epoch: 50, Loss: 1.4787, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 75, Loss: 1.4486, Train: 28.95%, Valid: 28.69%, Test: 28.97%
Epoch: 100, Loss: 1.4410, Train: 28.99%, Valid: 28.86%, Test: 29.15%
Epoch: 125, Loss: 1.4642, Train: 28.64%, Valid: 28.39%, Test: 28.72%
Epoch: 150, Loss: 1.4569, Train: 28.35%, Valid: 28.07%, Test: 28.44%
Epoch: 175, Loss: 1.4486, Train: 28.66%, Valid: 28.43%, Test: 28.73%
Epoch: 200, Loss: 1.4424, Train: 28.71%, Valid: 28.52%, Test: 28.80%
Epoch: 225, Loss: 1.4388, Train: 28.74%, Valid: 28.56%, Test: 28.82%
Epoch: 250, Loss: 1.4351, Train: 28.84%, Valid: 28.66%, Test: 28.93%
Epoch: 275, Loss: 1.4309, Train: 28.96%, Valid: 28.75%, Test: 29.01%
Epoch: 300, Loss: 1.4396, Train: 28.87%, Valid: 28.55%, Test: 28.88%
Epoch: 325, Loss: 1.4330, Train: 29.09%, Valid: 28.91%, Test: 29.08%
Epoch: 350, Loss: 1.4287, Train: 29.22%, Valid: 29.04%, Test: 29.28%
Epoch: 375, Loss: 1.4243, Train: 29.42%, Valid: 29.28%, Test: 29.49%
Epoch: 400, Loss: 1.4588, Train: 28.53%, Valid: 28.28%, Test: 28.61%
Epoch: 425, Loss: 1.4332, Train: 29.19%, Valid: 28.97%, Test: 29.21%
Epoch: 450, Loss: 1.4289, Train: 29.32%, Valid: 29.20%, Test: 29.29%
Epoch: 475, Loss: 1.4257, Train: 29.46%, Valid: 29.26%, Test: 29.46%
Epoch: 500, Loss: 1.4226, Train: 29.70%, Valid: 29.50%, Test: 29.73%
Epoch: 525, Loss: 1.4192, Train: 29.91%, Valid: 29.82%, Test: 30.03%
Epoch: 550, Loss: 1.5348, Train: 29.42%, Valid: 29.22%, Test: 29.46%
Epoch: 575, Loss: 1.4444, Train: 28.95%, Valid: 28.81%, Test: 29.07%
Epoch: 600, Loss: 1.4287, Train: 29.37%, Valid: 29.25%, Test: 29.43%
Epoch: 625, Loss: 1.4252, Train: 29.49%, Valid: 29.31%, Test: 29.53%
Epoch: 650, Loss: 1.4222, Train: 29.58%, Valid: 29.39%, Test: 29.58%
Epoch: 675, Loss: 1.4188, Train: 29.59%, Valid: 29.45%, Test: 29.66%
Epoch: 700, Loss: 1.4141, Train: 29.56%, Valid: 29.59%, Test: 29.72%
Epoch: 725, Loss: 1.4165, Train: 28.91%, Valid: 28.94%, Test: 29.12%
Epoch: 750, Loss: 1.4202, Train: 31.04%, Valid: 31.03%, Test: 31.27%
Epoch: 775, Loss: 1.4163, Train: 30.17%, Valid: 30.23%, Test: 30.36%
Epoch: 800, Loss: 1.4130, Train: 30.24%, Valid: 30.35%, Test: 30.52%
Epoch: 825, Loss: 1.4095, Train: 30.19%, Valid: 30.22%, Test: 30.44%
Epoch: 850, Loss: 1.4094, Train: 29.52%, Valid: 29.49%, Test: 29.61%
Epoch: 875, Loss: 1.4079, Train: 29.70%, Valid: 29.62%, Test: 29.95%
Epoch: 900, Loss: 1.4037, Train: 29.16%, Valid: 29.17%, Test: 29.29%
Epoch: 925, Loss: 1.4031, Train: 30.29%, Valid: 30.21%, Test: 30.52%
Epoch: 950, Loss: 1.3946, Train: 30.02%, Valid: 30.03%, Test: 30.21%
Epoch: 975, Loss: 1.4022, Train: 29.53%, Valid: 29.40%, Test: 29.65%
Run 01:
Highest Train: 33.11
Highest Valid: 32.97
  Final Train: 33.11
   Final Test: 33.68
All runs:
Highest Train: 33.11, nan
Highest Valid: 32.97, nan
  Final Train: 33.11, nan
   Final Test: 33.68, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6032, Train: 28.73%, Valid: 28.57%, Test: 28.86%
Epoch: 25, Loss: 1.5140, Train: 29.57%, Valid: 29.49%, Test: 29.77%
Epoch: 50, Loss: 1.4417, Train: 37.02%, Valid: 36.57%, Test: 37.12%
Epoch: 75, Loss: 1.4060, Train: 39.24%, Valid: 38.74%, Test: 38.98%
Epoch: 100, Loss: 1.3856, Train: 40.24%, Valid: 39.68%, Test: 39.95%
Epoch: 125, Loss: 1.3638, Train: 41.23%, Valid: 40.49%, Test: 40.73%
Epoch: 150, Loss: 1.3658, Train: 41.00%, Valid: 40.42%, Test: 40.43%
Epoch: 175, Loss: 1.3409, Train: 42.39%, Valid: 41.56%, Test: 41.78%
Epoch: 200, Loss: 1.3323, Train: 43.23%, Valid: 42.46%, Test: 42.61%
Epoch: 225, Loss: 1.3161, Train: 44.46%, Valid: 43.59%, Test: 43.85%
Epoch: 250, Loss: 1.2968, Train: 45.11%, Valid: 43.73%, Test: 44.09%
Epoch: 275, Loss: 1.2959, Train: 44.66%, Valid: 43.30%, Test: 43.36%
Epoch: 300, Loss: 1.2776, Train: 46.62%, Valid: 45.07%, Test: 45.30%
Epoch: 325, Loss: 1.2644, Train: 46.98%, Valid: 45.24%, Test: 45.33%
Epoch: 350, Loss: 1.2570, Train: 47.44%, Valid: 45.44%, Test: 45.59%
Epoch: 375, Loss: 1.2435, Train: 48.11%, Valid: 45.93%, Test: 45.98%
Epoch: 400, Loss: 1.2411, Train: 48.06%, Valid: 45.84%, Test: 45.95%
Epoch: 425, Loss: 1.2243, Train: 48.51%, Valid: 46.28%, Test: 46.13%
Epoch: 450, Loss: 1.2491, Train: 48.00%, Valid: 46.09%, Test: 45.93%
Epoch: 475, Loss: 1.2503, Train: 47.65%, Valid: 45.54%, Test: 45.69%
Epoch: 500, Loss: 1.2147, Train: 49.10%, Valid: 46.42%, Test: 46.56%
Epoch: 525, Loss: 1.2468, Train: 47.76%, Valid: 45.75%, Test: 45.61%
Epoch: 550, Loss: 1.2095, Train: 49.27%, Valid: 46.55%, Test: 46.70%
Epoch: 575, Loss: 1.2086, Train: 49.47%, Valid: 46.62%, Test: 46.66%
Epoch: 600, Loss: 1.2468, Train: 48.39%, Valid: 45.76%, Test: 45.93%
Epoch: 625, Loss: 1.1899, Train: 50.13%, Valid: 46.86%, Test: 47.03%
Epoch: 650, Loss: 1.1930, Train: 50.07%, Valid: 47.06%, Test: 47.22%
Epoch: 675, Loss: 1.1819, Train: 50.35%, Valid: 46.98%, Test: 47.13%
Epoch: 700, Loss: 1.1825, Train: 50.30%, Valid: 46.98%, Test: 47.00%
Epoch: 725, Loss: 1.1786, Train: 50.65%, Valid: 47.25%, Test: 47.22%
Epoch: 750, Loss: 1.1749, Train: 50.69%, Valid: 47.28%, Test: 47.35%
Epoch: 775, Loss: 1.1709, Train: 50.68%, Valid: 47.14%, Test: 47.20%
Epoch: 800, Loss: 1.1695, Train: 50.87%, Valid: 47.03%, Test: 47.32%
Epoch: 825, Loss: 1.1940, Train: 49.91%, Valid: 46.85%, Test: 47.10%
Epoch: 850, Loss: 1.1690, Train: 50.80%, Valid: 47.20%, Test: 47.61%
Epoch: 875, Loss: 1.2064, Train: 49.21%, Valid: 46.36%, Test: 46.40%
Epoch: 900, Loss: 1.1588, Train: 51.17%, Valid: 47.27%, Test: 47.58%
Epoch: 925, Loss: 1.1981, Train: 49.76%, Valid: 46.31%, Test: 46.80%
Epoch: 950, Loss: 1.1523, Train: 51.46%, Valid: 47.52%, Test: 47.90%
Epoch: 975, Loss: 1.1455, Train: 51.67%, Valid: 47.55%, Test: 47.89%
Run 01:
Highest Train: 51.76
Highest Valid: 47.67
  Final Train: 51.49
   Final Test: 47.70
All runs:
Highest Train: 51.76, nan
Highest Valid: 47.67, nan
  Final Train: 51.49, nan
   Final Test: 47.70, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.9527, Train: 24.04%, Valid: 23.82%, Test: 23.61%
Epoch: 25, Loss: 1.4862, Train: 26.78%, Valid: 26.42%, Test: 26.66%
Epoch: 50, Loss: 1.4567, Train: 28.97%, Valid: 28.68%, Test: 29.02%
Epoch: 75, Loss: 1.4428, Train: 28.77%, Valid: 28.54%, Test: 28.85%
Epoch: 100, Loss: 1.4396, Train: 29.18%, Valid: 28.84%, Test: 29.19%
Epoch: 125, Loss: 1.4235, Train: 29.43%, Valid: 29.22%, Test: 29.41%
Epoch: 150, Loss: 1.4126, Train: 29.36%, Valid: 29.37%, Test: 29.37%
Epoch: 175, Loss: 1.4220, Train: 29.51%, Valid: 29.44%, Test: 29.62%
Epoch: 200, Loss: 1.4018, Train: 37.24%, Valid: 37.39%, Test: 37.37%
Epoch: 225, Loss: 1.3770, Train: 40.46%, Valid: 40.52%, Test: 40.26%
Epoch: 250, Loss: 1.9502, Train: 26.95%, Valid: 26.66%, Test: 27.56%
Epoch: 275, Loss: 1.4507, Train: 28.04%, Valid: 27.86%, Test: 28.15%
Epoch: 300, Loss: 1.4138, Train: 40.27%, Valid: 40.02%, Test: 40.35%
Epoch: 325, Loss: 1.3882, Train: 41.08%, Valid: 40.81%, Test: 41.06%
Epoch: 350, Loss: 1.3882, Train: 41.16%, Valid: 40.81%, Test: 40.97%
Epoch: 375, Loss: 1.3651, Train: 41.68%, Valid: 41.54%, Test: 41.70%
Epoch: 400, Loss: 1.3547, Train: 42.23%, Valid: 42.08%, Test: 42.23%
Epoch: 425, Loss: 1.3461, Train: 42.60%, Valid: 42.44%, Test: 42.58%
Epoch: 450, Loss: 1.3632, Train: 42.54%, Valid: 42.24%, Test: 42.37%
Epoch: 475, Loss: 1.3359, Train: 42.63%, Valid: 42.31%, Test: 42.58%
Epoch: 500, Loss: 1.3235, Train: 42.73%, Valid: 42.53%, Test: 42.74%
Epoch: 525, Loss: 1.3130, Train: 43.03%, Valid: 42.82%, Test: 43.05%
Epoch: 550, Loss: 1.3153, Train: 43.17%, Valid: 42.70%, Test: 42.99%
Epoch: 575, Loss: 1.3029, Train: 43.47%, Valid: 43.12%, Test: 43.38%
Epoch: 600, Loss: 1.2961, Train: 43.72%, Valid: 43.48%, Test: 43.61%
Epoch: 625, Loss: 1.3916, Train: 42.38%, Valid: 42.25%, Test: 42.19%
Epoch: 650, Loss: 1.3255, Train: 43.13%, Valid: 42.87%, Test: 43.18%
Epoch: 675, Loss: 1.3094, Train: 43.34%, Valid: 43.09%, Test: 43.42%
Epoch: 700, Loss: 1.3011, Train: 43.56%, Valid: 43.36%, Test: 43.53%
Epoch: 725, Loss: 1.2954, Train: 43.80%, Valid: 43.47%, Test: 43.66%
Epoch: 750, Loss: 1.2911, Train: 43.91%, Valid: 43.67%, Test: 43.82%
Epoch: 775, Loss: 1.2875, Train: 44.16%, Valid: 43.94%, Test: 44.05%
Epoch: 800, Loss: 1.3039, Train: 44.41%, Valid: 44.24%, Test: 44.23%
Epoch: 825, Loss: 1.2841, Train: 44.32%, Valid: 43.97%, Test: 44.20%
Epoch: 850, Loss: 1.2797, Train: 44.57%, Valid: 44.35%, Test: 44.50%
Epoch: 875, Loss: 1.2764, Train: 44.75%, Valid: 44.54%, Test: 44.59%
Epoch: 900, Loss: 1.2878, Train: 44.04%, Valid: 44.03%, Test: 44.12%
Epoch: 925, Loss: 1.2750, Train: 44.60%, Valid: 44.40%, Test: 44.61%
Epoch: 950, Loss: 1.2714, Train: 45.00%, Valid: 44.72%, Test: 44.88%
Epoch: 975, Loss: 1.2723, Train: 44.99%, Valid: 44.68%, Test: 44.76%
Run 01:
Highest Train: 45.14
Highest Valid: 44.93
  Final Train: 45.14
   Final Test: 45.00
All runs:
Highest Train: 45.14, nan
Highest Valid: 44.93, nan
  Final Train: 45.14, nan
   Final Test: 45.00, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 6.1462, Train: 31.83%, Valid: 31.73%, Test: 32.22%
Epoch: 25, Loss: 1.4973, Train: 32.70%, Valid: 32.47%, Test: 33.23%
Epoch: 50, Loss: 1.4539, Train: 33.59%, Valid: 33.37%, Test: 34.13%
Epoch: 75, Loss: 1.4310, Train: 33.92%, Valid: 33.85%, Test: 34.54%
Epoch: 100, Loss: 1.4273, Train: 34.14%, Valid: 33.91%, Test: 34.67%
Epoch: 125, Loss: 1.4360, Train: 33.99%, Valid: 33.95%, Test: 34.60%
Epoch: 150, Loss: 1.4107, Train: 34.32%, Valid: 34.26%, Test: 34.96%
Epoch: 175, Loss: 1.3952, Train: 35.39%, Valid: 35.46%, Test: 36.11%
Epoch: 200, Loss: 1.4695, Train: 33.72%, Valid: 33.52%, Test: 34.15%
Epoch: 225, Loss: 1.4407, Train: 34.12%, Valid: 34.07%, Test: 34.68%
Epoch: 250, Loss: 1.4052, Train: 29.48%, Valid: 29.40%, Test: 29.59%
Epoch: 275, Loss: 1.3929, Train: 40.92%, Valid: 40.88%, Test: 40.62%
Epoch: 300, Loss: 1.4071, Train: 40.08%, Valid: 39.88%, Test: 39.89%
Epoch: 325, Loss: 1.3859, Train: 40.76%, Valid: 40.47%, Test: 40.55%
Epoch: 350, Loss: 1.3737, Train: 41.03%, Valid: 40.81%, Test: 40.98%
Epoch: 375, Loss: 1.3718, Train: 41.03%, Valid: 40.68%, Test: 40.75%
Epoch: 400, Loss: 1.3771, Train: 39.92%, Valid: 39.45%, Test: 39.53%
Epoch: 425, Loss: 1.3680, Train: 40.49%, Valid: 40.33%, Test: 40.19%
Epoch: 450, Loss: 1.3617, Train: 41.31%, Valid: 41.22%, Test: 41.18%
Epoch: 475, Loss: 1.3548, Train: 42.05%, Valid: 41.94%, Test: 41.88%
Epoch: 500, Loss: 1.3481, Train: 42.49%, Valid: 42.36%, Test: 42.35%
Epoch: 525, Loss: 1.3408, Train: 42.58%, Valid: 42.66%, Test: 42.55%
Epoch: 550, Loss: 1.3916, Train: 39.38%, Valid: 39.03%, Test: 39.00%
Epoch: 575, Loss: 1.3704, Train: 39.78%, Valid: 39.53%, Test: 39.47%
Epoch: 600, Loss: 1.3608, Train: 40.63%, Valid: 40.42%, Test: 40.44%
Epoch: 625, Loss: 1.3544, Train: 41.50%, Valid: 41.28%, Test: 41.12%
Epoch: 650, Loss: 1.3489, Train: 42.32%, Valid: 42.23%, Test: 42.25%
Epoch: 675, Loss: 1.3436, Train: 42.70%, Valid: 42.51%, Test: 42.60%
Epoch: 700, Loss: 1.3427, Train: 42.41%, Valid: 42.22%, Test: 42.26%
Epoch: 725, Loss: 1.3364, Train: 42.69%, Valid: 42.53%, Test: 42.59%
Epoch: 750, Loss: 1.3390, Train: 42.94%, Valid: 42.69%, Test: 42.78%
Epoch: 775, Loss: 1.3386, Train: 42.57%, Valid: 42.11%, Test: 42.38%
Epoch: 800, Loss: 1.3290, Train: 42.80%, Valid: 42.69%, Test: 42.93%
Epoch: 825, Loss: 1.3226, Train: 43.00%, Valid: 42.88%, Test: 43.21%
Epoch: 850, Loss: 1.4129, Train: 41.64%, Valid: 41.30%, Test: 41.77%
Epoch: 875, Loss: 1.3433, Train: 42.37%, Valid: 41.90%, Test: 42.18%
Epoch: 900, Loss: 1.3320, Train: 42.54%, Valid: 42.23%, Test: 42.49%
Epoch: 925, Loss: 1.3246, Train: 42.83%, Valid: 42.67%, Test: 43.00%
Epoch: 950, Loss: 1.3185, Train: 43.07%, Valid: 42.92%, Test: 43.33%
Epoch: 975, Loss: 1.4888, Train: 40.95%, Valid: 40.51%, Test: 40.80%
Run 01:
Highest Train: 43.19
Highest Valid: 43.01
  Final Train: 43.19
   Final Test: 43.41
All runs:
Highest Train: 43.19, nan
Highest Valid: 43.01, nan
  Final Train: 43.19, nan
   Final Test: 43.41, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6181, Train: 29.21%, Valid: 28.93%, Test: 29.36%
Epoch: 25, Loss: 1.5111, Train: 34.09%, Valid: 33.78%, Test: 34.05%
Epoch: 50, Loss: 1.4762, Train: 36.24%, Valid: 35.95%, Test: 36.20%
Epoch: 75, Loss: 1.4572, Train: 37.48%, Valid: 36.97%, Test: 37.44%
Epoch: 100, Loss: 1.4404, Train: 38.97%, Valid: 38.14%, Test: 38.49%
Epoch: 125, Loss: 1.4242, Train: 40.19%, Valid: 39.41%, Test: 39.79%
Epoch: 150, Loss: 1.4131, Train: 41.45%, Valid: 40.71%, Test: 40.91%
Epoch: 175, Loss: 1.4081, Train: 41.59%, Valid: 40.92%, Test: 41.00%
Epoch: 200, Loss: 1.3913, Train: 42.08%, Valid: 41.27%, Test: 41.52%
Epoch: 225, Loss: 1.3895, Train: 42.45%, Valid: 41.82%, Test: 41.84%
Epoch: 250, Loss: 1.3749, Train: 42.72%, Valid: 41.82%, Test: 42.01%
Epoch: 275, Loss: 1.3689, Train: 42.40%, Valid: 41.49%, Test: 41.71%
Epoch: 300, Loss: 1.3662, Train: 43.38%, Valid: 42.39%, Test: 42.49%
Epoch: 325, Loss: 1.3604, Train: 43.01%, Valid: 41.97%, Test: 42.10%
Epoch: 350, Loss: 1.3512, Train: 42.02%, Valid: 40.99%, Test: 41.02%
Epoch: 375, Loss: 1.3428, Train: 43.92%, Valid: 42.56%, Test: 42.76%
Epoch: 400, Loss: 1.3408, Train: 42.85%, Valid: 41.41%, Test: 41.55%
Epoch: 425, Loss: 1.3375, Train: 44.43%, Valid: 42.93%, Test: 42.91%
Epoch: 450, Loss: 1.3340, Train: 44.19%, Valid: 42.72%, Test: 42.74%
Epoch: 475, Loss: 1.3401, Train: 42.67%, Valid: 41.65%, Test: 41.46%
Epoch: 500, Loss: 1.3399, Train: 44.71%, Valid: 43.35%, Test: 43.62%
Epoch: 525, Loss: 1.3274, Train: 41.71%, Valid: 40.52%, Test: 40.57%
Epoch: 550, Loss: 1.3211, Train: 41.96%, Valid: 40.49%, Test: 40.55%
Epoch: 575, Loss: 1.3269, Train: 43.85%, Valid: 42.17%, Test: 42.53%
Epoch: 600, Loss: 1.3251, Train: 43.52%, Valid: 42.22%, Test: 42.36%
Epoch: 625, Loss: 1.3243, Train: 44.64%, Valid: 43.40%, Test: 43.49%
Epoch: 650, Loss: 1.3195, Train: 44.97%, Valid: 43.57%, Test: 43.75%
Epoch: 675, Loss: 1.3217, Train: 43.19%, Valid: 42.09%, Test: 42.05%
Epoch: 700, Loss: 1.3285, Train: 42.22%, Valid: 41.04%, Test: 41.35%
Epoch: 725, Loss: 1.3162, Train: 44.96%, Valid: 43.25%, Test: 43.84%
Epoch: 750, Loss: 1.3103, Train: 40.63%, Valid: 39.74%, Test: 39.84%
Epoch: 775, Loss: 1.3137, Train: 44.25%, Valid: 42.70%, Test: 42.88%
Epoch: 800, Loss: 1.3153, Train: 43.90%, Valid: 42.40%, Test: 42.72%
Epoch: 825, Loss: 1.3216, Train: 44.64%, Valid: 43.10%, Test: 43.47%
Epoch: 850, Loss: 1.3105, Train: 44.74%, Valid: 43.38%, Test: 43.45%
Epoch: 875, Loss: 1.3068, Train: 41.58%, Valid: 40.51%, Test: 40.59%
Epoch: 900, Loss: 1.3169, Train: 44.91%, Valid: 43.32%, Test: 43.47%
Epoch: 925, Loss: 1.3076, Train: 41.69%, Valid: 40.47%, Test: 40.47%
Epoch: 950, Loss: 1.3182, Train: 39.83%, Valid: 38.85%, Test: 39.04%
Epoch: 975, Loss: 1.3148, Train: 44.24%, Valid: 42.91%, Test: 42.97%
Run 01:
Highest Train: 46.22
Highest Valid: 44.61
  Final Train: 46.22
   Final Test: 44.80
All runs:
Highest Train: 46.22, nan
Highest Valid: 44.61, nan
  Final Train: 46.22, nan
   Final Test: 44.80, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6862, Train: 22.84%, Valid: 22.75%, Test: 22.83%
Epoch: 25, Loss: 1.5284, Train: 31.28%, Valid: 31.05%, Test: 31.02%
Epoch: 50, Loss: 1.4819, Train: 28.78%, Valid: 28.59%, Test: 28.89%
Epoch: 75, Loss: 1.4787, Train: 29.13%, Valid: 29.03%, Test: 29.35%
Epoch: 100, Loss: 1.4674, Train: 29.93%, Valid: 29.88%, Test: 30.34%
Epoch: 125, Loss: 1.4459, Train: 30.15%, Valid: 30.07%, Test: 30.47%
Epoch: 150, Loss: 1.4500, Train: 30.26%, Valid: 29.97%, Test: 30.57%
Epoch: 175, Loss: 1.4417, Train: 30.50%, Valid: 30.27%, Test: 30.79%
Epoch: 200, Loss: 1.4510, Train: 31.53%, Valid: 31.25%, Test: 32.00%
Epoch: 225, Loss: 1.4508, Train: 31.23%, Valid: 30.90%, Test: 31.70%
Epoch: 250, Loss: 1.4395, Train: 31.72%, Valid: 31.37%, Test: 32.32%
Epoch: 275, Loss: 1.4358, Train: 32.79%, Valid: 32.40%, Test: 33.35%
Epoch: 300, Loss: 1.4332, Train: 32.92%, Valid: 32.67%, Test: 33.45%
Epoch: 325, Loss: 1.4360, Train: 32.33%, Valid: 31.91%, Test: 32.87%
Epoch: 350, Loss: 1.4584, Train: 31.72%, Valid: 31.21%, Test: 32.23%
Epoch: 375, Loss: 1.4310, Train: 33.61%, Valid: 33.01%, Test: 33.74%
Epoch: 400, Loss: 1.4428, Train: 32.54%, Valid: 32.21%, Test: 33.08%
Epoch: 425, Loss: 1.4244, Train: 33.85%, Valid: 33.57%, Test: 34.25%
Epoch: 450, Loss: 1.4411, Train: 33.68%, Valid: 33.13%, Test: 33.99%
Epoch: 475, Loss: 1.4250, Train: 34.33%, Valid: 34.13%, Test: 34.69%
Epoch: 500, Loss: 1.4414, Train: 33.54%, Valid: 33.15%, Test: 34.04%
Epoch: 525, Loss: 1.4220, Train: 35.72%, Valid: 35.18%, Test: 36.17%
Epoch: 550, Loss: 1.4326, Train: 34.72%, Valid: 34.48%, Test: 35.34%
Epoch: 575, Loss: 1.4107, Train: 36.26%, Valid: 35.78%, Test: 36.75%
Epoch: 600, Loss: 1.4273, Train: 34.59%, Valid: 34.03%, Test: 34.90%
Epoch: 625, Loss: 1.4511, Train: 34.82%, Valid: 34.52%, Test: 35.57%
Epoch: 650, Loss: 1.4102, Train: 36.43%, Valid: 35.83%, Test: 36.67%
Epoch: 675, Loss: 1.4073, Train: 30.81%, Valid: 30.26%, Test: 31.50%
Epoch: 700, Loss: 1.4185, Train: 34.79%, Valid: 34.28%, Test: 35.08%
Epoch: 725, Loss: 1.4237, Train: 35.69%, Valid: 35.12%, Test: 35.98%
Epoch: 750, Loss: 1.4099, Train: 35.67%, Valid: 34.99%, Test: 36.25%
Epoch: 775, Loss: 1.4196, Train: 36.09%, Valid: 35.45%, Test: 36.78%
Epoch: 800, Loss: 1.4018, Train: 36.59%, Valid: 35.96%, Test: 37.17%
Epoch: 825, Loss: 1.4219, Train: 32.87%, Valid: 32.54%, Test: 33.63%
Epoch: 850, Loss: 1.4298, Train: 35.27%, Valid: 34.76%, Test: 35.44%
Epoch: 875, Loss: 1.4155, Train: 35.61%, Valid: 35.14%, Test: 36.08%
Epoch: 900, Loss: 1.4149, Train: 34.98%, Valid: 34.60%, Test: 35.67%
Epoch: 925, Loss: 1.4178, Train: 34.52%, Valid: 34.09%, Test: 34.68%
Epoch: 950, Loss: 1.4184, Train: 31.90%, Valid: 31.39%, Test: 32.00%
Epoch: 975, Loss: 1.4235, Train: 29.62%, Valid: 29.49%, Test: 30.14%
Run 01:
Highest Train: 37.77
Highest Valid: 37.31
  Final Train: 37.77
   Final Test: 38.06
All runs:
Highest Train: 37.77, nan
Highest Valid: 37.31, nan
  Final Train: 37.77, nan
   Final Test: 38.06, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 6.3693, Train: 12.13%, Valid: 12.17%, Test: 12.13%
Epoch: 25, Loss: 1.7113, Train: 28.06%, Valid: 27.90%, Test: 28.14%
Epoch: 50, Loss: 1.7358, Train: 28.63%, Valid: 28.57%, Test: 28.73%
Epoch: 75, Loss: 1.5356, Train: 30.69%, Valid: 30.53%, Test: 30.92%
Epoch: 100, Loss: 1.4713, Train: 29.44%, Valid: 29.33%, Test: 29.56%
Epoch: 125, Loss: 1.4729, Train: 29.77%, Valid: 29.48%, Test: 29.93%
Epoch: 150, Loss: 1.4695, Train: 29.32%, Valid: 29.14%, Test: 29.41%
Epoch: 175, Loss: 1.4676, Train: 30.33%, Valid: 30.01%, Test: 30.54%
Epoch: 200, Loss: 1.5247, Train: 30.06%, Valid: 29.89%, Test: 30.32%
Epoch: 225, Loss: 1.4728, Train: 30.22%, Valid: 29.91%, Test: 30.34%
Epoch: 250, Loss: 1.4509, Train: 30.55%, Valid: 30.27%, Test: 30.73%
Epoch: 275, Loss: 1.4466, Train: 30.16%, Valid: 29.95%, Test: 30.35%
Epoch: 300, Loss: 1.4481, Train: 30.17%, Valid: 29.89%, Test: 30.33%
Epoch: 325, Loss: 1.4499, Train: 31.38%, Valid: 30.92%, Test: 31.48%
Epoch: 350, Loss: 1.4420, Train: 31.11%, Valid: 30.66%, Test: 31.20%
Epoch: 375, Loss: 1.4449, Train: 29.78%, Valid: 29.65%, Test: 29.86%
Epoch: 400, Loss: 1.4429, Train: 30.34%, Valid: 30.08%, Test: 30.49%
Epoch: 425, Loss: 1.4396, Train: 31.46%, Valid: 31.03%, Test: 31.50%
Epoch: 450, Loss: 1.4382, Train: 30.96%, Valid: 30.65%, Test: 31.07%
Epoch: 475, Loss: 1.4441, Train: 31.62%, Valid: 31.12%, Test: 31.71%
Epoch: 500, Loss: 1.4379, Train: 30.62%, Valid: 30.30%, Test: 30.79%
Epoch: 525, Loss: 1.4400, Train: 29.56%, Valid: 29.38%, Test: 29.62%
Epoch: 550, Loss: 1.4432, Train: 30.81%, Valid: 30.48%, Test: 30.91%
Epoch: 575, Loss: 1.4444, Train: 30.60%, Valid: 29.98%, Test: 30.91%
Epoch: 600, Loss: 1.4440, Train: 29.63%, Valid: 29.32%, Test: 29.73%
Epoch: 625, Loss: 1.4342, Train: 29.49%, Valid: 29.15%, Test: 29.58%
Epoch: 650, Loss: 1.4421, Train: 29.53%, Valid: 29.17%, Test: 29.62%
Epoch: 675, Loss: 1.4300, Train: 30.00%, Valid: 29.61%, Test: 30.20%
Epoch: 700, Loss: 1.4383, Train: 30.43%, Valid: 30.09%, Test: 30.59%
Epoch: 725, Loss: 1.4312, Train: 30.90%, Valid: 30.56%, Test: 31.09%
Epoch: 750, Loss: 1.4309, Train: 30.55%, Valid: 30.30%, Test: 30.78%
Epoch: 775, Loss: 1.4286, Train: 30.56%, Valid: 30.28%, Test: 30.83%
Epoch: 800, Loss: 1.4384, Train: 30.70%, Valid: 30.48%, Test: 30.91%
Epoch: 825, Loss: 1.4269, Train: 30.66%, Valid: 30.42%, Test: 31.05%
Epoch: 850, Loss: 1.4224, Train: 33.01%, Valid: 32.87%, Test: 33.47%
Epoch: 875, Loss: 1.4219, Train: 31.90%, Valid: 31.71%, Test: 32.50%
Epoch: 900, Loss: 1.4237, Train: 33.54%, Valid: 33.31%, Test: 33.78%
Epoch: 925, Loss: 1.4281, Train: 32.90%, Valid: 32.79%, Test: 33.46%
Epoch: 950, Loss: 1.4209, Train: 33.41%, Valid: 33.27%, Test: 33.97%
Epoch: 975, Loss: 1.4205, Train: 32.61%, Valid: 32.53%, Test: 33.00%
Run 01:
Highest Train: 34.16
Highest Valid: 34.09
  Final Train: 34.13
   Final Test: 34.68
All runs:
Highest Train: 34.16, nan
Highest Valid: 34.09, nan
  Final Train: 34.13, nan
   Final Test: 34.68, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6160, Train: 29.29%, Valid: 28.98%, Test: 29.37%
Epoch: 25, Loss: 1.5257, Train: 29.04%, Valid: 28.77%, Test: 29.11%
Epoch: 50, Loss: 1.4791, Train: 36.32%, Valid: 35.97%, Test: 36.20%
Epoch: 75, Loss: 1.4495, Train: 38.39%, Valid: 37.76%, Test: 38.08%
Epoch: 100, Loss: 1.4205, Train: 39.61%, Valid: 38.89%, Test: 38.93%
Epoch: 125, Loss: 1.4002, Train: 41.88%, Valid: 41.19%, Test: 41.25%
Epoch: 150, Loss: 1.3836, Train: 42.26%, Valid: 41.31%, Test: 41.30%
Epoch: 175, Loss: 1.3663, Train: 43.21%, Valid: 42.43%, Test: 42.51%
Epoch: 200, Loss: 1.3540, Train: 43.74%, Valid: 43.18%, Test: 43.39%
Epoch: 225, Loss: 1.3560, Train: 43.24%, Valid: 42.25%, Test: 42.79%
Epoch: 250, Loss: 1.3343, Train: 44.03%, Valid: 43.21%, Test: 43.41%
Epoch: 275, Loss: 1.3433, Train: 43.50%, Valid: 42.78%, Test: 42.90%
Epoch: 300, Loss: 1.3324, Train: 43.19%, Valid: 42.22%, Test: 42.85%
Epoch: 325, Loss: 1.3225, Train: 43.89%, Valid: 43.20%, Test: 43.45%
Epoch: 350, Loss: 1.3256, Train: 44.07%, Valid: 43.19%, Test: 43.52%
Epoch: 375, Loss: 1.3186, Train: 43.26%, Valid: 42.35%, Test: 42.79%
Epoch: 400, Loss: 1.3357, Train: 43.58%, Valid: 42.78%, Test: 42.88%
Epoch: 425, Loss: 1.3147, Train: 43.39%, Valid: 42.54%, Test: 42.93%
Epoch: 450, Loss: 1.3232, Train: 44.39%, Valid: 43.75%, Test: 44.02%
Epoch: 475, Loss: 1.3160, Train: 44.19%, Valid: 43.44%, Test: 43.56%
Epoch: 500, Loss: 1.3225, Train: 44.44%, Valid: 43.55%, Test: 43.82%
Epoch: 525, Loss: 1.3059, Train: 43.79%, Valid: 43.07%, Test: 43.36%
Epoch: 550, Loss: 1.3077, Train: 44.21%, Valid: 43.42%, Test: 43.74%
Epoch: 575, Loss: 1.2965, Train: 45.04%, Valid: 44.19%, Test: 44.19%
Epoch: 600, Loss: 1.2989, Train: 44.79%, Valid: 43.95%, Test: 44.03%
Epoch: 625, Loss: 1.2997, Train: 45.26%, Valid: 44.45%, Test: 44.51%
Epoch: 650, Loss: 1.3021, Train: 44.76%, Valid: 43.99%, Test: 44.14%
Epoch: 675, Loss: 1.3160, Train: 44.58%, Valid: 43.88%, Test: 43.89%
Epoch: 700, Loss: 1.2966, Train: 44.52%, Valid: 43.64%, Test: 43.69%
Epoch: 725, Loss: 1.3037, Train: 44.96%, Valid: 44.12%, Test: 44.23%
Epoch: 750, Loss: 1.3051, Train: 44.66%, Valid: 43.90%, Test: 43.90%
Epoch: 775, Loss: 1.2950, Train: 44.73%, Valid: 44.05%, Test: 44.17%
Epoch: 800, Loss: 1.3166, Train: 44.87%, Valid: 43.96%, Test: 44.14%
Epoch: 825, Loss: 1.2980, Train: 45.14%, Valid: 44.27%, Test: 44.52%
Epoch: 850, Loss: 1.2859, Train: 45.61%, Valid: 44.72%, Test: 44.74%
Epoch: 875, Loss: 1.2919, Train: 44.64%, Valid: 43.94%, Test: 43.97%
Epoch: 900, Loss: 1.2946, Train: 44.68%, Valid: 43.87%, Test: 43.73%
Epoch: 925, Loss: 1.2885, Train: 45.00%, Valid: 43.98%, Test: 43.99%
Epoch: 950, Loss: 1.2912, Train: 44.87%, Valid: 43.94%, Test: 43.93%
Epoch: 975, Loss: 1.2968, Train: 45.29%, Valid: 44.16%, Test: 44.47%
Run 01:
Highest Train: 45.84
Highest Valid: 44.97
  Final Train: 45.82
   Final Test: 45.07
All runs:
Highest Train: 45.84, nan
Highest Valid: 44.97, nan
  Final Train: 45.82, nan
   Final Test: 45.07, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 2.0024, Train: 25.39%, Valid: 25.30%, Test: 25.36%
Epoch: 25, Loss: 1.5217, Train: 28.62%, Valid: 28.43%, Test: 28.69%
Epoch: 50, Loss: 1.4914, Train: 22.47%, Valid: 22.41%, Test: 22.57%
Epoch: 75, Loss: 1.4721, Train: 19.84%, Valid: 19.96%, Test: 19.77%
Epoch: 100, Loss: 1.4625, Train: 20.04%, Valid: 20.15%, Test: 19.91%
Epoch: 125, Loss: 1.4601, Train: 22.24%, Valid: 22.12%, Test: 22.21%
Epoch: 150, Loss: 1.4556, Train: 24.40%, Valid: 24.07%, Test: 24.44%
Epoch: 175, Loss: 1.4484, Train: 25.71%, Valid: 25.44%, Test: 25.71%
Epoch: 200, Loss: 1.4422, Train: 28.38%, Valid: 28.16%, Test: 28.41%
Epoch: 225, Loss: 1.4328, Train: 29.75%, Valid: 29.50%, Test: 29.75%
Epoch: 250, Loss: 1.4335, Train: 27.16%, Valid: 26.69%, Test: 27.16%
Epoch: 275, Loss: 1.4335, Train: 33.27%, Valid: 33.08%, Test: 33.14%
Epoch: 300, Loss: 1.4204, Train: 31.94%, Valid: 31.67%, Test: 31.91%
Epoch: 325, Loss: 1.4225, Train: 36.25%, Valid: 36.10%, Test: 36.20%
Epoch: 350, Loss: 1.4289, Train: 20.93%, Valid: 20.85%, Test: 21.13%
Epoch: 375, Loss: 1.4242, Train: 28.99%, Valid: 28.68%, Test: 29.37%
Epoch: 400, Loss: 1.4055, Train: 35.87%, Valid: 35.69%, Test: 35.76%
Epoch: 425, Loss: 1.4241, Train: 29.99%, Valid: 29.96%, Test: 30.23%
Epoch: 450, Loss: 1.4540, Train: 34.03%, Valid: 33.89%, Test: 34.15%
Epoch: 475, Loss: 1.4187, Train: 29.95%, Valid: 29.17%, Test: 30.10%
Epoch: 500, Loss: 1.4267, Train: 35.53%, Valid: 35.04%, Test: 35.84%
Epoch: 525, Loss: 1.4041, Train: 35.46%, Valid: 35.30%, Test: 35.31%
Epoch: 550, Loss: 1.4009, Train: 37.68%, Valid: 37.53%, Test: 37.39%
Epoch: 575, Loss: 1.3956, Train: 35.57%, Valid: 35.36%, Test: 35.30%
Epoch: 600, Loss: 1.4077, Train: 37.14%, Valid: 36.91%, Test: 36.92%
Epoch: 625, Loss: 1.3997, Train: 36.66%, Valid: 36.58%, Test: 36.32%
Epoch: 650, Loss: 1.3975, Train: 35.99%, Valid: 35.81%, Test: 35.75%
Epoch: 675, Loss: 1.4285, Train: 34.15%, Valid: 34.10%, Test: 34.22%
Epoch: 700, Loss: 1.4101, Train: 38.36%, Valid: 37.91%, Test: 38.44%
Epoch: 725, Loss: 1.3992, Train: 39.00%, Valid: 38.66%, Test: 38.70%
Epoch: 750, Loss: 1.4071, Train: 40.12%, Valid: 39.82%, Test: 40.08%
Epoch: 775, Loss: 1.3894, Train: 39.66%, Valid: 39.55%, Test: 39.40%
Epoch: 800, Loss: 1.3864, Train: 39.90%, Valid: 39.72%, Test: 39.97%
Epoch: 825, Loss: 1.3864, Train: 40.96%, Valid: 40.67%, Test: 41.17%
Epoch: 850, Loss: 1.4066, Train: 40.17%, Valid: 40.04%, Test: 40.30%
Epoch: 875, Loss: 1.3750, Train: 40.67%, Valid: 40.39%, Test: 40.71%
Epoch: 900, Loss: 1.3716, Train: 38.25%, Valid: 37.91%, Test: 38.50%
Epoch: 925, Loss: 1.3711, Train: 41.85%, Valid: 41.54%, Test: 41.78%
Epoch: 950, Loss: 1.3632, Train: 41.79%, Valid: 41.63%, Test: 41.83%
Epoch: 975, Loss: 1.3606, Train: 41.57%, Valid: 41.46%, Test: 41.49%
Run 01:
Highest Train: 42.65
Highest Valid: 42.35
  Final Train: 42.65
   Final Test: 42.75
All runs:
Highest Train: 42.65, nan
Highest Valid: 42.35, nan
  Final Train: 42.65, nan
   Final Test: 42.75, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 6.3589, Train: 26.55%, Valid: 26.33%, Test: 26.26%
Epoch: 25, Loss: 1.6700, Train: 11.90%, Valid: 11.95%, Test: 11.90%
Epoch: 50, Loss: 1.6464, Train: 20.98%, Valid: 20.93%, Test: 21.16%
Epoch: 75, Loss: 1.6315, Train: 13.70%, Valid: 13.70%, Test: 13.71%
Epoch: 100, Loss: 1.5326, Train: 23.82%, Valid: 23.61%, Test: 23.71%
Epoch: 125, Loss: 1.5114, Train: 27.52%, Valid: 27.36%, Test: 27.50%
Epoch: 150, Loss: 1.4684, Train: 26.59%, Valid: 26.48%, Test: 26.63%
Epoch: 175, Loss: 1.5015, Train: 20.24%, Valid: 20.31%, Test: 19.98%
Epoch: 200, Loss: 1.5098, Train: 28.55%, Valid: 28.35%, Test: 28.60%
Epoch: 225, Loss: 1.4814, Train: 28.50%, Valid: 28.29%, Test: 28.62%
Epoch: 250, Loss: 1.4592, Train: 28.24%, Valid: 28.01%, Test: 28.37%
Epoch: 275, Loss: 1.4664, Train: 28.61%, Valid: 28.38%, Test: 28.69%
Epoch: 300, Loss: 1.4601, Train: 28.40%, Valid: 28.18%, Test: 28.51%
Epoch: 325, Loss: 1.4513, Train: 28.56%, Valid: 28.35%, Test: 28.64%
Epoch: 350, Loss: 1.4591, Train: 28.66%, Valid: 28.46%, Test: 28.75%
Epoch: 375, Loss: 1.4549, Train: 28.65%, Valid: 28.44%, Test: 28.74%
Epoch: 400, Loss: 1.4445, Train: 28.73%, Valid: 28.54%, Test: 28.83%
Epoch: 425, Loss: 1.4440, Train: 28.73%, Valid: 28.53%, Test: 28.83%
Epoch: 450, Loss: 1.4444, Train: 28.72%, Valid: 28.53%, Test: 28.82%
Epoch: 475, Loss: 1.4389, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 500, Loss: 1.4415, Train: 28.73%, Valid: 28.54%, Test: 28.83%
Epoch: 525, Loss: 1.4396, Train: 28.71%, Valid: 28.52%, Test: 28.81%
Epoch: 550, Loss: 1.4405, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 575, Loss: 1.4484, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 600, Loss: 1.4389, Train: 28.66%, Valid: 28.49%, Test: 28.77%
Epoch: 625, Loss: 1.4412, Train: 28.71%, Valid: 28.53%, Test: 28.80%
Epoch: 650, Loss: 1.4356, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 675, Loss: 1.4372, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 700, Loss: 1.4421, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 725, Loss: 1.4399, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 750, Loss: 1.4298, Train: 28.40%, Valid: 28.27%, Test: 28.59%
Epoch: 775, Loss: 1.4320, Train: 28.07%, Valid: 27.82%, Test: 28.21%
Epoch: 800, Loss: 1.4344, Train: 28.79%, Valid: 28.65%, Test: 28.93%
Epoch: 825, Loss: 1.4322, Train: 27.32%, Valid: 27.11%, Test: 27.57%
Epoch: 850, Loss: 1.4352, Train: 26.95%, Valid: 26.81%, Test: 27.19%
Epoch: 875, Loss: 1.4442, Train: 27.72%, Valid: 27.54%, Test: 27.94%
Epoch: 900, Loss: 1.4275, Train: 27.53%, Valid: 27.27%, Test: 27.69%
Epoch: 925, Loss: 1.4379, Train: 28.74%, Valid: 28.56%, Test: 28.82%
Epoch: 950, Loss: 1.4277, Train: 28.73%, Valid: 28.56%, Test: 28.81%
Epoch: 975, Loss: 1.4299, Train: 28.77%, Valid: 28.64%, Test: 28.90%
Run 01:
Highest Train: 28.86
Highest Valid: 28.71
  Final Train: 28.78
   Final Test: 28.94
All runs:
Highest Train: 28.86, nan
Highest Valid: 28.71, nan
  Final Train: 28.78, nan
   Final Test: 28.94, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6207, Train: 22.16%, Valid: 22.17%, Test: 22.74%
Epoch: 25, Loss: 1.4798, Train: 35.00%, Valid: 34.62%, Test: 34.84%
Epoch: 50, Loss: 1.4482, Train: 36.65%, Valid: 36.25%, Test: 36.59%
Epoch: 75, Loss: 1.4173, Train: 38.21%, Valid: 37.58%, Test: 37.94%
Epoch: 100, Loss: 1.3878, Train: 39.87%, Valid: 39.00%, Test: 39.06%
Epoch: 125, Loss: 1.3638, Train: 40.34%, Valid: 39.56%, Test: 39.70%
Epoch: 150, Loss: 1.3407, Train: 42.05%, Valid: 40.88%, Test: 40.97%
Epoch: 175, Loss: 1.3213, Train: 42.15%, Valid: 41.06%, Test: 41.17%
Epoch: 200, Loss: 1.3105, Train: 43.42%, Valid: 42.01%, Test: 42.25%
Epoch: 225, Loss: 1.3044, Train: 43.72%, Valid: 42.16%, Test: 42.36%
Epoch: 250, Loss: 1.2853, Train: 44.35%, Valid: 42.52%, Test: 42.58%
Epoch: 275, Loss: 1.2701, Train: 45.22%, Valid: 43.10%, Test: 43.35%
Epoch: 300, Loss: 1.2620, Train: 45.41%, Valid: 43.31%, Test: 43.40%
Epoch: 325, Loss: 1.2564, Train: 45.50%, Valid: 43.08%, Test: 43.23%
Epoch: 350, Loss: 1.2464, Train: 46.02%, Valid: 43.52%, Test: 43.61%
Epoch: 375, Loss: 1.2438, Train: 46.52%, Valid: 43.86%, Test: 44.10%
Epoch: 400, Loss: 1.2476, Train: 46.89%, Valid: 43.99%, Test: 44.27%
Epoch: 425, Loss: 1.2298, Train: 46.90%, Valid: 44.18%, Test: 44.34%
Epoch: 450, Loss: 1.2271, Train: 47.50%, Valid: 44.25%, Test: 44.42%
Epoch: 475, Loss: 1.2181, Train: 47.74%, Valid: 44.37%, Test: 44.63%
Epoch: 500, Loss: 1.2122, Train: 47.68%, Valid: 44.26%, Test: 44.69%
Epoch: 525, Loss: 1.2048, Train: 48.00%, Valid: 44.38%, Test: 44.67%
Epoch: 550, Loss: 1.1999, Train: 48.17%, Valid: 44.42%, Test: 44.75%
Epoch: 575, Loss: 1.1976, Train: 48.10%, Valid: 44.16%, Test: 44.46%
Epoch: 600, Loss: 1.1983, Train: 48.79%, Valid: 44.66%, Test: 44.92%
Epoch: 625, Loss: 1.1939, Train: 48.36%, Valid: 44.36%, Test: 44.42%
Epoch: 650, Loss: 1.1921, Train: 48.66%, Valid: 44.43%, Test: 44.78%
Epoch: 675, Loss: 1.1891, Train: 49.03%, Valid: 44.65%, Test: 44.79%
Epoch: 700, Loss: 1.1816, Train: 49.24%, Valid: 44.97%, Test: 45.31%
Epoch: 725, Loss: 1.1782, Train: 49.04%, Valid: 44.57%, Test: 45.01%
Epoch: 750, Loss: 1.1744, Train: 49.52%, Valid: 44.80%, Test: 44.98%
Epoch: 775, Loss: 1.1758, Train: 49.07%, Valid: 44.75%, Test: 45.00%
Epoch: 800, Loss: 1.1692, Train: 49.93%, Valid: 45.39%, Test: 45.56%
Epoch: 825, Loss: 1.1733, Train: 49.59%, Valid: 44.81%, Test: 44.87%
Epoch: 850, Loss: 1.1654, Train: 49.90%, Valid: 45.03%, Test: 45.22%
Epoch: 875, Loss: 1.1624, Train: 49.91%, Valid: 44.93%, Test: 45.04%
Epoch: 900, Loss: 1.1589, Train: 50.27%, Valid: 45.37%, Test: 45.37%
Epoch: 925, Loss: 1.1818, Train: 49.82%, Valid: 44.86%, Test: 45.05%
Epoch: 950, Loss: 1.1536, Train: 50.49%, Valid: 45.56%, Test: 45.50%
Epoch: 975, Loss: 1.1555, Train: 50.54%, Valid: 45.70%, Test: 45.78%
Run 01:
Highest Train: 50.68
Highest Valid: 45.77
  Final Train: 50.60
   Final Test: 45.73
All runs:
Highest Train: 50.68, nan
Highest Valid: 45.77, nan
  Final Train: 50.60, nan
   Final Test: 45.73, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 2.1280, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4547, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 50, Loss: 1.4305, Train: 30.91%, Valid: 30.69%, Test: 31.00%
Epoch: 75, Loss: 1.4080, Train: 32.84%, Valid: 32.65%, Test: 32.94%
Epoch: 100, Loss: 1.3772, Train: 38.30%, Valid: 38.13%, Test: 38.20%
Epoch: 125, Loss: 1.3502, Train: 42.06%, Valid: 41.77%, Test: 42.03%
Epoch: 150, Loss: 1.3197, Train: 42.86%, Valid: 42.48%, Test: 42.74%
Epoch: 175, Loss: 1.3043, Train: 43.42%, Valid: 43.15%, Test: 43.42%
Epoch: 200, Loss: 1.2913, Train: 43.81%, Valid: 43.55%, Test: 43.95%
Epoch: 225, Loss: 1.2920, Train: 43.69%, Valid: 43.46%, Test: 43.81%
Epoch: 250, Loss: 1.2663, Train: 45.00%, Valid: 44.52%, Test: 44.67%
Epoch: 275, Loss: 1.2622, Train: 44.38%, Valid: 43.97%, Test: 44.20%
Epoch: 300, Loss: 1.2492, Train: 45.49%, Valid: 45.22%, Test: 45.22%
Epoch: 325, Loss: 1.2646, Train: 45.59%, Valid: 45.24%, Test: 45.04%
Epoch: 350, Loss: 1.2494, Train: 45.90%, Valid: 45.40%, Test: 45.43%
Epoch: 375, Loss: 1.2290, Train: 46.67%, Valid: 46.23%, Test: 46.19%
Epoch: 400, Loss: 1.2450, Train: 46.60%, Valid: 46.21%, Test: 45.98%
Epoch: 425, Loss: 1.2242, Train: 46.91%, Valid: 46.45%, Test: 46.44%
Epoch: 450, Loss: 1.2167, Train: 47.34%, Valid: 46.79%, Test: 46.80%
Epoch: 475, Loss: 1.2290, Train: 47.08%, Valid: 46.37%, Test: 46.35%
Epoch: 500, Loss: 1.2125, Train: 47.65%, Valid: 47.02%, Test: 46.91%
Epoch: 525, Loss: 1.2108, Train: 48.01%, Valid: 47.38%, Test: 47.21%
Epoch: 550, Loss: 1.2659, Train: 47.53%, Valid: 46.89%, Test: 46.87%
Epoch: 575, Loss: 1.2159, Train: 47.19%, Valid: 46.66%, Test: 46.55%
Epoch: 600, Loss: 1.2032, Train: 48.13%, Valid: 47.41%, Test: 47.28%
Epoch: 625, Loss: 1.1974, Train: 48.34%, Valid: 47.73%, Test: 47.44%
Epoch: 650, Loss: 1.2146, Train: 48.23%, Valid: 47.40%, Test: 47.54%
Epoch: 675, Loss: 1.1991, Train: 48.15%, Valid: 47.40%, Test: 47.63%
Epoch: 700, Loss: 1.1918, Train: 48.67%, Valid: 47.91%, Test: 47.92%
Epoch: 725, Loss: 1.2289, Train: 47.67%, Valid: 46.78%, Test: 47.13%
Epoch: 750, Loss: 1.2016, Train: 48.44%, Valid: 47.64%, Test: 47.81%
Epoch: 775, Loss: 1.1884, Train: 48.83%, Valid: 47.99%, Test: 48.06%
Epoch: 800, Loss: 1.1883, Train: 48.58%, Valid: 47.93%, Test: 47.96%
Epoch: 825, Loss: 1.2209, Train: 47.22%, Valid: 46.28%, Test: 46.41%
Epoch: 850, Loss: 1.1909, Train: 48.82%, Valid: 48.06%, Test: 48.02%
Epoch: 875, Loss: 1.1823, Train: 49.08%, Valid: 48.23%, Test: 48.31%
Epoch: 900, Loss: 1.1793, Train: 49.30%, Valid: 48.58%, Test: 48.48%
Epoch: 925, Loss: 1.1832, Train: 49.15%, Valid: 48.20%, Test: 48.31%
Epoch: 950, Loss: 1.1785, Train: 49.24%, Valid: 48.24%, Test: 48.33%
Epoch: 975, Loss: 1.2558, Train: 47.87%, Valid: 47.06%, Test: 47.18%
Run 01:
Highest Train: 49.52
Highest Valid: 48.69
  Final Train: 49.51
   Final Test: 48.74
All runs:
Highest Train: 49.52, nan
Highest Valid: 48.69, nan
  Final Train: 49.51, nan
   Final Test: 48.74, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 55.1842, Train: 17.64%, Valid: 17.78%, Test: 17.32%
Epoch: 25, Loss: 1.7043, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 50, Loss: 1.5134, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 75, Loss: 1.4974, Train: 28.74%, Valid: 28.56%, Test: 28.84%
Epoch: 100, Loss: 1.4881, Train: 28.73%, Valid: 28.56%, Test: 28.84%
Epoch: 125, Loss: 1.4809, Train: 28.73%, Valid: 28.54%, Test: 28.82%
Epoch: 150, Loss: 1.4746, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 175, Loss: 1.4688, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 200, Loss: 1.4638, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 225, Loss: 1.4595, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 250, Loss: 1.4556, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 275, Loss: 1.4520, Train: 28.72%, Valid: 28.55%, Test: 28.82%
Epoch: 300, Loss: 1.4485, Train: 28.72%, Valid: 28.55%, Test: 28.82%
Epoch: 325, Loss: 1.4452, Train: 28.72%, Valid: 28.55%, Test: 28.82%
Epoch: 350, Loss: 1.4420, Train: 28.72%, Valid: 28.56%, Test: 28.83%
Epoch: 375, Loss: 1.4390, Train: 28.72%, Valid: 28.57%, Test: 28.84%
Epoch: 400, Loss: 1.4359, Train: 28.75%, Valid: 28.57%, Test: 28.86%
Epoch: 425, Loss: 1.4328, Train: 28.75%, Valid: 28.59%, Test: 28.88%
Epoch: 450, Loss: 1.4297, Train: 28.78%, Valid: 28.63%, Test: 28.91%
Epoch: 475, Loss: 1.4265, Train: 28.83%, Valid: 28.70%, Test: 29.00%
Epoch: 500, Loss: 1.4233, Train: 28.92%, Valid: 28.81%, Test: 29.13%
Epoch: 525, Loss: 1.4199, Train: 29.12%, Valid: 28.99%, Test: 29.29%
Epoch: 550, Loss: 1.4164, Train: 29.38%, Valid: 29.28%, Test: 29.56%
Epoch: 575, Loss: 1.4130, Train: 29.73%, Valid: 29.61%, Test: 29.99%
Epoch: 600, Loss: 1.4095, Train: 30.13%, Valid: 30.05%, Test: 30.30%
Epoch: 625, Loss: 1.4062, Train: 30.37%, Valid: 30.27%, Test: 30.63%
Epoch: 650, Loss: 1.6172, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 675, Loss: 1.4528, Train: 31.06%, Valid: 30.84%, Test: 31.03%
Epoch: 700, Loss: 1.4404, Train: 31.19%, Valid: 31.03%, Test: 31.12%
Epoch: 725, Loss: 1.4322, Train: 28.78%, Valid: 28.59%, Test: 28.86%
Epoch: 750, Loss: 1.4277, Train: 28.73%, Valid: 28.53%, Test: 28.81%
Epoch: 775, Loss: 1.4244, Train: 28.73%, Valid: 28.52%, Test: 28.80%
Epoch: 800, Loss: 1.4212, Train: 28.72%, Valid: 28.52%, Test: 28.81%
Epoch: 825, Loss: 1.4174, Train: 28.72%, Valid: 28.53%, Test: 28.80%
Epoch: 850, Loss: 1.4130, Train: 28.71%, Valid: 28.53%, Test: 28.81%
Epoch: 875, Loss: 1.4081, Train: 28.83%, Valid: 28.70%, Test: 28.95%
Epoch: 900, Loss: 1.4031, Train: 29.01%, Valid: 28.91%, Test: 29.10%
Epoch: 925, Loss: 1.3978, Train: 29.14%, Valid: 28.98%, Test: 29.21%
Epoch: 950, Loss: 1.8125, Train: 22.35%, Valid: 22.61%, Test: 22.46%
Epoch: 975, Loss: 1.5203, Train: 29.10%, Valid: 29.03%, Test: 29.26%
Run 01:
Highest Train: 31.71
Highest Valid: 31.65
  Final Train: 31.71
   Final Test: 31.83
All runs:
Highest Train: 31.71, nan
Highest Valid: 31.65, nan
  Final Train: 31.71, nan
   Final Test: 31.83, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.5991, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4729, Train: 35.22%, Valid: 34.95%, Test: 35.25%
Epoch: 50, Loss: 1.4413, Train: 36.84%, Valid: 36.04%, Test: 36.55%
Epoch: 75, Loss: 1.4088, Train: 38.39%, Valid: 37.65%, Test: 37.88%
Epoch: 100, Loss: 1.3816, Train: 39.62%, Valid: 38.62%, Test: 38.96%
Epoch: 125, Loss: 1.3593, Train: 40.78%, Valid: 39.72%, Test: 40.11%
Epoch: 150, Loss: 1.3493, Train: 41.53%, Valid: 40.29%, Test: 40.62%
Epoch: 175, Loss: 1.3167, Train: 43.00%, Valid: 41.38%, Test: 41.60%
Epoch: 200, Loss: 1.3080, Train: 43.68%, Valid: 41.89%, Test: 42.43%
Epoch: 225, Loss: 1.2921, Train: 44.39%, Valid: 42.45%, Test: 42.76%
Epoch: 250, Loss: 1.2780, Train: 44.40%, Valid: 42.60%, Test: 42.70%
Epoch: 275, Loss: 1.2883, Train: 45.21%, Valid: 42.79%, Test: 43.03%
Epoch: 300, Loss: 1.2599, Train: 45.63%, Valid: 43.33%, Test: 43.32%
Epoch: 325, Loss: 1.2475, Train: 46.31%, Valid: 43.43%, Test: 43.68%
Epoch: 350, Loss: 1.2500, Train: 45.48%, Valid: 42.73%, Test: 42.84%
Epoch: 375, Loss: 1.2284, Train: 47.35%, Valid: 43.91%, Test: 44.24%
Epoch: 400, Loss: 1.2224, Train: 47.75%, Valid: 44.18%, Test: 44.33%
Epoch: 425, Loss: 1.2188, Train: 47.33%, Valid: 43.96%, Test: 44.20%
Epoch: 450, Loss: 1.2049, Train: 48.33%, Valid: 44.41%, Test: 44.57%
Epoch: 475, Loss: 1.1963, Train: 48.60%, Valid: 44.86%, Test: 45.03%
Epoch: 500, Loss: 1.2061, Train: 48.00%, Valid: 43.85%, Test: 44.13%
Epoch: 525, Loss: 1.2002, Train: 48.18%, Valid: 44.55%, Test: 44.71%
Epoch: 550, Loss: 1.1832, Train: 49.29%, Valid: 44.73%, Test: 45.02%
Epoch: 575, Loss: 1.1857, Train: 48.98%, Valid: 44.72%, Test: 44.85%
Epoch: 600, Loss: 1.1748, Train: 49.67%, Valid: 44.91%, Test: 45.32%
Epoch: 625, Loss: 1.1709, Train: 49.95%, Valid: 44.84%, Test: 45.29%
Epoch: 650, Loss: 1.1760, Train: 49.77%, Valid: 45.09%, Test: 45.47%
Epoch: 675, Loss: 1.1621, Train: 50.15%, Valid: 45.05%, Test: 45.22%
Epoch: 700, Loss: 1.1618, Train: 50.12%, Valid: 45.18%, Test: 45.43%
Epoch: 725, Loss: 1.1697, Train: 50.32%, Valid: 45.35%, Test: 45.70%
Epoch: 750, Loss: 1.1652, Train: 50.60%, Valid: 45.04%, Test: 45.40%
Epoch: 775, Loss: 1.1507, Train: 50.77%, Valid: 45.34%, Test: 45.75%
Epoch: 800, Loss: 1.1491, Train: 50.85%, Valid: 45.42%, Test: 45.88%
Epoch: 825, Loss: 1.1437, Train: 51.21%, Valid: 45.50%, Test: 45.99%
Epoch: 850, Loss: 1.1665, Train: 50.22%, Valid: 45.21%, Test: 45.48%
Epoch: 875, Loss: 1.1389, Train: 51.28%, Valid: 45.62%, Test: 46.20%
Epoch: 900, Loss: 1.1410, Train: 51.43%, Valid: 45.52%, Test: 46.09%
Epoch: 925, Loss: 1.1370, Train: 51.25%, Valid: 45.07%, Test: 45.61%
Epoch: 950, Loss: 1.1373, Train: 51.44%, Valid: 45.44%, Test: 45.90%
Epoch: 975, Loss: 1.1343, Train: 51.31%, Valid: 45.36%, Test: 45.90%
Run 01:
Highest Train: 51.99
Highest Valid: 45.85
  Final Train: 51.32
   Final Test: 46.21
All runs:
Highest Train: 51.99, nan
Highest Valid: 45.85, nan
  Final Train: 51.32, nan
   Final Test: 46.21, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 2.1212, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4558, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 50, Loss: 1.4292, Train: 30.73%, Valid: 30.49%, Test: 31.03%
Epoch: 75, Loss: 1.3961, Train: 36.44%, Valid: 36.04%, Test: 36.23%
Epoch: 100, Loss: 1.3696, Train: 39.83%, Valid: 39.71%, Test: 39.73%
Epoch: 125, Loss: 1.3473, Train: 41.41%, Valid: 41.21%, Test: 41.21%
Epoch: 150, Loss: 1.3313, Train: 41.93%, Valid: 41.65%, Test: 41.74%
Epoch: 175, Loss: 1.3486, Train: 41.54%, Valid: 41.27%, Test: 41.33%
Epoch: 200, Loss: 1.3128, Train: 42.68%, Valid: 42.35%, Test: 42.49%
Epoch: 225, Loss: 1.3014, Train: 42.95%, Valid: 42.82%, Test: 42.93%
Epoch: 250, Loss: 1.2999, Train: 43.20%, Valid: 43.03%, Test: 43.13%
Epoch: 275, Loss: 1.2864, Train: 43.86%, Valid: 43.66%, Test: 43.69%
Epoch: 300, Loss: 1.3158, Train: 44.21%, Valid: 43.88%, Test: 43.97%
Epoch: 325, Loss: 1.2735, Train: 44.47%, Valid: 44.20%, Test: 44.07%
Epoch: 350, Loss: 1.2637, Train: 44.87%, Valid: 44.59%, Test: 44.49%
Epoch: 375, Loss: 1.3001, Train: 42.76%, Valid: 42.40%, Test: 42.87%
Epoch: 400, Loss: 1.2704, Train: 44.60%, Valid: 44.35%, Test: 44.27%
Epoch: 425, Loss: 1.2575, Train: 45.11%, Valid: 44.78%, Test: 44.91%
Epoch: 450, Loss: 1.2502, Train: 45.53%, Valid: 45.14%, Test: 45.17%
Epoch: 475, Loss: 1.2697, Train: 44.27%, Valid: 44.08%, Test: 44.00%
Epoch: 500, Loss: 1.2523, Train: 45.44%, Valid: 45.05%, Test: 44.99%
Epoch: 525, Loss: 1.2435, Train: 45.90%, Valid: 45.50%, Test: 45.52%
Epoch: 550, Loss: 1.2376, Train: 46.12%, Valid: 45.78%, Test: 45.68%
Epoch: 575, Loss: 1.2583, Train: 45.80%, Valid: 45.44%, Test: 45.40%
Epoch: 600, Loss: 1.2357, Train: 46.30%, Valid: 45.92%, Test: 45.88%
Epoch: 625, Loss: 1.2409, Train: 46.50%, Valid: 46.07%, Test: 45.99%
Epoch: 650, Loss: 1.2273, Train: 46.69%, Valid: 46.23%, Test: 46.17%
Epoch: 675, Loss: 1.2285, Train: 46.92%, Valid: 46.46%, Test: 46.33%
Epoch: 700, Loss: 1.2218, Train: 46.88%, Valid: 46.57%, Test: 46.30%
Epoch: 725, Loss: 1.2612, Train: 46.23%, Valid: 45.76%, Test: 45.67%
Epoch: 750, Loss: 1.2173, Train: 47.36%, Valid: 46.88%, Test: 46.73%
Epoch: 775, Loss: 1.2247, Train: 46.07%, Valid: 45.57%, Test: 45.47%
Epoch: 800, Loss: 1.2580, Train: 45.23%, Valid: 44.69%, Test: 44.59%
Epoch: 825, Loss: 1.2328, Train: 46.66%, Valid: 46.37%, Test: 46.24%
Epoch: 850, Loss: 1.2210, Train: 47.12%, Valid: 46.82%, Test: 46.61%
Epoch: 875, Loss: 1.2140, Train: 47.36%, Valid: 47.04%, Test: 46.89%
Epoch: 900, Loss: 1.2256, Train: 47.07%, Valid: 46.62%, Test: 46.51%
Epoch: 925, Loss: 1.2071, Train: 47.75%, Valid: 47.26%, Test: 47.17%
Epoch: 950, Loss: 1.2085, Train: 47.75%, Valid: 47.35%, Test: 47.21%
Epoch: 975, Loss: 1.2032, Train: 47.99%, Valid: 47.52%, Test: 47.37%
Run 01:
Highest Train: 48.08
Highest Valid: 47.54
  Final Train: 48.08
   Final Test: 47.47
All runs:
Highest Train: 48.08, nan
Highest Valid: 47.54, nan
  Final Train: 48.08, nan
   Final Test: 47.47, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 8.1343, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 2.8444, Train: 17.83%, Valid: 17.76%, Test: 17.83%
Epoch: 50, Loss: 1.9697, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 75, Loss: 1.6899, Train: 30.38%, Valid: 30.30%, Test: 30.90%
Epoch: 100, Loss: 1.4876, Train: 33.01%, Valid: 32.93%, Test: 33.74%
Epoch: 125, Loss: 1.4445, Train: 33.44%, Valid: 33.32%, Test: 34.31%
Epoch: 150, Loss: 1.5656, Train: 21.73%, Valid: 21.72%, Test: 22.32%
Epoch: 175, Loss: 1.4864, Train: 29.43%, Valid: 29.26%, Test: 29.82%
Epoch: 200, Loss: 1.4443, Train: 33.05%, Valid: 32.91%, Test: 33.77%
Epoch: 225, Loss: 1.4333, Train: 33.32%, Valid: 33.19%, Test: 34.08%
Epoch: 250, Loss: 1.4199, Train: 33.79%, Valid: 33.68%, Test: 34.53%
Epoch: 275, Loss: 1.9632, Train: 28.72%, Valid: 28.54%, Test: 28.83%
Epoch: 300, Loss: 1.9364, Train: 32.30%, Valid: 32.04%, Test: 32.94%
Epoch: 325, Loss: 1.6876, Train: 33.98%, Valid: 33.79%, Test: 34.70%
Epoch: 350, Loss: 1.5643, Train: 34.28%, Valid: 34.03%, Test: 34.91%
Epoch: 375, Loss: 1.5173, Train: 34.12%, Valid: 34.01%, Test: 34.70%
Epoch: 400, Loss: 1.4908, Train: 34.19%, Valid: 34.05%, Test: 34.73%
Epoch: 425, Loss: 1.4721, Train: 34.33%, Valid: 34.16%, Test: 34.84%
Epoch: 450, Loss: 1.4573, Train: 34.49%, Valid: 34.36%, Test: 35.04%
Epoch: 475, Loss: 1.4454, Train: 34.65%, Valid: 34.51%, Test: 35.15%
Epoch: 500, Loss: 1.4356, Train: 34.81%, Valid: 34.69%, Test: 35.27%
Epoch: 525, Loss: 1.4271, Train: 35.04%, Valid: 34.82%, Test: 35.38%
Epoch: 550, Loss: 1.4195, Train: 35.19%, Valid: 34.95%, Test: 35.50%
Epoch: 575, Loss: 1.4125, Train: 35.29%, Valid: 35.04%, Test: 35.73%
Epoch: 600, Loss: 1.4061, Train: 35.47%, Valid: 35.16%, Test: 35.85%
Epoch: 625, Loss: 1.4007, Train: 35.63%, Valid: 35.27%, Test: 35.99%
Epoch: 650, Loss: 1.3973, Train: 35.65%, Valid: 35.32%, Test: 35.98%
Epoch: 675, Loss: 1.3947, Train: 35.73%, Valid: 35.43%, Test: 36.09%
Epoch: 700, Loss: 1.3913, Train: 35.78%, Valid: 35.49%, Test: 36.15%
Epoch: 725, Loss: 1.3863, Train: 35.92%, Valid: 35.51%, Test: 36.25%
Epoch: 750, Loss: 1.4539, Train: 39.03%, Valid: 38.80%, Test: 39.51%
Epoch: 775, Loss: 1.4146, Train: 40.03%, Valid: 39.66%, Test: 39.68%
Epoch: 800, Loss: 1.3779, Train: 40.03%, Valid: 39.54%, Test: 39.88%
Epoch: 825, Loss: 1.3692, Train: 40.63%, Valid: 40.21%, Test: 40.58%
Epoch: 850, Loss: 1.3654, Train: 40.72%, Valid: 40.38%, Test: 40.67%
Epoch: 875, Loss: 1.3634, Train: 40.76%, Valid: 40.41%, Test: 40.64%
Epoch: 900, Loss: 1.3619, Train: 40.81%, Valid: 40.46%, Test: 40.69%
Epoch: 925, Loss: 1.3606, Train: 40.85%, Valid: 40.48%, Test: 40.71%
Epoch: 950, Loss: 1.3594, Train: 40.93%, Valid: 40.49%, Test: 40.77%
Epoch: 975, Loss: 1.3583, Train: 40.93%, Valid: 40.53%, Test: 40.82%
Run 01:
Highest Train: 41.83
Highest Valid: 41.73
  Final Train: 41.83
   Final Test: 41.83
All runs:
Highest Train: 41.83, nan
Highest Valid: 41.73, nan
  Final Train: 41.83, nan
   Final Test: 41.83, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6154, Train: 28.72%, Valid: 28.56%, Test: 28.82%
Epoch: 25, Loss: 1.4867, Train: 35.14%, Valid: 34.82%, Test: 35.12%
Epoch: 50, Loss: 1.4528, Train: 36.96%, Valid: 36.32%, Test: 36.63%
Epoch: 75, Loss: 1.4241, Train: 38.38%, Valid: 37.31%, Test: 37.94%
Epoch: 100, Loss: 1.4042, Train: 39.59%, Valid: 38.56%, Test: 38.90%
Epoch: 125, Loss: 1.3825, Train: 40.61%, Valid: 39.55%, Test: 39.90%
Epoch: 150, Loss: 1.3726, Train: 40.71%, Valid: 39.72%, Test: 40.02%
Epoch: 175, Loss: 1.3641, Train: 42.03%, Valid: 41.07%, Test: 41.13%
Epoch: 200, Loss: 1.3581, Train: 41.66%, Valid: 40.81%, Test: 40.88%
Epoch: 225, Loss: 1.3396, Train: 43.01%, Valid: 42.01%, Test: 41.87%
Epoch: 250, Loss: 1.3307, Train: 43.45%, Valid: 42.28%, Test: 42.48%
Epoch: 275, Loss: 1.3318, Train: 43.55%, Valid: 42.53%, Test: 42.61%
Epoch: 300, Loss: 1.3238, Train: 44.03%, Valid: 42.94%, Test: 42.94%
Epoch: 325, Loss: 1.3198, Train: 43.53%, Valid: 42.54%, Test: 42.59%
Epoch: 350, Loss: 1.3165, Train: 43.91%, Valid: 42.92%, Test: 42.92%
Epoch: 375, Loss: 1.3152, Train: 43.79%, Valid: 42.61%, Test: 42.73%
Epoch: 400, Loss: 1.3103, Train: 44.67%, Valid: 43.50%, Test: 43.67%
Epoch: 425, Loss: 1.3105, Train: 44.35%, Valid: 43.33%, Test: 43.19%
Epoch: 450, Loss: 1.3022, Train: 44.82%, Valid: 43.58%, Test: 43.58%
Epoch: 475, Loss: 1.2983, Train: 45.02%, Valid: 43.70%, Test: 43.75%
Epoch: 500, Loss: 1.3211, Train: 44.93%, Valid: 43.35%, Test: 43.54%
Epoch: 525, Loss: 1.3009, Train: 45.12%, Valid: 44.03%, Test: 43.89%
Epoch: 550, Loss: 1.2948, Train: 45.14%, Valid: 43.85%, Test: 43.85%
Epoch: 575, Loss: 1.3032, Train: 44.81%, Valid: 43.39%, Test: 43.65%
Epoch: 600, Loss: 1.2995, Train: 44.71%, Valid: 43.62%, Test: 43.56%
Epoch: 625, Loss: 1.2912, Train: 45.51%, Valid: 44.25%, Test: 44.28%
Epoch: 650, Loss: 1.3073, Train: 44.98%, Valid: 43.64%, Test: 43.79%
Epoch: 675, Loss: 1.3007, Train: 45.91%, Valid: 44.56%, Test: 44.50%
Epoch: 700, Loss: 1.2880, Train: 45.15%, Valid: 43.92%, Test: 43.99%
Epoch: 725, Loss: 1.2901, Train: 45.87%, Valid: 44.65%, Test: 44.51%
Epoch: 750, Loss: 1.2937, Train: 45.32%, Valid: 44.07%, Test: 44.11%
Epoch: 775, Loss: 1.2827, Train: 45.60%, Valid: 44.24%, Test: 44.33%
Epoch: 800, Loss: 1.2840, Train: 46.04%, Valid: 44.57%, Test: 44.66%
Epoch: 825, Loss: 1.2890, Train: 45.52%, Valid: 44.25%, Test: 44.24%
Epoch: 850, Loss: 1.2824, Train: 45.81%, Valid: 44.39%, Test: 44.43%
Epoch: 875, Loss: 1.2944, Train: 45.06%, Valid: 44.02%, Test: 43.93%
Epoch: 900, Loss: 1.2869, Train: 45.91%, Valid: 44.64%, Test: 44.58%
Epoch: 925, Loss: 1.2861, Train: 44.94%, Valid: 43.89%, Test: 43.89%
Epoch: 950, Loss: 1.2831, Train: 46.40%, Valid: 44.98%, Test: 44.89%
Epoch: 975, Loss: 1.2813, Train: 45.97%, Valid: 44.67%, Test: 44.55%
Run 01:
Highest Train: 46.78
Highest Valid: 45.19
  Final Train: 46.78
   Final Test: 45.17
All runs:
Highest Train: 46.78, nan
Highest Valid: 45.19, nan
  Final Train: 46.78, nan
   Final Test: 45.17, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.9685, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4621, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 50, Loss: 1.4346, Train: 28.76%, Valid: 28.59%, Test: 28.86%
Epoch: 75, Loss: 1.4241, Train: 30.46%, Valid: 30.37%, Test: 30.44%
Epoch: 100, Loss: 1.4028, Train: 36.67%, Valid: 36.54%, Test: 36.53%
Epoch: 125, Loss: 1.3835, Train: 37.59%, Valid: 37.39%, Test: 37.54%
Epoch: 150, Loss: 1.3758, Train: 40.17%, Valid: 39.87%, Test: 40.13%
Epoch: 175, Loss: 1.3733, Train: 41.12%, Valid: 40.82%, Test: 41.02%
Epoch: 200, Loss: 1.3554, Train: 42.51%, Valid: 42.20%, Test: 42.28%
Epoch: 225, Loss: 1.3486, Train: 42.71%, Valid: 42.53%, Test: 42.69%
Epoch: 250, Loss: 1.3491, Train: 41.23%, Valid: 40.90%, Test: 41.01%
Epoch: 275, Loss: 1.3356, Train: 43.12%, Valid: 42.85%, Test: 42.71%
Epoch: 300, Loss: 1.3277, Train: 40.20%, Valid: 39.78%, Test: 40.20%
Epoch: 325, Loss: 1.3324, Train: 43.30%, Valid: 42.97%, Test: 42.84%
Epoch: 350, Loss: 1.3287, Train: 43.00%, Valid: 42.89%, Test: 42.95%
Epoch: 375, Loss: 1.3444, Train: 43.95%, Valid: 43.80%, Test: 43.79%
Epoch: 400, Loss: 1.3284, Train: 43.27%, Valid: 43.09%, Test: 43.15%
Epoch: 425, Loss: 1.3158, Train: 43.89%, Valid: 43.70%, Test: 43.63%
Epoch: 450, Loss: 1.3283, Train: 42.81%, Valid: 42.49%, Test: 42.34%
Epoch: 475, Loss: 1.3120, Train: 43.81%, Valid: 43.72%, Test: 43.86%
Epoch: 500, Loss: 1.3198, Train: 42.26%, Valid: 42.34%, Test: 42.39%
Epoch: 525, Loss: 1.3169, Train: 43.39%, Valid: 43.14%, Test: 43.21%
Epoch: 550, Loss: 1.3166, Train: 44.07%, Valid: 43.80%, Test: 43.61%
Epoch: 575, Loss: 1.3135, Train: 44.11%, Valid: 43.85%, Test: 43.95%
Epoch: 600, Loss: 1.3009, Train: 44.40%, Valid: 44.08%, Test: 44.20%
Epoch: 625, Loss: 1.3134, Train: 44.11%, Valid: 43.68%, Test: 44.00%
Epoch: 650, Loss: 1.3074, Train: 44.24%, Valid: 43.97%, Test: 44.13%
Epoch: 675, Loss: 1.3007, Train: 45.16%, Valid: 44.84%, Test: 44.81%
Epoch: 700, Loss: 1.3084, Train: 44.22%, Valid: 43.90%, Test: 44.04%
Epoch: 725, Loss: 1.3168, Train: 44.04%, Valid: 43.66%, Test: 43.87%
Epoch: 750, Loss: 1.3027, Train: 44.55%, Valid: 44.30%, Test: 44.52%
Epoch: 775, Loss: 1.2974, Train: 44.66%, Valid: 44.28%, Test: 44.37%
Epoch: 800, Loss: 1.2952, Train: 44.36%, Valid: 44.17%, Test: 44.07%
Epoch: 825, Loss: 1.3050, Train: 44.65%, Valid: 44.29%, Test: 44.31%
Epoch: 850, Loss: 1.2940, Train: 43.23%, Valid: 42.84%, Test: 43.06%
Epoch: 875, Loss: 1.2949, Train: 45.09%, Valid: 44.63%, Test: 44.79%
Epoch: 900, Loss: 1.2886, Train: 45.01%, Valid: 44.73%, Test: 44.92%
Epoch: 925, Loss: 1.2944, Train: 45.45%, Valid: 45.23%, Test: 45.11%
Epoch: 950, Loss: 1.3216, Train: 45.12%, Valid: 44.84%, Test: 44.77%
Epoch: 975, Loss: 1.3011, Train: 44.14%, Valid: 43.80%, Test: 43.82%
Run 01:
Highest Train: 45.60
Highest Valid: 45.28
  Final Train: 45.58
   Final Test: 45.19
All runs:
Highest Train: 45.60, nan
Highest Valid: 45.28, nan
  Final Train: 45.58, nan
   Final Test: 45.19, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 74.2695, Train: 18.42%, Valid: 18.53%, Test: 18.74%
Epoch: 25, Loss: 2.4333, Train: 19.50%, Valid: 19.51%, Test: 19.64%
Epoch: 50, Loss: 1.5774, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 75, Loss: 1.5104, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 100, Loss: 1.4847, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 125, Loss: 1.4747, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 150, Loss: 1.4682, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 175, Loss: 1.4672, Train: 28.74%, Valid: 28.52%, Test: 28.88%
Epoch: 200, Loss: 1.4577, Train: 28.74%, Valid: 28.55%, Test: 28.87%
Epoch: 225, Loss: 1.4564, Train: 28.27%, Valid: 28.14%, Test: 28.40%
Epoch: 250, Loss: 1.4528, Train: 25.94%, Valid: 25.87%, Test: 25.89%
Epoch: 275, Loss: 1.4517, Train: 25.62%, Valid: 25.35%, Test: 25.37%
Epoch: 300, Loss: 1.4488, Train: 18.61%, Valid: 18.71%, Test: 18.32%
Epoch: 325, Loss: 1.4431, Train: 22.18%, Valid: 22.11%, Test: 21.99%
Epoch: 350, Loss: 1.4469, Train: 19.11%, Valid: 19.16%, Test: 18.96%
Epoch: 375, Loss: 1.4416, Train: 18.76%, Valid: 18.89%, Test: 18.53%
Epoch: 400, Loss: 1.4407, Train: 19.02%, Valid: 19.09%, Test: 18.87%
Epoch: 425, Loss: 1.4367, Train: 18.19%, Valid: 18.26%, Test: 17.93%
Epoch: 450, Loss: 1.4367, Train: 18.73%, Valid: 18.82%, Test: 18.53%
Epoch: 475, Loss: 1.4340, Train: 28.58%, Valid: 28.37%, Test: 28.71%
Epoch: 500, Loss: 1.4370, Train: 18.78%, Valid: 18.84%, Test: 18.57%
Epoch: 525, Loss: 1.4393, Train: 18.66%, Valid: 18.84%, Test: 18.52%
Epoch: 550, Loss: 1.4315, Train: 18.40%, Valid: 18.41%, Test: 18.01%
Epoch: 575, Loss: 1.4317, Train: 17.99%, Valid: 18.01%, Test: 17.59%
Epoch: 600, Loss: 1.4345, Train: 18.92%, Valid: 19.06%, Test: 18.84%
Epoch: 625, Loss: 1.4278, Train: 19.08%, Valid: 19.12%, Test: 18.88%
Epoch: 650, Loss: 1.4283, Train: 19.29%, Valid: 19.46%, Test: 19.18%
Epoch: 675, Loss: 1.4272, Train: 19.91%, Valid: 19.92%, Test: 19.92%
Epoch: 700, Loss: 1.4361, Train: 21.06%, Valid: 20.91%, Test: 21.09%
Epoch: 725, Loss: 1.4249, Train: 23.64%, Valid: 23.54%, Test: 23.99%
Epoch: 750, Loss: 1.4324, Train: 30.56%, Valid: 30.25%, Test: 30.69%
Epoch: 775, Loss: 1.4211, Train: 21.42%, Valid: 21.37%, Test: 21.43%
Epoch: 800, Loss: 1.4171, Train: 24.48%, Valid: 24.13%, Test: 24.60%
Epoch: 825, Loss: 1.4239, Train: 29.28%, Valid: 28.90%, Test: 29.63%
Epoch: 850, Loss: 1.4206, Train: 28.99%, Valid: 28.69%, Test: 29.27%
Epoch: 875, Loss: 1.4157, Train: 25.13%, Valid: 25.33%, Test: 25.23%
Epoch: 900, Loss: 1.4246, Train: 26.18%, Valid: 26.17%, Test: 26.15%
Epoch: 925, Loss: 1.4130, Train: 31.92%, Valid: 31.87%, Test: 32.20%
Epoch: 950, Loss: 1.4137, Train: 31.77%, Valid: 31.69%, Test: 31.94%
Epoch: 975, Loss: 1.4102, Train: 28.67%, Valid: 28.58%, Test: 28.82%
Run 01:
Highest Train: 34.85
Highest Valid: 34.52
  Final Train: 34.85
   Final Test: 35.10
All runs:
Highest Train: 34.85, nan
Highest Valid: 34.52, nan
  Final Train: 34.85, nan
   Final Test: 35.10, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6201, Train: 25.64%, Valid: 25.37%, Test: 25.90%
Epoch: 25, Loss: 1.4879, Train: 35.01%, Valid: 34.72%, Test: 35.04%
Epoch: 50, Loss: 1.4553, Train: 36.87%, Valid: 36.28%, Test: 36.71%
Epoch: 75, Loss: 1.4281, Train: 38.19%, Valid: 37.35%, Test: 37.88%
Epoch: 100, Loss: 1.4069, Train: 39.04%, Valid: 38.33%, Test: 38.61%
Epoch: 125, Loss: 1.3915, Train: 39.97%, Valid: 39.06%, Test: 39.36%
Epoch: 150, Loss: 1.3878, Train: 40.42%, Valid: 39.40%, Test: 39.63%
Epoch: 175, Loss: 1.3792, Train: 41.59%, Valid: 40.47%, Test: 40.82%
Epoch: 200, Loss: 1.3644, Train: 42.17%, Valid: 41.35%, Test: 41.37%
Epoch: 225, Loss: 1.3621, Train: 42.76%, Valid: 41.80%, Test: 42.02%
Epoch: 250, Loss: 1.3631, Train: 42.70%, Valid: 41.88%, Test: 41.98%
Epoch: 275, Loss: 1.3497, Train: 43.82%, Valid: 42.66%, Test: 43.10%
Epoch: 300, Loss: 1.3375, Train: 43.26%, Valid: 42.34%, Test: 42.34%
Epoch: 325, Loss: 1.3411, Train: 44.28%, Valid: 43.10%, Test: 43.20%
Epoch: 350, Loss: 1.3319, Train: 44.62%, Valid: 43.16%, Test: 43.70%
Epoch: 375, Loss: 1.3341, Train: 44.70%, Valid: 43.51%, Test: 43.62%
Epoch: 400, Loss: 1.3301, Train: 44.73%, Valid: 43.47%, Test: 43.57%
Epoch: 425, Loss: 1.3209, Train: 44.90%, Valid: 43.72%, Test: 43.69%
Epoch: 450, Loss: 1.3219, Train: 44.82%, Valid: 43.59%, Test: 43.69%
Epoch: 475, Loss: 1.3143, Train: 45.14%, Valid: 43.83%, Test: 43.85%
Epoch: 500, Loss: 1.3161, Train: 45.21%, Valid: 43.97%, Test: 44.00%
Epoch: 525, Loss: 1.3255, Train: 45.02%, Valid: 43.71%, Test: 43.92%
Epoch: 550, Loss: 1.3124, Train: 45.47%, Valid: 44.07%, Test: 44.14%
Epoch: 575, Loss: 1.3145, Train: 45.47%, Valid: 43.85%, Test: 43.99%
Epoch: 600, Loss: 1.3062, Train: 45.63%, Valid: 44.03%, Test: 44.23%
Epoch: 625, Loss: 1.3191, Train: 45.65%, Valid: 44.17%, Test: 44.31%
Epoch: 650, Loss: 1.3122, Train: 45.91%, Valid: 44.31%, Test: 44.51%
Epoch: 675, Loss: 1.3084, Train: 45.92%, Valid: 44.44%, Test: 44.55%
Epoch: 700, Loss: 1.3046, Train: 45.62%, Valid: 44.06%, Test: 44.29%
Epoch: 725, Loss: 1.3056, Train: 45.92%, Valid: 44.53%, Test: 44.54%
Epoch: 750, Loss: 1.3001, Train: 45.56%, Valid: 44.08%, Test: 44.41%
Epoch: 775, Loss: 1.3013, Train: 45.56%, Valid: 44.14%, Test: 44.31%
Epoch: 800, Loss: 1.3082, Train: 45.69%, Valid: 44.20%, Test: 44.32%
Epoch: 825, Loss: 1.3018, Train: 46.31%, Valid: 44.72%, Test: 44.78%
Epoch: 850, Loss: 1.2993, Train: 46.05%, Valid: 44.64%, Test: 44.67%
Epoch: 875, Loss: 1.2949, Train: 45.95%, Valid: 44.59%, Test: 44.65%
Epoch: 900, Loss: 1.2916, Train: 46.00%, Valid: 44.57%, Test: 44.61%
Epoch: 925, Loss: 1.2947, Train: 45.60%, Valid: 44.34%, Test: 44.38%
Epoch: 950, Loss: 1.3050, Train: 46.17%, Valid: 44.64%, Test: 44.70%
Epoch: 975, Loss: 1.2980, Train: 45.83%, Valid: 44.50%, Test: 44.52%
Run 01:
Highest Train: 46.50
Highest Valid: 44.93
  Final Train: 46.45
   Final Test: 44.85
All runs:
Highest Train: 46.50, nan
Highest Valid: 44.93, nan
  Final Train: 46.45, nan
   Final Test: 44.85, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 2.0729, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4785, Train: 28.74%, Valid: 28.55%, Test: 28.86%
Epoch: 50, Loss: 1.4567, Train: 28.98%, Valid: 28.89%, Test: 29.10%
Epoch: 75, Loss: 1.4445, Train: 29.20%, Valid: 29.10%, Test: 29.26%
Epoch: 100, Loss: 1.4401, Train: 27.18%, Valid: 27.21%, Test: 27.24%
Epoch: 125, Loss: 1.4338, Train: 30.68%, Valid: 30.50%, Test: 30.87%
Epoch: 150, Loss: 1.4191, Train: 36.37%, Valid: 35.84%, Test: 36.29%
Epoch: 175, Loss: 1.4127, Train: 37.16%, Valid: 36.97%, Test: 37.03%
Epoch: 200, Loss: 1.4148, Train: 38.54%, Valid: 38.36%, Test: 38.52%
Epoch: 225, Loss: 1.4182, Train: 38.28%, Valid: 38.03%, Test: 38.10%
Epoch: 250, Loss: 1.3964, Train: 37.36%, Valid: 37.01%, Test: 37.24%
Epoch: 275, Loss: 1.4332, Train: 37.59%, Valid: 37.51%, Test: 37.62%
Epoch: 300, Loss: 1.4483, Train: 37.97%, Valid: 37.83%, Test: 37.83%
Epoch: 325, Loss: 1.4014, Train: 37.98%, Valid: 37.82%, Test: 37.87%
Epoch: 350, Loss: 1.4121, Train: 39.09%, Valid: 38.63%, Test: 38.90%
Epoch: 375, Loss: 1.3958, Train: 38.54%, Valid: 38.22%, Test: 38.27%
Epoch: 400, Loss: 1.3822, Train: 40.37%, Valid: 40.15%, Test: 40.36%
Epoch: 425, Loss: 1.3998, Train: 34.49%, Valid: 34.39%, Test: 34.71%
Epoch: 450, Loss: 1.3879, Train: 40.04%, Valid: 39.88%, Test: 40.07%
Epoch: 475, Loss: 1.3961, Train: 40.10%, Valid: 39.90%, Test: 40.31%
Epoch: 500, Loss: 1.3770, Train: 40.97%, Valid: 40.60%, Test: 41.28%
Epoch: 525, Loss: 1.3880, Train: 41.61%, Valid: 41.27%, Test: 41.70%
Epoch: 550, Loss: 1.3915, Train: 40.91%, Valid: 40.85%, Test: 40.92%
Epoch: 575, Loss: 1.3710, Train: 41.56%, Valid: 41.31%, Test: 41.42%
Epoch: 600, Loss: 1.3709, Train: 40.07%, Valid: 39.64%, Test: 40.22%
Epoch: 625, Loss: 1.3876, Train: 42.03%, Valid: 41.70%, Test: 41.86%
Epoch: 650, Loss: 1.3687, Train: 41.92%, Valid: 41.62%, Test: 41.80%
Epoch: 675, Loss: 1.4047, Train: 42.01%, Valid: 41.54%, Test: 41.73%
Epoch: 700, Loss: 1.3883, Train: 41.18%, Valid: 41.05%, Test: 41.31%
Epoch: 725, Loss: 1.3813, Train: 41.54%, Valid: 41.17%, Test: 41.46%
Epoch: 750, Loss: 1.3718, Train: 42.66%, Valid: 42.33%, Test: 42.41%
Epoch: 775, Loss: 1.3530, Train: 42.17%, Valid: 41.99%, Test: 42.08%
Epoch: 800, Loss: 1.3579, Train: 37.19%, Valid: 37.20%, Test: 37.08%
Epoch: 825, Loss: 1.3549, Train: 40.47%, Valid: 40.14%, Test: 40.12%
Epoch: 850, Loss: 1.3777, Train: 41.30%, Valid: 40.96%, Test: 41.09%
Epoch: 875, Loss: 1.3637, Train: 42.51%, Valid: 42.10%, Test: 42.25%
Epoch: 900, Loss: 1.3545, Train: 42.47%, Valid: 42.29%, Test: 42.32%
Epoch: 925, Loss: 1.3579, Train: 38.78%, Valid: 38.47%, Test: 38.82%
Epoch: 950, Loss: 1.3411, Train: 41.87%, Valid: 41.81%, Test: 41.93%
Epoch: 975, Loss: 1.3468, Train: 42.45%, Valid: 42.10%, Test: 42.29%
Run 01:
Highest Train: 43.07
Highest Valid: 42.81
  Final Train: 43.06
   Final Test: 42.79
All runs:
Highest Train: 43.07, nan
Highest Valid: 42.81, nan
  Final Train: 43.06, nan
   Final Test: 42.79, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 84.7753, Train: 28.72%, Valid: 28.56%, Test: 28.82%
Epoch: 25, Loss: 2.6624, Train: 27.46%, Valid: 27.49%, Test: 27.69%
Epoch: 50, Loss: 1.8678, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 75, Loss: 1.6161, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 100, Loss: 1.5495, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 125, Loss: 1.5253, Train: 28.72%, Valid: 28.54%, Test: 28.83%
Epoch: 150, Loss: 1.4969, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 175, Loss: 1.4869, Train: 28.69%, Valid: 28.52%, Test: 28.80%
Epoch: 200, Loss: 1.4793, Train: 28.59%, Valid: 28.46%, Test: 28.71%
Epoch: 225, Loss: 1.4699, Train: 28.51%, Valid: 28.33%, Test: 28.62%
Epoch: 250, Loss: 1.4656, Train: 28.26%, Valid: 28.09%, Test: 28.34%
Epoch: 275, Loss: 1.4573, Train: 28.35%, Valid: 28.15%, Test: 28.46%
Epoch: 300, Loss: 1.4541, Train: 28.41%, Valid: 28.30%, Test: 28.57%
Epoch: 325, Loss: 1.4503, Train: 28.06%, Valid: 27.91%, Test: 28.00%
Epoch: 350, Loss: 1.4469, Train: 19.20%, Valid: 19.45%, Test: 18.93%
Epoch: 375, Loss: 1.4451, Train: 19.52%, Valid: 19.64%, Test: 19.44%
Epoch: 400, Loss: 1.4407, Train: 21.29%, Valid: 21.14%, Test: 21.19%
Epoch: 425, Loss: 1.4388, Train: 21.12%, Valid: 21.04%, Test: 21.07%
Epoch: 450, Loss: 1.4393, Train: 22.97%, Valid: 23.00%, Test: 23.05%
Epoch: 475, Loss: 1.4358, Train: 23.17%, Valid: 23.05%, Test: 23.48%
Epoch: 500, Loss: 1.4285, Train: 23.02%, Valid: 23.05%, Test: 23.07%
Epoch: 525, Loss: 1.4307, Train: 23.53%, Valid: 23.51%, Test: 23.50%
Epoch: 550, Loss: 1.4274, Train: 25.00%, Valid: 24.69%, Test: 24.90%
Epoch: 575, Loss: 1.4308, Train: 27.74%, Valid: 27.71%, Test: 27.92%
Epoch: 600, Loss: 1.4267, Train: 28.24%, Valid: 28.33%, Test: 28.47%
Epoch: 625, Loss: 1.4273, Train: 25.78%, Valid: 25.51%, Test: 25.72%
Epoch: 650, Loss: 1.4222, Train: 28.13%, Valid: 28.07%, Test: 28.13%
Epoch: 675, Loss: 1.4180, Train: 33.48%, Valid: 33.29%, Test: 33.71%
Epoch: 700, Loss: 1.4184, Train: 30.32%, Valid: 30.40%, Test: 30.51%
Epoch: 725, Loss: 1.4192, Train: 30.16%, Valid: 30.11%, Test: 30.20%
Epoch: 750, Loss: 1.4158, Train: 37.00%, Valid: 36.90%, Test: 37.24%
Epoch: 775, Loss: 1.4163, Train: 32.88%, Valid: 33.00%, Test: 33.11%
Epoch: 800, Loss: 1.4149, Train: 34.25%, Valid: 34.18%, Test: 34.59%
Epoch: 825, Loss: 1.4161, Train: 32.58%, Valid: 32.57%, Test: 32.54%
Epoch: 850, Loss: 1.4113, Train: 38.01%, Valid: 37.89%, Test: 38.39%
Epoch: 875, Loss: 1.4105, Train: 38.70%, Valid: 38.46%, Test: 38.91%
Epoch: 900, Loss: 1.4131, Train: 36.26%, Valid: 36.13%, Test: 36.56%
Epoch: 925, Loss: 1.4076, Train: 31.81%, Valid: 31.76%, Test: 31.74%
Epoch: 950, Loss: 1.4077, Train: 34.06%, Valid: 33.93%, Test: 34.02%
Epoch: 975, Loss: 1.4084, Train: 39.05%, Valid: 38.96%, Test: 39.23%
Run 01:
Highest Train: 39.31
Highest Valid: 39.09
  Final Train: 39.18
   Final Test: 39.29
All runs:
Highest Train: 39.31, nan
Highest Valid: 39.09, nan
  Final Train: 39.18, nan
   Final Test: 39.29, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6111, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4727, Train: 35.26%, Valid: 34.86%, Test: 35.24%
Epoch: 50, Loss: 1.4388, Train: 37.22%, Valid: 36.67%, Test: 36.84%
Epoch: 75, Loss: 1.4130, Train: 38.43%, Valid: 37.90%, Test: 38.05%
Epoch: 100, Loss: 1.3769, Train: 40.27%, Valid: 39.51%, Test: 39.72%
Epoch: 125, Loss: 1.3508, Train: 41.65%, Valid: 40.76%, Test: 40.91%
Epoch: 150, Loss: 1.3328, Train: 42.89%, Valid: 41.77%, Test: 41.93%
Epoch: 175, Loss: 1.3423, Train: 41.21%, Valid: 40.14%, Test: 40.56%
Epoch: 200, Loss: 1.2996, Train: 43.66%, Valid: 42.49%, Test: 42.66%
Epoch: 225, Loss: 1.3165, Train: 41.99%, Valid: 41.03%, Test: 41.41%
Epoch: 250, Loss: 1.2884, Train: 44.21%, Valid: 43.00%, Test: 43.20%
Epoch: 275, Loss: 1.2959, Train: 44.29%, Valid: 43.01%, Test: 43.10%
Epoch: 300, Loss: 1.2682, Train: 44.58%, Valid: 43.29%, Test: 43.47%
Epoch: 325, Loss: 1.2590, Train: 45.57%, Valid: 44.01%, Test: 44.06%
Epoch: 350, Loss: 1.2443, Train: 46.33%, Valid: 44.37%, Test: 44.51%
Epoch: 375, Loss: 1.2378, Train: 46.83%, Valid: 44.89%, Test: 45.02%
Epoch: 400, Loss: 1.2385, Train: 46.48%, Valid: 44.68%, Test: 44.81%
Epoch: 425, Loss: 1.2212, Train: 47.09%, Valid: 45.06%, Test: 45.07%
Epoch: 450, Loss: 1.2254, Train: 46.75%, Valid: 44.68%, Test: 44.81%
Epoch: 475, Loss: 1.2108, Train: 48.00%, Valid: 45.76%, Test: 45.97%
Epoch: 500, Loss: 1.2159, Train: 47.91%, Valid: 45.51%, Test: 45.74%
Epoch: 525, Loss: 1.1969, Train: 48.55%, Valid: 46.10%, Test: 46.37%
Epoch: 550, Loss: 1.2115, Train: 47.85%, Valid: 45.35%, Test: 45.44%
Epoch: 575, Loss: 1.1886, Train: 48.94%, Valid: 46.17%, Test: 46.46%
Epoch: 600, Loss: 1.2002, Train: 48.84%, Valid: 45.95%, Test: 46.45%
Epoch: 625, Loss: 1.1786, Train: 49.38%, Valid: 46.59%, Test: 46.85%
Epoch: 650, Loss: 1.1983, Train: 48.63%, Valid: 46.08%, Test: 46.17%
Epoch: 675, Loss: 1.1741, Train: 49.65%, Valid: 46.79%, Test: 47.02%
Epoch: 700, Loss: 1.1926, Train: 49.60%, Valid: 46.72%, Test: 47.05%
Epoch: 725, Loss: 1.1869, Train: 49.18%, Valid: 46.47%, Test: 46.89%
Epoch: 750, Loss: 1.1713, Train: 49.69%, Valid: 46.88%, Test: 47.19%
Epoch: 775, Loss: 1.1902, Train: 48.05%, Valid: 45.43%, Test: 45.44%
Epoch: 800, Loss: 1.1617, Train: 50.14%, Valid: 47.22%, Test: 47.35%
Epoch: 825, Loss: 1.1601, Train: 50.27%, Valid: 46.94%, Test: 47.21%
Epoch: 850, Loss: 1.1512, Train: 50.80%, Valid: 47.20%, Test: 47.55%
Epoch: 875, Loss: 1.1665, Train: 50.53%, Valid: 46.96%, Test: 47.18%
Epoch: 900, Loss: 1.1450, Train: 50.88%, Valid: 47.54%, Test: 47.60%
Epoch: 925, Loss: 1.1495, Train: 50.68%, Valid: 47.25%, Test: 47.63%
Epoch: 950, Loss: 1.1472, Train: 50.83%, Valid: 47.19%, Test: 47.23%
Epoch: 975, Loss: 1.1599, Train: 49.88%, Valid: 46.38%, Test: 46.66%
Run 01:
Highest Train: 51.40
Highest Valid: 47.71
  Final Train: 51.14
   Final Test: 47.65
All runs:
Highest Train: 51.40, nan
Highest Valid: 47.71, nan
  Final Train: 51.14, nan
   Final Test: 47.65, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6467, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4820, Train: 28.72%, Valid: 28.40%, Test: 28.85%
Epoch: 50, Loss: 1.4600, Train: 34.35%, Valid: 34.02%, Test: 34.95%
Epoch: 75, Loss: 1.4256, Train: 30.30%, Valid: 29.77%, Test: 30.81%
Epoch: 100, Loss: 1.4076, Train: 33.86%, Valid: 33.31%, Test: 34.49%
Epoch: 125, Loss: 1.4289, Train: 34.10%, Valid: 33.70%, Test: 34.61%
Epoch: 150, Loss: 1.4060, Train: 34.47%, Valid: 34.09%, Test: 35.22%
Epoch: 175, Loss: 1.3863, Train: 34.51%, Valid: 34.21%, Test: 35.40%
Epoch: 200, Loss: 1.4059, Train: 34.18%, Valid: 33.75%, Test: 35.04%
Epoch: 225, Loss: 1.3844, Train: 34.95%, Valid: 34.52%, Test: 35.58%
Epoch: 250, Loss: 1.4565, Train: 35.36%, Valid: 34.95%, Test: 35.90%
Epoch: 275, Loss: 1.3804, Train: 35.18%, Valid: 34.81%, Test: 35.87%
Epoch: 300, Loss: 1.3737, Train: 35.65%, Valid: 35.42%, Test: 36.28%
Epoch: 325, Loss: 1.3957, Train: 35.68%, Valid: 35.41%, Test: 36.18%
Epoch: 350, Loss: 1.3831, Train: 35.34%, Valid: 35.12%, Test: 36.00%
Epoch: 375, Loss: 1.3708, Train: 35.81%, Valid: 35.51%, Test: 36.49%
Epoch: 400, Loss: 1.3944, Train: 35.66%, Valid: 35.44%, Test: 36.27%
Epoch: 425, Loss: 1.3643, Train: 35.79%, Valid: 35.59%, Test: 36.46%
Epoch: 450, Loss: 1.3868, Train: 35.72%, Valid: 35.73%, Test: 36.54%
Epoch: 475, Loss: 1.3548, Train: 36.49%, Valid: 36.26%, Test: 37.13%
Epoch: 500, Loss: 1.3638, Train: 31.93%, Valid: 32.13%, Test: 32.64%
Epoch: 525, Loss: 1.3585, Train: 36.70%, Valid: 36.54%, Test: 37.28%
Epoch: 550, Loss: 1.3478, Train: 36.84%, Valid: 36.77%, Test: 37.44%
Epoch: 575, Loss: 1.5494, Train: 32.38%, Valid: 32.20%, Test: 33.17%
Epoch: 600, Loss: 1.4097, Train: 34.66%, Valid: 34.36%, Test: 35.42%
Epoch: 625, Loss: 1.3914, Train: 35.26%, Valid: 34.95%, Test: 36.10%
Epoch: 650, Loss: 1.3813, Train: 35.41%, Valid: 35.19%, Test: 36.25%
Epoch: 675, Loss: 1.3748, Train: 35.61%, Valid: 35.43%, Test: 36.49%
Epoch: 700, Loss: 1.3718, Train: 35.86%, Valid: 35.70%, Test: 36.73%
Epoch: 725, Loss: 1.3677, Train: 36.07%, Valid: 35.89%, Test: 36.87%
Epoch: 750, Loss: 1.3633, Train: 36.13%, Valid: 36.00%, Test: 36.99%
Epoch: 775, Loss: 1.3639, Train: 35.93%, Valid: 35.69%, Test: 36.60%
Epoch: 800, Loss: 1.3924, Train: 35.43%, Valid: 35.24%, Test: 36.23%
Epoch: 825, Loss: 1.3682, Train: 35.87%, Valid: 35.75%, Test: 36.63%
Epoch: 850, Loss: 1.3611, Train: 36.07%, Valid: 36.00%, Test: 36.83%
Epoch: 875, Loss: 1.3538, Train: 36.68%, Valid: 36.65%, Test: 37.50%
Epoch: 900, Loss: 1.3527, Train: 37.36%, Valid: 37.21%, Test: 37.94%
Epoch: 925, Loss: 1.3509, Train: 37.65%, Valid: 37.54%, Test: 38.21%
Epoch: 950, Loss: 1.4162, Train: 32.51%, Valid: 32.56%, Test: 33.05%
Epoch: 975, Loss: 1.3763, Train: 35.46%, Valid: 35.23%, Test: 36.38%
Run 01:
Highest Train: 39.27
Highest Valid: 39.19
  Final Train: 39.27
   Final Test: 39.19
All runs:
Highest Train: 39.27, nan
Highest Valid: 39.19, nan
  Final Train: 39.27, nan
   Final Test: 39.19, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 5.8649, Train: 24.14%, Valid: 23.95%, Test: 23.94%
Epoch: 25, Loss: 1.9289, Train: 26.33%, Valid: 26.29%, Test: 26.19%
Epoch: 50, Loss: 1.5661, Train: 31.42%, Valid: 31.47%, Test: 31.92%
Epoch: 75, Loss: 1.4636, Train: 33.75%, Valid: 34.02%, Test: 33.96%
Epoch: 100, Loss: 1.4319, Train: 35.32%, Valid: 35.81%, Test: 35.68%
Epoch: 125, Loss: 1.4628, Train: 37.58%, Valid: 37.89%, Test: 37.68%
Epoch: 150, Loss: 1.4259, Train: 36.68%, Valid: 37.10%, Test: 36.97%
Epoch: 175, Loss: 1.4273, Train: 38.09%, Valid: 38.35%, Test: 38.05%
Epoch: 200, Loss: 1.4161, Train: 38.14%, Valid: 38.45%, Test: 38.25%
Epoch: 225, Loss: 1.4167, Train: 36.48%, Valid: 36.91%, Test: 36.71%
Epoch: 250, Loss: 1.4208, Train: 38.64%, Valid: 38.98%, Test: 38.78%
Epoch: 275, Loss: 1.4110, Train: 38.77%, Valid: 39.00%, Test: 38.93%
Epoch: 300, Loss: 1.4060, Train: 37.23%, Valid: 37.38%, Test: 37.36%
Epoch: 325, Loss: 1.4129, Train: 38.94%, Valid: 39.20%, Test: 39.16%
Epoch: 350, Loss: 1.4027, Train: 39.05%, Valid: 39.20%, Test: 39.28%
Epoch: 375, Loss: 1.3983, Train: 37.51%, Valid: 37.58%, Test: 37.79%
Epoch: 400, Loss: 1.4062, Train: 39.28%, Valid: 39.25%, Test: 39.36%
Epoch: 425, Loss: 1.3961, Train: 39.39%, Valid: 39.31%, Test: 39.43%
Epoch: 450, Loss: 1.3915, Train: 38.06%, Valid: 37.97%, Test: 38.24%
Epoch: 475, Loss: 1.3920, Train: 39.49%, Valid: 39.45%, Test: 39.45%
Epoch: 500, Loss: 1.5785, Train: 30.99%, Valid: 30.72%, Test: 31.33%
Epoch: 525, Loss: 1.5007, Train: 33.55%, Valid: 33.61%, Test: 33.60%
Epoch: 550, Loss: 1.4682, Train: 33.20%, Valid: 33.20%, Test: 33.30%
Epoch: 575, Loss: 1.4561, Train: 34.55%, Valid: 34.50%, Test: 34.61%
Epoch: 600, Loss: 1.4508, Train: 35.05%, Valid: 35.03%, Test: 35.18%
Epoch: 625, Loss: 1.4455, Train: 35.32%, Valid: 35.27%, Test: 35.39%
Epoch: 650, Loss: 1.4473, Train: 35.52%, Valid: 35.62%, Test: 35.51%
Epoch: 675, Loss: 1.4433, Train: 36.12%, Valid: 35.98%, Test: 35.99%
Epoch: 700, Loss: 1.4323, Train: 36.34%, Valid: 36.26%, Test: 36.41%
Epoch: 725, Loss: 1.4212, Train: 37.23%, Valid: 37.04%, Test: 37.26%
Epoch: 750, Loss: 1.4639, Train: 37.66%, Valid: 37.47%, Test: 37.56%
Epoch: 775, Loss: 1.4905, Train: 33.51%, Valid: 33.17%, Test: 33.57%
Epoch: 800, Loss: 1.4215, Train: 38.28%, Valid: 38.10%, Test: 38.40%
Epoch: 825, Loss: 1.4251, Train: 38.97%, Valid: 38.73%, Test: 39.09%
Epoch: 850, Loss: 1.4193, Train: 38.03%, Valid: 37.82%, Test: 38.20%
Epoch: 875, Loss: 1.4017, Train: 38.55%, Valid: 38.26%, Test: 38.44%
Epoch: 900, Loss: 1.3940, Train: 39.00%, Valid: 38.78%, Test: 38.91%
Epoch: 925, Loss: 1.4139, Train: 38.38%, Valid: 38.09%, Test: 38.10%
Epoch: 950, Loss: 1.3991, Train: 39.28%, Valid: 39.02%, Test: 38.96%
Epoch: 975, Loss: 1.3881, Train: 39.40%, Valid: 39.12%, Test: 39.14%
Run 01:
Highest Train: 39.63
Highest Valid: 39.59
  Final Train: 39.63
   Final Test: 39.58
All runs:
Highest Train: 39.63, nan
Highest Valid: 39.59, nan
  Final Train: 39.63, nan
   Final Test: 39.58, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.5914, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4700, Train: 35.62%, Valid: 35.20%, Test: 35.60%
Epoch: 50, Loss: 1.4381, Train: 37.20%, Valid: 36.72%, Test: 36.86%
Epoch: 75, Loss: 1.4031, Train: 38.99%, Valid: 38.42%, Test: 38.48%
Epoch: 100, Loss: 1.3763, Train: 40.24%, Valid: 39.56%, Test: 39.62%
Epoch: 125, Loss: 1.3491, Train: 41.14%, Valid: 40.62%, Test: 40.55%
Epoch: 150, Loss: 1.3236, Train: 41.87%, Valid: 41.00%, Test: 41.37%
Epoch: 175, Loss: 1.3068, Train: 43.37%, Valid: 42.47%, Test: 42.79%
Epoch: 200, Loss: 1.2941, Train: 44.03%, Valid: 42.94%, Test: 43.31%
Epoch: 225, Loss: 1.2768, Train: 44.85%, Valid: 43.68%, Test: 43.82%
Epoch: 250, Loss: 1.2671, Train: 45.11%, Valid: 43.79%, Test: 44.07%
Epoch: 275, Loss: 1.2661, Train: 45.70%, Valid: 44.35%, Test: 44.55%
Epoch: 300, Loss: 1.2463, Train: 46.27%, Valid: 44.71%, Test: 44.98%
Epoch: 325, Loss: 1.2494, Train: 45.86%, Valid: 44.20%, Test: 44.70%
Epoch: 350, Loss: 1.2323, Train: 47.17%, Valid: 45.38%, Test: 45.68%
Epoch: 375, Loss: 1.2257, Train: 47.44%, Valid: 45.46%, Test: 45.95%
Epoch: 400, Loss: 1.2257, Train: 47.28%, Valid: 45.18%, Test: 45.47%
Epoch: 425, Loss: 1.2148, Train: 47.86%, Valid: 45.51%, Test: 45.96%
Epoch: 450, Loss: 1.2201, Train: 47.63%, Valid: 45.42%, Test: 45.94%
Epoch: 475, Loss: 1.2037, Train: 48.63%, Valid: 45.94%, Test: 46.54%
Epoch: 500, Loss: 1.1981, Train: 48.94%, Valid: 46.15%, Test: 46.67%
Epoch: 525, Loss: 1.2034, Train: 48.23%, Valid: 45.95%, Test: 46.18%
Epoch: 550, Loss: 1.2009, Train: 49.05%, Valid: 46.44%, Test: 46.70%
Epoch: 575, Loss: 1.2013, Train: 47.63%, Valid: 44.83%, Test: 45.13%
Epoch: 600, Loss: 1.1928, Train: 49.29%, Valid: 46.55%, Test: 46.90%
Epoch: 625, Loss: 1.1781, Train: 49.86%, Valid: 47.02%, Test: 47.16%
Epoch: 650, Loss: 1.1785, Train: 49.84%, Valid: 46.95%, Test: 46.93%
Epoch: 675, Loss: 1.1818, Train: 49.88%, Valid: 47.10%, Test: 47.14%
Epoch: 700, Loss: 1.1717, Train: 49.97%, Valid: 47.17%, Test: 47.13%
Epoch: 725, Loss: 1.1780, Train: 49.84%, Valid: 46.87%, Test: 46.98%
Epoch: 750, Loss: 1.1643, Train: 50.43%, Valid: 47.38%, Test: 47.37%
Epoch: 775, Loss: 1.1611, Train: 50.62%, Valid: 47.36%, Test: 47.52%
Epoch: 800, Loss: 1.2363, Train: 48.48%, Valid: 46.51%, Test: 46.19%
Epoch: 825, Loss: 1.1678, Train: 50.48%, Valid: 47.68%, Test: 47.49%
Epoch: 850, Loss: 1.1560, Train: 50.97%, Valid: 47.65%, Test: 47.62%
Epoch: 875, Loss: 1.1544, Train: 50.92%, Valid: 47.62%, Test: 47.64%
Epoch: 900, Loss: 1.1556, Train: 51.06%, Valid: 47.65%, Test: 47.64%
Epoch: 925, Loss: 1.1524, Train: 50.79%, Valid: 47.60%, Test: 47.46%
Epoch: 950, Loss: 1.1539, Train: 50.98%, Valid: 47.53%, Test: 47.56%
Epoch: 975, Loss: 1.1490, Train: 50.99%, Valid: 47.44%, Test: 47.42%
Run 01:
Highest Train: 51.60
Highest Valid: 48.00
  Final Train: 51.41
   Final Test: 47.98
All runs:
Highest Train: 51.60, nan
Highest Valid: 48.00, nan
  Final Train: 51.41, nan
   Final Test: 47.98, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 2.0173, Train: 22.43%, Valid: 22.40%, Test: 22.95%
Epoch: 25, Loss: 1.4139, Train: 39.45%, Valid: 39.17%, Test: 39.54%
Epoch: 50, Loss: 1.3702, Train: 41.06%, Valid: 40.70%, Test: 40.98%
Epoch: 75, Loss: 1.3635, Train: 41.05%, Valid: 40.72%, Test: 41.07%
Epoch: 100, Loss: 1.3728, Train: 39.95%, Valid: 39.74%, Test: 40.01%
Epoch: 125, Loss: 1.3542, Train: 41.14%, Valid: 40.85%, Test: 41.21%
Epoch: 150, Loss: 1.3463, Train: 41.45%, Valid: 41.24%, Test: 41.65%
Epoch: 175, Loss: 1.3866, Train: 40.24%, Valid: 39.87%, Test: 40.38%
Epoch: 200, Loss: 1.3510, Train: 41.01%, Valid: 40.81%, Test: 41.27%
Epoch: 225, Loss: 1.3411, Train: 41.84%, Valid: 41.54%, Test: 41.88%
Epoch: 250, Loss: 1.3574, Train: 40.52%, Valid: 40.32%, Test: 40.75%
Epoch: 275, Loss: 1.3361, Train: 42.01%, Valid: 41.81%, Test: 42.25%
Epoch: 300, Loss: 1.3445, Train: 42.38%, Valid: 42.10%, Test: 42.24%
Epoch: 325, Loss: 1.3348, Train: 41.75%, Valid: 41.49%, Test: 41.82%
Epoch: 350, Loss: 1.3286, Train: 40.88%, Valid: 40.73%, Test: 41.11%
Epoch: 375, Loss: 1.3717, Train: 40.87%, Valid: 40.47%, Test: 40.97%
Epoch: 400, Loss: 1.3463, Train: 41.40%, Valid: 41.22%, Test: 41.62%
Epoch: 425, Loss: 1.3475, Train: 42.00%, Valid: 41.55%, Test: 41.99%
Epoch: 450, Loss: 1.3347, Train: 42.13%, Valid: 41.81%, Test: 42.21%
Epoch: 475, Loss: 1.3446, Train: 42.36%, Valid: 42.09%, Test: 42.32%
Epoch: 500, Loss: 1.3269, Train: 42.30%, Valid: 42.04%, Test: 42.32%
Epoch: 525, Loss: 1.5222, Train: 33.08%, Valid: 32.89%, Test: 32.94%
Epoch: 550, Loss: 1.4058, Train: 38.21%, Valid: 37.99%, Test: 38.05%
Epoch: 575, Loss: 1.3664, Train: 38.32%, Valid: 38.30%, Test: 38.58%
Epoch: 600, Loss: 1.3557, Train: 41.14%, Valid: 40.86%, Test: 41.19%
Epoch: 625, Loss: 1.3406, Train: 41.59%, Valid: 41.43%, Test: 41.83%
Epoch: 650, Loss: 1.3344, Train: 42.05%, Valid: 41.87%, Test: 42.22%
Epoch: 675, Loss: 1.3330, Train: 41.70%, Valid: 41.40%, Test: 41.81%
Epoch: 700, Loss: 1.3251, Train: 42.26%, Valid: 42.20%, Test: 42.51%
Epoch: 725, Loss: 1.3593, Train: 41.91%, Valid: 41.72%, Test: 42.01%
Epoch: 750, Loss: 1.3275, Train: 42.49%, Valid: 42.27%, Test: 42.66%
Epoch: 775, Loss: 1.3192, Train: 42.88%, Valid: 42.63%, Test: 42.92%
Epoch: 800, Loss: 1.3945, Train: 40.73%, Valid: 40.42%, Test: 40.65%
Epoch: 825, Loss: 1.3388, Train: 41.31%, Valid: 41.10%, Test: 41.41%
Epoch: 850, Loss: 1.3269, Train: 42.52%, Valid: 42.27%, Test: 42.60%
Epoch: 875, Loss: 1.3209, Train: 42.42%, Valid: 42.28%, Test: 42.54%
Epoch: 900, Loss: 1.3327, Train: 42.29%, Valid: 41.97%, Test: 42.40%
Epoch: 925, Loss: 1.3200, Train: 42.68%, Valid: 42.45%, Test: 42.85%
Epoch: 950, Loss: 1.3142, Train: 43.18%, Valid: 42.96%, Test: 43.12%
Epoch: 975, Loss: 1.3237, Train: 42.61%, Valid: 42.34%, Test: 42.63%
Run 01:
Highest Train: 43.34
Highest Valid: 43.01
  Final Train: 43.34
   Final Test: 43.23
All runs:
Highest Train: 43.34, nan
Highest Valid: 43.01, nan
  Final Train: 43.34, nan
   Final Test: 43.23, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 27.9089, Train: 36.73%, Valid: 36.34%, Test: 36.35%
Epoch: 25, Loss: 1.6573, Train: 22.87%, Valid: 22.85%, Test: 22.93%
Epoch: 50, Loss: 1.5461, Train: 30.71%, Valid: 30.80%, Test: 30.91%
Epoch: 75, Loss: 1.5144, Train: 32.15%, Valid: 32.08%, Test: 32.25%
Epoch: 100, Loss: 1.4979, Train: 32.84%, Valid: 32.64%, Test: 32.99%
Epoch: 125, Loss: 1.4852, Train: 33.76%, Valid: 33.49%, Test: 34.12%
Epoch: 150, Loss: 1.4747, Train: 34.04%, Valid: 33.48%, Test: 34.28%
Epoch: 175, Loss: 1.4665, Train: 34.17%, Valid: 33.51%, Test: 34.42%
Epoch: 200, Loss: 1.4604, Train: 34.37%, Valid: 33.74%, Test: 34.61%
Epoch: 225, Loss: 1.4549, Train: 34.61%, Valid: 34.09%, Test: 34.99%
Epoch: 250, Loss: 1.4494, Train: 35.05%, Valid: 34.69%, Test: 35.27%
Epoch: 275, Loss: 1.4430, Train: 35.61%, Valid: 35.25%, Test: 35.71%
Epoch: 300, Loss: 1.4349, Train: 35.99%, Valid: 35.55%, Test: 36.01%
Epoch: 325, Loss: 1.4279, Train: 36.29%, Valid: 35.89%, Test: 36.36%
Epoch: 350, Loss: 1.4223, Train: 36.83%, Valid: 36.26%, Test: 36.80%
Epoch: 375, Loss: 1.4173, Train: 37.27%, Valid: 36.56%, Test: 37.31%
Epoch: 400, Loss: 1.4467, Train: 36.24%, Valid: 35.97%, Test: 36.51%
Epoch: 425, Loss: 1.4161, Train: 37.24%, Valid: 36.65%, Test: 37.41%
Epoch: 450, Loss: 1.4099, Train: 37.89%, Valid: 37.33%, Test: 38.14%
Epoch: 475, Loss: 1.4055, Train: 38.19%, Valid: 37.68%, Test: 38.46%
Epoch: 500, Loss: 1.4019, Train: 38.52%, Valid: 38.09%, Test: 38.78%
Epoch: 525, Loss: 1.3987, Train: 38.73%, Valid: 38.33%, Test: 39.12%
Epoch: 550, Loss: 1.5893, Train: 37.84%, Valid: 37.48%, Test: 37.87%
Epoch: 575, Loss: 1.4409, Train: 35.25%, Valid: 34.86%, Test: 35.43%
Epoch: 600, Loss: 1.4105, Train: 37.77%, Valid: 37.25%, Test: 38.02%
Epoch: 625, Loss: 1.4026, Train: 38.01%, Valid: 37.40%, Test: 38.19%
Epoch: 650, Loss: 1.3979, Train: 38.39%, Valid: 37.90%, Test: 38.49%
Epoch: 675, Loss: 1.3942, Train: 38.72%, Valid: 38.37%, Test: 38.77%
Epoch: 700, Loss: 1.3907, Train: 39.07%, Valid: 38.66%, Test: 39.11%
Epoch: 725, Loss: 1.3874, Train: 39.42%, Valid: 39.02%, Test: 39.45%
Epoch: 750, Loss: 1.3844, Train: 39.64%, Valid: 39.29%, Test: 39.62%
Epoch: 775, Loss: 1.4655, Train: 35.20%, Valid: 34.97%, Test: 35.30%
Epoch: 800, Loss: 1.3932, Train: 38.98%, Valid: 38.54%, Test: 39.16%
Epoch: 825, Loss: 1.3866, Train: 39.65%, Valid: 39.26%, Test: 39.71%
Epoch: 850, Loss: 1.3827, Train: 39.66%, Valid: 39.27%, Test: 39.73%
Epoch: 875, Loss: 1.3797, Train: 39.78%, Valid: 39.41%, Test: 39.87%
Epoch: 900, Loss: 1.3769, Train: 39.91%, Valid: 39.53%, Test: 39.93%
Epoch: 925, Loss: 1.7427, Train: 36.95%, Valid: 36.62%, Test: 37.00%
Epoch: 950, Loss: 1.4239, Train: 37.85%, Valid: 37.41%, Test: 37.87%
Epoch: 975, Loss: 1.3998, Train: 39.06%, Valid: 38.64%, Test: 39.07%
Run 01:
Highest Train: 40.15
Highest Valid: 39.71
  Final Train: 40.05
   Final Test: 40.14
All runs:
Highest Train: 40.15, nan
Highest Valid: 39.71, nan
  Final Train: 40.05, nan
   Final Test: 40.14, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6028, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4784, Train: 35.20%, Valid: 35.02%, Test: 35.12%
Epoch: 50, Loss: 1.4447, Train: 37.15%, Valid: 36.58%, Test: 36.85%
Epoch: 75, Loss: 1.4145, Train: 39.00%, Valid: 38.23%, Test: 38.68%
Epoch: 100, Loss: 1.3879, Train: 40.45%, Valid: 39.67%, Test: 39.86%
Epoch: 125, Loss: 1.3679, Train: 41.82%, Valid: 40.78%, Test: 41.13%
Epoch: 150, Loss: 1.3565, Train: 42.72%, Valid: 41.68%, Test: 42.20%
Epoch: 175, Loss: 1.3500, Train: 42.21%, Valid: 41.44%, Test: 41.94%
Epoch: 200, Loss: 1.3400, Train: 43.53%, Valid: 42.54%, Test: 42.95%
Epoch: 225, Loss: 1.3314, Train: 43.74%, Valid: 42.76%, Test: 43.44%
Epoch: 250, Loss: 1.3428, Train: 43.22%, Valid: 42.47%, Test: 42.77%
Epoch: 275, Loss: 1.3341, Train: 44.16%, Valid: 43.04%, Test: 43.54%
Epoch: 300, Loss: 1.3170, Train: 44.64%, Valid: 43.73%, Test: 44.29%
Epoch: 325, Loss: 1.3177, Train: 44.86%, Valid: 43.80%, Test: 44.29%
Epoch: 350, Loss: 1.3225, Train: 44.51%, Valid: 43.59%, Test: 43.96%
Epoch: 375, Loss: 1.3179, Train: 44.73%, Valid: 43.86%, Test: 44.38%
Epoch: 400, Loss: 1.3237, Train: 43.54%, Valid: 42.78%, Test: 43.21%
Epoch: 425, Loss: 1.3086, Train: 44.93%, Valid: 44.24%, Test: 44.46%
Epoch: 450, Loss: 1.3111, Train: 45.12%, Valid: 44.34%, Test: 44.62%
Epoch: 475, Loss: 1.3151, Train: 43.94%, Valid: 43.20%, Test: 43.50%
Epoch: 500, Loss: 1.2979, Train: 44.73%, Valid: 44.03%, Test: 44.12%
Epoch: 525, Loss: 1.3084, Train: 44.32%, Valid: 43.80%, Test: 43.79%
Epoch: 550, Loss: 1.2992, Train: 44.10%, Valid: 43.55%, Test: 43.81%
Epoch: 575, Loss: 1.3062, Train: 45.64%, Valid: 44.91%, Test: 44.87%
Epoch: 600, Loss: 1.3050, Train: 45.88%, Valid: 45.08%, Test: 45.25%
Epoch: 625, Loss: 1.2991, Train: 45.72%, Valid: 45.05%, Test: 45.31%
Epoch: 650, Loss: 1.2845, Train: 46.04%, Valid: 45.07%, Test: 45.30%
Epoch: 675, Loss: 1.3023, Train: 45.75%, Valid: 45.08%, Test: 45.14%
Epoch: 700, Loss: 1.2960, Train: 45.77%, Valid: 44.90%, Test: 45.02%
Epoch: 725, Loss: 1.2936, Train: 44.56%, Valid: 43.81%, Test: 43.89%
Epoch: 750, Loss: 1.3120, Train: 44.68%, Valid: 44.14%, Test: 44.05%
Epoch: 775, Loss: 1.3057, Train: 45.72%, Valid: 44.86%, Test: 45.01%
Epoch: 800, Loss: 1.2963, Train: 45.32%, Valid: 44.69%, Test: 44.65%
Epoch: 825, Loss: 1.3107, Train: 44.94%, Valid: 44.13%, Test: 44.10%
Epoch: 850, Loss: 1.3059, Train: 45.77%, Valid: 44.95%, Test: 44.99%
Epoch: 875, Loss: 1.3033, Train: 44.37%, Valid: 43.79%, Test: 43.65%
Epoch: 900, Loss: 1.2977, Train: 45.67%, Valid: 45.01%, Test: 45.09%
Epoch: 925, Loss: 1.2954, Train: 44.82%, Valid: 44.11%, Test: 44.11%
Epoch: 950, Loss: 1.2786, Train: 45.74%, Valid: 45.06%, Test: 45.07%
Epoch: 975, Loss: 1.2902, Train: 45.05%, Valid: 44.18%, Test: 44.18%
Run 01:
Highest Train: 47.00
Highest Valid: 45.81
  Final Train: 46.51
   Final Test: 45.80
All runs:
Highest Train: 47.00, nan
Highest Valid: 45.81, nan
  Final Train: 46.51, nan
   Final Test: 45.80, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 2.1263, Train: 21.95%, Valid: 22.09%, Test: 21.80%
Epoch: 25, Loss: 1.4953, Train: 30.14%, Valid: 29.71%, Test: 30.39%
Epoch: 50, Loss: 1.4992, Train: 28.75%, Valid: 28.56%, Test: 28.84%
Epoch: 75, Loss: 1.4749, Train: 34.23%, Valid: 34.06%, Test: 34.96%
Epoch: 100, Loss: 1.4719, Train: 34.25%, Valid: 34.17%, Test: 34.90%
Epoch: 125, Loss: 1.4621, Train: 34.22%, Valid: 33.86%, Test: 34.78%
Epoch: 150, Loss: 1.4888, Train: 33.89%, Valid: 33.79%, Test: 34.59%
Epoch: 175, Loss: 1.4467, Train: 33.01%, Valid: 32.85%, Test: 33.80%
Epoch: 200, Loss: 1.4367, Train: 33.32%, Valid: 33.14%, Test: 33.96%
Epoch: 225, Loss: 1.4343, Train: 33.37%, Valid: 33.17%, Test: 34.08%
Epoch: 250, Loss: 1.4477, Train: 32.82%, Valid: 32.65%, Test: 33.46%
Epoch: 275, Loss: 1.4266, Train: 32.80%, Valid: 32.66%, Test: 33.58%
Epoch: 300, Loss: 1.4388, Train: 32.68%, Valid: 32.49%, Test: 33.45%
Epoch: 325, Loss: 1.4206, Train: 33.56%, Valid: 33.35%, Test: 34.23%
Epoch: 350, Loss: 1.4287, Train: 33.02%, Valid: 32.94%, Test: 33.79%
Epoch: 375, Loss: 1.4219, Train: 33.64%, Valid: 33.50%, Test: 34.26%
Epoch: 400, Loss: 1.4305, Train: 33.70%, Valid: 33.54%, Test: 34.34%
Epoch: 425, Loss: 1.4207, Train: 34.13%, Valid: 33.90%, Test: 34.66%
Epoch: 450, Loss: 1.4509, Train: 33.66%, Valid: 33.52%, Test: 34.13%
Epoch: 475, Loss: 1.4166, Train: 34.24%, Valid: 34.20%, Test: 34.75%
Epoch: 500, Loss: 1.4209, Train: 34.30%, Valid: 34.14%, Test: 34.81%
Epoch: 525, Loss: 1.4124, Train: 33.69%, Valid: 33.67%, Test: 34.15%
Epoch: 550, Loss: 1.4306, Train: 34.02%, Valid: 33.76%, Test: 34.57%
Epoch: 575, Loss: 1.4201, Train: 32.97%, Valid: 32.75%, Test: 33.51%
Epoch: 600, Loss: 1.4143, Train: 34.89%, Valid: 34.74%, Test: 35.33%
Epoch: 625, Loss: 1.4136, Train: 34.37%, Valid: 34.30%, Test: 34.87%
Epoch: 650, Loss: 1.4133, Train: 35.19%, Valid: 35.12%, Test: 35.55%
Epoch: 675, Loss: 1.4197, Train: 25.32%, Valid: 25.45%, Test: 25.71%
Epoch: 700, Loss: 1.4440, Train: 33.27%, Valid: 33.26%, Test: 33.94%
Epoch: 725, Loss: 1.4245, Train: 33.05%, Valid: 33.19%, Test: 33.74%
Epoch: 750, Loss: 1.4256, Train: 34.42%, Valid: 34.46%, Test: 34.94%
Epoch: 775, Loss: 1.4162, Train: 34.90%, Valid: 34.91%, Test: 35.52%
Epoch: 800, Loss: 1.4068, Train: 32.72%, Valid: 32.64%, Test: 33.26%
Epoch: 825, Loss: 1.4433, Train: 31.28%, Valid: 31.13%, Test: 31.85%
Epoch: 850, Loss: 1.4303, Train: 33.33%, Valid: 33.22%, Test: 33.86%
Epoch: 875, Loss: 1.4213, Train: 33.03%, Valid: 32.85%, Test: 33.69%
Epoch: 900, Loss: 1.4136, Train: 34.49%, Valid: 34.39%, Test: 35.02%
Epoch: 925, Loss: 1.4122, Train: 33.39%, Valid: 33.36%, Test: 33.99%
Epoch: 950, Loss: 1.4077, Train: 34.73%, Valid: 34.62%, Test: 35.28%
Epoch: 975, Loss: 1.4204, Train: 34.94%, Valid: 34.88%, Test: 35.51%
Run 01:
Highest Train: 35.75
Highest Valid: 35.64
  Final Train: 35.68
   Final Test: 36.10
All runs:
Highest Train: 35.75, nan
Highest Valid: 35.64, nan
  Final Train: 35.68, nan
   Final Test: 36.10, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 28.4600, Train: 14.85%, Valid: 14.63%, Test: 14.64%
Epoch: 25, Loss: 2.6114, Train: 19.13%, Valid: 19.00%, Test: 19.13%
Epoch: 50, Loss: 2.0582, Train: 24.21%, Valid: 23.93%, Test: 24.29%
Epoch: 75, Loss: 2.4559, Train: 28.56%, Valid: 28.32%, Test: 28.64%
Epoch: 100, Loss: 1.7488, Train: 23.08%, Valid: 22.81%, Test: 23.18%
Epoch: 125, Loss: 1.7104, Train: 28.70%, Valid: 28.50%, Test: 28.79%
Epoch: 150, Loss: 1.6815, Train: 28.62%, Valid: 28.39%, Test: 28.70%
Epoch: 175, Loss: 1.6292, Train: 28.39%, Valid: 28.23%, Test: 28.49%
Epoch: 200, Loss: 1.6155, Train: 28.87%, Valid: 28.66%, Test: 28.93%
Epoch: 225, Loss: 1.5876, Train: 27.94%, Valid: 27.70%, Test: 28.01%
Epoch: 250, Loss: 1.5805, Train: 28.59%, Valid: 28.40%, Test: 28.66%
Epoch: 275, Loss: 1.5357, Train: 28.66%, Valid: 28.48%, Test: 28.72%
Epoch: 300, Loss: 1.5196, Train: 28.76%, Valid: 28.61%, Test: 28.86%
Epoch: 325, Loss: 1.5313, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 350, Loss: 1.5059, Train: 28.75%, Valid: 28.59%, Test: 28.84%
Epoch: 375, Loss: 1.4909, Train: 28.78%, Valid: 28.61%, Test: 28.90%
Epoch: 400, Loss: 1.4855, Train: 28.78%, Valid: 28.61%, Test: 28.89%
Epoch: 425, Loss: 1.4922, Train: 28.79%, Valid: 28.61%, Test: 28.88%
Epoch: 450, Loss: 1.4928, Train: 28.73%, Valid: 28.54%, Test: 28.82%
Epoch: 475, Loss: 1.5106, Train: 28.76%, Valid: 28.59%, Test: 28.88%
Epoch: 500, Loss: 1.5067, Train: 28.75%, Valid: 28.57%, Test: 28.85%
Epoch: 525, Loss: 1.5277, Train: 28.73%, Valid: 28.54%, Test: 28.84%
Epoch: 550, Loss: 1.4601, Train: 28.74%, Valid: 28.55%, Test: 28.84%
Epoch: 575, Loss: 1.4702, Train: 28.73%, Valid: 28.54%, Test: 28.83%
Epoch: 600, Loss: 1.4881, Train: 28.74%, Valid: 28.55%, Test: 28.84%
Epoch: 625, Loss: 1.4613, Train: 28.73%, Valid: 28.55%, Test: 28.82%
Epoch: 650, Loss: 1.4794, Train: 28.74%, Valid: 28.55%, Test: 28.84%
Epoch: 675, Loss: 1.4480, Train: 28.73%, Valid: 28.57%, Test: 28.84%
Epoch: 700, Loss: 1.4508, Train: 28.71%, Valid: 28.55%, Test: 28.81%
Epoch: 725, Loss: 1.4539, Train: 28.76%, Valid: 28.60%, Test: 28.86%
Epoch: 750, Loss: 1.4522, Train: 28.88%, Valid: 28.69%, Test: 28.97%
Epoch: 775, Loss: 1.4745, Train: 31.83%, Valid: 31.73%, Test: 31.96%
Epoch: 800, Loss: 1.4547, Train: 35.19%, Valid: 34.95%, Test: 35.05%
Epoch: 825, Loss: 1.4561, Train: 30.79%, Valid: 30.58%, Test: 30.85%
Epoch: 850, Loss: 1.4673, Train: 28.71%, Valid: 28.55%, Test: 28.81%
Epoch: 875, Loss: 1.4431, Train: 29.22%, Valid: 29.00%, Test: 29.28%
Epoch: 900, Loss: 1.4758, Train: 36.77%, Valid: 36.73%, Test: 36.71%
Epoch: 925, Loss: 1.4421, Train: 31.46%, Valid: 31.32%, Test: 31.52%
Epoch: 950, Loss: 1.4514, Train: 34.90%, Valid: 34.74%, Test: 34.89%
Epoch: 975, Loss: 1.4337, Train: 34.99%, Valid: 34.75%, Test: 34.91%
Run 01:
Highest Train: 38.78
Highest Valid: 38.69
  Final Train: 38.78
   Final Test: 38.68
All runs:
Highest Train: 38.78, nan
Highest Valid: 38.69, nan
  Final Train: 38.78, nan
   Final Test: 38.68, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6037, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4813, Train: 35.30%, Valid: 35.10%, Test: 35.15%
Epoch: 50, Loss: 1.4444, Train: 37.48%, Valid: 36.62%, Test: 37.29%
Epoch: 75, Loss: 1.4077, Train: 38.94%, Valid: 38.16%, Test: 38.67%
Epoch: 100, Loss: 1.3776, Train: 40.61%, Valid: 39.91%, Test: 40.32%
Epoch: 125, Loss: 1.3648, Train: 40.97%, Valid: 40.23%, Test: 40.49%
Epoch: 150, Loss: 1.3500, Train: 42.67%, Valid: 41.96%, Test: 42.21%
Epoch: 175, Loss: 1.3421, Train: 42.30%, Valid: 41.66%, Test: 41.93%
Epoch: 200, Loss: 1.3257, Train: 42.63%, Valid: 41.92%, Test: 42.11%
Epoch: 225, Loss: 1.3344, Train: 43.63%, Valid: 42.80%, Test: 43.18%
Epoch: 250, Loss: 1.3293, Train: 42.36%, Valid: 41.58%, Test: 41.88%
Epoch: 275, Loss: 1.3240, Train: 43.34%, Valid: 42.43%, Test: 42.57%
Epoch: 300, Loss: 1.3265, Train: 43.08%, Valid: 42.34%, Test: 42.54%
Epoch: 325, Loss: 1.3189, Train: 44.41%, Valid: 43.57%, Test: 43.94%
Epoch: 350, Loss: 1.3211, Train: 43.76%, Valid: 42.99%, Test: 43.19%
Epoch: 375, Loss: 1.3195, Train: 43.05%, Valid: 42.34%, Test: 42.32%
Epoch: 400, Loss: 1.3197, Train: 43.23%, Valid: 42.66%, Test: 42.73%
Epoch: 425, Loss: 1.3002, Train: 45.10%, Valid: 44.00%, Test: 44.52%
Epoch: 450, Loss: 1.2995, Train: 43.60%, Valid: 42.78%, Test: 42.91%
Epoch: 475, Loss: 1.3078, Train: 43.15%, Valid: 42.37%, Test: 42.58%
Epoch: 500, Loss: 1.3029, Train: 44.55%, Valid: 43.94%, Test: 44.06%
Epoch: 525, Loss: 1.3075, Train: 43.33%, Valid: 42.48%, Test: 42.83%
Epoch: 550, Loss: 1.3324, Train: 43.01%, Valid: 42.31%, Test: 42.48%
Epoch: 575, Loss: 1.2927, Train: 45.52%, Valid: 44.42%, Test: 44.97%
Epoch: 600, Loss: 1.3194, Train: 45.09%, Valid: 44.19%, Test: 44.31%
Epoch: 625, Loss: 1.3015, Train: 44.52%, Valid: 43.65%, Test: 43.95%
Epoch: 650, Loss: 1.3103, Train: 44.46%, Valid: 43.60%, Test: 43.87%
Epoch: 675, Loss: 1.2987, Train: 43.65%, Valid: 42.56%, Test: 43.15%
Epoch: 700, Loss: 1.2911, Train: 44.27%, Valid: 43.46%, Test: 43.60%
Epoch: 725, Loss: 1.3035, Train: 43.67%, Valid: 42.64%, Test: 43.04%
Epoch: 750, Loss: 1.2984, Train: 44.97%, Valid: 43.96%, Test: 44.36%
Epoch: 775, Loss: 1.2864, Train: 44.16%, Valid: 43.29%, Test: 43.40%
Epoch: 800, Loss: 1.3217, Train: 45.24%, Valid: 44.33%, Test: 44.53%
Epoch: 825, Loss: 1.2984, Train: 45.44%, Valid: 44.53%, Test: 44.84%
Epoch: 850, Loss: 1.2923, Train: 44.47%, Valid: 43.59%, Test: 43.75%
Epoch: 875, Loss: 1.2815, Train: 44.40%, Valid: 43.58%, Test: 43.84%
Epoch: 900, Loss: 1.2851, Train: 43.83%, Valid: 43.05%, Test: 43.15%
Epoch: 925, Loss: 1.2816, Train: 46.08%, Valid: 45.15%, Test: 45.51%
Epoch: 950, Loss: 1.2837, Train: 45.34%, Valid: 44.32%, Test: 44.87%
Epoch: 975, Loss: 1.2862, Train: 45.66%, Valid: 44.83%, Test: 45.05%
Run 01:
Highest Train: 46.08
Highest Valid: 45.15
  Final Train: 46.08
   Final Test: 45.51
All runs:
Highest Train: 46.08, nan
Highest Valid: 45.15, nan
  Final Train: 46.08, nan
   Final Test: 45.51, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.8504, Train: 24.28%, Valid: 23.99%, Test: 24.38%
Epoch: 25, Loss: 1.4967, Train: 28.78%, Valid: 28.58%, Test: 28.84%
Epoch: 50, Loss: 1.4535, Train: 30.19%, Valid: 30.01%, Test: 30.15%
Epoch: 75, Loss: 1.4313, Train: 38.28%, Valid: 37.88%, Test: 38.10%
Epoch: 100, Loss: 1.4200, Train: 40.13%, Valid: 39.65%, Test: 39.81%
Epoch: 125, Loss: 1.4040, Train: 40.14%, Valid: 39.80%, Test: 39.81%
Epoch: 150, Loss: 1.4026, Train: 39.68%, Valid: 39.30%, Test: 39.44%
Epoch: 175, Loss: 1.4155, Train: 39.82%, Valid: 39.50%, Test: 39.48%
Epoch: 200, Loss: 1.4065, Train: 39.63%, Valid: 39.43%, Test: 39.41%
Epoch: 225, Loss: 1.4164, Train: 39.22%, Valid: 38.92%, Test: 39.19%
Epoch: 250, Loss: 1.4128, Train: 39.38%, Valid: 38.97%, Test: 39.10%
Epoch: 275, Loss: 1.4053, Train: 39.62%, Valid: 39.24%, Test: 39.39%
Epoch: 300, Loss: 1.3917, Train: 39.76%, Valid: 39.38%, Test: 39.62%
Epoch: 325, Loss: 1.3943, Train: 39.99%, Valid: 39.74%, Test: 39.76%
Epoch: 350, Loss: 1.3852, Train: 40.11%, Valid: 39.61%, Test: 39.72%
Epoch: 375, Loss: 1.3932, Train: 40.11%, Valid: 39.71%, Test: 39.85%
Epoch: 400, Loss: 1.3828, Train: 40.15%, Valid: 39.70%, Test: 39.88%
Epoch: 425, Loss: 1.3780, Train: 40.45%, Valid: 39.94%, Test: 40.21%
Epoch: 450, Loss: 1.3745, Train: 40.08%, Valid: 39.61%, Test: 39.77%
Epoch: 475, Loss: 1.3793, Train: 40.61%, Valid: 40.13%, Test: 40.34%
Epoch: 500, Loss: 1.3774, Train: 40.58%, Valid: 40.25%, Test: 40.24%
Epoch: 525, Loss: 1.3776, Train: 40.62%, Valid: 40.31%, Test: 40.30%
Epoch: 550, Loss: 1.3790, Train: 40.35%, Valid: 39.81%, Test: 40.00%
Epoch: 575, Loss: 1.3736, Train: 40.52%, Valid: 40.02%, Test: 40.12%
Epoch: 600, Loss: 1.3832, Train: 40.23%, Valid: 39.99%, Test: 40.13%
Epoch: 625, Loss: 1.3646, Train: 40.97%, Valid: 40.47%, Test: 40.70%
Epoch: 650, Loss: 1.3779, Train: 39.86%, Valid: 39.63%, Test: 39.54%
Epoch: 675, Loss: 1.3805, Train: 40.55%, Valid: 40.17%, Test: 40.17%
Epoch: 700, Loss: 1.3843, Train: 40.52%, Valid: 40.01%, Test: 40.09%
Epoch: 725, Loss: 1.3822, Train: 40.37%, Valid: 40.11%, Test: 40.05%
Epoch: 750, Loss: 1.3812, Train: 40.41%, Valid: 39.98%, Test: 39.98%
Epoch: 775, Loss: 1.3759, Train: 40.11%, Valid: 39.61%, Test: 39.71%
Epoch: 800, Loss: 1.3689, Train: 40.90%, Valid: 40.43%, Test: 40.56%
Epoch: 825, Loss: 1.3835, Train: 39.83%, Valid: 39.53%, Test: 39.63%
Epoch: 850, Loss: 1.3743, Train: 40.44%, Valid: 39.96%, Test: 40.06%
Epoch: 875, Loss: 1.3778, Train: 40.42%, Valid: 40.01%, Test: 39.97%
Epoch: 900, Loss: 1.3707, Train: 40.86%, Valid: 40.42%, Test: 40.46%
Epoch: 925, Loss: 1.3678, Train: 41.11%, Valid: 40.70%, Test: 40.71%
Epoch: 950, Loss: 1.3735, Train: 39.85%, Valid: 39.34%, Test: 39.54%
Epoch: 975, Loss: 1.3846, Train: 40.67%, Valid: 40.44%, Test: 40.39%
Run 01:
Highest Train: 41.90
Highest Valid: 41.46
  Final Train: 41.84
   Final Test: 41.59
All runs:
Highest Train: 41.90, nan
Highest Valid: 41.46, nan
  Final Train: 41.84, nan
   Final Test: 41.59, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 5.4335, Train: 14.66%, Valid: 14.86%, Test: 14.51%
Epoch: 25, Loss: 2.2032, Train: 17.43%, Valid: 17.39%, Test: 17.36%
Epoch: 50, Loss: 2.9293, Train: 13.14%, Valid: 13.15%, Test: 12.73%
Epoch: 75, Loss: 2.0253, Train: 27.41%, Valid: 27.23%, Test: 27.55%
Epoch: 100, Loss: 1.7816, Train: 18.51%, Valid: 18.64%, Test: 18.21%
Epoch: 125, Loss: 1.7536, Train: 21.42%, Valid: 21.53%, Test: 21.36%
Epoch: 150, Loss: 1.7263, Train: 27.14%, Valid: 27.01%, Test: 27.16%
Epoch: 175, Loss: 1.6147, Train: 28.48%, Valid: 28.46%, Test: 28.51%
Epoch: 200, Loss: 1.5634, Train: 28.45%, Valid: 28.41%, Test: 28.50%
Epoch: 225, Loss: 1.5781, Train: 25.73%, Valid: 25.53%, Test: 25.54%
Epoch: 250, Loss: 1.5725, Train: 27.85%, Valid: 27.64%, Test: 27.60%
Epoch: 275, Loss: 1.5275, Train: 28.81%, Valid: 28.67%, Test: 28.82%
Epoch: 300, Loss: 1.5412, Train: 28.99%, Valid: 28.70%, Test: 28.93%
Epoch: 325, Loss: 1.5122, Train: 29.01%, Valid: 28.82%, Test: 29.01%
Epoch: 350, Loss: 1.5340, Train: 28.84%, Valid: 28.67%, Test: 28.82%
Epoch: 375, Loss: 1.5011, Train: 28.60%, Valid: 28.37%, Test: 28.56%
Epoch: 400, Loss: 1.4902, Train: 28.80%, Valid: 28.58%, Test: 28.71%
Epoch: 425, Loss: 1.5060, Train: 27.65%, Valid: 27.42%, Test: 27.73%
Epoch: 450, Loss: 1.4926, Train: 28.69%, Valid: 28.44%, Test: 28.65%
Epoch: 475, Loss: 1.4562, Train: 29.27%, Valid: 29.07%, Test: 29.25%
Epoch: 500, Loss: 1.4436, Train: 31.21%, Valid: 31.03%, Test: 31.27%
Epoch: 525, Loss: 1.4412, Train: 33.57%, Valid: 33.23%, Test: 34.07%
Epoch: 550, Loss: 1.4358, Train: 33.83%, Valid: 33.66%, Test: 34.41%
Epoch: 575, Loss: 1.4308, Train: 33.92%, Valid: 33.70%, Test: 34.42%
Epoch: 600, Loss: 1.4277, Train: 33.83%, Valid: 33.58%, Test: 34.48%
Epoch: 625, Loss: 1.4300, Train: 33.76%, Valid: 33.50%, Test: 34.41%
Epoch: 650, Loss: 1.4221, Train: 34.06%, Valid: 33.70%, Test: 34.67%
Epoch: 675, Loss: 1.4220, Train: 34.05%, Valid: 33.83%, Test: 34.66%
Epoch: 700, Loss: 1.4246, Train: 33.77%, Valid: 33.48%, Test: 34.41%
Epoch: 725, Loss: 1.4186, Train: 34.08%, Valid: 33.85%, Test: 34.73%
Epoch: 750, Loss: 1.4176, Train: 34.46%, Valid: 34.19%, Test: 35.12%
Epoch: 775, Loss: 1.4142, Train: 34.68%, Valid: 34.52%, Test: 35.35%
Epoch: 800, Loss: 1.4136, Train: 36.15%, Valid: 35.88%, Test: 36.48%
Epoch: 825, Loss: 1.4163, Train: 35.77%, Valid: 35.55%, Test: 36.23%
Epoch: 850, Loss: 1.4109, Train: 37.42%, Valid: 37.08%, Test: 37.81%
Epoch: 875, Loss: 1.4078, Train: 37.84%, Valid: 37.60%, Test: 38.26%
Epoch: 900, Loss: 1.4092, Train: 38.39%, Valid: 38.38%, Test: 38.79%
Epoch: 925, Loss: 1.4090, Train: 38.63%, Valid: 38.53%, Test: 38.89%
Epoch: 950, Loss: 1.4032, Train: 39.25%, Valid: 38.94%, Test: 39.33%
Epoch: 975, Loss: 1.4052, Train: 39.39%, Valid: 38.96%, Test: 39.47%
Run 01:
Highest Train: 39.62
Highest Valid: 39.22
  Final Train: 39.62
   Final Test: 39.62
All runs:
Highest Train: 39.62, nan
Highest Valid: 39.22, nan
  Final Train: 39.62, nan
   Final Test: 39.62, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6104, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.5008, Train: 34.15%, Valid: 34.04%, Test: 34.24%
Epoch: 50, Loss: 1.4827, Train: 35.05%, Valid: 34.64%, Test: 34.85%
Epoch: 75, Loss: 1.4641, Train: 36.31%, Valid: 35.43%, Test: 35.67%
Epoch: 100, Loss: 1.4503, Train: 36.92%, Valid: 35.72%, Test: 36.06%
Epoch: 125, Loss: 1.4380, Train: 37.72%, Valid: 36.16%, Test: 36.45%
Epoch: 150, Loss: 1.4310, Train: 37.84%, Valid: 35.71%, Test: 36.17%
Epoch: 175, Loss: 1.4179, Train: 38.69%, Valid: 35.66%, Test: 36.52%
Epoch: 200, Loss: 1.4124, Train: 38.89%, Valid: 35.53%, Test: 36.36%
Epoch: 225, Loss: 1.4163, Train: 39.23%, Valid: 36.09%, Test: 36.48%
Epoch: 250, Loss: 1.3972, Train: 39.73%, Valid: 35.84%, Test: 36.20%
Epoch: 275, Loss: 1.3913, Train: 39.96%, Valid: 35.27%, Test: 35.92%
Epoch: 300, Loss: 1.3863, Train: 40.18%, Valid: 35.88%, Test: 36.17%
Epoch: 325, Loss: 1.3787, Train: 40.59%, Valid: 35.54%, Test: 36.00%
Epoch: 350, Loss: 1.3711, Train: 40.99%, Valid: 35.20%, Test: 35.74%
Epoch: 375, Loss: 1.3706, Train: 41.04%, Valid: 34.83%, Test: 35.65%
Epoch: 400, Loss: 1.3610, Train: 41.66%, Valid: 35.37%, Test: 35.68%
Epoch: 425, Loss: 1.3534, Train: 41.71%, Valid: 35.05%, Test: 35.53%
Epoch: 450, Loss: 1.3519, Train: 41.93%, Valid: 34.48%, Test: 35.12%
Epoch: 475, Loss: 1.3413, Train: 42.79%, Valid: 34.75%, Test: 35.44%
Epoch: 500, Loss: 1.3466, Train: 42.85%, Valid: 34.47%, Test: 35.01%
Epoch: 525, Loss: 1.3345, Train: 42.70%, Valid: 35.02%, Test: 35.60%
Epoch: 550, Loss: 1.3293, Train: 43.40%, Valid: 34.14%, Test: 34.86%
Epoch: 575, Loss: 1.3256, Train: 43.58%, Valid: 33.91%, Test: 34.84%
Epoch: 600, Loss: 1.3201, Train: 43.88%, Valid: 34.24%, Test: 35.10%
Epoch: 625, Loss: 1.3160, Train: 44.12%, Valid: 34.37%, Test: 34.89%
Epoch: 650, Loss: 1.3093, Train: 44.41%, Valid: 33.19%, Test: 34.27%
Epoch: 675, Loss: 1.3122, Train: 44.04%, Valid: 33.03%, Test: 33.85%
Epoch: 700, Loss: 1.3090, Train: 43.78%, Valid: 34.04%, Test: 34.78%
Epoch: 725, Loss: 1.3271, Train: 44.56%, Valid: 33.36%, Test: 34.26%
Epoch: 750, Loss: 1.2956, Train: 44.94%, Valid: 34.17%, Test: 34.60%
Epoch: 775, Loss: 1.2903, Train: 45.29%, Valid: 33.49%, Test: 34.21%
Epoch: 800, Loss: 1.2944, Train: 45.52%, Valid: 33.42%, Test: 34.09%
Epoch: 825, Loss: 1.2843, Train: 45.77%, Valid: 33.42%, Test: 34.21%
Epoch: 850, Loss: 1.2974, Train: 45.67%, Valid: 33.56%, Test: 34.32%
Epoch: 875, Loss: 1.2751, Train: 46.12%, Valid: 33.56%, Test: 34.44%
Epoch: 900, Loss: 1.2869, Train: 45.64%, Valid: 33.62%, Test: 33.97%
Epoch: 925, Loss: 1.2693, Train: 46.35%, Valid: 33.03%, Test: 33.79%
Epoch: 950, Loss: 1.2826, Train: 45.42%, Valid: 32.07%, Test: 33.17%
Epoch: 975, Loss: 1.2693, Train: 46.11%, Valid: 33.17%, Test: 33.82%
Run 01:
Highest Train: 46.57
Highest Valid: 36.19
  Final Train: 38.45
   Final Test: 36.60
All runs:
Highest Train: 46.57, nan
Highest Valid: 36.19, nan
  Final Train: 38.45, nan
   Final Test: 36.60, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6112, Train: 30.80%, Valid: 30.69%, Test: 30.89%
Epoch: 25, Loss: 1.4971, Train: 34.33%, Valid: 34.12%, Test: 34.37%
Epoch: 50, Loss: 1.4728, Train: 36.17%, Valid: 36.06%, Test: 36.32%
Epoch: 75, Loss: 1.4512, Train: 35.96%, Valid: 35.58%, Test: 35.86%
Epoch: 100, Loss: 1.4288, Train: 37.16%, Valid: 36.78%, Test: 36.99%
Epoch: 125, Loss: 1.4124, Train: 37.43%, Valid: 36.75%, Test: 37.14%
Epoch: 150, Loss: 1.3820, Train: 39.91%, Valid: 38.78%, Test: 39.26%
Epoch: 175, Loss: 1.3605, Train: 41.19%, Valid: 39.70%, Test: 40.09%
Epoch: 200, Loss: 1.3685, Train: 40.71%, Valid: 39.34%, Test: 39.79%
Epoch: 225, Loss: 1.3466, Train: 41.82%, Valid: 40.28%, Test: 40.67%
Epoch: 250, Loss: 1.3677, Train: 40.89%, Valid: 39.48%, Test: 39.98%
Epoch: 275, Loss: 1.3417, Train: 41.74%, Valid: 39.79%, Test: 40.17%
Epoch: 300, Loss: 1.3317, Train: 42.24%, Valid: 40.21%, Test: 40.32%
Epoch: 325, Loss: 1.3321, Train: 42.23%, Valid: 40.04%, Test: 40.34%
Epoch: 350, Loss: 1.3210, Train: 42.64%, Valid: 40.36%, Test: 40.53%
Epoch: 375, Loss: 1.3389, Train: 41.24%, Valid: 38.95%, Test: 39.28%
Epoch: 400, Loss: 1.3175, Train: 42.98%, Valid: 40.45%, Test: 40.70%
Epoch: 425, Loss: 1.3366, Train: 41.69%, Valid: 39.68%, Test: 39.90%
Epoch: 450, Loss: 1.3131, Train: 42.95%, Valid: 39.99%, Test: 40.42%
Epoch: 475, Loss: 1.3086, Train: 43.07%, Valid: 39.91%, Test: 40.20%
Epoch: 500, Loss: 1.2992, Train: 43.68%, Valid: 40.52%, Test: 40.67%
Epoch: 525, Loss: 1.2967, Train: 43.85%, Valid: 40.39%, Test: 40.80%
Epoch: 550, Loss: 1.3139, Train: 43.29%, Valid: 40.50%, Test: 40.76%
Epoch: 575, Loss: 1.3128, Train: 42.24%, Valid: 39.06%, Test: 39.22%
Epoch: 600, Loss: 1.2898, Train: 44.17%, Valid: 40.89%, Test: 40.98%
Epoch: 625, Loss: 1.3271, Train: 42.36%, Valid: 39.42%, Test: 39.67%
Epoch: 650, Loss: 1.2905, Train: 44.06%, Valid: 40.71%, Test: 40.70%
Epoch: 675, Loss: 1.2784, Train: 44.55%, Valid: 41.04%, Test: 41.14%
Epoch: 700, Loss: 1.3541, Train: 41.42%, Valid: 38.93%, Test: 39.20%
Epoch: 725, Loss: 1.3000, Train: 43.79%, Valid: 41.07%, Test: 41.29%
Epoch: 750, Loss: 1.2815, Train: 44.54%, Valid: 41.67%, Test: 41.84%
Epoch: 775, Loss: 1.2828, Train: 44.04%, Valid: 41.23%, Test: 41.16%
Epoch: 800, Loss: 1.2743, Train: 44.64%, Valid: 41.46%, Test: 41.35%
Epoch: 825, Loss: 1.2671, Train: 45.08%, Valid: 41.92%, Test: 41.86%
Epoch: 850, Loss: 1.2662, Train: 44.92%, Valid: 41.52%, Test: 41.47%
Epoch: 875, Loss: 1.2590, Train: 45.25%, Valid: 42.06%, Test: 42.13%
Epoch: 900, Loss: 1.2623, Train: 45.28%, Valid: 41.92%, Test: 41.94%
Epoch: 925, Loss: 1.2563, Train: 45.68%, Valid: 42.00%, Test: 42.20%
Epoch: 950, Loss: 1.2903, Train: 44.27%, Valid: 41.44%, Test: 41.49%
Epoch: 975, Loss: 1.2506, Train: 46.05%, Valid: 42.43%, Test: 42.62%
Run 01:
Highest Train: 46.09
Highest Valid: 42.43
  Final Train: 46.05
   Final Test: 42.62
All runs:
Highest Train: 46.09, nan
Highest Valid: 42.43, nan
  Final Train: 46.05, nan
   Final Test: 42.62, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.7169, Train: 24.66%, Valid: 24.40%, Test: 24.58%
Epoch: 25, Loss: 1.4850, Train: 32.50%, Valid: 32.35%, Test: 33.22%
Epoch: 50, Loss: 1.4187, Train: 37.84%, Valid: 37.62%, Test: 37.80%
Epoch: 75, Loss: 1.4537, Train: 36.90%, Valid: 36.56%, Test: 37.24%
Epoch: 100, Loss: 1.3919, Train: 38.72%, Valid: 38.43%, Test: 38.76%
Epoch: 125, Loss: 1.3790, Train: 39.13%, Valid: 38.81%, Test: 39.21%
Epoch: 150, Loss: 1.3937, Train: 38.91%, Valid: 38.72%, Test: 39.03%
Epoch: 175, Loss: 1.3752, Train: 40.11%, Valid: 39.89%, Test: 40.21%
Epoch: 200, Loss: 1.4282, Train: 37.82%, Valid: 37.86%, Test: 38.13%
Epoch: 225, Loss: 1.3750, Train: 39.38%, Valid: 39.14%, Test: 39.51%
Epoch: 250, Loss: 1.3645, Train: 40.32%, Valid: 40.01%, Test: 40.39%
Epoch: 275, Loss: 1.3405, Train: 41.16%, Valid: 40.77%, Test: 41.19%
Epoch: 300, Loss: 1.3468, Train: 41.11%, Valid: 40.75%, Test: 41.10%
Epoch: 325, Loss: 1.3294, Train: 41.96%, Valid: 41.65%, Test: 42.02%
Epoch: 350, Loss: 1.3695, Train: 40.83%, Valid: 40.38%, Test: 40.52%
Epoch: 375, Loss: 1.3454, Train: 41.66%, Valid: 41.34%, Test: 41.61%
Epoch: 400, Loss: 1.3321, Train: 42.36%, Valid: 42.00%, Test: 42.27%
Epoch: 425, Loss: 1.3490, Train: 41.45%, Valid: 40.94%, Test: 41.00%
Epoch: 450, Loss: 1.3745, Train: 38.28%, Valid: 37.75%, Test: 37.79%
Epoch: 475, Loss: 1.3603, Train: 40.10%, Valid: 39.45%, Test: 40.06%
Epoch: 500, Loss: 1.3517, Train: 41.08%, Valid: 40.61%, Test: 40.82%
Epoch: 525, Loss: 1.3284, Train: 42.17%, Valid: 41.77%, Test: 41.90%
Epoch: 550, Loss: 1.3519, Train: 41.62%, Valid: 41.09%, Test: 41.47%
Epoch: 575, Loss: 1.3285, Train: 42.31%, Valid: 41.67%, Test: 41.85%
Epoch: 600, Loss: 1.3211, Train: 42.12%, Valid: 41.37%, Test: 41.50%
Epoch: 625, Loss: 1.3187, Train: 40.60%, Valid: 40.05%, Test: 40.30%
Epoch: 650, Loss: 1.3198, Train: 42.65%, Valid: 42.13%, Test: 42.04%
Epoch: 675, Loss: 1.3212, Train: 41.89%, Valid: 41.15%, Test: 41.29%
Epoch: 700, Loss: 1.3053, Train: 43.04%, Valid: 42.32%, Test: 42.41%
Epoch: 725, Loss: 1.3049, Train: 42.99%, Valid: 42.04%, Test: 42.34%
Epoch: 750, Loss: 1.2986, Train: 43.27%, Valid: 42.57%, Test: 42.79%
Epoch: 775, Loss: 1.2956, Train: 43.48%, Valid: 42.72%, Test: 42.83%
Epoch: 800, Loss: 1.3008, Train: 43.51%, Valid: 42.56%, Test: 42.82%
Epoch: 825, Loss: 1.3414, Train: 41.75%, Valid: 40.97%, Test: 41.29%
Epoch: 850, Loss: 1.3797, Train: 39.63%, Valid: 39.23%, Test: 39.31%
Epoch: 875, Loss: 1.3245, Train: 42.64%, Valid: 42.41%, Test: 42.47%
Epoch: 900, Loss: 1.3103, Train: 42.15%, Valid: 41.58%, Test: 41.89%
Epoch: 925, Loss: 1.3030, Train: 42.91%, Valid: 42.37%, Test: 42.49%
Epoch: 950, Loss: 1.3011, Train: 43.05%, Valid: 42.57%, Test: 42.58%
Epoch: 975, Loss: 1.2969, Train: 42.90%, Valid: 42.35%, Test: 42.53%
Run 01:
Highest Train: 43.52
Highest Valid: 42.82
  Final Train: 43.14
   Final Test: 42.64
All runs:
Highest Train: 43.52, nan
Highest Valid: 42.82, nan
  Final Train: 43.14, nan
   Final Test: 42.64, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6094, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.5010, Train: 34.22%, Valid: 34.00%, Test: 34.21%
Epoch: 50, Loss: 1.4820, Train: 35.21%, Valid: 34.81%, Test: 35.08%
Epoch: 75, Loss: 1.4680, Train: 36.13%, Valid: 35.32%, Test: 35.62%
Epoch: 100, Loss: 1.4525, Train: 36.67%, Valid: 35.58%, Test: 36.03%
Epoch: 125, Loss: 1.4410, Train: 37.41%, Valid: 35.91%, Test: 36.07%
Epoch: 150, Loss: 1.4279, Train: 38.04%, Valid: 35.71%, Test: 36.09%
Epoch: 175, Loss: 1.4184, Train: 38.60%, Valid: 35.85%, Test: 36.41%
Epoch: 200, Loss: 1.4143, Train: 38.73%, Valid: 35.90%, Test: 36.34%
Epoch: 225, Loss: 1.4030, Train: 39.36%, Valid: 35.23%, Test: 36.11%
Epoch: 250, Loss: 1.3949, Train: 39.60%, Valid: 35.75%, Test: 36.17%
Epoch: 275, Loss: 1.3879, Train: 39.90%, Valid: 34.45%, Test: 35.33%
Epoch: 300, Loss: 1.3812, Train: 40.05%, Valid: 35.54%, Test: 36.00%
Epoch: 325, Loss: 1.3725, Train: 40.99%, Valid: 35.33%, Test: 35.82%
Epoch: 350, Loss: 1.3690, Train: 41.01%, Valid: 35.40%, Test: 35.88%
Epoch: 375, Loss: 1.3699, Train: 40.97%, Valid: 34.72%, Test: 35.22%
Epoch: 400, Loss: 1.3783, Train: 40.67%, Valid: 34.30%, Test: 34.87%
Epoch: 425, Loss: 1.3516, Train: 41.75%, Valid: 34.92%, Test: 35.56%
Epoch: 450, Loss: 1.3446, Train: 42.10%, Valid: 34.87%, Test: 35.43%
Epoch: 475, Loss: 1.3392, Train: 42.54%, Valid: 34.57%, Test: 35.22%
Epoch: 500, Loss: 1.3422, Train: 42.22%, Valid: 33.99%, Test: 34.65%
Epoch: 525, Loss: 1.3316, Train: 42.76%, Valid: 34.49%, Test: 35.09%
Epoch: 550, Loss: 1.3281, Train: 43.17%, Valid: 34.98%, Test: 35.42%
Epoch: 575, Loss: 1.3446, Train: 42.60%, Valid: 34.21%, Test: 34.66%
Epoch: 600, Loss: 1.3160, Train: 43.64%, Valid: 34.17%, Test: 34.75%
Epoch: 625, Loss: 1.3204, Train: 43.46%, Valid: 34.58%, Test: 35.23%
Epoch: 650, Loss: 1.3180, Train: 43.64%, Valid: 34.01%, Test: 34.71%
Epoch: 675, Loss: 1.3127, Train: 43.60%, Valid: 33.53%, Test: 34.13%
Epoch: 700, Loss: 1.3137, Train: 43.87%, Valid: 34.00%, Test: 34.49%
Epoch: 725, Loss: 1.2993, Train: 44.51%, Valid: 33.48%, Test: 33.98%
Epoch: 750, Loss: 1.3086, Train: 44.02%, Valid: 34.05%, Test: 34.40%
Epoch: 775, Loss: 1.3117, Train: 44.10%, Valid: 33.24%, Test: 33.84%
Epoch: 800, Loss: 1.3017, Train: 44.59%, Valid: 33.20%, Test: 33.77%
Epoch: 825, Loss: 1.3000, Train: 44.60%, Valid: 34.07%, Test: 34.48%
Epoch: 850, Loss: 1.2896, Train: 45.26%, Valid: 34.03%, Test: 34.43%
Epoch: 875, Loss: 1.2865, Train: 44.44%, Valid: 32.11%, Test: 32.79%
Epoch: 900, Loss: 1.2833, Train: 45.39%, Valid: 33.19%, Test: 33.75%
Epoch: 925, Loss: 1.2792, Train: 45.64%, Valid: 33.36%, Test: 33.92%
Epoch: 950, Loss: 1.3011, Train: 45.14%, Valid: 33.92%, Test: 34.50%
Epoch: 975, Loss: 1.2754, Train: 45.85%, Valid: 32.88%, Test: 33.55%
Run 01:
Highest Train: 46.02
Highest Valid: 36.16
  Final Train: 37.78
   Final Test: 36.31
All runs:
Highest Train: 46.02, nan
Highest Valid: 36.16, nan
  Final Train: 37.78, nan
   Final Test: 36.31, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6105, Train: 32.52%, Valid: 32.31%, Test: 32.42%
Epoch: 25, Loss: 1.5256, Train: 28.68%, Valid: 28.41%, Test: 28.73%
Epoch: 50, Loss: 1.4674, Train: 30.64%, Valid: 30.70%, Test: 31.11%
Epoch: 75, Loss: 1.4274, Train: 37.50%, Valid: 37.15%, Test: 37.31%
Epoch: 100, Loss: 1.4082, Train: 39.94%, Valid: 39.84%, Test: 40.33%
Epoch: 125, Loss: 1.4201, Train: 39.00%, Valid: 38.46%, Test: 39.22%
Epoch: 150, Loss: 1.4072, Train: 39.47%, Valid: 39.11%, Test: 39.58%
Epoch: 175, Loss: 1.3961, Train: 40.02%, Valid: 39.22%, Test: 39.79%
Epoch: 200, Loss: 1.3872, Train: 39.99%, Valid: 39.18%, Test: 39.60%
Epoch: 225, Loss: 1.3868, Train: 39.88%, Valid: 38.88%, Test: 39.35%
Epoch: 250, Loss: 1.3706, Train: 40.43%, Valid: 39.43%, Test: 39.68%
Epoch: 275, Loss: 1.3742, Train: 40.09%, Valid: 39.10%, Test: 39.30%
Epoch: 300, Loss: 1.3591, Train: 40.75%, Valid: 39.74%, Test: 40.04%
Epoch: 325, Loss: 1.3493, Train: 41.07%, Valid: 39.96%, Test: 40.36%
Epoch: 350, Loss: 1.3517, Train: 41.19%, Valid: 39.93%, Test: 40.31%
Epoch: 375, Loss: 1.3441, Train: 41.41%, Valid: 40.21%, Test: 40.57%
Epoch: 400, Loss: 1.3406, Train: 41.66%, Valid: 40.44%, Test: 40.83%
Epoch: 425, Loss: 1.3326, Train: 41.80%, Valid: 40.42%, Test: 40.78%
Epoch: 450, Loss: 1.3504, Train: 41.20%, Valid: 39.68%, Test: 40.01%
Epoch: 475, Loss: 1.3431, Train: 41.30%, Valid: 40.14%, Test: 40.34%
Epoch: 500, Loss: 1.3278, Train: 42.05%, Valid: 40.62%, Test: 41.00%
Epoch: 525, Loss: 1.3255, Train: 41.79%, Valid: 40.46%, Test: 40.65%
Epoch: 550, Loss: 1.3440, Train: 41.70%, Valid: 40.52%, Test: 40.90%
Epoch: 575, Loss: 1.3253, Train: 42.30%, Valid: 41.03%, Test: 41.49%
Epoch: 600, Loss: 1.3200, Train: 42.45%, Valid: 40.99%, Test: 41.43%
Epoch: 625, Loss: 1.3175, Train: 42.60%, Valid: 40.98%, Test: 41.47%
Epoch: 650, Loss: 1.3454, Train: 40.56%, Valid: 39.23%, Test: 39.78%
Epoch: 675, Loss: 1.3216, Train: 42.48%, Valid: 41.29%, Test: 41.33%
Epoch: 700, Loss: 1.3323, Train: 41.93%, Valid: 40.63%, Test: 40.84%
Epoch: 725, Loss: 1.3154, Train: 42.58%, Valid: 41.50%, Test: 41.50%
Epoch: 750, Loss: 1.3746, Train: 40.33%, Valid: 39.58%, Test: 40.16%
Epoch: 775, Loss: 1.3356, Train: 42.22%, Valid: 41.34%, Test: 41.62%
Epoch: 800, Loss: 1.3270, Train: 42.34%, Valid: 41.31%, Test: 41.51%
Epoch: 825, Loss: 1.3196, Train: 42.52%, Valid: 41.58%, Test: 41.77%
Epoch: 850, Loss: 1.3189, Train: 42.46%, Valid: 41.45%, Test: 41.60%
Epoch: 875, Loss: 1.3116, Train: 42.96%, Valid: 41.80%, Test: 41.97%
Epoch: 900, Loss: 1.3092, Train: 43.03%, Valid: 41.97%, Test: 42.21%
Epoch: 925, Loss: 1.3054, Train: 43.34%, Valid: 42.15%, Test: 42.22%
Epoch: 950, Loss: 1.3024, Train: 43.53%, Valid: 42.27%, Test: 42.45%
Epoch: 975, Loss: 1.3525, Train: 42.55%, Valid: 41.49%, Test: 41.82%
Run 01:
Highest Train: 43.55
Highest Valid: 42.31
  Final Train: 43.20
   Final Test: 42.11
All runs:
Highest Train: 43.55, nan
Highest Valid: 42.31, nan
  Final Train: 43.20, nan
   Final Test: 42.11, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.5913, Train: 32.82%, Valid: 32.53%, Test: 33.38%
Epoch: 25, Loss: 1.4664, Train: 34.59%, Valid: 34.40%, Test: 35.21%
Epoch: 50, Loss: 1.4846, Train: 35.39%, Valid: 35.35%, Test: 35.50%
Epoch: 75, Loss: 1.4965, Train: 35.53%, Valid: 35.14%, Test: 35.43%
Epoch: 100, Loss: 1.4197, Train: 37.96%, Valid: 38.01%, Test: 38.19%
Epoch: 125, Loss: 1.4036, Train: 38.59%, Valid: 38.73%, Test: 39.08%
Epoch: 150, Loss: 1.4466, Train: 37.17%, Valid: 37.17%, Test: 37.63%
Epoch: 175, Loss: 1.3945, Train: 39.04%, Valid: 38.95%, Test: 39.38%
Epoch: 200, Loss: 1.4733, Train: 34.93%, Valid: 34.99%, Test: 35.36%
Epoch: 225, Loss: 1.4055, Train: 38.20%, Valid: 38.12%, Test: 38.43%
Epoch: 250, Loss: 1.4029, Train: 38.94%, Valid: 38.68%, Test: 39.04%
Epoch: 275, Loss: 1.3732, Train: 39.71%, Valid: 39.61%, Test: 39.93%
Epoch: 300, Loss: 1.3850, Train: 38.74%, Valid: 38.69%, Test: 39.02%
Epoch: 325, Loss: 1.3726, Train: 40.11%, Valid: 39.79%, Test: 40.15%
Epoch: 350, Loss: 1.3692, Train: 40.18%, Valid: 39.75%, Test: 40.06%
Epoch: 375, Loss: 1.4069, Train: 37.75%, Valid: 37.62%, Test: 37.89%
Epoch: 400, Loss: 1.3622, Train: 40.13%, Valid: 39.70%, Test: 40.13%
Epoch: 425, Loss: 1.3899, Train: 38.99%, Valid: 38.52%, Test: 38.87%
Epoch: 450, Loss: 1.3616, Train: 40.67%, Valid: 40.01%, Test: 40.48%
Epoch: 475, Loss: 1.4141, Train: 38.59%, Valid: 37.81%, Test: 38.50%
Epoch: 500, Loss: 1.3454, Train: 41.23%, Valid: 40.80%, Test: 41.21%
Epoch: 525, Loss: 1.3443, Train: 41.47%, Valid: 40.89%, Test: 41.15%
Epoch: 550, Loss: 1.3181, Train: 42.28%, Valid: 41.78%, Test: 42.04%
Epoch: 575, Loss: 1.5641, Train: 38.25%, Valid: 38.36%, Test: 38.54%
Epoch: 600, Loss: 1.3870, Train: 40.21%, Valid: 40.05%, Test: 40.36%
Epoch: 625, Loss: 1.3750, Train: 40.00%, Valid: 39.87%, Test: 40.30%
Epoch: 650, Loss: 1.3423, Train: 40.78%, Valid: 40.71%, Test: 41.04%
Epoch: 675, Loss: 1.3499, Train: 40.57%, Valid: 40.36%, Test: 40.69%
Epoch: 700, Loss: 1.3241, Train: 41.90%, Valid: 41.74%, Test: 41.84%
Epoch: 725, Loss: 1.4015, Train: 39.01%, Valid: 38.44%, Test: 38.85%
Epoch: 750, Loss: 1.3633, Train: 40.06%, Valid: 39.80%, Test: 40.15%
Epoch: 775, Loss: 1.3592, Train: 40.22%, Valid: 39.94%, Test: 40.21%
Epoch: 800, Loss: 1.4279, Train: 38.16%, Valid: 37.45%, Test: 37.79%
Epoch: 825, Loss: 1.3670, Train: 39.99%, Valid: 39.35%, Test: 39.81%
Epoch: 850, Loss: 1.3818, Train: 38.96%, Valid: 38.58%, Test: 38.97%
Epoch: 875, Loss: 1.3522, Train: 40.62%, Valid: 40.08%, Test: 40.41%
Epoch: 900, Loss: 1.3666, Train: 39.82%, Valid: 39.16%, Test: 39.60%
Epoch: 925, Loss: 1.3536, Train: 40.44%, Valid: 39.72%, Test: 40.13%
Epoch: 950, Loss: 1.3395, Train: 41.16%, Valid: 40.47%, Test: 40.80%
Epoch: 975, Loss: 1.3480, Train: 40.75%, Valid: 40.22%, Test: 40.46%
Run 01:
Highest Train: 42.28
Highest Valid: 42.05
  Final Train: 42.15
   Final Test: 42.10
All runs:
Highest Train: 42.28, nan
Highest Valid: 42.05, nan
  Final Train: 42.15, nan
   Final Test: 42.10, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6089, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.5036, Train: 34.48%, Valid: 34.20%, Test: 34.44%
Epoch: 50, Loss: 1.4862, Train: 35.30%, Valid: 34.83%, Test: 35.17%
Epoch: 75, Loss: 1.4733, Train: 36.53%, Valid: 35.66%, Test: 36.00%
Epoch: 100, Loss: 1.4641, Train: 37.19%, Valid: 36.02%, Test: 36.34%
Epoch: 125, Loss: 1.4562, Train: 37.73%, Valid: 36.26%, Test: 36.66%
Epoch: 150, Loss: 1.4486, Train: 38.21%, Valid: 36.25%, Test: 36.73%
Epoch: 175, Loss: 1.4441, Train: 38.61%, Valid: 36.36%, Test: 36.80%
Epoch: 200, Loss: 1.4422, Train: 38.90%, Valid: 36.44%, Test: 36.71%
Epoch: 225, Loss: 1.4340, Train: 39.21%, Valid: 36.33%, Test: 36.73%
Epoch: 250, Loss: 1.4307, Train: 39.40%, Valid: 36.46%, Test: 36.82%
Epoch: 275, Loss: 1.4285, Train: 39.57%, Valid: 36.51%, Test: 36.58%
Epoch: 300, Loss: 1.4272, Train: 39.89%, Valid: 36.31%, Test: 36.66%
Epoch: 325, Loss: 1.4239, Train: 40.03%, Valid: 36.37%, Test: 36.58%
Epoch: 350, Loss: 1.4217, Train: 40.21%, Valid: 36.37%, Test: 36.55%
Epoch: 375, Loss: 1.4207, Train: 40.28%, Valid: 36.35%, Test: 36.68%
Epoch: 400, Loss: 1.4191, Train: 40.44%, Valid: 36.31%, Test: 36.67%
Epoch: 425, Loss: 1.4166, Train: 40.55%, Valid: 36.32%, Test: 36.62%
Epoch: 450, Loss: 1.4150, Train: 40.67%, Valid: 36.21%, Test: 36.74%
Epoch: 475, Loss: 1.4117, Train: 40.75%, Valid: 36.29%, Test: 36.63%
Epoch: 500, Loss: 1.4128, Train: 40.87%, Valid: 36.13%, Test: 36.61%
Epoch: 525, Loss: 1.4111, Train: 40.81%, Valid: 36.18%, Test: 36.59%
Epoch: 550, Loss: 1.4096, Train: 40.89%, Valid: 36.11%, Test: 36.50%
Epoch: 575, Loss: 1.4108, Train: 41.00%, Valid: 36.26%, Test: 36.49%
Epoch: 600, Loss: 1.4104, Train: 41.13%, Valid: 36.06%, Test: 36.55%
Epoch: 625, Loss: 1.4059, Train: 41.17%, Valid: 36.14%, Test: 36.51%
Epoch: 650, Loss: 1.4074, Train: 41.35%, Valid: 36.15%, Test: 36.56%
Epoch: 675, Loss: 1.4079, Train: 41.32%, Valid: 36.09%, Test: 36.53%
Epoch: 700, Loss: 1.4070, Train: 41.30%, Valid: 36.04%, Test: 36.56%
Epoch: 725, Loss: 1.4041, Train: 41.45%, Valid: 36.01%, Test: 36.55%
Epoch: 750, Loss: 1.4047, Train: 41.39%, Valid: 36.03%, Test: 36.55%
Epoch: 775, Loss: 1.4038, Train: 41.51%, Valid: 36.09%, Test: 36.53%
Epoch: 800, Loss: 1.4041, Train: 41.64%, Valid: 35.96%, Test: 36.37%
Epoch: 825, Loss: 1.4033, Train: 41.57%, Valid: 36.02%, Test: 36.46%
Epoch: 850, Loss: 1.4011, Train: 41.60%, Valid: 35.98%, Test: 36.54%
Epoch: 875, Loss: 1.3996, Train: 41.76%, Valid: 35.97%, Test: 36.35%
Epoch: 900, Loss: 1.3997, Train: 41.74%, Valid: 35.98%, Test: 36.42%
Epoch: 925, Loss: 1.4011, Train: 41.73%, Valid: 36.02%, Test: 36.45%
Epoch: 950, Loss: 1.3991, Train: 41.71%, Valid: 36.00%, Test: 36.41%
Epoch: 975, Loss: 1.3990, Train: 41.66%, Valid: 36.07%, Test: 36.55%
Run 01:
Highest Train: 41.90
Highest Valid: 36.51
  Final Train: 39.69
   Final Test: 36.79
All runs:
Highest Train: 41.90, nan
Highest Valid: 36.51, nan
  Final Train: 39.69, nan
   Final Test: 36.79, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6107, Train: 28.72%, Valid: 28.53%, Test: 28.82%
Epoch: 25, Loss: 1.5075, Train: 34.27%, Valid: 34.10%, Test: 34.30%
Epoch: 50, Loss: 1.4891, Train: 35.08%, Valid: 34.64%, Test: 34.94%
Epoch: 75, Loss: 1.4785, Train: 36.12%, Valid: 35.40%, Test: 35.80%
Epoch: 100, Loss: 1.4712, Train: 36.81%, Valid: 35.73%, Test: 36.07%
Epoch: 125, Loss: 1.4624, Train: 37.50%, Valid: 36.19%, Test: 36.51%
Epoch: 150, Loss: 1.4561, Train: 37.95%, Valid: 36.28%, Test: 36.51%
Epoch: 175, Loss: 1.4509, Train: 38.34%, Valid: 36.44%, Test: 36.70%
Epoch: 200, Loss: 1.4414, Train: 38.85%, Valid: 36.61%, Test: 37.00%
Epoch: 225, Loss: 1.4388, Train: 39.11%, Valid: 36.72%, Test: 37.12%
Epoch: 250, Loss: 1.4325, Train: 39.43%, Valid: 36.92%, Test: 37.18%
Epoch: 275, Loss: 1.4256, Train: 39.76%, Valid: 37.12%, Test: 37.43%
Epoch: 300, Loss: 1.4160, Train: 40.12%, Valid: 37.11%, Test: 37.42%
Epoch: 325, Loss: 1.4241, Train: 39.77%, Valid: 36.70%, Test: 37.06%
Epoch: 350, Loss: 1.4183, Train: 40.11%, Valid: 37.08%, Test: 37.42%
Epoch: 375, Loss: 1.4176, Train: 40.36%, Valid: 37.29%, Test: 37.71%
Epoch: 400, Loss: 1.4277, Train: 39.31%, Valid: 36.51%, Test: 36.84%
Epoch: 425, Loss: 1.4186, Train: 39.60%, Valid: 36.95%, Test: 37.43%
Epoch: 450, Loss: 1.4110, Train: 40.23%, Valid: 37.70%, Test: 38.01%
Epoch: 475, Loss: 1.4252, Train: 39.65%, Valid: 37.17%, Test: 37.41%
Epoch: 500, Loss: 1.4209, Train: 40.00%, Valid: 37.60%, Test: 37.73%
Epoch: 525, Loss: 1.4206, Train: 39.51%, Valid: 37.27%, Test: 37.46%
Epoch: 550, Loss: 1.4171, Train: 39.45%, Valid: 37.47%, Test: 37.65%
Epoch: 575, Loss: 1.4097, Train: 40.03%, Valid: 37.76%, Test: 37.97%
Epoch: 600, Loss: 1.4410, Train: 39.93%, Valid: 37.70%, Test: 37.89%
Epoch: 625, Loss: 1.4093, Train: 40.35%, Valid: 37.84%, Test: 38.08%
Epoch: 650, Loss: 1.4167, Train: 39.98%, Valid: 37.58%, Test: 37.92%
Epoch: 675, Loss: 1.4221, Train: 39.78%, Valid: 37.52%, Test: 37.71%
Epoch: 700, Loss: 1.4151, Train: 39.89%, Valid: 37.58%, Test: 37.78%
Epoch: 725, Loss: 1.4247, Train: 39.26%, Valid: 37.07%, Test: 37.44%
Epoch: 750, Loss: 1.4220, Train: 39.37%, Valid: 37.18%, Test: 37.66%
Epoch: 775, Loss: 1.4487, Train: 39.83%, Valid: 37.47%, Test: 37.91%
Epoch: 800, Loss: 1.4156, Train: 40.07%, Valid: 37.82%, Test: 38.14%
Epoch: 825, Loss: 1.4339, Train: 39.57%, Valid: 37.49%, Test: 37.83%
Epoch: 850, Loss: 1.4223, Train: 39.59%, Valid: 37.55%, Test: 37.85%
Epoch: 875, Loss: 1.4181, Train: 39.69%, Valid: 37.43%, Test: 37.98%
Epoch: 900, Loss: 1.4264, Train: 38.89%, Valid: 37.14%, Test: 37.40%
Epoch: 925, Loss: 1.4281, Train: 39.18%, Valid: 37.34%, Test: 37.72%
Epoch: 950, Loss: 1.4100, Train: 40.24%, Valid: 38.20%, Test: 38.57%
Epoch: 975, Loss: 1.4498, Train: 39.76%, Valid: 37.87%, Test: 38.26%
Run 01:
Highest Train: 40.90
Highest Valid: 38.40
  Final Train: 40.81
   Final Test: 38.57
All runs:
Highest Train: 40.90, nan
Highest Valid: 38.40, nan
  Final Train: 40.81, nan
   Final Test: 38.57, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6092, Train: 27.50%, Valid: 27.37%, Test: 27.64%
Epoch: 25, Loss: 1.4743, Train: 29.44%, Valid: 29.36%, Test: 29.87%
Epoch: 50, Loss: 1.4441, Train: 36.40%, Valid: 36.21%, Test: 36.29%
Epoch: 75, Loss: 1.4254, Train: 30.49%, Valid: 30.52%, Test: 30.20%
Epoch: 100, Loss: 1.4100, Train: 39.67%, Valid: 39.27%, Test: 39.42%
Epoch: 125, Loss: 1.4530, Train: 36.05%, Valid: 35.65%, Test: 35.78%
Epoch: 150, Loss: 1.4029, Train: 39.35%, Valid: 38.92%, Test: 39.13%
Epoch: 175, Loss: 1.3921, Train: 39.88%, Valid: 39.57%, Test: 39.57%
Epoch: 200, Loss: 1.3878, Train: 40.04%, Valid: 39.57%, Test: 39.78%
Epoch: 225, Loss: 1.3790, Train: 39.73%, Valid: 39.47%, Test: 39.60%
Epoch: 250, Loss: 1.5513, Train: 25.29%, Valid: 25.21%, Test: 25.21%
Epoch: 275, Loss: 1.4648, Train: 34.93%, Valid: 34.61%, Test: 35.46%
Epoch: 300, Loss: 1.4201, Train: 37.04%, Valid: 36.76%, Test: 37.14%
Epoch: 325, Loss: 1.4112, Train: 40.23%, Valid: 39.71%, Test: 40.13%
Epoch: 350, Loss: 1.4022, Train: 40.61%, Valid: 40.02%, Test: 40.34%
Epoch: 375, Loss: 1.3915, Train: 40.95%, Valid: 40.61%, Test: 40.67%
Epoch: 400, Loss: 1.4050, Train: 40.89%, Valid: 40.48%, Test: 40.78%
Epoch: 425, Loss: 1.3957, Train: 37.34%, Valid: 37.23%, Test: 37.35%
Epoch: 450, Loss: 1.3797, Train: 41.18%, Valid: 40.98%, Test: 41.06%
Epoch: 475, Loss: 1.3878, Train: 41.16%, Valid: 40.77%, Test: 40.94%
Epoch: 500, Loss: 1.3772, Train: 41.70%, Valid: 41.43%, Test: 41.54%
Epoch: 525, Loss: 1.3784, Train: 40.56%, Valid: 40.13%, Test: 40.36%
Epoch: 550, Loss: 1.3669, Train: 39.74%, Valid: 39.35%, Test: 39.86%
Epoch: 575, Loss: 1.3650, Train: 39.49%, Valid: 39.04%, Test: 39.68%
Epoch: 600, Loss: 1.3736, Train: 40.89%, Valid: 40.68%, Test: 40.90%
Epoch: 625, Loss: 1.3741, Train: 41.70%, Valid: 41.44%, Test: 41.62%
Epoch: 650, Loss: 1.3600, Train: 42.00%, Valid: 41.74%, Test: 41.95%
Epoch: 675, Loss: 1.3594, Train: 41.89%, Valid: 41.64%, Test: 42.01%
Epoch: 700, Loss: 1.3650, Train: 41.48%, Valid: 41.23%, Test: 41.42%
Epoch: 725, Loss: 1.3645, Train: 39.62%, Valid: 39.49%, Test: 39.76%
Epoch: 750, Loss: 1.3737, Train: 40.61%, Valid: 40.30%, Test: 40.51%
Epoch: 775, Loss: 1.3716, Train: 38.22%, Valid: 37.74%, Test: 38.39%
Epoch: 800, Loss: 1.3651, Train: 41.28%, Valid: 40.89%, Test: 41.18%
Epoch: 825, Loss: 1.3642, Train: 39.76%, Valid: 39.62%, Test: 39.59%
Epoch: 850, Loss: 1.3616, Train: 40.04%, Valid: 39.89%, Test: 39.96%
Epoch: 875, Loss: 1.3982, Train: 39.91%, Valid: 39.82%, Test: 40.19%
Epoch: 900, Loss: 1.3720, Train: 40.42%, Valid: 40.24%, Test: 40.48%
Epoch: 925, Loss: 1.3667, Train: 40.19%, Valid: 39.83%, Test: 40.24%
Epoch: 950, Loss: 1.3749, Train: 40.73%, Valid: 40.39%, Test: 40.65%
Epoch: 975, Loss: 1.4345, Train: 36.02%, Valid: 35.80%, Test: 36.35%
Run 01:
Highest Train: 42.63
Highest Valid: 42.36
  Final Train: 42.63
   Final Test: 42.45
All runs:
Highest Train: 42.63, nan
Highest Valid: 42.36, nan
  Final Train: 42.63, nan
   Final Test: 42.45, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6109, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.5023, Train: 34.48%, Valid: 34.22%, Test: 34.49%
Epoch: 50, Loss: 1.4838, Train: 35.50%, Valid: 34.90%, Test: 35.38%
Epoch: 75, Loss: 1.4700, Train: 36.71%, Valid: 35.71%, Test: 36.02%
Epoch: 100, Loss: 1.4624, Train: 37.27%, Valid: 36.01%, Test: 36.39%
Epoch: 125, Loss: 1.4537, Train: 37.90%, Valid: 36.31%, Test: 36.47%
Epoch: 150, Loss: 1.4482, Train: 38.25%, Valid: 36.38%, Test: 36.69%
Epoch: 175, Loss: 1.4427, Train: 38.71%, Valid: 36.31%, Test: 36.82%
Epoch: 200, Loss: 1.4378, Train: 39.15%, Valid: 36.23%, Test: 36.71%
Epoch: 225, Loss: 1.4372, Train: 39.16%, Valid: 36.32%, Test: 36.80%
Epoch: 250, Loss: 1.4314, Train: 39.57%, Valid: 36.36%, Test: 36.79%
Epoch: 275, Loss: 1.4265, Train: 39.89%, Valid: 36.44%, Test: 36.85%
Epoch: 300, Loss: 1.4253, Train: 40.00%, Valid: 36.40%, Test: 36.82%
Epoch: 325, Loss: 1.4232, Train: 40.24%, Valid: 36.55%, Test: 36.77%
Epoch: 350, Loss: 1.4211, Train: 40.18%, Valid: 36.49%, Test: 36.76%
Epoch: 375, Loss: 1.4186, Train: 40.53%, Valid: 36.51%, Test: 36.75%
Epoch: 400, Loss: 1.4178, Train: 40.58%, Valid: 36.41%, Test: 36.81%
Epoch: 425, Loss: 1.4150, Train: 40.60%, Valid: 36.37%, Test: 36.78%
Epoch: 450, Loss: 1.4151, Train: 40.68%, Valid: 36.33%, Test: 36.74%
Epoch: 475, Loss: 1.4137, Train: 40.93%, Valid: 36.32%, Test: 36.73%
Epoch: 500, Loss: 1.4155, Train: 40.87%, Valid: 36.39%, Test: 36.75%
Epoch: 525, Loss: 1.4113, Train: 40.98%, Valid: 36.25%, Test: 36.78%
Epoch: 550, Loss: 1.4096, Train: 41.08%, Valid: 36.38%, Test: 36.80%
Epoch: 575, Loss: 1.4099, Train: 41.23%, Valid: 36.26%, Test: 36.63%
Epoch: 600, Loss: 1.4086, Train: 41.37%, Valid: 36.25%, Test: 36.74%
Epoch: 625, Loss: 1.4097, Train: 41.21%, Valid: 36.28%, Test: 36.61%
Epoch: 650, Loss: 1.4084, Train: 41.40%, Valid: 36.33%, Test: 36.71%
Epoch: 675, Loss: 1.4093, Train: 41.42%, Valid: 36.34%, Test: 36.67%
Epoch: 700, Loss: 1.4080, Train: 41.50%, Valid: 36.41%, Test: 36.58%
Epoch: 725, Loss: 1.4067, Train: 41.44%, Valid: 36.40%, Test: 36.58%
Epoch: 750, Loss: 1.4059, Train: 41.45%, Valid: 36.32%, Test: 36.50%
Epoch: 775, Loss: 1.4053, Train: 41.70%, Valid: 36.37%, Test: 36.58%
Epoch: 800, Loss: 1.4013, Train: 41.76%, Valid: 36.37%, Test: 36.69%
Epoch: 825, Loss: 1.4048, Train: 41.73%, Valid: 36.23%, Test: 36.58%
Epoch: 850, Loss: 1.4020, Train: 41.88%, Valid: 36.27%, Test: 36.54%
Epoch: 875, Loss: 1.4045, Train: 41.76%, Valid: 36.32%, Test: 36.66%
Epoch: 900, Loss: 1.4018, Train: 41.82%, Valid: 36.35%, Test: 36.67%
Epoch: 925, Loss: 1.4012, Train: 41.97%, Valid: 36.32%, Test: 36.54%
Epoch: 950, Loss: 1.3995, Train: 41.79%, Valid: 36.37%, Test: 36.56%
Epoch: 975, Loss: 1.4023, Train: 41.85%, Valid: 36.24%, Test: 36.59%
Run 01:
Highest Train: 42.07
Highest Valid: 36.57
  Final Train: 39.80
   Final Test: 36.89
All runs:
Highest Train: 42.07, nan
Highest Valid: 36.57, nan
  Final Train: 39.80, nan
   Final Test: 36.89, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6088, Train: 28.70%, Valid: 28.52%, Test: 28.81%
Epoch: 25, Loss: 1.5078, Train: 34.33%, Valid: 34.09%, Test: 34.34%
Epoch: 50, Loss: 1.4870, Train: 35.30%, Valid: 34.77%, Test: 35.11%
Epoch: 75, Loss: 1.4732, Train: 36.52%, Valid: 35.60%, Test: 36.03%
Epoch: 100, Loss: 1.4645, Train: 37.24%, Valid: 35.94%, Test: 36.32%
Epoch: 125, Loss: 1.4570, Train: 37.80%, Valid: 36.10%, Test: 36.58%
Epoch: 150, Loss: 1.4507, Train: 38.36%, Valid: 36.29%, Test: 36.74%
Epoch: 175, Loss: 1.4454, Train: 38.71%, Valid: 36.48%, Test: 36.72%
Epoch: 200, Loss: 1.4391, Train: 39.00%, Valid: 36.42%, Test: 36.73%
Epoch: 225, Loss: 1.4369, Train: 39.25%, Valid: 36.46%, Test: 36.76%
Epoch: 250, Loss: 1.4306, Train: 39.71%, Valid: 36.59%, Test: 37.07%
Epoch: 275, Loss: 1.4234, Train: 40.06%, Valid: 36.90%, Test: 37.29%
Epoch: 300, Loss: 1.4107, Train: 40.35%, Valid: 37.26%, Test: 37.30%
Epoch: 325, Loss: 1.4280, Train: 39.59%, Valid: 36.48%, Test: 36.76%
Epoch: 350, Loss: 1.4148, Train: 40.26%, Valid: 37.03%, Test: 37.30%
Epoch: 375, Loss: 1.4136, Train: 40.59%, Valid: 37.25%, Test: 37.51%
Epoch: 400, Loss: 1.4025, Train: 41.28%, Valid: 38.15%, Test: 38.12%
Epoch: 425, Loss: 1.4177, Train: 40.57%, Valid: 37.54%, Test: 37.87%
Epoch: 450, Loss: 1.4162, Train: 39.66%, Valid: 36.87%, Test: 37.36%
Epoch: 475, Loss: 1.4077, Train: 41.22%, Valid: 38.51%, Test: 38.37%
Epoch: 500, Loss: 1.4006, Train: 41.19%, Valid: 38.22%, Test: 38.31%
Epoch: 525, Loss: 1.3920, Train: 41.47%, Valid: 38.54%, Test: 38.74%
Epoch: 550, Loss: 1.4090, Train: 41.57%, Valid: 38.89%, Test: 39.12%
Epoch: 575, Loss: 1.4281, Train: 39.08%, Valid: 37.00%, Test: 37.55%
Epoch: 600, Loss: 1.4041, Train: 41.57%, Valid: 39.53%, Test: 39.89%
Epoch: 625, Loss: 1.4001, Train: 40.66%, Valid: 38.55%, Test: 39.04%
Epoch: 650, Loss: 1.3978, Train: 33.74%, Valid: 31.93%, Test: 31.97%
Epoch: 675, Loss: 1.4004, Train: 41.72%, Valid: 40.25%, Test: 40.65%
Epoch: 700, Loss: 1.4074, Train: 42.04%, Valid: 40.51%, Test: 40.77%
Epoch: 725, Loss: 1.4865, Train: 40.40%, Valid: 38.90%, Test: 39.05%
Epoch: 750, Loss: 1.3977, Train: 41.30%, Valid: 40.17%, Test: 40.72%
Epoch: 775, Loss: 1.4067, Train: 39.76%, Valid: 38.45%, Test: 38.69%
Epoch: 800, Loss: 1.4251, Train: 37.77%, Valid: 36.40%, Test: 36.73%
Epoch: 825, Loss: 1.4120, Train: 40.63%, Valid: 39.61%, Test: 39.93%
Epoch: 850, Loss: 1.3926, Train: 41.27%, Valid: 40.38%, Test: 40.89%
Epoch: 875, Loss: 1.3942, Train: 37.70%, Valid: 36.75%, Test: 37.14%
Epoch: 900, Loss: 1.4608, Train: 40.25%, Valid: 39.32%, Test: 39.87%
Epoch: 925, Loss: 1.3905, Train: 41.53%, Valid: 40.51%, Test: 40.93%
Epoch: 950, Loss: 1.4441, Train: 40.47%, Valid: 39.69%, Test: 39.99%
Epoch: 975, Loss: 1.5671, Train: 31.09%, Valid: 30.64%, Test: 31.18%
Run 01:
Highest Train: 42.24
Highest Valid: 41.27
  Final Train: 41.82
   Final Test: 41.39
All runs:
Highest Train: 42.24, nan
Highest Valid: 41.27, nan
  Final Train: 41.82, nan
   Final Test: 41.39, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6212, Train: 25.57%, Valid: 25.41%, Test: 25.32%
Epoch: 25, Loss: 1.5135, Train: 28.68%, Valid: 28.46%, Test: 28.75%
Epoch: 50, Loss: 1.4435, Train: 30.74%, Valid: 30.57%, Test: 30.51%
Epoch: 75, Loss: 1.4383, Train: 30.93%, Valid: 30.79%, Test: 30.64%
Epoch: 100, Loss: 1.4063, Train: 38.50%, Valid: 38.07%, Test: 38.16%
Epoch: 125, Loss: 1.3914, Train: 39.53%, Valid: 39.19%, Test: 39.42%
Epoch: 150, Loss: 1.3821, Train: 34.93%, Valid: 34.85%, Test: 34.52%
Epoch: 175, Loss: 1.3763, Train: 41.06%, Valid: 40.68%, Test: 40.83%
Epoch: 200, Loss: 1.3753, Train: 39.79%, Valid: 39.63%, Test: 39.60%
Epoch: 225, Loss: 1.3726, Train: 33.40%, Valid: 33.19%, Test: 32.97%
Epoch: 250, Loss: 1.4115, Train: 38.28%, Valid: 38.05%, Test: 37.70%
Epoch: 275, Loss: 1.4317, Train: 32.02%, Valid: 31.74%, Test: 31.67%
Epoch: 300, Loss: 1.3808, Train: 40.48%, Valid: 40.18%, Test: 40.31%
Epoch: 325, Loss: 1.3696, Train: 41.10%, Valid: 40.79%, Test: 40.89%
Epoch: 350, Loss: 1.3570, Train: 32.88%, Valid: 32.65%, Test: 32.44%
Epoch: 375, Loss: 1.3777, Train: 37.04%, Valid: 36.61%, Test: 36.54%
Epoch: 400, Loss: 1.3610, Train: 40.84%, Valid: 40.51%, Test: 40.44%
Epoch: 425, Loss: 1.3638, Train: 40.25%, Valid: 39.95%, Test: 40.00%
Epoch: 450, Loss: 1.3560, Train: 40.44%, Valid: 40.19%, Test: 40.23%
Epoch: 475, Loss: 1.4066, Train: 40.89%, Valid: 40.58%, Test: 40.79%
Epoch: 500, Loss: 1.3832, Train: 41.50%, Valid: 41.07%, Test: 41.18%
Epoch: 525, Loss: 1.3679, Train: 40.81%, Valid: 40.43%, Test: 40.49%
Epoch: 550, Loss: 1.3525, Train: 40.35%, Valid: 39.97%, Test: 40.10%
Epoch: 575, Loss: 1.3527, Train: 41.03%, Valid: 40.60%, Test: 40.64%
Epoch: 600, Loss: 1.3523, Train: 40.78%, Valid: 40.54%, Test: 40.62%
Epoch: 625, Loss: 1.3723, Train: 41.10%, Valid: 40.70%, Test: 40.79%
Epoch: 650, Loss: 1.3548, Train: 40.84%, Valid: 40.33%, Test: 40.45%
Epoch: 675, Loss: 1.3674, Train: 40.50%, Valid: 40.06%, Test: 40.32%
Epoch: 700, Loss: 1.3746, Train: 39.91%, Valid: 39.58%, Test: 39.71%
Epoch: 725, Loss: 1.3511, Train: 41.46%, Valid: 41.11%, Test: 41.02%
Epoch: 750, Loss: 1.3760, Train: 41.09%, Valid: 40.64%, Test: 40.84%
Epoch: 775, Loss: 1.3823, Train: 40.20%, Valid: 39.95%, Test: 40.00%
Epoch: 800, Loss: 1.3539, Train: 40.43%, Valid: 40.12%, Test: 40.32%
Epoch: 825, Loss: 1.3475, Train: 40.66%, Valid: 40.39%, Test: 40.35%
Epoch: 850, Loss: 1.3495, Train: 34.76%, Valid: 34.54%, Test: 34.41%
Epoch: 875, Loss: 1.3626, Train: 41.59%, Valid: 41.24%, Test: 41.13%
Epoch: 900, Loss: 1.3431, Train: 40.48%, Valid: 40.23%, Test: 40.26%
Epoch: 925, Loss: 1.3482, Train: 40.75%, Valid: 40.43%, Test: 40.40%
Epoch: 950, Loss: 1.3468, Train: 40.90%, Valid: 40.69%, Test: 40.65%
Epoch: 975, Loss: 1.3402, Train: 41.55%, Valid: 41.35%, Test: 41.20%
Run 01:
Highest Train: 41.99
Highest Valid: 41.66
  Final Train: 41.99
   Final Test: 41.53
All runs:
Highest Train: 41.99, nan
Highest Valid: 41.66, nan
  Final Train: 41.99, nan
   Final Test: 41.53, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6103, Train: 28.73%, Valid: 28.57%, Test: 28.81%
Epoch: 25, Loss: 1.5008, Train: 34.20%, Valid: 34.00%, Test: 34.27%
Epoch: 50, Loss: 1.4837, Train: 35.03%, Valid: 34.64%, Test: 34.91%
Epoch: 75, Loss: 1.4715, Train: 35.91%, Valid: 35.08%, Test: 35.65%
Epoch: 100, Loss: 1.4574, Train: 36.62%, Valid: 35.76%, Test: 35.88%
Epoch: 125, Loss: 1.4505, Train: 37.18%, Valid: 36.00%, Test: 36.02%
Epoch: 150, Loss: 1.4580, Train: 37.41%, Valid: 35.79%, Test: 36.29%
Epoch: 175, Loss: 1.4249, Train: 38.33%, Valid: 36.08%, Test: 36.38%
Epoch: 200, Loss: 1.4258, Train: 38.26%, Valid: 36.04%, Test: 36.27%
Epoch: 225, Loss: 1.4087, Train: 39.16%, Valid: 35.94%, Test: 36.30%
Epoch: 250, Loss: 1.4023, Train: 39.29%, Valid: 36.01%, Test: 36.46%
Epoch: 275, Loss: 1.3958, Train: 39.77%, Valid: 35.38%, Test: 35.85%
Epoch: 300, Loss: 1.3901, Train: 40.40%, Valid: 35.30%, Test: 35.82%
Epoch: 325, Loss: 1.3817, Train: 39.87%, Valid: 34.34%, Test: 34.97%
Epoch: 350, Loss: 1.3735, Train: 40.57%, Valid: 34.44%, Test: 34.93%
Epoch: 375, Loss: 1.3682, Train: 41.16%, Valid: 34.18%, Test: 34.74%
Epoch: 400, Loss: 1.3522, Train: 42.19%, Valid: 34.48%, Test: 35.05%
Epoch: 425, Loss: 1.3488, Train: 42.62%, Valid: 34.66%, Test: 35.18%
Epoch: 450, Loss: 1.3455, Train: 42.55%, Valid: 34.18%, Test: 34.72%
Epoch: 475, Loss: 1.3477, Train: 42.83%, Valid: 34.72%, Test: 35.30%
Epoch: 500, Loss: 1.3325, Train: 43.06%, Valid: 33.36%, Test: 33.98%
Epoch: 525, Loss: 1.3224, Train: 43.63%, Valid: 33.79%, Test: 34.64%
Epoch: 550, Loss: 1.3398, Train: 43.05%, Valid: 33.79%, Test: 34.30%
Epoch: 575, Loss: 1.3189, Train: 43.95%, Valid: 33.23%, Test: 34.10%
Epoch: 600, Loss: 1.3133, Train: 44.16%, Valid: 34.54%, Test: 35.05%
Epoch: 625, Loss: 1.3213, Train: 44.50%, Valid: 34.13%, Test: 34.73%
Epoch: 650, Loss: 1.3011, Train: 44.60%, Valid: 33.90%, Test: 34.37%
Epoch: 675, Loss: 1.3009, Train: 44.76%, Valid: 34.16%, Test: 34.73%
Epoch: 700, Loss: 1.2922, Train: 45.09%, Valid: 33.41%, Test: 34.23%
Epoch: 725, Loss: 1.2906, Train: 45.29%, Valid: 33.96%, Test: 34.59%
Epoch: 750, Loss: 1.2910, Train: 45.17%, Valid: 33.90%, Test: 34.62%
Epoch: 775, Loss: 1.2851, Train: 45.62%, Valid: 33.63%, Test: 34.28%
Epoch: 800, Loss: 1.2856, Train: 44.85%, Valid: 33.03%, Test: 33.48%
Epoch: 825, Loss: 1.2789, Train: 45.58%, Valid: 34.35%, Test: 34.66%
Epoch: 850, Loss: 1.2801, Train: 46.13%, Valid: 32.98%, Test: 33.69%
Epoch: 875, Loss: 1.2758, Train: 46.07%, Valid: 32.67%, Test: 33.46%
Epoch: 900, Loss: 1.2829, Train: 45.84%, Valid: 32.80%, Test: 33.34%
Epoch: 925, Loss: 1.2879, Train: 45.51%, Valid: 33.07%, Test: 33.41%
Epoch: 950, Loss: 1.2767, Train: 46.15%, Valid: 33.61%, Test: 34.02%
Epoch: 975, Loss: 1.2918, Train: 45.56%, Valid: 32.13%, Test: 32.69%
Run 01:
Highest Train: 46.98
Highest Valid: 36.31
  Final Train: 38.14
   Final Test: 36.69
All runs:
Highest Train: 46.98, nan
Highest Valid: 36.31, nan
  Final Train: 38.14, nan
   Final Test: 36.69, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6094, Train: 28.18%, Valid: 27.98%, Test: 28.27%
Epoch: 25, Loss: 1.5309, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 50, Loss: 1.5130, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 75, Loss: 1.4918, Train: 34.59%, Valid: 34.38%, Test: 34.50%
Epoch: 100, Loss: 1.4828, Train: 34.94%, Valid: 34.60%, Test: 34.80%
Epoch: 125, Loss: 1.4553, Train: 36.93%, Valid: 36.40%, Test: 36.61%
Epoch: 150, Loss: 1.4318, Train: 38.05%, Valid: 37.47%, Test: 37.96%
Epoch: 175, Loss: 1.4253, Train: 38.21%, Valid: 37.55%, Test: 37.91%
Epoch: 200, Loss: 1.4218, Train: 37.39%, Valid: 36.77%, Test: 36.97%
Epoch: 225, Loss: 1.3882, Train: 39.58%, Valid: 38.62%, Test: 38.99%
Epoch: 250, Loss: 1.3783, Train: 39.79%, Valid: 38.73%, Test: 38.97%
Epoch: 275, Loss: 1.3645, Train: 40.51%, Valid: 39.15%, Test: 39.45%
Epoch: 300, Loss: 1.3581, Train: 40.66%, Valid: 39.09%, Test: 39.56%
Epoch: 325, Loss: 1.3715, Train: 40.52%, Valid: 39.25%, Test: 39.73%
Epoch: 350, Loss: 1.3555, Train: 40.84%, Valid: 39.24%, Test: 39.50%
Epoch: 375, Loss: 1.3311, Train: 42.26%, Valid: 40.47%, Test: 40.91%
Epoch: 400, Loss: 1.3298, Train: 42.52%, Valid: 40.62%, Test: 40.83%
Epoch: 425, Loss: 1.3200, Train: 42.92%, Valid: 40.94%, Test: 41.02%
Epoch: 450, Loss: 1.3323, Train: 42.03%, Valid: 40.33%, Test: 40.68%
Epoch: 475, Loss: 1.3086, Train: 42.94%, Valid: 41.03%, Test: 41.50%
Epoch: 500, Loss: 1.3258, Train: 42.14%, Valid: 40.22%, Test: 40.49%
Epoch: 525, Loss: 1.2922, Train: 43.84%, Valid: 41.69%, Test: 41.92%
Epoch: 550, Loss: 1.2855, Train: 44.02%, Valid: 41.81%, Test: 42.09%
Epoch: 575, Loss: 1.2800, Train: 44.82%, Valid: 42.33%, Test: 42.28%
Epoch: 600, Loss: 1.2897, Train: 44.94%, Valid: 42.63%, Test: 42.86%
Epoch: 625, Loss: 1.2554, Train: 45.55%, Valid: 43.08%, Test: 43.22%
Epoch: 650, Loss: 1.2601, Train: 45.63%, Valid: 43.17%, Test: 43.10%
Epoch: 675, Loss: 1.2505, Train: 46.19%, Valid: 43.38%, Test: 43.54%
Epoch: 700, Loss: 1.2529, Train: 45.51%, Valid: 43.15%, Test: 43.23%
Epoch: 725, Loss: 1.2307, Train: 47.13%, Valid: 43.89%, Test: 44.19%
Epoch: 750, Loss: 1.2248, Train: 46.66%, Valid: 43.91%, Test: 44.07%
Epoch: 775, Loss: 1.3268, Train: 42.93%, Valid: 41.31%, Test: 41.54%
Epoch: 800, Loss: 1.2595, Train: 45.53%, Valid: 43.68%, Test: 43.85%
Epoch: 825, Loss: 1.2504, Train: 46.02%, Valid: 43.81%, Test: 44.11%
Epoch: 850, Loss: 1.2409, Train: 46.28%, Valid: 44.10%, Test: 44.22%
Epoch: 875, Loss: 1.2394, Train: 46.60%, Valid: 44.22%, Test: 44.32%
Epoch: 900, Loss: 1.2430, Train: 45.70%, Valid: 43.71%, Test: 43.75%
Epoch: 925, Loss: 1.2222, Train: 47.18%, Valid: 44.71%, Test: 44.87%
Epoch: 950, Loss: 1.2172, Train: 47.25%, Valid: 44.85%, Test: 44.87%
Epoch: 975, Loss: 1.2167, Train: 47.18%, Valid: 45.09%, Test: 44.79%
Run 01:
Highest Train: 48.08
Highest Valid: 45.46
  Final Train: 48.08
   Final Test: 45.44
All runs:
Highest Train: 48.08, nan
Highest Valid: 45.46, nan
  Final Train: 48.08, nan
   Final Test: 45.44, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6594, Train: 28.61%, Valid: 28.44%, Test: 28.71%
Epoch: 25, Loss: 1.4781, Train: 33.00%, Valid: 32.77%, Test: 33.67%
Epoch: 50, Loss: 1.4745, Train: 31.39%, Valid: 30.99%, Test: 31.57%
Epoch: 75, Loss: 1.4490, Train: 33.27%, Valid: 33.05%, Test: 33.93%
Epoch: 100, Loss: 1.4370, Train: 33.33%, Valid: 33.24%, Test: 34.10%
Epoch: 125, Loss: 1.4517, Train: 32.84%, Valid: 32.59%, Test: 33.52%
Epoch: 150, Loss: 1.4450, Train: 32.60%, Valid: 32.20%, Test: 32.98%
Epoch: 175, Loss: 1.4422, Train: 32.15%, Valid: 31.79%, Test: 32.33%
Epoch: 200, Loss: 1.4455, Train: 31.79%, Valid: 31.40%, Test: 31.87%
Epoch: 225, Loss: 1.4442, Train: 32.61%, Valid: 32.21%, Test: 32.91%
Epoch: 250, Loss: 1.4450, Train: 31.33%, Valid: 31.01%, Test: 31.43%
Epoch: 275, Loss: 1.4309, Train: 33.82%, Valid: 33.44%, Test: 34.35%
Epoch: 300, Loss: 1.4451, Train: 33.17%, Valid: 32.89%, Test: 33.81%
Epoch: 325, Loss: 1.4235, Train: 37.37%, Valid: 36.97%, Test: 37.30%
Epoch: 350, Loss: 1.4483, Train: 36.07%, Valid: 35.93%, Test: 36.00%
Epoch: 375, Loss: 1.3930, Train: 38.32%, Valid: 38.10%, Test: 38.26%
Epoch: 400, Loss: 1.4076, Train: 38.51%, Valid: 38.25%, Test: 38.40%
Epoch: 425, Loss: 1.4492, Train: 38.93%, Valid: 38.56%, Test: 38.99%
Epoch: 450, Loss: 1.3830, Train: 38.66%, Valid: 38.38%, Test: 38.85%
Epoch: 475, Loss: 1.3659, Train: 39.81%, Valid: 39.47%, Test: 39.90%
Epoch: 500, Loss: 1.3575, Train: 40.22%, Valid: 39.65%, Test: 40.28%
Epoch: 525, Loss: 1.3982, Train: 39.57%, Valid: 38.89%, Test: 39.51%
Epoch: 550, Loss: 1.3676, Train: 40.76%, Valid: 40.18%, Test: 40.57%
Epoch: 575, Loss: 1.3901, Train: 38.75%, Valid: 38.69%, Test: 38.85%
Epoch: 600, Loss: 1.4197, Train: 34.42%, Valid: 34.28%, Test: 34.53%
Epoch: 625, Loss: 1.3639, Train: 39.76%, Valid: 39.38%, Test: 39.64%
Epoch: 650, Loss: 1.3520, Train: 40.11%, Valid: 39.73%, Test: 40.02%
Epoch: 675, Loss: 1.3550, Train: 40.03%, Valid: 39.65%, Test: 39.88%
Epoch: 700, Loss: 1.3477, Train: 41.11%, Valid: 40.85%, Test: 41.00%
Epoch: 725, Loss: 1.3437, Train: 41.63%, Valid: 41.23%, Test: 41.49%
Epoch: 750, Loss: 1.3295, Train: 41.68%, Valid: 41.25%, Test: 41.76%
Epoch: 775, Loss: 1.3324, Train: 39.86%, Valid: 39.68%, Test: 39.71%
Epoch: 800, Loss: 1.4509, Train: 35.61%, Valid: 34.95%, Test: 35.46%
Epoch: 825, Loss: 1.3792, Train: 39.60%, Valid: 39.11%, Test: 39.61%
Epoch: 850, Loss: 1.3697, Train: 38.83%, Valid: 38.15%, Test: 38.61%
Epoch: 875, Loss: 1.3564, Train: 41.12%, Valid: 40.64%, Test: 40.94%
Epoch: 900, Loss: 1.3386, Train: 41.70%, Valid: 41.49%, Test: 41.73%
Epoch: 925, Loss: 1.3634, Train: 40.03%, Valid: 39.62%, Test: 40.04%
Epoch: 950, Loss: 1.3597, Train: 40.24%, Valid: 40.02%, Test: 39.89%
Epoch: 975, Loss: 1.5426, Train: 32.18%, Valid: 31.94%, Test: 32.18%
Run 01:
Highest Train: 42.30
Highest Valid: 41.83
  Final Train: 42.30
   Final Test: 42.12
All runs:
Highest Train: 42.30, nan
Highest Valid: 41.83, nan
  Final Train: 42.30, nan
   Final Test: 42.12, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6099, Train: 28.73%, Valid: 28.55%, Test: 28.84%
Epoch: 25, Loss: 1.5041, Train: 34.12%, Valid: 33.95%, Test: 34.19%
Epoch: 50, Loss: 1.4860, Train: 34.82%, Valid: 34.59%, Test: 34.77%
Epoch: 75, Loss: 1.4743, Train: 35.68%, Valid: 35.04%, Test: 35.53%
Epoch: 100, Loss: 1.4632, Train: 36.25%, Valid: 35.37%, Test: 35.77%
Epoch: 125, Loss: 1.4525, Train: 36.82%, Valid: 35.75%, Test: 36.13%
Epoch: 150, Loss: 1.4469, Train: 37.27%, Valid: 36.04%, Test: 36.38%
Epoch: 175, Loss: 1.4389, Train: 37.73%, Valid: 36.25%, Test: 36.56%
Epoch: 200, Loss: 1.4451, Train: 37.87%, Valid: 36.18%, Test: 36.41%
Epoch: 225, Loss: 1.4226, Train: 38.46%, Valid: 36.23%, Test: 36.54%
Epoch: 250, Loss: 1.4176, Train: 38.69%, Valid: 35.86%, Test: 36.34%
Epoch: 275, Loss: 1.4087, Train: 39.05%, Valid: 35.81%, Test: 36.24%
Epoch: 300, Loss: 1.4065, Train: 39.20%, Valid: 36.27%, Test: 36.52%
Epoch: 325, Loss: 1.4023, Train: 39.55%, Valid: 36.03%, Test: 36.28%
Epoch: 350, Loss: 1.3977, Train: 39.92%, Valid: 35.87%, Test: 36.11%
Epoch: 375, Loss: 1.3887, Train: 39.88%, Valid: 35.01%, Test: 35.32%
Epoch: 400, Loss: 1.3936, Train: 40.32%, Valid: 35.44%, Test: 35.77%
Epoch: 425, Loss: 1.3970, Train: 39.66%, Valid: 35.19%, Test: 35.57%
Epoch: 450, Loss: 1.3753, Train: 40.77%, Valid: 34.99%, Test: 35.35%
Epoch: 475, Loss: 1.3920, Train: 40.71%, Valid: 35.10%, Test: 35.39%
Epoch: 500, Loss: 1.3678, Train: 41.21%, Valid: 35.52%, Test: 35.80%
Epoch: 525, Loss: 1.3734, Train: 40.84%, Valid: 33.89%, Test: 34.39%
Epoch: 550, Loss: 1.3576, Train: 41.76%, Valid: 35.23%, Test: 35.56%
Epoch: 575, Loss: 1.3644, Train: 41.69%, Valid: 35.54%, Test: 35.77%
Epoch: 600, Loss: 1.3596, Train: 41.94%, Valid: 35.30%, Test: 35.69%
Epoch: 625, Loss: 1.3768, Train: 40.75%, Valid: 35.05%, Test: 35.53%
Epoch: 650, Loss: 1.3425, Train: 42.51%, Valid: 34.72%, Test: 34.96%
Epoch: 675, Loss: 1.3416, Train: 42.37%, Valid: 35.14%, Test: 35.51%
Epoch: 700, Loss: 1.3408, Train: 42.50%, Valid: 34.31%, Test: 34.55%
Epoch: 725, Loss: 1.3451, Train: 42.84%, Valid: 34.99%, Test: 35.16%
Epoch: 750, Loss: 1.3457, Train: 42.30%, Valid: 34.10%, Test: 34.04%
Epoch: 775, Loss: 1.3353, Train: 43.21%, Valid: 34.60%, Test: 34.79%
Epoch: 800, Loss: 1.3365, Train: 43.34%, Valid: 34.43%, Test: 34.57%
Epoch: 825, Loss: 1.3186, Train: 43.91%, Valid: 33.86%, Test: 34.29%
Epoch: 850, Loss: 1.3193, Train: 43.80%, Valid: 33.92%, Test: 34.11%
Epoch: 875, Loss: 1.3157, Train: 43.82%, Valid: 33.92%, Test: 34.12%
Epoch: 900, Loss: 1.3182, Train: 43.47%, Valid: 34.26%, Test: 34.50%
Epoch: 925, Loss: 1.3113, Train: 43.88%, Valid: 34.13%, Test: 34.28%
Epoch: 950, Loss: 1.3219, Train: 43.84%, Valid: 33.30%, Test: 33.68%
Epoch: 975, Loss: 1.3081, Train: 44.16%, Valid: 33.76%, Test: 33.88%
Run 01:
Highest Train: 44.58
Highest Valid: 36.48
  Final Train: 38.80
   Final Test: 36.52
All runs:
Highest Train: 44.58, nan
Highest Valid: 36.48, nan
  Final Train: 38.80, nan
   Final Test: 36.52, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6095, Train: 26.70%, Valid: 26.52%, Test: 26.80%
Epoch: 25, Loss: 1.5543, Train: 28.72%, Valid: 28.53%, Test: 28.82%
Epoch: 50, Loss: 1.5097, Train: 34.44%, Valid: 34.16%, Test: 34.29%
Epoch: 75, Loss: 1.4751, Train: 35.11%, Valid: 34.88%, Test: 35.01%
Epoch: 100, Loss: 1.4831, Train: 35.50%, Valid: 35.12%, Test: 35.38%
Epoch: 125, Loss: 1.4443, Train: 36.11%, Valid: 35.72%, Test: 35.96%
Epoch: 150, Loss: 1.4201, Train: 36.37%, Valid: 36.02%, Test: 36.15%
Epoch: 175, Loss: 1.4127, Train: 37.32%, Valid: 37.00%, Test: 36.94%
Epoch: 200, Loss: 1.3992, Train: 38.28%, Valid: 37.66%, Test: 37.83%
Epoch: 225, Loss: 1.3824, Train: 38.44%, Valid: 37.69%, Test: 37.81%
Epoch: 250, Loss: 1.3704, Train: 39.29%, Valid: 38.38%, Test: 38.58%
Epoch: 275, Loss: 1.3625, Train: 39.83%, Valid: 38.85%, Test: 38.94%
Epoch: 300, Loss: 1.3589, Train: 40.45%, Valid: 39.44%, Test: 39.65%
Epoch: 325, Loss: 1.3541, Train: 39.42%, Valid: 38.62%, Test: 38.66%
Epoch: 350, Loss: 1.3581, Train: 41.13%, Valid: 39.85%, Test: 40.15%
Epoch: 375, Loss: 1.4351, Train: 40.52%, Valid: 39.41%, Test: 39.58%
Epoch: 400, Loss: 1.3560, Train: 40.82%, Valid: 39.74%, Test: 39.83%
Epoch: 425, Loss: 1.3497, Train: 40.84%, Valid: 39.75%, Test: 39.81%
Epoch: 450, Loss: 1.3388, Train: 40.86%, Valid: 39.66%, Test: 39.67%
Epoch: 475, Loss: 1.3384, Train: 42.04%, Valid: 40.46%, Test: 40.76%
Epoch: 500, Loss: 1.3303, Train: 41.90%, Valid: 40.52%, Test: 40.55%
Epoch: 525, Loss: 1.3183, Train: 42.33%, Valid: 40.85%, Test: 41.03%
Epoch: 550, Loss: 1.3120, Train: 42.56%, Valid: 40.96%, Test: 40.98%
Epoch: 575, Loss: 1.3928, Train: 38.32%, Valid: 37.60%, Test: 37.77%
Epoch: 600, Loss: 1.3479, Train: 40.75%, Valid: 40.21%, Test: 40.38%
Epoch: 625, Loss: 1.3655, Train: 39.65%, Valid: 39.01%, Test: 39.27%
Epoch: 650, Loss: 1.3450, Train: 41.44%, Valid: 40.91%, Test: 40.90%
Epoch: 675, Loss: 1.3867, Train: 38.45%, Valid: 37.96%, Test: 38.33%
Epoch: 700, Loss: 1.3777, Train: 41.86%, Valid: 41.48%, Test: 41.48%
Epoch: 725, Loss: 1.3166, Train: 42.39%, Valid: 41.82%, Test: 42.13%
Epoch: 750, Loss: 1.3249, Train: 41.81%, Valid: 41.19%, Test: 41.45%
Epoch: 775, Loss: 1.3031, Train: 42.99%, Valid: 42.38%, Test: 42.62%
Epoch: 800, Loss: 1.3054, Train: 43.15%, Valid: 42.48%, Test: 42.77%
Epoch: 825, Loss: 1.2938, Train: 42.98%, Valid: 42.54%, Test: 42.61%
Epoch: 850, Loss: 1.2936, Train: 42.42%, Valid: 42.01%, Test: 42.24%
Epoch: 875, Loss: 1.2883, Train: 43.52%, Valid: 42.81%, Test: 43.18%
Epoch: 900, Loss: 1.2827, Train: 43.80%, Valid: 43.09%, Test: 43.31%
Epoch: 925, Loss: 1.3114, Train: 42.53%, Valid: 41.91%, Test: 42.23%
Epoch: 950, Loss: 1.2902, Train: 43.37%, Valid: 42.74%, Test: 43.04%
Epoch: 975, Loss: 1.2824, Train: 43.64%, Valid: 42.97%, Test: 43.22%
Run 01:
Highest Train: 44.05
Highest Valid: 43.35
  Final Train: 44.05
   Final Test: 43.56
All runs:
Highest Train: 44.05, nan
Highest Valid: 43.35, nan
  Final Train: 44.05, nan
   Final Test: 43.56, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.5801, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.5084, Train: 32.73%, Valid: 32.55%, Test: 33.39%
Epoch: 50, Loss: 1.4630, Train: 33.08%, Valid: 32.78%, Test: 33.73%
Epoch: 75, Loss: 1.4551, Train: 33.12%, Valid: 32.85%, Test: 33.83%
Epoch: 100, Loss: 1.4536, Train: 33.18%, Valid: 32.93%, Test: 33.89%
Epoch: 125, Loss: 1.4435, Train: 35.84%, Valid: 35.62%, Test: 35.71%
Epoch: 150, Loss: 1.4511, Train: 35.72%, Valid: 35.51%, Test: 35.72%
Epoch: 175, Loss: 1.4716, Train: 32.52%, Valid: 32.46%, Test: 32.71%
Epoch: 200, Loss: 1.4592, Train: 33.34%, Valid: 33.20%, Test: 33.36%
Epoch: 225, Loss: 1.4514, Train: 34.36%, Valid: 33.99%, Test: 34.21%
Epoch: 250, Loss: 1.4392, Train: 36.26%, Valid: 35.93%, Test: 36.27%
Epoch: 275, Loss: 1.4798, Train: 31.46%, Valid: 31.00%, Test: 31.86%
Epoch: 300, Loss: 1.4341, Train: 37.34%, Valid: 37.09%, Test: 37.32%
Epoch: 325, Loss: 1.4196, Train: 37.70%, Valid: 37.52%, Test: 37.68%
Epoch: 350, Loss: 1.6505, Train: 36.93%, Valid: 36.47%, Test: 37.02%
Epoch: 375, Loss: 1.4621, Train: 35.64%, Valid: 35.40%, Test: 35.32%
Epoch: 400, Loss: 1.4261, Train: 37.77%, Valid: 37.44%, Test: 37.59%
Epoch: 425, Loss: 1.4382, Train: 37.67%, Valid: 37.21%, Test: 37.56%
Epoch: 450, Loss: 1.4030, Train: 38.24%, Valid: 37.91%, Test: 38.02%
Epoch: 475, Loss: 1.6053, Train: 24.40%, Valid: 24.39%, Test: 25.06%
Epoch: 500, Loss: 1.5001, Train: 33.21%, Valid: 33.08%, Test: 33.37%
Epoch: 525, Loss: 1.4677, Train: 35.10%, Valid: 34.85%, Test: 35.23%
Epoch: 550, Loss: 1.4556, Train: 35.49%, Valid: 35.14%, Test: 35.72%
Epoch: 575, Loss: 1.4460, Train: 36.27%, Valid: 36.05%, Test: 36.37%
Epoch: 600, Loss: 1.4386, Train: 36.16%, Valid: 35.74%, Test: 36.11%
Epoch: 625, Loss: 1.4666, Train: 35.60%, Valid: 35.33%, Test: 35.88%
Epoch: 650, Loss: 1.4435, Train: 37.14%, Valid: 36.72%, Test: 37.47%
Epoch: 675, Loss: 1.4412, Train: 36.85%, Valid: 36.34%, Test: 36.94%
Epoch: 700, Loss: 1.4399, Train: 36.87%, Valid: 36.44%, Test: 36.88%
Epoch: 725, Loss: 1.4278, Train: 35.02%, Valid: 34.25%, Test: 35.14%
Epoch: 750, Loss: 1.4582, Train: 36.73%, Valid: 36.20%, Test: 36.96%
Epoch: 775, Loss: 1.4421, Train: 37.02%, Valid: 36.54%, Test: 37.25%
Epoch: 800, Loss: 1.4283, Train: 37.47%, Valid: 36.98%, Test: 37.61%
Epoch: 825, Loss: 1.4413, Train: 36.86%, Valid: 36.27%, Test: 36.87%
Epoch: 850, Loss: 1.4265, Train: 37.70%, Valid: 37.29%, Test: 37.86%
Epoch: 875, Loss: 1.4263, Train: 37.53%, Valid: 37.05%, Test: 37.67%
Epoch: 900, Loss: 1.4558, Train: 36.40%, Valid: 36.00%, Test: 36.62%
Epoch: 925, Loss: 1.4379, Train: 36.98%, Valid: 36.59%, Test: 37.33%
Epoch: 950, Loss: 1.4235, Train: 37.66%, Valid: 37.11%, Test: 37.78%
Epoch: 975, Loss: 1.4143, Train: 37.22%, Valid: 36.81%, Test: 37.44%
Run 01:
Highest Train: 38.85
Highest Valid: 38.26
  Final Train: 38.84
   Final Test: 38.63
All runs:
Highest Train: 38.85, nan
Highest Valid: 38.26, nan
  Final Train: 38.84, nan
   Final Test: 38.63, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6107, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.5088, Train: 34.16%, Valid: 33.98%, Test: 34.24%
Epoch: 50, Loss: 1.4895, Train: 34.97%, Valid: 34.61%, Test: 34.82%
Epoch: 75, Loss: 1.4786, Train: 36.14%, Valid: 35.31%, Test: 35.69%
Epoch: 100, Loss: 1.4686, Train: 36.91%, Valid: 35.82%, Test: 36.19%
Epoch: 125, Loss: 1.4591, Train: 37.56%, Valid: 36.28%, Test: 36.51%
Epoch: 150, Loss: 1.4532, Train: 38.04%, Valid: 36.55%, Test: 36.70%
Epoch: 175, Loss: 1.4470, Train: 38.46%, Valid: 36.51%, Test: 36.73%
Epoch: 200, Loss: 1.4416, Train: 38.91%, Valid: 36.60%, Test: 36.78%
Epoch: 225, Loss: 1.4378, Train: 39.17%, Valid: 36.53%, Test: 36.71%
Epoch: 250, Loss: 1.4357, Train: 39.41%, Valid: 36.59%, Test: 36.81%
Epoch: 275, Loss: 1.4326, Train: 39.68%, Valid: 36.64%, Test: 36.95%
Epoch: 300, Loss: 1.4315, Train: 39.76%, Valid: 36.49%, Test: 36.84%
Epoch: 325, Loss: 1.4272, Train: 39.81%, Valid: 36.43%, Test: 36.94%
Epoch: 350, Loss: 1.4262, Train: 39.99%, Valid: 36.46%, Test: 36.83%
Epoch: 375, Loss: 1.4245, Train: 40.24%, Valid: 36.28%, Test: 36.81%
Epoch: 400, Loss: 1.4222, Train: 40.36%, Valid: 36.32%, Test: 36.79%
Epoch: 425, Loss: 1.4215, Train: 40.40%, Valid: 36.46%, Test: 36.78%
Epoch: 450, Loss: 1.4171, Train: 40.51%, Valid: 36.40%, Test: 36.84%
Epoch: 475, Loss: 1.4196, Train: 40.58%, Valid: 36.34%, Test: 36.76%
Epoch: 500, Loss: 1.4172, Train: 40.55%, Valid: 36.42%, Test: 36.69%
Epoch: 525, Loss: 1.4149, Train: 40.74%, Valid: 36.23%, Test: 36.71%
Epoch: 550, Loss: 1.4161, Train: 40.73%, Valid: 36.43%, Test: 36.67%
Epoch: 575, Loss: 1.4150, Train: 40.77%, Valid: 36.36%, Test: 36.64%
Epoch: 600, Loss: 1.4128, Train: 40.78%, Valid: 36.17%, Test: 36.67%
Epoch: 625, Loss: 1.4140, Train: 40.93%, Valid: 36.27%, Test: 36.74%
Epoch: 650, Loss: 1.4118, Train: 41.01%, Valid: 36.26%, Test: 36.69%
Epoch: 675, Loss: 1.4098, Train: 40.98%, Valid: 36.19%, Test: 36.68%
Epoch: 700, Loss: 1.4136, Train: 41.01%, Valid: 36.11%, Test: 36.58%
Epoch: 725, Loss: 1.4105, Train: 41.08%, Valid: 36.18%, Test: 36.47%
Epoch: 750, Loss: 1.4088, Train: 41.24%, Valid: 36.14%, Test: 36.67%
Epoch: 775, Loss: 1.4084, Train: 41.27%, Valid: 36.21%, Test: 36.61%
Epoch: 800, Loss: 1.4089, Train: 41.35%, Valid: 36.22%, Test: 36.59%
Epoch: 825, Loss: 1.4066, Train: 41.26%, Valid: 36.14%, Test: 36.58%
Epoch: 850, Loss: 1.4081, Train: 41.35%, Valid: 36.18%, Test: 36.49%
Epoch: 875, Loss: 1.4033, Train: 41.34%, Valid: 36.17%, Test: 36.60%
Epoch: 900, Loss: 1.4072, Train: 41.34%, Valid: 36.17%, Test: 36.54%
Epoch: 925, Loss: 1.4038, Train: 41.47%, Valid: 36.29%, Test: 36.61%
Epoch: 950, Loss: 1.4035, Train: 41.52%, Valid: 36.12%, Test: 36.53%
Epoch: 975, Loss: 1.4053, Train: 41.37%, Valid: 36.15%, Test: 36.52%
Run 01:
Highest Train: 41.59
Highest Valid: 36.72
  Final Train: 39.51
   Final Test: 36.86
All runs:
Highest Train: 41.59, nan
Highest Valid: 36.72, nan
  Final Train: 39.51, nan
   Final Test: 36.86, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6096, Train: 34.75%, Valid: 34.43%, Test: 34.65%
Epoch: 25, Loss: 1.5059, Train: 34.29%, Valid: 34.09%, Test: 34.25%
Epoch: 50, Loss: 1.4873, Train: 35.25%, Valid: 34.89%, Test: 35.02%
Epoch: 75, Loss: 1.4716, Train: 36.41%, Valid: 35.62%, Test: 35.92%
Epoch: 100, Loss: 1.4593, Train: 37.25%, Valid: 36.15%, Test: 36.41%
Epoch: 125, Loss: 1.4482, Train: 38.04%, Valid: 36.38%, Test: 36.75%
Epoch: 150, Loss: 1.4341, Train: 38.82%, Valid: 36.91%, Test: 37.23%
Epoch: 175, Loss: 1.4383, Train: 39.09%, Valid: 36.91%, Test: 37.39%
Epoch: 200, Loss: 1.4206, Train: 39.56%, Valid: 37.41%, Test: 37.73%
Epoch: 225, Loss: 1.4342, Train: 39.38%, Valid: 37.37%, Test: 37.81%
Epoch: 250, Loss: 1.4164, Train: 40.01%, Valid: 37.82%, Test: 38.34%
Epoch: 275, Loss: 1.4100, Train: 39.84%, Valid: 37.87%, Test: 38.15%
Epoch: 300, Loss: 1.4085, Train: 41.24%, Valid: 39.09%, Test: 39.38%
Epoch: 325, Loss: 1.3944, Train: 41.65%, Valid: 39.43%, Test: 39.55%
Epoch: 350, Loss: 1.3905, Train: 41.97%, Valid: 39.59%, Test: 39.76%
Epoch: 375, Loss: 1.3900, Train: 42.19%, Valid: 39.74%, Test: 39.86%
Epoch: 400, Loss: 1.4063, Train: 40.58%, Valid: 38.67%, Test: 38.85%
Epoch: 425, Loss: 1.4029, Train: 41.06%, Valid: 39.30%, Test: 39.53%
Epoch: 450, Loss: 1.4072, Train: 41.65%, Valid: 40.05%, Test: 40.39%
Epoch: 475, Loss: 1.4122, Train: 40.65%, Valid: 39.47%, Test: 39.71%
Epoch: 500, Loss: 1.4123, Train: 40.49%, Valid: 39.10%, Test: 39.37%
Epoch: 525, Loss: 1.4186, Train: 41.98%, Valid: 40.46%, Test: 40.99%
Epoch: 550, Loss: 1.4186, Train: 41.49%, Valid: 40.40%, Test: 40.85%
Epoch: 575, Loss: 1.4057, Train: 41.52%, Valid: 40.41%, Test: 40.80%
Epoch: 600, Loss: 1.4241, Train: 40.53%, Valid: 39.79%, Test: 40.00%
Epoch: 625, Loss: 1.4663, Train: 38.76%, Valid: 38.48%, Test: 38.47%
Epoch: 650, Loss: 1.4257, Train: 39.35%, Valid: 39.11%, Test: 39.11%
Epoch: 675, Loss: 1.4251, Train: 40.02%, Valid: 39.63%, Test: 39.87%
Epoch: 700, Loss: 1.6018, Train: 40.52%, Valid: 40.23%, Test: 40.27%
Epoch: 725, Loss: 1.4372, Train: 34.27%, Valid: 33.88%, Test: 34.12%
Epoch: 750, Loss: 1.4190, Train: 38.24%, Valid: 37.88%, Test: 38.04%
Epoch: 775, Loss: 1.4948, Train: 38.73%, Valid: 38.60%, Test: 38.41%
Epoch: 800, Loss: 1.4904, Train: 21.01%, Valid: 20.91%, Test: 21.13%
Epoch: 825, Loss: 1.5255, Train: 23.06%, Valid: 22.92%, Test: 23.28%
Epoch: 850, Loss: 1.4592, Train: 37.40%, Valid: 37.13%, Test: 37.24%
Epoch: 875, Loss: 1.4666, Train: 37.94%, Valid: 37.65%, Test: 37.91%
Epoch: 900, Loss: 1.4391, Train: 37.84%, Valid: 37.59%, Test: 37.68%
Epoch: 925, Loss: 1.4773, Train: 34.57%, Valid: 34.66%, Test: 34.42%
Epoch: 950, Loss: 1.6085, Train: 23.78%, Valid: 23.49%, Test: 23.96%
Epoch: 975, Loss: 1.5371, Train: 18.19%, Valid: 18.34%, Test: 18.15%
Run 01:
Highest Train: 42.58
Highest Valid: 40.92
  Final Train: 42.19
   Final Test: 41.39
All runs:
Highest Train: 42.58, nan
Highest Valid: 40.92, nan
  Final Train: 42.19, nan
   Final Test: 41.39, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6006, Train: 21.47%, Valid: 21.71%, Test: 21.91%
Epoch: 25, Loss: 1.5488, Train: 28.67%, Valid: 28.43%, Test: 28.84%
Epoch: 50, Loss: 1.4971, Train: 30.55%, Valid: 30.16%, Test: 30.35%
Epoch: 75, Loss: 1.4548, Train: 37.00%, Valid: 36.80%, Test: 37.29%
Epoch: 100, Loss: 1.4302, Train: 38.00%, Valid: 37.60%, Test: 38.01%
Epoch: 125, Loss: 1.4331, Train: 37.89%, Valid: 37.30%, Test: 37.74%
Epoch: 150, Loss: 1.4066, Train: 38.22%, Valid: 37.51%, Test: 37.90%
Epoch: 175, Loss: 1.4202, Train: 37.05%, Valid: 36.38%, Test: 36.88%
Epoch: 200, Loss: 1.4165, Train: 38.63%, Valid: 38.14%, Test: 38.82%
Epoch: 225, Loss: 1.4013, Train: 38.69%, Valid: 38.03%, Test: 38.38%
Epoch: 250, Loss: 1.4148, Train: 39.14%, Valid: 38.54%, Test: 38.82%
Epoch: 275, Loss: 1.4018, Train: 39.10%, Valid: 38.42%, Test: 38.76%
Epoch: 300, Loss: 1.4056, Train: 39.74%, Valid: 38.95%, Test: 39.23%
Epoch: 325, Loss: 1.3998, Train: 39.90%, Valid: 39.01%, Test: 39.53%
Epoch: 350, Loss: 1.3975, Train: 40.12%, Valid: 39.25%, Test: 39.87%
Epoch: 375, Loss: 1.3879, Train: 39.25%, Valid: 38.34%, Test: 38.91%
Epoch: 400, Loss: 1.3836, Train: 36.10%, Valid: 35.02%, Test: 35.87%
Epoch: 425, Loss: 1.3793, Train: 39.40%, Valid: 38.55%, Test: 39.12%
Epoch: 450, Loss: 1.3880, Train: 40.85%, Valid: 40.08%, Test: 40.56%
Epoch: 475, Loss: 1.3831, Train: 41.30%, Valid: 40.53%, Test: 40.86%
Epoch: 500, Loss: 1.3849, Train: 40.84%, Valid: 39.86%, Test: 40.36%
Epoch: 525, Loss: 1.4007, Train: 40.63%, Valid: 39.96%, Test: 40.28%
Epoch: 550, Loss: 1.3850, Train: 40.67%, Valid: 39.93%, Test: 40.35%
Epoch: 575, Loss: 1.3734, Train: 38.67%, Valid: 37.75%, Test: 38.50%
Epoch: 600, Loss: 1.3865, Train: 40.98%, Valid: 40.08%, Test: 40.51%
Epoch: 625, Loss: 1.3907, Train: 40.94%, Valid: 40.21%, Test: 40.48%
Epoch: 650, Loss: 1.3833, Train: 39.30%, Valid: 38.53%, Test: 39.14%
Epoch: 675, Loss: 1.3877, Train: 40.55%, Valid: 39.91%, Test: 40.14%
Epoch: 700, Loss: 1.3848, Train: 39.51%, Valid: 38.79%, Test: 39.51%
Epoch: 725, Loss: 1.3656, Train: 30.00%, Valid: 29.32%, Test: 29.52%
Epoch: 750, Loss: 1.3686, Train: 41.89%, Valid: 40.97%, Test: 41.32%
Epoch: 775, Loss: 1.3779, Train: 41.57%, Valid: 41.00%, Test: 41.24%
Epoch: 800, Loss: 1.3850, Train: 40.83%, Valid: 40.13%, Test: 40.81%
Epoch: 825, Loss: 1.3924, Train: 41.16%, Valid: 40.35%, Test: 40.61%
Epoch: 850, Loss: 1.3895, Train: 40.64%, Valid: 40.14%, Test: 40.71%
Epoch: 875, Loss: 1.3700, Train: 42.12%, Valid: 41.30%, Test: 41.72%
Epoch: 900, Loss: 1.3746, Train: 41.98%, Valid: 41.21%, Test: 41.57%
Epoch: 925, Loss: 1.3662, Train: 40.98%, Valid: 40.17%, Test: 40.70%
Epoch: 950, Loss: 1.3602, Train: 32.68%, Valid: 32.22%, Test: 32.49%
Epoch: 975, Loss: 1.3713, Train: 42.62%, Valid: 41.87%, Test: 42.41%
Run 01:
Highest Train: 42.67
Highest Valid: 41.89
  Final Train: 42.62
   Final Test: 42.36
All runs:
Highest Train: 42.67, nan
Highest Valid: 41.89, nan
  Final Train: 42.62, nan
   Final Test: 42.36, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6091, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.5082, Train: 34.14%, Valid: 33.92%, Test: 34.22%
Epoch: 50, Loss: 1.4889, Train: 35.16%, Valid: 34.73%, Test: 34.95%
Epoch: 75, Loss: 1.4761, Train: 36.28%, Valid: 35.42%, Test: 35.93%
Epoch: 100, Loss: 1.4687, Train: 36.91%, Valid: 35.87%, Test: 36.24%
Epoch: 125, Loss: 1.4605, Train: 37.52%, Valid: 36.22%, Test: 36.50%
Epoch: 150, Loss: 1.4548, Train: 38.00%, Valid: 36.38%, Test: 36.69%
Epoch: 175, Loss: 1.4478, Train: 38.49%, Valid: 36.49%, Test: 36.73%
Epoch: 200, Loss: 1.4468, Train: 38.70%, Valid: 36.41%, Test: 36.66%
Epoch: 225, Loss: 1.4399, Train: 39.10%, Valid: 36.53%, Test: 36.84%
Epoch: 250, Loss: 1.4376, Train: 39.31%, Valid: 36.51%, Test: 36.83%
Epoch: 275, Loss: 1.4326, Train: 39.55%, Valid: 36.54%, Test: 36.91%
Epoch: 300, Loss: 1.4328, Train: 39.77%, Valid: 36.52%, Test: 36.83%
Epoch: 325, Loss: 1.4290, Train: 39.85%, Valid: 36.48%, Test: 36.91%
Epoch: 350, Loss: 1.4261, Train: 40.05%, Valid: 36.52%, Test: 36.89%
Epoch: 375, Loss: 1.4239, Train: 40.29%, Valid: 36.40%, Test: 36.93%
Epoch: 400, Loss: 1.4238, Train: 40.32%, Valid: 36.49%, Test: 36.83%
Epoch: 425, Loss: 1.4238, Train: 40.37%, Valid: 36.43%, Test: 36.97%
Epoch: 450, Loss: 1.4203, Train: 40.50%, Valid: 36.31%, Test: 36.92%
Epoch: 475, Loss: 1.4207, Train: 40.60%, Valid: 36.40%, Test: 36.87%
Epoch: 500, Loss: 1.4175, Train: 40.59%, Valid: 36.48%, Test: 36.87%
Epoch: 525, Loss: 1.4176, Train: 40.82%, Valid: 36.43%, Test: 36.82%
Epoch: 550, Loss: 1.4161, Train: 40.61%, Valid: 36.44%, Test: 36.78%
Epoch: 575, Loss: 1.4172, Train: 40.74%, Valid: 36.38%, Test: 36.75%
Epoch: 600, Loss: 1.4163, Train: 41.03%, Valid: 36.36%, Test: 36.92%
Epoch: 625, Loss: 1.4131, Train: 40.85%, Valid: 36.28%, Test: 36.90%
Epoch: 650, Loss: 1.4141, Train: 41.01%, Valid: 36.42%, Test: 36.75%
Epoch: 675, Loss: 1.4146, Train: 40.94%, Valid: 36.34%, Test: 36.82%
Epoch: 700, Loss: 1.4138, Train: 41.06%, Valid: 36.36%, Test: 36.77%
Epoch: 725, Loss: 1.4131, Train: 41.06%, Valid: 36.34%, Test: 36.67%
Epoch: 750, Loss: 1.4126, Train: 41.28%, Valid: 36.31%, Test: 36.61%
Epoch: 775, Loss: 1.4113, Train: 41.15%, Valid: 36.36%, Test: 36.72%
Epoch: 800, Loss: 1.4105, Train: 41.17%, Valid: 36.20%, Test: 36.66%
Epoch: 825, Loss: 1.4103, Train: 41.17%, Valid: 36.21%, Test: 36.64%
Epoch: 850, Loss: 1.4110, Train: 41.19%, Valid: 36.29%, Test: 36.83%
Epoch: 875, Loss: 1.4100, Train: 41.35%, Valid: 36.30%, Test: 36.80%
Epoch: 900, Loss: 1.4086, Train: 41.19%, Valid: 36.29%, Test: 36.65%
Epoch: 925, Loss: 1.4069, Train: 41.34%, Valid: 36.24%, Test: 36.61%
Epoch: 950, Loss: 1.4069, Train: 41.33%, Valid: 36.30%, Test: 36.71%
Epoch: 975, Loss: 1.4114, Train: 41.39%, Valid: 36.31%, Test: 36.75%
Run 01:
Highest Train: 41.52
Highest Valid: 36.65
  Final Train: 40.14
   Final Test: 36.83
All runs:
Highest Train: 41.52, nan
Highest Valid: 36.65, nan
  Final Train: 40.14, nan
   Final Test: 36.83, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6086, Train: 28.33%, Valid: 28.09%, Test: 28.42%
Epoch: 25, Loss: 1.5054, Train: 34.32%, Valid: 34.17%, Test: 34.34%
Epoch: 50, Loss: 1.4862, Train: 35.21%, Valid: 34.78%, Test: 35.11%
Epoch: 75, Loss: 1.4728, Train: 36.31%, Valid: 35.51%, Test: 36.05%
Epoch: 100, Loss: 1.4624, Train: 37.20%, Valid: 36.03%, Test: 36.52%
Epoch: 125, Loss: 1.4529, Train: 37.75%, Valid: 36.23%, Test: 36.81%
Epoch: 150, Loss: 1.4452, Train: 38.35%, Valid: 36.50%, Test: 36.82%
Epoch: 175, Loss: 1.4393, Train: 38.86%, Valid: 36.67%, Test: 37.06%
Epoch: 200, Loss: 1.4276, Train: 39.62%, Valid: 37.24%, Test: 37.86%
Epoch: 225, Loss: 1.4338, Train: 39.55%, Valid: 37.10%, Test: 37.59%
Epoch: 250, Loss: 1.4250, Train: 39.22%, Valid: 36.77%, Test: 37.28%
Epoch: 275, Loss: 1.4157, Train: 40.14%, Valid: 37.66%, Test: 38.04%
Epoch: 300, Loss: 1.4177, Train: 39.66%, Valid: 37.46%, Test: 37.84%
Epoch: 325, Loss: 1.4356, Train: 38.89%, Valid: 36.70%, Test: 37.16%
Epoch: 350, Loss: 1.4269, Train: 40.83%, Valid: 38.67%, Test: 39.02%
Epoch: 375, Loss: 1.4121, Train: 40.77%, Valid: 38.29%, Test: 38.95%
Epoch: 400, Loss: 1.4082, Train: 40.60%, Valid: 38.19%, Test: 38.60%
Epoch: 425, Loss: 1.4117, Train: 40.09%, Valid: 37.69%, Test: 38.30%
Epoch: 450, Loss: 1.4026, Train: 41.20%, Valid: 38.37%, Test: 38.92%
Epoch: 475, Loss: 1.3990, Train: 41.58%, Valid: 38.64%, Test: 39.18%
Epoch: 500, Loss: 1.3979, Train: 41.10%, Valid: 38.45%, Test: 38.92%
Epoch: 525, Loss: 1.3971, Train: 40.80%, Valid: 38.27%, Test: 38.64%
Epoch: 550, Loss: 1.4008, Train: 41.61%, Valid: 39.00%, Test: 39.47%
Epoch: 575, Loss: 1.4080, Train: 41.46%, Valid: 38.83%, Test: 39.08%
Epoch: 600, Loss: 1.4761, Train: 42.12%, Valid: 39.55%, Test: 39.99%
Epoch: 625, Loss: 1.3835, Train: 42.16%, Valid: 39.72%, Test: 40.16%
Epoch: 650, Loss: 1.3788, Train: 42.37%, Valid: 40.15%, Test: 40.40%
Epoch: 675, Loss: 1.3898, Train: 42.54%, Valid: 40.48%, Test: 40.57%
Epoch: 700, Loss: 1.3821, Train: 43.12%, Valid: 41.17%, Test: 41.25%
Epoch: 725, Loss: 1.6270, Train: 28.81%, Valid: 28.53%, Test: 28.85%
Epoch: 750, Loss: 1.5887, Train: 28.85%, Valid: 28.51%, Test: 28.94%
Epoch: 775, Loss: 1.5824, Train: 28.86%, Valid: 28.52%, Test: 28.94%
Epoch: 800, Loss: 1.5203, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 825, Loss: 1.5816, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 850, Loss: 1.5025, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 875, Loss: 1.5977, Train: 27.71%, Valid: 27.53%, Test: 27.87%
Epoch: 900, Loss: 1.5860, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 925, Loss: 1.5149, Train: 20.60%, Valid: 20.64%, Test: 20.66%
Epoch: 950, Loss: 1.5355, Train: 22.21%, Valid: 22.21%, Test: 22.79%
Epoch: 975, Loss: 1.5019, Train: 16.12%, Valid: 16.35%, Test: 16.31%
Run 01:
Highest Train: 43.50
Highest Valid: 41.27
  Final Train: 43.22
   Final Test: 41.46
All runs:
Highest Train: 43.50, nan
Highest Valid: 41.27, nan
  Final Train: 43.22, nan
   Final Test: 41.46, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6203, Train: 28.66%, Valid: 28.46%, Test: 28.74%
Epoch: 25, Loss: 1.5152, Train: 32.48%, Valid: 32.29%, Test: 33.18%
Epoch: 50, Loss: 1.4856, Train: 32.88%, Valid: 32.63%, Test: 33.54%
Epoch: 75, Loss: 1.4703, Train: 25.79%, Valid: 25.71%, Test: 26.36%
Epoch: 100, Loss: 1.4533, Train: 36.36%, Valid: 36.13%, Test: 36.24%
Epoch: 125, Loss: 1.4579, Train: 35.21%, Valid: 34.87%, Test: 35.20%
Epoch: 150, Loss: 1.4359, Train: 35.43%, Valid: 35.12%, Test: 35.29%
Epoch: 175, Loss: 1.4490, Train: 36.04%, Valid: 35.74%, Test: 35.94%
Epoch: 200, Loss: 1.4429, Train: 26.32%, Valid: 26.29%, Test: 26.61%
Epoch: 225, Loss: 1.4335, Train: 22.54%, Valid: 22.53%, Test: 22.58%
Epoch: 250, Loss: 1.4200, Train: 37.13%, Valid: 36.84%, Test: 37.07%
Epoch: 275, Loss: 1.4474, Train: 24.43%, Valid: 24.33%, Test: 24.38%
Epoch: 300, Loss: 1.4411, Train: 37.86%, Valid: 37.75%, Test: 37.78%
Epoch: 325, Loss: 1.4249, Train: 38.00%, Valid: 37.67%, Test: 37.91%
Epoch: 350, Loss: 1.4347, Train: 38.54%, Valid: 38.36%, Test: 38.37%
Epoch: 375, Loss: 1.4374, Train: 38.19%, Valid: 37.92%, Test: 38.15%
Epoch: 400, Loss: 1.4289, Train: 38.68%, Valid: 38.49%, Test: 38.47%
Epoch: 425, Loss: 1.4055, Train: 39.04%, Valid: 38.83%, Test: 38.86%
Epoch: 450, Loss: 1.5272, Train: 14.51%, Valid: 14.56%, Test: 14.21%
Epoch: 475, Loss: 1.4681, Train: 26.10%, Valid: 26.04%, Test: 26.11%
Epoch: 500, Loss: 1.4798, Train: 27.28%, Valid: 27.42%, Test: 27.24%
Epoch: 525, Loss: 1.4631, Train: 34.91%, Valid: 34.77%, Test: 34.95%
Epoch: 550, Loss: 1.4824, Train: 29.01%, Valid: 29.06%, Test: 28.90%
Epoch: 575, Loss: 1.4481, Train: 26.80%, Valid: 26.78%, Test: 26.84%
Epoch: 600, Loss: 1.4568, Train: 34.06%, Valid: 33.94%, Test: 34.14%
Epoch: 625, Loss: 1.4579, Train: 34.40%, Valid: 34.22%, Test: 34.35%
Epoch: 650, Loss: 1.4418, Train: 34.81%, Valid: 34.46%, Test: 34.73%
Epoch: 675, Loss: 1.4457, Train: 26.62%, Valid: 26.61%, Test: 26.55%
Epoch: 700, Loss: 1.4526, Train: 30.38%, Valid: 30.24%, Test: 30.13%
Epoch: 725, Loss: 1.4352, Train: 36.65%, Valid: 36.27%, Test: 36.51%
Epoch: 750, Loss: 1.4348, Train: 32.16%, Valid: 32.03%, Test: 31.88%
Epoch: 775, Loss: 1.4544, Train: 38.04%, Valid: 37.84%, Test: 38.02%
Epoch: 800, Loss: 1.4433, Train: 26.13%, Valid: 26.32%, Test: 26.06%
Epoch: 825, Loss: 1.4400, Train: 36.12%, Valid: 35.90%, Test: 36.05%
Epoch: 850, Loss: 1.4311, Train: 33.20%, Valid: 32.80%, Test: 32.96%
Epoch: 875, Loss: 1.4388, Train: 32.18%, Valid: 31.99%, Test: 32.07%
Epoch: 900, Loss: 1.4284, Train: 37.37%, Valid: 37.14%, Test: 37.26%
Epoch: 925, Loss: 1.4367, Train: 33.41%, Valid: 33.23%, Test: 33.41%
Epoch: 950, Loss: 1.4285, Train: 34.65%, Valid: 34.26%, Test: 34.44%
Epoch: 975, Loss: 1.4269, Train: 29.51%, Valid: 29.48%, Test: 29.19%
Run 01:
Highest Train: 39.34
Highest Valid: 39.05
  Final Train: 39.34
   Final Test: 39.14
All runs:
Highest Train: 39.34, nan
Highest Valid: 39.05, nan
  Final Train: 39.34, nan
   Final Test: 39.14, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6230, Train: 23.40%, Valid: 23.49%, Test: 23.93%
Epoch: 25, Loss: 1.4969, Train: 34.43%, Valid: 34.17%, Test: 34.41%
Epoch: 50, Loss: 1.4746, Train: 35.45%, Valid: 35.00%, Test: 35.25%
Epoch: 75, Loss: 1.4564, Train: 36.69%, Valid: 35.82%, Test: 36.17%
Epoch: 100, Loss: 1.4430, Train: 37.24%, Valid: 36.05%, Test: 36.35%
Epoch: 125, Loss: 1.4328, Train: 37.72%, Valid: 36.10%, Test: 36.56%
Epoch: 150, Loss: 1.4245, Train: 38.24%, Valid: 35.89%, Test: 36.37%
Epoch: 175, Loss: 1.4157, Train: 38.87%, Valid: 35.79%, Test: 36.27%
Epoch: 200, Loss: 1.4037, Train: 39.23%, Valid: 35.72%, Test: 36.03%
Epoch: 225, Loss: 1.4024, Train: 39.33%, Valid: 35.83%, Test: 36.02%
Epoch: 250, Loss: 1.3837, Train: 40.30%, Valid: 35.26%, Test: 35.62%
Epoch: 275, Loss: 1.3821, Train: 40.59%, Valid: 35.59%, Test: 36.07%
Epoch: 300, Loss: 1.3713, Train: 41.00%, Valid: 35.41%, Test: 35.61%
Epoch: 325, Loss: 1.3670, Train: 41.35%, Valid: 34.94%, Test: 35.41%
Epoch: 350, Loss: 1.3649, Train: 41.42%, Valid: 34.98%, Test: 35.00%
Epoch: 375, Loss: 1.3548, Train: 41.65%, Valid: 34.30%, Test: 34.56%
Epoch: 400, Loss: 1.3475, Train: 41.98%, Valid: 34.44%, Test: 34.53%
Epoch: 425, Loss: 1.3388, Train: 42.64%, Valid: 34.85%, Test: 35.01%
Epoch: 450, Loss: 1.3398, Train: 42.68%, Valid: 34.24%, Test: 34.36%
Epoch: 475, Loss: 1.3394, Train: 42.58%, Valid: 34.74%, Test: 34.72%
Epoch: 500, Loss: 1.3302, Train: 43.18%, Valid: 33.86%, Test: 33.99%
Epoch: 525, Loss: 1.3300, Train: 43.37%, Valid: 34.43%, Test: 34.42%
Epoch: 550, Loss: 1.3166, Train: 43.70%, Valid: 34.49%, Test: 34.61%
Epoch: 575, Loss: 1.3182, Train: 44.07%, Valid: 34.15%, Test: 34.14%
Epoch: 600, Loss: 1.3098, Train: 44.12%, Valid: 34.38%, Test: 34.43%
Epoch: 625, Loss: 1.3087, Train: 44.13%, Valid: 34.30%, Test: 34.07%
Epoch: 650, Loss: 1.3027, Train: 44.48%, Valid: 33.42%, Test: 33.51%
Epoch: 675, Loss: 1.3101, Train: 44.12%, Valid: 33.55%, Test: 33.76%
Epoch: 700, Loss: 1.3028, Train: 44.27%, Valid: 32.97%, Test: 33.20%
Epoch: 725, Loss: 1.3015, Train: 44.73%, Valid: 33.69%, Test: 33.68%
Epoch: 750, Loss: 1.3009, Train: 44.80%, Valid: 32.98%, Test: 33.06%
Epoch: 775, Loss: 1.2896, Train: 44.81%, Valid: 33.28%, Test: 33.47%
Epoch: 800, Loss: 1.2885, Train: 45.26%, Valid: 33.50%, Test: 33.58%
Epoch: 825, Loss: 1.2918, Train: 45.07%, Valid: 33.53%, Test: 33.57%
Epoch: 850, Loss: 1.2961, Train: 45.35%, Valid: 33.67%, Test: 33.62%
Epoch: 875, Loss: 1.2869, Train: 45.20%, Valid: 32.65%, Test: 32.83%
Epoch: 900, Loss: 1.2809, Train: 45.49%, Valid: 33.58%, Test: 33.60%
Epoch: 925, Loss: 1.2739, Train: 45.95%, Valid: 33.47%, Test: 33.42%
Epoch: 950, Loss: 1.2761, Train: 45.98%, Valid: 33.45%, Test: 33.41%
Epoch: 975, Loss: 1.2810, Train: 45.56%, Valid: 33.34%, Test: 33.43%
Run 01:
Highest Train: 46.14
Highest Valid: 36.20
  Final Train: 37.58
   Final Test: 36.59
All runs:
Highest Train: 46.14, nan
Highest Valid: 36.20, nan
  Final Train: 37.58, nan
   Final Test: 36.59, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6174, Train: 28.72%, Valid: 28.54%, Test: 28.83%
Epoch: 25, Loss: 1.4807, Train: 35.05%, Valid: 34.78%, Test: 34.98%
Epoch: 50, Loss: 1.4344, Train: 37.39%, Valid: 36.71%, Test: 37.22%
Epoch: 75, Loss: 1.4114, Train: 38.47%, Valid: 37.84%, Test: 38.00%
Epoch: 100, Loss: 1.4005, Train: 39.09%, Valid: 37.88%, Test: 38.33%
Epoch: 125, Loss: 1.3635, Train: 40.46%, Valid: 39.28%, Test: 39.44%
Epoch: 150, Loss: 1.3708, Train: 40.08%, Valid: 38.55%, Test: 39.08%
Epoch: 175, Loss: 1.3364, Train: 42.31%, Valid: 40.81%, Test: 41.03%
Epoch: 200, Loss: 1.3185, Train: 43.10%, Valid: 41.12%, Test: 41.40%
Epoch: 225, Loss: 1.3086, Train: 43.54%, Valid: 41.31%, Test: 41.70%
Epoch: 250, Loss: 1.3041, Train: 43.67%, Valid: 41.42%, Test: 41.69%
Epoch: 275, Loss: 1.2971, Train: 43.96%, Valid: 41.35%, Test: 41.63%
Epoch: 300, Loss: 1.2933, Train: 44.43%, Valid: 41.39%, Test: 41.81%
Epoch: 325, Loss: 1.2728, Train: 45.18%, Valid: 41.95%, Test: 42.31%
Epoch: 350, Loss: 1.2745, Train: 45.31%, Valid: 42.43%, Test: 42.60%
Epoch: 375, Loss: 1.2885, Train: 45.02%, Valid: 41.53%, Test: 41.60%
Epoch: 400, Loss: 1.2785, Train: 45.57%, Valid: 42.02%, Test: 42.33%
Epoch: 425, Loss: 1.2487, Train: 46.31%, Valid: 42.37%, Test: 42.76%
Epoch: 450, Loss: 1.2571, Train: 46.26%, Valid: 41.84%, Test: 42.15%
Epoch: 475, Loss: 1.2716, Train: 46.35%, Valid: 42.94%, Test: 43.12%
Epoch: 500, Loss: 1.2533, Train: 46.67%, Valid: 42.32%, Test: 42.71%
Epoch: 525, Loss: 1.2318, Train: 47.23%, Valid: 42.57%, Test: 42.88%
Epoch: 550, Loss: 1.2244, Train: 47.62%, Valid: 42.66%, Test: 43.00%
Epoch: 575, Loss: 1.2294, Train: 47.57%, Valid: 43.06%, Test: 43.22%
Epoch: 600, Loss: 1.2103, Train: 48.11%, Valid: 43.22%, Test: 43.35%
Epoch: 625, Loss: 1.2050, Train: 48.34%, Valid: 43.08%, Test: 43.23%
Epoch: 650, Loss: 1.2767, Train: 45.66%, Valid: 41.43%, Test: 41.84%
Epoch: 675, Loss: 1.2317, Train: 47.66%, Valid: 43.29%, Test: 43.36%
Epoch: 700, Loss: 1.2176, Train: 47.87%, Valid: 42.42%, Test: 42.59%
Epoch: 725, Loss: 1.1962, Train: 48.65%, Valid: 42.95%, Test: 43.07%
Epoch: 750, Loss: 1.1913, Train: 48.16%, Valid: 42.14%, Test: 42.36%
Epoch: 775, Loss: 1.1867, Train: 49.03%, Valid: 43.02%, Test: 43.15%
Epoch: 800, Loss: 1.2632, Train: 46.59%, Valid: 42.73%, Test: 43.06%
Epoch: 825, Loss: 1.2258, Train: 47.64%, Valid: 43.21%, Test: 43.14%
Epoch: 850, Loss: 1.2638, Train: 46.31%, Valid: 42.36%, Test: 42.63%
Epoch: 875, Loss: 1.2142, Train: 47.90%, Valid: 43.25%, Test: 43.15%
Epoch: 900, Loss: 1.1962, Train: 48.64%, Valid: 43.54%, Test: 43.58%
Epoch: 925, Loss: 1.1944, Train: 48.62%, Valid: 43.09%, Test: 43.21%
Epoch: 950, Loss: 1.1802, Train: 49.26%, Valid: 43.38%, Test: 43.51%
Epoch: 975, Loss: 1.1762, Train: 49.52%, Valid: 43.36%, Test: 43.33%
Run 01:
Highest Train: 49.84
Highest Valid: 43.65
  Final Train: 48.21
   Final Test: 43.45
All runs:
Highest Train: 49.84, nan
Highest Valid: 43.65, nan
  Final Train: 48.21, nan
   Final Test: 43.45, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.7430, Train: 23.67%, Valid: 24.02%, Test: 23.63%
Epoch: 25, Loss: 1.4313, Train: 34.66%, Valid: 34.22%, Test: 34.72%
Epoch: 50, Loss: 1.4092, Train: 33.83%, Valid: 33.53%, Test: 34.46%
Epoch: 75, Loss: 1.3787, Train: 39.11%, Valid: 39.06%, Test: 39.37%
Epoch: 100, Loss: 1.3839, Train: 39.20%, Valid: 39.03%, Test: 39.32%
Epoch: 125, Loss: 1.3622, Train: 40.36%, Valid: 40.13%, Test: 40.29%
Epoch: 150, Loss: 1.3915, Train: 38.42%, Valid: 37.87%, Test: 37.98%
Epoch: 175, Loss: 1.3587, Train: 40.35%, Valid: 40.17%, Test: 40.35%
Epoch: 200, Loss: 1.3470, Train: 41.07%, Valid: 40.75%, Test: 40.86%
Epoch: 225, Loss: 1.3477, Train: 40.44%, Valid: 40.34%, Test: 40.62%
Epoch: 250, Loss: 1.3446, Train: 41.31%, Valid: 41.03%, Test: 41.37%
Epoch: 275, Loss: 1.4847, Train: 33.02%, Valid: 32.57%, Test: 33.13%
Epoch: 300, Loss: 1.5058, Train: 32.24%, Valid: 32.14%, Test: 32.47%
Epoch: 325, Loss: 1.4078, Train: 36.82%, Valid: 36.48%, Test: 36.67%
Epoch: 350, Loss: 1.4174, Train: 35.90%, Valid: 35.48%, Test: 35.93%
Epoch: 375, Loss: 1.3885, Train: 38.11%, Valid: 37.77%, Test: 38.09%
Epoch: 400, Loss: 1.3755, Train: 39.35%, Valid: 38.93%, Test: 39.32%
Epoch: 425, Loss: 1.4108, Train: 36.86%, Valid: 36.44%, Test: 36.71%
Epoch: 450, Loss: 1.3839, Train: 38.39%, Valid: 38.03%, Test: 38.22%
Epoch: 475, Loss: 1.3734, Train: 39.31%, Valid: 38.90%, Test: 39.15%
Epoch: 500, Loss: 1.3781, Train: 38.39%, Valid: 37.99%, Test: 38.51%
Epoch: 525, Loss: 1.3630, Train: 40.08%, Valid: 39.74%, Test: 40.04%
Epoch: 550, Loss: 1.3562, Train: 40.65%, Valid: 40.33%, Test: 40.65%
Epoch: 575, Loss: 1.3586, Train: 40.51%, Valid: 40.22%, Test: 40.63%
Epoch: 600, Loss: 1.3511, Train: 40.00%, Valid: 39.87%, Test: 40.18%
Epoch: 625, Loss: 1.3571, Train: 41.01%, Valid: 40.88%, Test: 41.05%
Epoch: 650, Loss: 1.3457, Train: 41.09%, Valid: 40.95%, Test: 41.07%
Epoch: 675, Loss: 1.3532, Train: 40.33%, Valid: 40.12%, Test: 40.35%
Epoch: 700, Loss: 1.3460, Train: 40.61%, Valid: 40.45%, Test: 40.74%
Epoch: 725, Loss: 1.4515, Train: 35.87%, Valid: 35.33%, Test: 35.83%
Epoch: 750, Loss: 1.4283, Train: 34.93%, Valid: 34.52%, Test: 34.86%
Epoch: 775, Loss: 1.3854, Train: 38.10%, Valid: 37.73%, Test: 38.09%
Epoch: 800, Loss: 1.3691, Train: 39.48%, Valid: 38.79%, Test: 39.22%
Epoch: 825, Loss: 1.3631, Train: 39.95%, Valid: 39.51%, Test: 39.67%
Epoch: 850, Loss: 1.3574, Train: 40.18%, Valid: 39.79%, Test: 40.06%
Epoch: 875, Loss: 1.3541, Train: 40.44%, Valid: 40.07%, Test: 40.25%
Epoch: 900, Loss: 1.3611, Train: 39.89%, Valid: 39.56%, Test: 39.80%
Epoch: 925, Loss: 1.3524, Train: 40.27%, Valid: 39.83%, Test: 40.01%
Epoch: 950, Loss: 1.3490, Train: 40.66%, Valid: 40.26%, Test: 40.46%
Epoch: 975, Loss: 1.3466, Train: 40.72%, Valid: 40.30%, Test: 40.52%
Run 01:
Highest Train: 41.74
Highest Valid: 41.54
  Final Train: 41.74
   Final Test: 41.65
All runs:
Highest Train: 41.74, nan
Highest Valid: 41.54, nan
  Final Train: 41.74, nan
   Final Test: 41.65, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6018, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4878, Train: 34.84%, Valid: 34.41%, Test: 34.76%
Epoch: 50, Loss: 1.4635, Train: 36.26%, Valid: 35.57%, Test: 35.91%
Epoch: 75, Loss: 1.4424, Train: 37.10%, Valid: 36.02%, Test: 36.15%
Epoch: 100, Loss: 1.4337, Train: 37.97%, Valid: 36.07%, Test: 36.29%
Epoch: 125, Loss: 1.4178, Train: 38.31%, Valid: 35.45%, Test: 36.04%
Epoch: 150, Loss: 1.4290, Train: 38.18%, Valid: 35.29%, Test: 35.89%
Epoch: 175, Loss: 1.3943, Train: 40.03%, Valid: 36.00%, Test: 36.16%
Epoch: 200, Loss: 1.3862, Train: 39.91%, Valid: 35.83%, Test: 35.99%
Epoch: 225, Loss: 1.3711, Train: 41.15%, Valid: 35.49%, Test: 35.98%
Epoch: 250, Loss: 1.3644, Train: 41.77%, Valid: 34.92%, Test: 35.53%
Epoch: 275, Loss: 1.3669, Train: 40.90%, Valid: 34.16%, Test: 34.76%
Epoch: 300, Loss: 1.3473, Train: 42.22%, Valid: 34.48%, Test: 35.00%
Epoch: 325, Loss: 1.3395, Train: 42.58%, Valid: 34.82%, Test: 35.44%
Epoch: 350, Loss: 1.3647, Train: 42.42%, Valid: 34.05%, Test: 34.30%
Epoch: 375, Loss: 1.3160, Train: 43.93%, Valid: 34.48%, Test: 34.90%
Epoch: 400, Loss: 1.3162, Train: 43.98%, Valid: 34.19%, Test: 34.51%
Epoch: 425, Loss: 1.3026, Train: 44.69%, Valid: 34.40%, Test: 34.42%
Epoch: 450, Loss: 1.3013, Train: 44.78%, Valid: 34.11%, Test: 34.13%
Epoch: 475, Loss: 1.2929, Train: 45.24%, Valid: 34.35%, Test: 34.28%
Epoch: 500, Loss: 1.2898, Train: 45.43%, Valid: 33.70%, Test: 33.54%
Epoch: 525, Loss: 1.2836, Train: 45.75%, Valid: 34.11%, Test: 34.01%
Epoch: 550, Loss: 1.2931, Train: 44.51%, Valid: 32.93%, Test: 32.69%
Epoch: 575, Loss: 1.2703, Train: 46.38%, Valid: 33.80%, Test: 33.50%
Epoch: 600, Loss: 1.2709, Train: 46.39%, Valid: 34.07%, Test: 33.79%
Epoch: 625, Loss: 1.2609, Train: 46.76%, Valid: 33.68%, Test: 33.60%
Epoch: 650, Loss: 1.2544, Train: 47.26%, Valid: 33.62%, Test: 33.37%
Epoch: 675, Loss: 1.2544, Train: 47.29%, Valid: 33.43%, Test: 33.26%
Epoch: 700, Loss: 1.2484, Train: 47.30%, Valid: 32.78%, Test: 32.78%
Epoch: 725, Loss: 1.2447, Train: 47.87%, Valid: 33.30%, Test: 33.22%
Epoch: 750, Loss: 1.2410, Train: 47.58%, Valid: 32.51%, Test: 32.24%
Epoch: 775, Loss: 1.2463, Train: 47.49%, Valid: 32.57%, Test: 32.75%
Epoch: 800, Loss: 1.2355, Train: 48.33%, Valid: 32.76%, Test: 32.82%
Epoch: 825, Loss: 1.2328, Train: 48.06%, Valid: 32.51%, Test: 32.49%
Epoch: 850, Loss: 1.2298, Train: 48.38%, Valid: 32.81%, Test: 32.74%
Epoch: 875, Loss: 1.2268, Train: 48.14%, Valid: 32.25%, Test: 32.05%
Epoch: 900, Loss: 1.2256, Train: 48.85%, Valid: 32.76%, Test: 32.69%
Epoch: 925, Loss: 1.2256, Train: 48.66%, Valid: 32.64%, Test: 32.49%
Epoch: 950, Loss: 1.2149, Train: 49.27%, Valid: 32.56%, Test: 32.73%
Epoch: 975, Loss: 1.2139, Train: 48.88%, Valid: 32.68%, Test: 32.86%
Run 01:
Highest Train: 49.34
Highest Valid: 36.28
  Final Train: 37.94
   Final Test: 36.39
All runs:
Highest Train: 49.34, nan
Highest Valid: 36.28, nan
  Final Train: 37.94, nan
   Final Test: 36.39, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6115, Train: 29.00%, Valid: 28.74%, Test: 29.12%
Epoch: 25, Loss: 1.4786, Train: 35.05%, Valid: 34.60%, Test: 34.98%
Epoch: 50, Loss: 1.4352, Train: 37.25%, Valid: 36.81%, Test: 36.94%
Epoch: 75, Loss: 1.4070, Train: 38.63%, Valid: 37.79%, Test: 38.05%
Epoch: 100, Loss: 1.3855, Train: 39.47%, Valid: 38.30%, Test: 38.70%
Epoch: 125, Loss: 1.3797, Train: 40.64%, Valid: 39.47%, Test: 39.66%
Epoch: 150, Loss: 1.3361, Train: 42.27%, Valid: 40.81%, Test: 41.14%
Epoch: 175, Loss: 1.3791, Train: 40.79%, Valid: 39.47%, Test: 40.04%
Epoch: 200, Loss: 1.3385, Train: 42.09%, Valid: 40.57%, Test: 40.96%
Epoch: 225, Loss: 1.3306, Train: 42.55%, Valid: 41.06%, Test: 41.43%
Epoch: 250, Loss: 1.3045, Train: 43.62%, Valid: 41.74%, Test: 41.97%
Epoch: 275, Loss: 1.2968, Train: 43.38%, Valid: 41.56%, Test: 41.72%
Epoch: 300, Loss: 1.2808, Train: 44.55%, Valid: 42.18%, Test: 42.51%
Epoch: 325, Loss: 1.3074, Train: 43.33%, Valid: 41.54%, Test: 41.93%
Epoch: 350, Loss: 1.2773, Train: 44.44%, Valid: 42.08%, Test: 42.50%
Epoch: 375, Loss: 1.2713, Train: 44.89%, Valid: 42.39%, Test: 42.76%
Epoch: 400, Loss: 1.2592, Train: 45.47%, Valid: 42.62%, Test: 42.95%
Epoch: 425, Loss: 1.2532, Train: 45.71%, Valid: 42.87%, Test: 43.06%
Epoch: 450, Loss: 1.2506, Train: 46.28%, Valid: 43.00%, Test: 43.39%
Epoch: 475, Loss: 1.2386, Train: 46.63%, Valid: 43.04%, Test: 43.39%
Epoch: 500, Loss: 1.2343, Train: 47.09%, Valid: 43.11%, Test: 43.49%
Epoch: 525, Loss: 1.2416, Train: 46.82%, Valid: 43.24%, Test: 43.32%
Epoch: 550, Loss: 1.2295, Train: 47.08%, Valid: 43.20%, Test: 43.56%
Epoch: 575, Loss: 1.2150, Train: 47.72%, Valid: 43.21%, Test: 43.67%
Epoch: 600, Loss: 1.2263, Train: 47.32%, Valid: 43.31%, Test: 43.70%
Epoch: 625, Loss: 1.2077, Train: 48.05%, Valid: 43.41%, Test: 43.60%
Epoch: 650, Loss: 1.2074, Train: 48.14%, Valid: 43.24%, Test: 43.36%
Epoch: 675, Loss: 1.1979, Train: 48.44%, Valid: 43.42%, Test: 43.64%
Epoch: 700, Loss: 1.2055, Train: 48.50%, Valid: 43.61%, Test: 43.91%
Epoch: 725, Loss: 1.2069, Train: 48.29%, Valid: 43.54%, Test: 43.74%
Epoch: 750, Loss: 1.1892, Train: 48.88%, Valid: 43.61%, Test: 43.60%
Epoch: 775, Loss: 1.1823, Train: 49.24%, Valid: 43.50%, Test: 43.45%
Epoch: 800, Loss: 1.2467, Train: 44.67%, Valid: 40.06%, Test: 39.81%
Epoch: 825, Loss: 1.1986, Train: 48.47%, Valid: 43.55%, Test: 43.62%
Epoch: 850, Loss: 1.1845, Train: 49.04%, Valid: 43.63%, Test: 43.71%
Epoch: 875, Loss: 1.1745, Train: 49.25%, Valid: 43.17%, Test: 43.10%
Epoch: 900, Loss: 1.1679, Train: 49.94%, Valid: 43.80%, Test: 43.67%
Epoch: 925, Loss: 1.1743, Train: 49.74%, Valid: 43.15%, Test: 43.20%
Epoch: 950, Loss: 1.2421, Train: 47.08%, Valid: 43.19%, Test: 43.25%
Epoch: 975, Loss: 1.2056, Train: 47.83%, Valid: 42.53%, Test: 42.52%
Run 01:
Highest Train: 50.35
Highest Valid: 43.94
  Final Train: 49.33
   Final Test: 43.94
All runs:
Highest Train: 50.35, nan
Highest Valid: 43.94, nan
  Final Train: 49.33, nan
   Final Test: 43.94, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.7943, Train: 19.38%, Valid: 19.15%, Test: 19.79%
Epoch: 25, Loss: 1.4489, Train: 36.24%, Valid: 36.04%, Test: 36.24%
Epoch: 50, Loss: 1.4314, Train: 32.63%, Valid: 32.40%, Test: 32.73%
Epoch: 75, Loss: 1.4058, Train: 38.00%, Valid: 37.25%, Test: 37.71%
Epoch: 100, Loss: 1.4030, Train: 35.47%, Valid: 34.95%, Test: 35.56%
Epoch: 125, Loss: 1.3885, Train: 39.00%, Valid: 38.72%, Test: 39.11%
Epoch: 150, Loss: 1.3856, Train: 39.26%, Valid: 38.83%, Test: 38.94%
Epoch: 175, Loss: 1.3663, Train: 40.18%, Valid: 39.91%, Test: 40.01%
Epoch: 200, Loss: 1.3727, Train: 40.44%, Valid: 40.16%, Test: 40.47%
Epoch: 225, Loss: 1.3474, Train: 40.54%, Valid: 40.23%, Test: 40.71%
Epoch: 250, Loss: 1.3597, Train: 40.63%, Valid: 40.50%, Test: 40.70%
Epoch: 275, Loss: 1.3415, Train: 41.60%, Valid: 41.44%, Test: 41.60%
Epoch: 300, Loss: 1.3491, Train: 41.55%, Valid: 41.22%, Test: 41.40%
Epoch: 325, Loss: 1.3272, Train: 42.43%, Valid: 42.08%, Test: 42.36%
Epoch: 350, Loss: 1.3292, Train: 42.81%, Valid: 42.49%, Test: 42.49%
Epoch: 375, Loss: 1.3137, Train: 42.65%, Valid: 42.49%, Test: 42.76%
Epoch: 400, Loss: 1.3183, Train: 42.25%, Valid: 42.08%, Test: 42.45%
Epoch: 425, Loss: 1.3062, Train: 42.95%, Valid: 42.70%, Test: 42.97%
Epoch: 450, Loss: 1.3117, Train: 42.62%, Valid: 42.37%, Test: 42.82%
Epoch: 475, Loss: 1.2995, Train: 43.32%, Valid: 43.06%, Test: 43.12%
Epoch: 500, Loss: 1.3709, Train: 40.11%, Valid: 39.73%, Test: 39.91%
Epoch: 525, Loss: 1.3192, Train: 42.39%, Valid: 42.28%, Test: 42.40%
Epoch: 550, Loss: 1.3065, Train: 42.00%, Valid: 41.84%, Test: 42.21%
Epoch: 575, Loss: 1.3080, Train: 42.56%, Valid: 42.25%, Test: 42.50%
Epoch: 600, Loss: 1.2954, Train: 43.31%, Valid: 43.12%, Test: 43.19%
Epoch: 625, Loss: 1.3092, Train: 43.29%, Valid: 42.83%, Test: 42.88%
Epoch: 650, Loss: 1.2914, Train: 43.48%, Valid: 43.17%, Test: 43.30%
Epoch: 675, Loss: 1.2891, Train: 44.02%, Valid: 43.68%, Test: 43.77%
Epoch: 700, Loss: 1.2924, Train: 44.20%, Valid: 43.84%, Test: 43.94%
Epoch: 725, Loss: 1.2817, Train: 44.02%, Valid: 43.71%, Test: 43.83%
Epoch: 750, Loss: 1.2922, Train: 44.32%, Valid: 43.82%, Test: 43.75%
Epoch: 775, Loss: 1.2810, Train: 43.93%, Valid: 43.39%, Test: 43.57%
Epoch: 800, Loss: 1.2749, Train: 44.66%, Valid: 44.28%, Test: 44.22%
Epoch: 825, Loss: 1.2829, Train: 44.69%, Valid: 44.25%, Test: 44.08%
Epoch: 850, Loss: 1.2694, Train: 44.65%, Valid: 44.04%, Test: 44.25%
Epoch: 875, Loss: 1.2892, Train: 44.10%, Valid: 43.71%, Test: 44.02%
Epoch: 900, Loss: 1.2718, Train: 44.77%, Valid: 44.44%, Test: 44.50%
Epoch: 925, Loss: 1.2744, Train: 44.83%, Valid: 44.25%, Test: 44.38%
Epoch: 950, Loss: 1.2674, Train: 45.20%, Valid: 44.55%, Test: 44.64%
Epoch: 975, Loss: 1.2684, Train: 45.17%, Valid: 44.60%, Test: 44.63%
Run 01:
Highest Train: 45.46
Highest Valid: 44.81
  Final Train: 45.41
   Final Test: 44.67
All runs:
Highest Train: 45.46, nan
Highest Valid: 44.81, nan
  Final Train: 45.41, nan
   Final Test: 44.67, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6120, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4916, Train: 34.89%, Valid: 34.66%, Test: 34.72%
Epoch: 50, Loss: 1.4709, Train: 36.53%, Valid: 35.77%, Test: 36.02%
Epoch: 75, Loss: 1.4586, Train: 37.44%, Valid: 36.07%, Test: 36.48%
Epoch: 100, Loss: 1.4516, Train: 38.11%, Valid: 36.29%, Test: 36.72%
Epoch: 125, Loss: 1.4445, Train: 38.36%, Valid: 36.36%, Test: 36.88%
Epoch: 150, Loss: 1.4407, Train: 38.78%, Valid: 36.41%, Test: 36.80%
Epoch: 175, Loss: 1.4361, Train: 39.10%, Valid: 36.45%, Test: 36.96%
Epoch: 200, Loss: 1.4315, Train: 39.22%, Valid: 36.41%, Test: 36.83%
Epoch: 225, Loss: 1.4290, Train: 39.51%, Valid: 36.37%, Test: 36.96%
Epoch: 250, Loss: 1.4248, Train: 39.67%, Valid: 36.25%, Test: 36.91%
Epoch: 275, Loss: 1.4241, Train: 39.83%, Valid: 36.32%, Test: 36.87%
Epoch: 300, Loss: 1.4226, Train: 39.99%, Valid: 36.28%, Test: 36.78%
Epoch: 325, Loss: 1.4211, Train: 40.10%, Valid: 36.22%, Test: 36.85%
Epoch: 350, Loss: 1.4202, Train: 40.18%, Valid: 36.36%, Test: 36.97%
Epoch: 375, Loss: 1.4178, Train: 40.26%, Valid: 36.30%, Test: 36.79%
Epoch: 400, Loss: 1.4190, Train: 40.55%, Valid: 36.26%, Test: 36.76%
Epoch: 425, Loss: 1.4173, Train: 40.43%, Valid: 36.36%, Test: 36.75%
Epoch: 450, Loss: 1.4157, Train: 40.49%, Valid: 36.44%, Test: 36.72%
Epoch: 475, Loss: 1.4158, Train: 40.83%, Valid: 36.31%, Test: 36.80%
Epoch: 500, Loss: 1.4126, Train: 40.71%, Valid: 36.37%, Test: 36.81%
Epoch: 525, Loss: 1.4115, Train: 40.92%, Valid: 36.44%, Test: 36.84%
Epoch: 550, Loss: 1.4090, Train: 40.84%, Valid: 36.49%, Test: 36.89%
Epoch: 575, Loss: 1.4093, Train: 41.10%, Valid: 36.50%, Test: 36.87%
Epoch: 600, Loss: 1.4078, Train: 41.04%, Valid: 36.49%, Test: 36.90%
Epoch: 625, Loss: 1.4051, Train: 41.24%, Valid: 36.46%, Test: 36.91%
Epoch: 650, Loss: 1.4080, Train: 41.36%, Valid: 36.61%, Test: 37.06%
Epoch: 675, Loss: 1.4068, Train: 41.17%, Valid: 36.81%, Test: 37.03%
Epoch: 700, Loss: 1.4055, Train: 41.56%, Valid: 36.74%, Test: 37.06%
Epoch: 725, Loss: 1.4068, Train: 41.57%, Valid: 36.84%, Test: 37.03%
Epoch: 750, Loss: 1.4023, Train: 41.74%, Valid: 37.06%, Test: 37.28%
Epoch: 775, Loss: 1.4002, Train: 41.92%, Valid: 37.01%, Test: 37.29%
Epoch: 800, Loss: 1.4006, Train: 41.81%, Valid: 37.14%, Test: 37.19%
Epoch: 825, Loss: 1.3972, Train: 41.84%, Valid: 36.91%, Test: 37.36%
Epoch: 850, Loss: 1.3956, Train: 41.96%, Valid: 37.10%, Test: 37.36%
Epoch: 875, Loss: 1.3987, Train: 41.96%, Valid: 37.22%, Test: 37.64%
Epoch: 900, Loss: 1.3974, Train: 41.94%, Valid: 37.28%, Test: 37.36%
Epoch: 925, Loss: 1.3941, Train: 42.34%, Valid: 37.38%, Test: 37.62%
Epoch: 950, Loss: 1.3951, Train: 42.31%, Valid: 37.48%, Test: 37.87%
Epoch: 975, Loss: 1.3928, Train: 42.48%, Valid: 37.42%, Test: 37.81%
Run 01:
Highest Train: 42.83
Highest Valid: 37.62
  Final Train: 42.80
   Final Test: 38.00
All runs:
Highest Train: 42.83, nan
Highest Valid: 37.62, nan
  Final Train: 42.80, nan
   Final Test: 38.00, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6158, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4979, Train: 34.68%, Valid: 34.37%, Test: 34.61%
Epoch: 50, Loss: 1.4712, Train: 36.34%, Valid: 35.71%, Test: 35.92%
Epoch: 75, Loss: 1.4374, Train: 38.37%, Valid: 37.41%, Test: 37.65%
Epoch: 100, Loss: 1.4237, Train: 38.18%, Valid: 37.15%, Test: 37.34%
Epoch: 125, Loss: 1.4091, Train: 36.15%, Valid: 34.87%, Test: 35.07%
Epoch: 150, Loss: 1.3994, Train: 37.76%, Valid: 36.60%, Test: 36.99%
Epoch: 175, Loss: 1.3992, Train: 40.01%, Valid: 38.34%, Test: 39.21%
Epoch: 200, Loss: 1.3874, Train: 41.03%, Valid: 39.45%, Test: 39.68%
Epoch: 225, Loss: 1.3746, Train: 40.39%, Valid: 38.83%, Test: 39.30%
Epoch: 250, Loss: 1.3852, Train: 41.46%, Valid: 40.17%, Test: 40.14%
Epoch: 275, Loss: 1.3854, Train: 41.26%, Valid: 39.81%, Test: 40.08%
Epoch: 300, Loss: 1.3618, Train: 41.20%, Valid: 39.97%, Test: 40.19%
Epoch: 325, Loss: 1.3928, Train: 41.33%, Valid: 39.81%, Test: 40.47%
Epoch: 350, Loss: 1.3617, Train: 42.05%, Valid: 40.40%, Test: 40.71%
Epoch: 375, Loss: 1.3586, Train: 42.97%, Valid: 41.30%, Test: 41.35%
Epoch: 400, Loss: 1.3571, Train: 42.78%, Valid: 41.34%, Test: 41.26%
Epoch: 425, Loss: 1.3847, Train: 41.70%, Valid: 40.17%, Test: 40.60%
Epoch: 450, Loss: 1.3460, Train: 42.58%, Valid: 41.19%, Test: 41.21%
Epoch: 475, Loss: 1.3803, Train: 40.28%, Valid: 39.29%, Test: 39.47%
Epoch: 500, Loss: 1.3636, Train: 42.47%, Valid: 41.19%, Test: 41.46%
Epoch: 525, Loss: 1.3590, Train: 41.93%, Valid: 40.69%, Test: 40.74%
Epoch: 550, Loss: 1.3707, Train: 41.11%, Valid: 39.96%, Test: 40.17%
Epoch: 575, Loss: 1.3541, Train: 42.49%, Valid: 41.20%, Test: 41.59%
Epoch: 600, Loss: 1.3609, Train: 43.27%, Valid: 42.09%, Test: 42.48%
Epoch: 625, Loss: 1.3571, Train: 42.05%, Valid: 40.86%, Test: 41.04%
Epoch: 650, Loss: 1.3395, Train: 44.04%, Valid: 42.71%, Test: 42.88%
Epoch: 675, Loss: 1.3478, Train: 43.01%, Valid: 41.64%, Test: 41.85%
Epoch: 700, Loss: 1.3959, Train: 39.92%, Valid: 38.99%, Test: 39.19%
Epoch: 725, Loss: 1.3591, Train: 43.57%, Valid: 42.63%, Test: 42.73%
Epoch: 750, Loss: 1.3451, Train: 44.27%, Valid: 43.10%, Test: 43.41%
Epoch: 775, Loss: 1.3556, Train: 43.30%, Valid: 42.06%, Test: 42.25%
Epoch: 800, Loss: 1.3372, Train: 44.02%, Valid: 42.99%, Test: 42.95%
Epoch: 825, Loss: 1.3374, Train: 42.76%, Valid: 41.61%, Test: 41.84%
Epoch: 850, Loss: 1.3497, Train: 42.14%, Valid: 41.44%, Test: 41.42%
Epoch: 875, Loss: 1.3355, Train: 43.64%, Valid: 42.58%, Test: 42.88%
Epoch: 900, Loss: 1.3481, Train: 42.52%, Valid: 41.78%, Test: 41.71%
Epoch: 925, Loss: 1.3546, Train: 42.90%, Valid: 42.00%, Test: 41.94%
Epoch: 950, Loss: 1.3650, Train: 42.70%, Valid: 41.88%, Test: 42.14%
Epoch: 975, Loss: 1.3421, Train: 43.11%, Valid: 42.30%, Test: 42.43%
Run 01:
Highest Train: 44.61
Highest Valid: 43.43
  Final Train: 44.27
   Final Test: 43.54
All runs:
Highest Train: 44.61, nan
Highest Valid: 43.43, nan
  Final Train: 44.27, nan
   Final Test: 43.54, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.7632, Train: 20.61%, Valid: 20.67%, Test: 20.43%
Epoch: 25, Loss: 1.4379, Train: 31.34%, Valid: 31.13%, Test: 31.47%
Epoch: 50, Loss: 1.4222, Train: 38.43%, Valid: 37.97%, Test: 38.19%
Epoch: 75, Loss: 1.4058, Train: 33.78%, Valid: 33.54%, Test: 33.93%
Epoch: 100, Loss: 1.3967, Train: 35.46%, Valid: 35.03%, Test: 35.00%
Epoch: 125, Loss: 1.3880, Train: 38.87%, Valid: 38.43%, Test: 38.59%
Epoch: 150, Loss: 1.3958, Train: 39.25%, Valid: 38.80%, Test: 39.01%
Epoch: 175, Loss: 1.3747, Train: 39.73%, Valid: 39.22%, Test: 39.64%
Epoch: 200, Loss: 1.3908, Train: 36.81%, Valid: 36.40%, Test: 36.67%
Epoch: 225, Loss: 1.3645, Train: 38.95%, Valid: 38.60%, Test: 38.89%
Epoch: 250, Loss: 1.3813, Train: 39.25%, Valid: 38.84%, Test: 38.84%
Epoch: 275, Loss: 1.3784, Train: 41.08%, Valid: 40.73%, Test: 41.07%
Epoch: 300, Loss: 1.3655, Train: 42.78%, Valid: 42.22%, Test: 42.83%
Epoch: 325, Loss: 1.3592, Train: 41.66%, Valid: 41.18%, Test: 41.40%
Epoch: 350, Loss: 1.3579, Train: 42.80%, Valid: 42.22%, Test: 42.72%
Epoch: 375, Loss: 1.3744, Train: 40.76%, Valid: 40.49%, Test: 40.52%
Epoch: 400, Loss: 1.3550, Train: 42.51%, Valid: 42.10%, Test: 42.30%
Epoch: 425, Loss: 1.3593, Train: 41.26%, Valid: 40.94%, Test: 41.06%
Epoch: 450, Loss: 1.3470, Train: 42.53%, Valid: 42.10%, Test: 42.34%
Epoch: 475, Loss: 1.3730, Train: 40.34%, Valid: 39.97%, Test: 40.10%
Epoch: 500, Loss: 1.3714, Train: 41.60%, Valid: 41.10%, Test: 41.43%
Epoch: 525, Loss: 1.3496, Train: 41.41%, Valid: 40.94%, Test: 41.09%
Epoch: 550, Loss: 1.3436, Train: 42.37%, Valid: 42.07%, Test: 42.18%
Epoch: 575, Loss: 1.3626, Train: 40.95%, Valid: 40.70%, Test: 40.70%
Epoch: 600, Loss: 1.3532, Train: 41.25%, Valid: 40.68%, Test: 40.97%
Epoch: 625, Loss: 1.3554, Train: 41.27%, Valid: 40.70%, Test: 40.94%
Epoch: 650, Loss: 1.3446, Train: 42.76%, Valid: 42.33%, Test: 42.68%
Epoch: 675, Loss: 1.3410, Train: 42.01%, Valid: 41.65%, Test: 41.78%
Epoch: 700, Loss: 1.3353, Train: 42.02%, Valid: 41.41%, Test: 41.68%
Epoch: 725, Loss: 1.3388, Train: 42.23%, Valid: 42.01%, Test: 42.17%
Epoch: 750, Loss: 1.3420, Train: 42.08%, Valid: 41.87%, Test: 42.05%
Epoch: 775, Loss: 1.3485, Train: 41.63%, Valid: 41.21%, Test: 41.38%
Epoch: 800, Loss: 1.3491, Train: 42.78%, Valid: 42.33%, Test: 42.55%
Epoch: 825, Loss: 1.3454, Train: 41.13%, Valid: 41.00%, Test: 41.12%
Epoch: 850, Loss: 1.3541, Train: 42.04%, Valid: 41.55%, Test: 41.59%
Epoch: 875, Loss: 1.3482, Train: 41.31%, Valid: 40.92%, Test: 41.10%
Epoch: 900, Loss: 1.3327, Train: 41.60%, Valid: 41.15%, Test: 41.28%
Epoch: 925, Loss: 1.3415, Train: 41.77%, Valid: 41.34%, Test: 41.43%
Epoch: 950, Loss: 1.3370, Train: 42.56%, Valid: 42.10%, Test: 42.26%
Epoch: 975, Loss: 1.4980, Train: 38.34%, Valid: 38.10%, Test: 38.10%
Run 01:
Highest Train: 43.34
Highest Valid: 42.94
  Final Train: 43.34
   Final Test: 43.07
All runs:
Highest Train: 43.34, nan
Highest Valid: 42.94, nan
  Final Train: 43.34, nan
   Final Test: 43.07, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6028, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4934, Train: 34.86%, Valid: 34.62%, Test: 34.71%
Epoch: 50, Loss: 1.4738, Train: 36.47%, Valid: 35.70%, Test: 36.07%
Epoch: 75, Loss: 1.4634, Train: 37.30%, Valid: 36.01%, Test: 36.45%
Epoch: 100, Loss: 1.4562, Train: 37.90%, Valid: 36.24%, Test: 36.67%
Epoch: 125, Loss: 1.4516, Train: 38.28%, Valid: 36.29%, Test: 36.70%
Epoch: 150, Loss: 1.4470, Train: 38.42%, Valid: 36.48%, Test: 36.76%
Epoch: 175, Loss: 1.4424, Train: 38.81%, Valid: 36.45%, Test: 36.78%
Epoch: 200, Loss: 1.4410, Train: 38.98%, Valid: 36.45%, Test: 36.65%
Epoch: 225, Loss: 1.4373, Train: 39.14%, Valid: 36.53%, Test: 36.71%
Epoch: 250, Loss: 1.4359, Train: 39.23%, Valid: 36.46%, Test: 36.74%
Epoch: 275, Loss: 1.4335, Train: 39.37%, Valid: 36.42%, Test: 36.71%
Epoch: 300, Loss: 1.4312, Train: 39.46%, Valid: 36.37%, Test: 36.66%
Epoch: 325, Loss: 1.4322, Train: 39.58%, Valid: 36.30%, Test: 36.73%
Epoch: 350, Loss: 1.4298, Train: 39.67%, Valid: 36.33%, Test: 36.76%
Epoch: 375, Loss: 1.4278, Train: 39.78%, Valid: 36.28%, Test: 36.69%
Epoch: 400, Loss: 1.4272, Train: 39.98%, Valid: 36.32%, Test: 36.65%
Epoch: 425, Loss: 1.4241, Train: 40.01%, Valid: 36.30%, Test: 36.78%
Epoch: 450, Loss: 1.4231, Train: 40.10%, Valid: 36.35%, Test: 36.88%
Epoch: 475, Loss: 1.4228, Train: 40.17%, Valid: 36.34%, Test: 36.79%
Epoch: 500, Loss: 1.4196, Train: 40.27%, Valid: 36.43%, Test: 36.88%
Epoch: 525, Loss: 1.4215, Train: 40.39%, Valid: 36.49%, Test: 36.86%
Epoch: 550, Loss: 1.4210, Train: 40.40%, Valid: 36.49%, Test: 36.87%
Epoch: 575, Loss: 1.4186, Train: 40.55%, Valid: 36.55%, Test: 36.86%
Epoch: 600, Loss: 1.4181, Train: 40.79%, Valid: 36.67%, Test: 36.84%
Epoch: 625, Loss: 1.4183, Train: 40.71%, Valid: 36.55%, Test: 37.04%
Epoch: 650, Loss: 1.4179, Train: 40.81%, Valid: 36.76%, Test: 37.02%
Epoch: 675, Loss: 1.4154, Train: 40.94%, Valid: 36.70%, Test: 37.11%
Epoch: 700, Loss: 1.4144, Train: 41.00%, Valid: 36.77%, Test: 37.20%
Epoch: 725, Loss: 1.4133, Train: 41.01%, Valid: 36.89%, Test: 37.14%
Epoch: 750, Loss: 1.4140, Train: 41.23%, Valid: 36.98%, Test: 37.30%
Epoch: 775, Loss: 1.4110, Train: 41.25%, Valid: 37.00%, Test: 37.33%
Epoch: 800, Loss: 1.4123, Train: 41.22%, Valid: 37.11%, Test: 37.42%
Epoch: 825, Loss: 1.4109, Train: 41.43%, Valid: 37.13%, Test: 37.53%
Epoch: 850, Loss: 1.4094, Train: 41.12%, Valid: 37.06%, Test: 37.28%
Epoch: 875, Loss: 1.4110, Train: 41.18%, Valid: 37.13%, Test: 37.41%
Epoch: 900, Loss: 1.4022, Train: 41.77%, Valid: 37.38%, Test: 37.62%
Epoch: 925, Loss: 1.4028, Train: 41.72%, Valid: 37.37%, Test: 37.97%
Epoch: 950, Loss: 1.3973, Train: 41.67%, Valid: 37.35%, Test: 37.66%
Epoch: 975, Loss: 1.3998, Train: 41.62%, Valid: 37.58%, Test: 37.80%
Run 01:
Highest Train: 41.90
Highest Valid: 37.70
  Final Train: 41.84
   Final Test: 37.87
All runs:
Highest Train: 41.90, nan
Highest Valid: 37.70, nan
  Final Train: 41.84, nan
   Final Test: 37.87, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6044, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4942, Train: 34.83%, Valid: 34.65%, Test: 34.78%
Epoch: 50, Loss: 1.4564, Train: 36.87%, Valid: 36.20%, Test: 36.49%
Epoch: 75, Loss: 1.4265, Train: 39.21%, Valid: 38.21%, Test: 38.31%
Epoch: 100, Loss: 1.4059, Train: 38.75%, Valid: 37.30%, Test: 37.52%
Epoch: 125, Loss: 1.4028, Train: 39.34%, Valid: 37.97%, Test: 38.17%
Epoch: 150, Loss: 1.3916, Train: 39.97%, Valid: 38.49%, Test: 38.51%
Epoch: 175, Loss: 1.3998, Train: 40.44%, Valid: 38.90%, Test: 39.37%
Epoch: 200, Loss: 1.3960, Train: 40.86%, Valid: 39.51%, Test: 39.66%
Epoch: 225, Loss: 1.3890, Train: 40.69%, Valid: 39.09%, Test: 39.73%
Epoch: 250, Loss: 1.3951, Train: 41.69%, Valid: 40.05%, Test: 40.47%
Epoch: 275, Loss: 1.3730, Train: 41.99%, Valid: 40.30%, Test: 40.72%
Epoch: 300, Loss: 1.3831, Train: 41.32%, Valid: 39.94%, Test: 40.03%
Epoch: 325, Loss: 1.3920, Train: 41.54%, Valid: 40.01%, Test: 40.33%
Epoch: 350, Loss: 1.3851, Train: 41.47%, Valid: 40.52%, Test: 40.58%
Epoch: 375, Loss: 1.3659, Train: 42.71%, Valid: 41.65%, Test: 41.93%
Epoch: 400, Loss: 1.3605, Train: 43.02%, Valid: 41.69%, Test: 41.91%
Epoch: 425, Loss: 1.3689, Train: 43.05%, Valid: 41.94%, Test: 41.92%
Epoch: 450, Loss: 1.3523, Train: 41.66%, Valid: 40.51%, Test: 40.94%
Epoch: 475, Loss: 1.3679, Train: 42.28%, Valid: 41.02%, Test: 41.16%
Epoch: 500, Loss: 1.3523, Train: 42.93%, Valid: 41.80%, Test: 41.90%
Epoch: 525, Loss: 1.3421, Train: 43.69%, Valid: 42.53%, Test: 42.61%
Epoch: 550, Loss: 1.3432, Train: 43.67%, Valid: 42.48%, Test: 42.67%
Epoch: 575, Loss: 1.3352, Train: 43.72%, Valid: 42.35%, Test: 42.68%
Epoch: 600, Loss: 1.3489, Train: 44.04%, Valid: 42.76%, Test: 42.97%
Epoch: 625, Loss: 1.3466, Train: 43.79%, Valid: 42.86%, Test: 42.91%
Epoch: 650, Loss: 1.3364, Train: 43.22%, Valid: 42.20%, Test: 42.23%
Epoch: 675, Loss: 1.3377, Train: 44.15%, Valid: 42.88%, Test: 42.87%
Epoch: 700, Loss: 1.3382, Train: 43.44%, Valid: 42.14%, Test: 42.32%
Epoch: 725, Loss: 1.3494, Train: 43.27%, Valid: 42.23%, Test: 42.36%
Epoch: 750, Loss: 1.3487, Train: 43.70%, Valid: 42.57%, Test: 42.76%
Epoch: 775, Loss: 1.3662, Train: 42.30%, Valid: 41.11%, Test: 41.28%
Epoch: 800, Loss: 1.3473, Train: 42.71%, Valid: 41.61%, Test: 41.77%
Epoch: 825, Loss: 1.3410, Train: 43.81%, Valid: 42.47%, Test: 42.96%
Epoch: 850, Loss: 1.3329, Train: 44.44%, Valid: 43.08%, Test: 43.42%
Epoch: 875, Loss: 1.3238, Train: 43.93%, Valid: 42.82%, Test: 43.00%
Epoch: 900, Loss: 1.3892, Train: 44.02%, Valid: 42.87%, Test: 43.03%
Epoch: 925, Loss: 1.3228, Train: 44.13%, Valid: 43.11%, Test: 43.00%
Epoch: 950, Loss: 1.3326, Train: 44.27%, Valid: 43.19%, Test: 43.42%
Epoch: 975, Loss: 1.3759, Train: 42.27%, Valid: 41.35%, Test: 41.41%
Run 01:
Highest Train: 44.53
Highest Valid: 43.50
  Final Train: 44.51
   Final Test: 43.42
All runs:
Highest Train: 44.53, nan
Highest Valid: 43.50, nan
  Final Train: 44.51, nan
   Final Test: 43.42, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.5672, Train: 33.04%, Valid: 32.74%, Test: 33.61%
Epoch: 25, Loss: 1.4475, Train: 34.00%, Valid: 33.91%, Test: 34.58%
Epoch: 50, Loss: 1.4258, Train: 38.76%, Valid: 38.38%, Test: 38.84%
Epoch: 75, Loss: 1.3996, Train: 38.02%, Valid: 37.51%, Test: 37.81%
Epoch: 100, Loss: 1.3893, Train: 40.48%, Valid: 40.08%, Test: 40.27%
Epoch: 125, Loss: 1.3928, Train: 39.60%, Valid: 39.27%, Test: 39.43%
Epoch: 150, Loss: 1.3940, Train: 40.20%, Valid: 39.82%, Test: 39.92%
Epoch: 175, Loss: 1.3730, Train: 39.96%, Valid: 39.50%, Test: 39.86%
Epoch: 200, Loss: 1.4047, Train: 39.58%, Valid: 39.28%, Test: 39.44%
Epoch: 225, Loss: 1.3767, Train: 40.51%, Valid: 40.10%, Test: 40.30%
Epoch: 250, Loss: 1.3772, Train: 39.84%, Valid: 39.12%, Test: 39.59%
Epoch: 275, Loss: 1.3781, Train: 40.14%, Valid: 39.93%, Test: 40.09%
Epoch: 300, Loss: 1.3785, Train: 39.19%, Valid: 38.92%, Test: 38.88%
Epoch: 325, Loss: 1.3649, Train: 41.45%, Valid: 40.99%, Test: 41.23%
Epoch: 350, Loss: 1.3510, Train: 38.93%, Valid: 38.49%, Test: 38.85%
Epoch: 375, Loss: 1.3716, Train: 41.53%, Valid: 41.11%, Test: 41.23%
Epoch: 400, Loss: 1.3494, Train: 40.05%, Valid: 39.40%, Test: 39.79%
Epoch: 425, Loss: 1.3458, Train: 38.49%, Valid: 37.92%, Test: 38.43%
Epoch: 450, Loss: 1.3651, Train: 41.97%, Valid: 41.33%, Test: 41.48%
Epoch: 475, Loss: 1.3526, Train: 42.00%, Valid: 41.21%, Test: 41.51%
Epoch: 500, Loss: 1.3815, Train: 40.73%, Valid: 40.46%, Test: 40.78%
Epoch: 525, Loss: 1.3620, Train: 41.53%, Valid: 41.18%, Test: 41.32%
Epoch: 550, Loss: 1.3561, Train: 42.43%, Valid: 41.63%, Test: 42.05%
Epoch: 575, Loss: 1.3631, Train: 41.43%, Valid: 40.75%, Test: 41.09%
Epoch: 600, Loss: 1.3612, Train: 42.43%, Valid: 41.75%, Test: 42.09%
Epoch: 625, Loss: 1.3623, Train: 41.79%, Valid: 41.31%, Test: 41.60%
Epoch: 650, Loss: 1.3310, Train: 42.13%, Valid: 41.52%, Test: 41.85%
Epoch: 675, Loss: 1.3586, Train: 41.85%, Valid: 41.14%, Test: 41.25%
Epoch: 700, Loss: 1.3689, Train: 41.60%, Valid: 41.00%, Test: 41.58%
Epoch: 725, Loss: 1.3478, Train: 42.46%, Valid: 42.05%, Test: 42.27%
Epoch: 750, Loss: 1.3528, Train: 42.95%, Valid: 42.31%, Test: 42.52%
Epoch: 775, Loss: 1.3337, Train: 43.08%, Valid: 42.57%, Test: 42.78%
Epoch: 800, Loss: 1.3438, Train: 43.36%, Valid: 43.00%, Test: 42.97%
Epoch: 825, Loss: 1.3522, Train: 42.83%, Valid: 42.45%, Test: 42.44%
Epoch: 850, Loss: 1.3253, Train: 43.05%, Valid: 42.66%, Test: 42.81%
Epoch: 875, Loss: 1.3501, Train: 42.28%, Valid: 41.84%, Test: 42.03%
Epoch: 900, Loss: 1.3363, Train: 42.61%, Valid: 42.37%, Test: 42.30%
Epoch: 925, Loss: 1.3359, Train: 43.15%, Valid: 42.75%, Test: 42.97%
Epoch: 950, Loss: 1.3315, Train: 43.48%, Valid: 42.95%, Test: 43.04%
Epoch: 975, Loss: 1.3322, Train: 43.17%, Valid: 42.62%, Test: 42.73%
Run 01:
Highest Train: 43.70
Highest Valid: 43.28
  Final Train: 43.56
   Final Test: 43.32
All runs:
Highest Train: 43.70, nan
Highest Valid: 43.28, nan
  Final Train: 43.56, nan
   Final Test: 43.32, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6119, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4900, Train: 34.74%, Valid: 34.41%, Test: 34.79%
Epoch: 50, Loss: 1.4651, Train: 36.05%, Valid: 35.11%, Test: 35.67%
Epoch: 75, Loss: 1.4488, Train: 36.88%, Valid: 35.86%, Test: 36.21%
Epoch: 100, Loss: 1.4306, Train: 37.91%, Valid: 36.17%, Test: 36.76%
Epoch: 125, Loss: 1.4201, Train: 38.44%, Valid: 36.06%, Test: 36.56%
Epoch: 150, Loss: 1.4087, Train: 38.65%, Valid: 36.03%, Test: 36.55%
Epoch: 175, Loss: 1.4030, Train: 39.53%, Valid: 35.74%, Test: 36.24%
Epoch: 200, Loss: 1.3901, Train: 40.12%, Valid: 35.69%, Test: 36.27%
Epoch: 225, Loss: 1.3862, Train: 40.22%, Valid: 35.68%, Test: 36.41%
Epoch: 250, Loss: 1.3685, Train: 41.03%, Valid: 35.39%, Test: 35.85%
Epoch: 275, Loss: 1.3658, Train: 41.06%, Valid: 34.92%, Test: 35.43%
Epoch: 300, Loss: 1.3852, Train: 41.14%, Valid: 34.97%, Test: 35.72%
Epoch: 325, Loss: 1.3464, Train: 42.04%, Valid: 34.82%, Test: 35.40%
Epoch: 350, Loss: 1.3406, Train: 42.47%, Valid: 34.37%, Test: 35.00%
Epoch: 375, Loss: 1.3357, Train: 42.94%, Valid: 35.07%, Test: 35.45%
Epoch: 400, Loss: 1.3364, Train: 43.32%, Valid: 34.44%, Test: 35.00%
Epoch: 425, Loss: 1.3455, Train: 42.87%, Valid: 33.79%, Test: 34.10%
Epoch: 450, Loss: 1.3053, Train: 44.21%, Valid: 33.60%, Test: 33.94%
Epoch: 475, Loss: 1.3088, Train: 44.32%, Valid: 34.41%, Test: 34.62%
Epoch: 500, Loss: 1.3052, Train: 44.60%, Valid: 33.41%, Test: 33.73%
Epoch: 525, Loss: 1.2936, Train: 44.22%, Valid: 32.91%, Test: 33.25%
Epoch: 550, Loss: 1.2937, Train: 44.81%, Valid: 33.65%, Test: 34.06%
Epoch: 575, Loss: 1.2748, Train: 45.29%, Valid: 34.03%, Test: 34.36%
Epoch: 600, Loss: 1.2704, Train: 46.01%, Valid: 33.93%, Test: 34.41%
Epoch: 625, Loss: 1.2830, Train: 45.24%, Valid: 32.93%, Test: 33.38%
Epoch: 650, Loss: 1.2778, Train: 45.18%, Valid: 33.07%, Test: 33.24%
Epoch: 675, Loss: 1.2597, Train: 46.49%, Valid: 33.35%, Test: 33.77%
Epoch: 700, Loss: 1.2535, Train: 46.84%, Valid: 33.80%, Test: 34.35%
Epoch: 725, Loss: 1.2536, Train: 47.09%, Valid: 33.21%, Test: 33.59%
Epoch: 750, Loss: 1.2420, Train: 47.21%, Valid: 33.16%, Test: 33.68%
Epoch: 775, Loss: 1.2605, Train: 46.74%, Valid: 33.48%, Test: 33.94%
Epoch: 800, Loss: 1.2377, Train: 47.89%, Valid: 33.37%, Test: 34.07%
Epoch: 825, Loss: 1.2297, Train: 47.94%, Valid: 33.20%, Test: 33.76%
Epoch: 850, Loss: 1.2375, Train: 47.28%, Valid: 32.62%, Test: 33.14%
Epoch: 875, Loss: 1.2347, Train: 47.49%, Valid: 32.91%, Test: 33.69%
Epoch: 900, Loss: 1.2300, Train: 47.90%, Valid: 32.94%, Test: 33.47%
Epoch: 925, Loss: 1.2240, Train: 48.73%, Valid: 33.62%, Test: 34.19%
Epoch: 950, Loss: 1.2408, Train: 47.79%, Valid: 34.03%, Test: 34.51%
Epoch: 975, Loss: 1.2053, Train: 49.19%, Valid: 33.26%, Test: 33.82%
Run 01:
Highest Train: 49.62
Highest Valid: 36.26
  Final Train: 38.24
   Final Test: 36.77
All runs:
Highest Train: 49.62, nan
Highest Valid: 36.26, nan
  Final Train: 38.24, nan
   Final Test: 36.77, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6104, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4859, Train: 35.00%, Valid: 34.68%, Test: 34.99%
Epoch: 50, Loss: 1.4408, Train: 37.40%, Valid: 36.77%, Test: 36.92%
Epoch: 75, Loss: 1.4002, Train: 39.07%, Valid: 37.99%, Test: 38.31%
Epoch: 100, Loss: 1.3773, Train: 39.95%, Valid: 38.75%, Test: 39.09%
Epoch: 125, Loss: 1.3624, Train: 40.74%, Valid: 39.78%, Test: 40.06%
Epoch: 150, Loss: 1.3217, Train: 42.90%, Valid: 41.59%, Test: 42.09%
Epoch: 175, Loss: 1.2921, Train: 44.38%, Valid: 42.86%, Test: 43.09%
Epoch: 200, Loss: 1.2695, Train: 44.95%, Valid: 43.18%, Test: 43.25%
Epoch: 225, Loss: 1.2556, Train: 45.74%, Valid: 43.95%, Test: 43.92%
Epoch: 250, Loss: 1.2405, Train: 46.36%, Valid: 44.27%, Test: 44.31%
Epoch: 275, Loss: 1.2357, Train: 46.98%, Valid: 44.50%, Test: 44.76%
Epoch: 300, Loss: 1.2207, Train: 47.46%, Valid: 44.94%, Test: 45.08%
Epoch: 325, Loss: 1.2054, Train: 48.23%, Valid: 45.16%, Test: 45.36%
Epoch: 350, Loss: 1.2028, Train: 48.18%, Valid: 45.19%, Test: 45.44%
Epoch: 375, Loss: 1.2225, Train: 48.70%, Valid: 45.34%, Test: 45.45%
Epoch: 400, Loss: 1.1951, Train: 48.14%, Valid: 45.43%, Test: 45.40%
Epoch: 425, Loss: 1.1802, Train: 49.16%, Valid: 45.64%, Test: 45.86%
Epoch: 450, Loss: 1.1625, Train: 49.55%, Valid: 45.70%, Test: 45.82%
Epoch: 475, Loss: 1.1704, Train: 49.31%, Valid: 45.93%, Test: 46.05%
Epoch: 500, Loss: 1.1611, Train: 50.05%, Valid: 45.90%, Test: 45.93%
Epoch: 525, Loss: 1.2649, Train: 48.29%, Valid: 44.81%, Test: 45.10%
Epoch: 550, Loss: 1.1797, Train: 49.28%, Valid: 45.86%, Test: 46.10%
Epoch: 575, Loss: 1.1533, Train: 50.91%, Valid: 46.25%, Test: 46.49%
Epoch: 600, Loss: 1.1319, Train: 51.02%, Valid: 45.84%, Test: 46.03%
Epoch: 625, Loss: 1.1263, Train: 51.32%, Valid: 45.90%, Test: 46.01%
Epoch: 650, Loss: 1.1194, Train: 52.08%, Valid: 46.37%, Test: 46.51%
Epoch: 675, Loss: 1.1781, Train: 49.27%, Valid: 46.03%, Test: 46.15%
Epoch: 700, Loss: 1.1325, Train: 51.16%, Valid: 46.25%, Test: 46.28%
Epoch: 725, Loss: 1.1104, Train: 51.95%, Valid: 46.31%, Test: 46.46%
Epoch: 750, Loss: 1.1142, Train: 52.40%, Valid: 46.13%, Test: 46.18%
Epoch: 775, Loss: 1.0920, Train: 52.85%, Valid: 46.10%, Test: 46.40%
Epoch: 800, Loss: 1.0872, Train: 53.32%, Valid: 46.70%, Test: 46.70%
Epoch: 825, Loss: 1.0914, Train: 52.95%, Valid: 46.54%, Test: 46.67%
Epoch: 850, Loss: 1.2091, Train: 50.11%, Valid: 45.34%, Test: 45.49%
Epoch: 875, Loss: 1.1051, Train: 52.37%, Valid: 46.35%, Test: 46.72%
Epoch: 900, Loss: 1.0943, Train: 53.47%, Valid: 46.18%, Test: 46.59%
Epoch: 925, Loss: 1.0686, Train: 54.05%, Valid: 46.62%, Test: 46.91%
Epoch: 950, Loss: 1.0714, Train: 53.86%, Valid: 46.27%, Test: 46.68%
Epoch: 975, Loss: 1.0727, Train: 53.79%, Valid: 46.43%, Test: 46.89%
Run 01:
Highest Train: 54.58
Highest Valid: 46.93
  Final Train: 52.76
   Final Test: 47.16
All runs:
Highest Train: 54.58, nan
Highest Valid: 46.93, nan
  Final Train: 52.76, nan
   Final Test: 47.16, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.7646, Train: 20.53%, Valid: 20.41%, Test: 20.17%
Epoch: 25, Loss: 1.4849, Train: 33.94%, Valid: 33.70%, Test: 34.38%
Epoch: 50, Loss: 1.4478, Train: 35.88%, Valid: 35.44%, Test: 36.01%
Epoch: 75, Loss: 1.4204, Train: 38.10%, Valid: 37.64%, Test: 38.34%
Epoch: 100, Loss: 1.4024, Train: 38.07%, Valid: 37.70%, Test: 38.10%
Epoch: 125, Loss: 1.3678, Train: 40.06%, Valid: 39.57%, Test: 39.97%
Epoch: 150, Loss: 1.3511, Train: 40.69%, Valid: 40.15%, Test: 40.29%
Epoch: 175, Loss: 1.4232, Train: 36.33%, Valid: 36.05%, Test: 36.46%
Epoch: 200, Loss: 1.3433, Train: 40.96%, Valid: 40.60%, Test: 40.71%
Epoch: 225, Loss: 1.3263, Train: 41.23%, Valid: 40.63%, Test: 40.99%
Epoch: 250, Loss: 1.3258, Train: 41.58%, Valid: 40.96%, Test: 40.90%
Epoch: 275, Loss: 1.3460, Train: 40.49%, Valid: 39.91%, Test: 40.25%
Epoch: 300, Loss: 1.3179, Train: 42.00%, Valid: 41.30%, Test: 41.56%
Epoch: 325, Loss: 1.3067, Train: 42.45%, Valid: 41.56%, Test: 41.73%
Epoch: 350, Loss: 1.3032, Train: 42.80%, Valid: 41.86%, Test: 42.10%
Epoch: 375, Loss: 1.3480, Train: 41.94%, Valid: 41.25%, Test: 41.46%
Epoch: 400, Loss: 1.3063, Train: 42.74%, Valid: 41.68%, Test: 42.05%
Epoch: 425, Loss: 1.2937, Train: 43.12%, Valid: 42.27%, Test: 42.28%
Epoch: 450, Loss: 1.2914, Train: 43.48%, Valid: 42.39%, Test: 42.69%
Epoch: 475, Loss: 1.2869, Train: 43.13%, Valid: 42.28%, Test: 42.26%
Epoch: 500, Loss: 1.3022, Train: 42.17%, Valid: 41.48%, Test: 41.64%
Epoch: 525, Loss: 1.2848, Train: 43.02%, Valid: 42.05%, Test: 42.20%
Epoch: 550, Loss: 1.2754, Train: 43.91%, Valid: 42.69%, Test: 42.80%
Epoch: 575, Loss: 1.2835, Train: 43.87%, Valid: 42.62%, Test: 42.93%
Epoch: 600, Loss: 1.2625, Train: 44.64%, Valid: 43.44%, Test: 43.65%
Epoch: 625, Loss: 1.2774, Train: 44.27%, Valid: 43.11%, Test: 43.30%
Epoch: 650, Loss: 1.2572, Train: 45.13%, Valid: 43.80%, Test: 43.85%
Epoch: 675, Loss: 1.2637, Train: 44.43%, Valid: 43.04%, Test: 43.40%
Epoch: 700, Loss: 1.2596, Train: 45.28%, Valid: 43.82%, Test: 43.92%
Epoch: 725, Loss: 1.2487, Train: 45.61%, Valid: 43.95%, Test: 44.10%
Epoch: 750, Loss: 1.2478, Train: 45.74%, Valid: 44.03%, Test: 44.25%
Epoch: 775, Loss: 1.3000, Train: 43.61%, Valid: 41.95%, Test: 42.88%
Epoch: 800, Loss: 1.2584, Train: 45.04%, Valid: 43.69%, Test: 43.95%
Epoch: 825, Loss: 1.2790, Train: 45.31%, Valid: 43.52%, Test: 43.72%
Epoch: 850, Loss: 1.2465, Train: 45.56%, Valid: 43.98%, Test: 44.14%
Epoch: 875, Loss: 1.2879, Train: 45.74%, Valid: 43.84%, Test: 44.12%
Epoch: 900, Loss: 1.2544, Train: 45.62%, Valid: 43.96%, Test: 44.09%
Epoch: 925, Loss: 1.2411, Train: 45.61%, Valid: 43.85%, Test: 44.16%
Epoch: 950, Loss: 1.2328, Train: 46.10%, Valid: 44.16%, Test: 44.51%
Epoch: 975, Loss: 1.2412, Train: 46.26%, Valid: 44.31%, Test: 44.64%
Run 01:
Highest Train: 46.76
Highest Valid: 44.61
  Final Train: 46.76
   Final Test: 44.91
All runs:
Highest Train: 46.76, nan
Highest Valid: 44.61, nan
  Final Train: 46.76, nan
   Final Test: 44.91, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6105, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4913, Train: 34.48%, Valid: 34.23%, Test: 34.49%
Epoch: 50, Loss: 1.4674, Train: 36.05%, Valid: 35.31%, Test: 35.48%
Epoch: 75, Loss: 1.4504, Train: 36.93%, Valid: 35.94%, Test: 36.33%
Epoch: 100, Loss: 1.4415, Train: 37.57%, Valid: 36.17%, Test: 36.44%
Epoch: 125, Loss: 1.4287, Train: 37.66%, Valid: 36.08%, Test: 36.61%
Epoch: 150, Loss: 1.4173, Train: 38.17%, Valid: 35.83%, Test: 36.28%
Epoch: 175, Loss: 1.4031, Train: 39.02%, Valid: 36.05%, Test: 36.54%
Epoch: 200, Loss: 1.3965, Train: 39.02%, Valid: 36.09%, Test: 36.48%
Epoch: 225, Loss: 1.3890, Train: 40.06%, Valid: 35.48%, Test: 35.91%
Epoch: 250, Loss: 1.3743, Train: 40.68%, Valid: 34.82%, Test: 35.47%
Epoch: 275, Loss: 1.3640, Train: 41.08%, Valid: 35.11%, Test: 35.78%
Epoch: 300, Loss: 1.3550, Train: 41.66%, Valid: 34.20%, Test: 34.74%
Epoch: 325, Loss: 1.3506, Train: 42.53%, Valid: 34.56%, Test: 35.21%
Epoch: 350, Loss: 1.3464, Train: 42.07%, Valid: 34.80%, Test: 35.35%
Epoch: 375, Loss: 1.3300, Train: 43.23%, Valid: 34.25%, Test: 34.80%
Epoch: 400, Loss: 1.3288, Train: 43.43%, Valid: 34.15%, Test: 35.06%
Epoch: 425, Loss: 1.3212, Train: 43.88%, Valid: 34.03%, Test: 34.62%
Epoch: 450, Loss: 1.3213, Train: 43.80%, Valid: 34.30%, Test: 34.96%
Epoch: 475, Loss: 1.3176, Train: 43.72%, Valid: 33.96%, Test: 34.20%
Epoch: 500, Loss: 1.3061, Train: 44.48%, Valid: 34.38%, Test: 34.97%
Epoch: 525, Loss: 1.3022, Train: 44.85%, Valid: 33.48%, Test: 33.98%
Epoch: 550, Loss: 1.2980, Train: 45.13%, Valid: 33.81%, Test: 34.44%
Epoch: 575, Loss: 1.3017, Train: 44.96%, Valid: 33.68%, Test: 34.28%
Epoch: 600, Loss: 1.2972, Train: 44.70%, Valid: 33.36%, Test: 33.99%
Epoch: 625, Loss: 1.2770, Train: 46.01%, Valid: 33.58%, Test: 34.27%
Epoch: 650, Loss: 1.2832, Train: 45.66%, Valid: 33.98%, Test: 34.45%
Epoch: 675, Loss: 1.2848, Train: 45.28%, Valid: 33.67%, Test: 34.00%
Epoch: 700, Loss: 1.2959, Train: 43.01%, Valid: 31.79%, Test: 32.24%
Epoch: 725, Loss: 1.2688, Train: 46.50%, Valid: 33.70%, Test: 34.23%
Epoch: 750, Loss: 1.2673, Train: 46.69%, Valid: 33.32%, Test: 33.74%
Epoch: 775, Loss: 1.2569, Train: 46.40%, Valid: 33.55%, Test: 33.92%
Epoch: 800, Loss: 1.2584, Train: 46.79%, Valid: 34.02%, Test: 34.32%
Epoch: 825, Loss: 1.2599, Train: 47.15%, Valid: 34.11%, Test: 34.44%
Epoch: 850, Loss: 1.2619, Train: 46.70%, Valid: 32.62%, Test: 33.28%
Epoch: 875, Loss: 1.2567, Train: 47.56%, Valid: 33.17%, Test: 33.60%
Epoch: 900, Loss: 1.2430, Train: 47.65%, Valid: 33.04%, Test: 33.43%
Epoch: 925, Loss: 1.2358, Train: 46.69%, Valid: 33.64%, Test: 33.90%
Epoch: 950, Loss: 1.2433, Train: 48.26%, Valid: 33.71%, Test: 34.00%
Epoch: 975, Loss: 1.2240, Train: 48.53%, Valid: 33.68%, Test: 34.07%
Run 01:
Highest Train: 48.79
Highest Valid: 36.30
  Final Train: 38.83
   Final Test: 36.66
All runs:
Highest Train: 48.79, nan
Highest Valid: 36.30, nan
  Final Train: 38.83, nan
   Final Test: 36.66, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6092, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4887, Train: 35.06%, Valid: 34.69%, Test: 35.05%
Epoch: 50, Loss: 1.4496, Train: 36.86%, Valid: 36.36%, Test: 36.61%
Epoch: 75, Loss: 1.4022, Train: 39.40%, Valid: 38.51%, Test: 38.68%
Epoch: 100, Loss: 1.3577, Train: 41.44%, Valid: 40.18%, Test: 40.37%
Epoch: 125, Loss: 1.3245, Train: 42.65%, Valid: 41.28%, Test: 41.42%
Epoch: 150, Loss: 1.3012, Train: 43.53%, Valid: 41.88%, Test: 41.97%
Epoch: 175, Loss: 1.5148, Train: 39.22%, Valid: 37.21%, Test: 37.50%
Epoch: 200, Loss: 1.2834, Train: 44.13%, Valid: 42.73%, Test: 42.87%
Epoch: 225, Loss: 1.2545, Train: 45.33%, Valid: 43.54%, Test: 43.91%
Epoch: 250, Loss: 1.2371, Train: 46.07%, Valid: 43.95%, Test: 44.27%
Epoch: 275, Loss: 1.2342, Train: 46.24%, Valid: 44.06%, Test: 44.39%
Epoch: 300, Loss: 1.2183, Train: 47.20%, Valid: 44.67%, Test: 44.89%
Epoch: 325, Loss: 1.2163, Train: 47.09%, Valid: 44.58%, Test: 44.76%
Epoch: 350, Loss: 1.2150, Train: 47.23%, Valid: 44.93%, Test: 45.03%
Epoch: 375, Loss: 1.1958, Train: 48.22%, Valid: 45.39%, Test: 45.73%
Epoch: 400, Loss: 1.2142, Train: 47.42%, Valid: 45.01%, Test: 45.25%
Epoch: 425, Loss: 1.1922, Train: 48.47%, Valid: 45.96%, Test: 45.97%
Epoch: 450, Loss: 1.1785, Train: 48.57%, Valid: 45.66%, Test: 45.89%
Epoch: 475, Loss: 1.1944, Train: 48.72%, Valid: 45.78%, Test: 45.84%
Epoch: 500, Loss: 1.1620, Train: 49.51%, Valid: 46.27%, Test: 46.23%
Epoch: 525, Loss: 1.1618, Train: 49.62%, Valid: 46.08%, Test: 46.07%
Epoch: 550, Loss: 1.1926, Train: 48.55%, Valid: 45.18%, Test: 45.07%
Epoch: 575, Loss: 1.1530, Train: 50.09%, Valid: 46.41%, Test: 46.42%
Epoch: 600, Loss: 1.1488, Train: 50.42%, Valid: 46.27%, Test: 46.23%
Epoch: 625, Loss: 1.6068, Train: 34.71%, Valid: 34.20%, Test: 34.70%
Epoch: 650, Loss: 1.3489, Train: 41.14%, Valid: 40.96%, Test: 40.99%
Epoch: 675, Loss: 1.2964, Train: 43.52%, Valid: 43.02%, Test: 43.00%
Epoch: 700, Loss: 1.2768, Train: 44.03%, Valid: 43.54%, Test: 43.54%
Epoch: 725, Loss: 1.2641, Train: 44.09%, Valid: 43.60%, Test: 43.73%
Epoch: 750, Loss: 1.2548, Train: 45.39%, Valid: 44.57%, Test: 44.77%
Epoch: 775, Loss: 1.2481, Train: 45.26%, Valid: 44.59%, Test: 44.62%
Epoch: 800, Loss: 1.2394, Train: 46.12%, Valid: 45.10%, Test: 45.17%
Epoch: 825, Loss: 1.2331, Train: 46.63%, Valid: 45.41%, Test: 45.40%
Epoch: 850, Loss: 1.2450, Train: 46.35%, Valid: 45.27%, Test: 45.40%
Epoch: 875, Loss: 1.2239, Train: 46.97%, Valid: 45.71%, Test: 45.78%
Epoch: 900, Loss: 1.2375, Train: 46.24%, Valid: 45.07%, Test: 45.10%
Epoch: 925, Loss: 1.2194, Train: 46.19%, Valid: 44.97%, Test: 45.22%
Epoch: 950, Loss: 1.2106, Train: 47.42%, Valid: 46.01%, Test: 46.05%
Epoch: 975, Loss: 1.2221, Train: 46.90%, Valid: 45.65%, Test: 45.78%
Run 01:
Highest Train: 51.01
Highest Valid: 46.91
  Final Train: 50.89
   Final Test: 46.83
All runs:
Highest Train: 51.01, nan
Highest Valid: 46.91, nan
  Final Train: 50.89, nan
   Final Test: 46.83, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6951, Train: 28.93%, Valid: 28.73%, Test: 28.76%
Epoch: 25, Loss: 1.4467, Train: 37.23%, Valid: 37.01%, Test: 37.43%
Epoch: 50, Loss: 1.4225, Train: 37.94%, Valid: 37.69%, Test: 38.45%
Epoch: 75, Loss: 1.3896, Train: 40.17%, Valid: 39.87%, Test: 40.30%
Epoch: 100, Loss: 1.4563, Train: 37.86%, Valid: 37.51%, Test: 37.89%
Epoch: 125, Loss: 1.3639, Train: 41.35%, Valid: 40.87%, Test: 41.12%
Epoch: 150, Loss: 1.3453, Train: 41.69%, Valid: 41.48%, Test: 41.92%
Epoch: 175, Loss: 1.3239, Train: 42.44%, Valid: 42.25%, Test: 42.50%
Epoch: 200, Loss: 1.3164, Train: 42.80%, Valid: 42.66%, Test: 42.79%
Epoch: 225, Loss: 1.2994, Train: 43.42%, Valid: 43.19%, Test: 43.55%
Epoch: 250, Loss: 1.3337, Train: 42.86%, Valid: 42.40%, Test: 42.61%
Epoch: 275, Loss: 1.3094, Train: 43.13%, Valid: 42.72%, Test: 42.96%
Epoch: 300, Loss: 1.3003, Train: 43.30%, Valid: 42.80%, Test: 43.11%
Epoch: 325, Loss: 1.2868, Train: 43.37%, Valid: 42.87%, Test: 43.31%
Epoch: 350, Loss: 1.2938, Train: 43.90%, Valid: 43.53%, Test: 43.86%
Epoch: 375, Loss: 1.2904, Train: 44.26%, Valid: 43.69%, Test: 43.93%
Epoch: 400, Loss: 1.2765, Train: 44.59%, Valid: 44.03%, Test: 44.19%
Epoch: 425, Loss: 1.2719, Train: 44.67%, Valid: 44.04%, Test: 44.46%
Epoch: 450, Loss: 1.2662, Train: 44.98%, Valid: 44.40%, Test: 44.66%
Epoch: 475, Loss: 1.3667, Train: 40.84%, Valid: 40.31%, Test: 40.85%
Epoch: 500, Loss: 1.2962, Train: 43.79%, Valid: 43.36%, Test: 43.72%
Epoch: 525, Loss: 1.2913, Train: 43.98%, Valid: 43.54%, Test: 43.89%
Epoch: 550, Loss: 1.2726, Train: 44.67%, Valid: 44.19%, Test: 44.42%
Epoch: 575, Loss: 1.2894, Train: 45.09%, Valid: 44.28%, Test: 44.46%
Epoch: 600, Loss: 1.2725, Train: 44.91%, Valid: 44.27%, Test: 44.66%
Epoch: 625, Loss: 1.2697, Train: 45.03%, Valid: 44.28%, Test: 44.62%
Epoch: 650, Loss: 1.2574, Train: 45.48%, Valid: 44.91%, Test: 45.01%
Epoch: 675, Loss: 1.3736, Train: 40.96%, Valid: 40.51%, Test: 40.62%
Epoch: 700, Loss: 1.3076, Train: 43.22%, Valid: 42.86%, Test: 43.18%
Epoch: 725, Loss: 1.2893, Train: 43.64%, Valid: 43.13%, Test: 43.26%
Epoch: 750, Loss: 1.2801, Train: 44.14%, Valid: 43.61%, Test: 43.82%
Epoch: 775, Loss: 1.2717, Train: 44.66%, Valid: 44.07%, Test: 44.28%
Epoch: 800, Loss: 1.2725, Train: 45.02%, Valid: 44.48%, Test: 44.71%
Epoch: 825, Loss: 1.2752, Train: 44.44%, Valid: 43.80%, Test: 44.29%
Epoch: 850, Loss: 1.2622, Train: 45.13%, Valid: 44.58%, Test: 44.81%
Epoch: 875, Loss: 1.2935, Train: 44.70%, Valid: 44.15%, Test: 44.38%
Epoch: 900, Loss: 1.2621, Train: 45.48%, Valid: 44.83%, Test: 44.99%
Epoch: 925, Loss: 1.2612, Train: 44.26%, Valid: 43.78%, Test: 43.89%
Epoch: 950, Loss: 1.2521, Train: 45.79%, Valid: 45.08%, Test: 45.30%
Epoch: 975, Loss: 1.2753, Train: 44.98%, Valid: 44.38%, Test: 44.68%
Run 01:
Highest Train: 46.02
Highest Valid: 45.41
  Final Train: 46.02
   Final Test: 45.51
All runs:
Highest Train: 46.02, nan
Highest Valid: 45.41, nan
  Final Train: 46.02, nan
   Final Test: 45.51, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6100, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4980, Train: 34.77%, Valid: 34.55%, Test: 34.82%
Epoch: 50, Loss: 1.4785, Train: 36.29%, Valid: 35.49%, Test: 35.88%
Epoch: 75, Loss: 1.4666, Train: 37.03%, Valid: 35.93%, Test: 36.35%
Epoch: 100, Loss: 1.4588, Train: 37.59%, Valid: 36.08%, Test: 36.69%
Epoch: 125, Loss: 1.4534, Train: 38.15%, Valid: 36.25%, Test: 36.64%
Epoch: 150, Loss: 1.4472, Train: 38.46%, Valid: 36.30%, Test: 36.63%
Epoch: 175, Loss: 1.4437, Train: 38.75%, Valid: 36.36%, Test: 36.75%
Epoch: 200, Loss: 1.4411, Train: 38.76%, Valid: 36.30%, Test: 36.81%
Epoch: 225, Loss: 1.4380, Train: 39.04%, Valid: 36.34%, Test: 36.73%
Epoch: 250, Loss: 1.4362, Train: 39.23%, Valid: 36.33%, Test: 36.73%
Epoch: 275, Loss: 1.4339, Train: 39.19%, Valid: 36.37%, Test: 36.74%
Epoch: 300, Loss: 1.4311, Train: 39.46%, Valid: 36.32%, Test: 36.65%
Epoch: 325, Loss: 1.4282, Train: 39.64%, Valid: 36.52%, Test: 36.87%
Epoch: 350, Loss: 1.4245, Train: 39.77%, Valid: 36.37%, Test: 36.82%
Epoch: 375, Loss: 1.4247, Train: 39.80%, Valid: 36.71%, Test: 36.98%
Epoch: 400, Loss: 1.4170, Train: 40.27%, Valid: 37.00%, Test: 37.34%
Epoch: 425, Loss: 1.4198, Train: 40.19%, Valid: 37.13%, Test: 37.38%
Epoch: 450, Loss: 1.4164, Train: 40.53%, Valid: 37.53%, Test: 37.66%
Epoch: 475, Loss: 1.4057, Train: 40.24%, Valid: 37.48%, Test: 37.63%
Epoch: 500, Loss: 1.4044, Train: 41.93%, Valid: 38.60%, Test: 38.78%
Epoch: 525, Loss: 1.3972, Train: 41.77%, Valid: 38.51%, Test: 38.87%
Epoch: 550, Loss: 1.3943, Train: 41.86%, Valid: 38.65%, Test: 39.01%
Epoch: 575, Loss: 1.3856, Train: 42.87%, Valid: 39.49%, Test: 39.65%
Epoch: 600, Loss: 1.3930, Train: 42.96%, Valid: 39.50%, Test: 39.70%
Epoch: 625, Loss: 1.3817, Train: 42.92%, Valid: 39.59%, Test: 39.77%
Epoch: 650, Loss: 1.3853, Train: 43.28%, Valid: 39.87%, Test: 39.99%
Epoch: 675, Loss: 1.3838, Train: 43.36%, Valid: 39.95%, Test: 39.97%
Epoch: 700, Loss: 1.3751, Train: 43.64%, Valid: 40.24%, Test: 40.30%
Epoch: 725, Loss: 1.3849, Train: 42.82%, Valid: 39.54%, Test: 39.87%
Epoch: 750, Loss: 1.3856, Train: 43.53%, Valid: 40.12%, Test: 40.15%
Epoch: 775, Loss: 1.3754, Train: 43.36%, Valid: 40.08%, Test: 40.32%
Epoch: 800, Loss: 1.3705, Train: 43.99%, Valid: 40.65%, Test: 40.72%
Epoch: 825, Loss: 1.3809, Train: 43.31%, Valid: 40.33%, Test: 40.45%
Epoch: 850, Loss: 1.3735, Train: 43.16%, Valid: 40.13%, Test: 40.49%
Epoch: 875, Loss: 1.3782, Train: 43.46%, Valid: 40.51%, Test: 40.70%
Epoch: 900, Loss: 1.3665, Train: 42.93%, Valid: 39.62%, Test: 39.91%
Epoch: 925, Loss: 1.3623, Train: 44.42%, Valid: 41.47%, Test: 41.84%
Epoch: 950, Loss: 1.3614, Train: 44.46%, Valid: 41.67%, Test: 42.02%
Epoch: 975, Loss: 1.3571, Train: 44.79%, Valid: 42.04%, Test: 42.45%
Run 01:
Highest Train: 45.18
Highest Valid: 42.48
  Final Train: 44.91
   Final Test: 42.61
All runs:
Highest Train: 45.18, nan
Highest Valid: 42.48, nan
  Final Train: 44.91, nan
   Final Test: 42.61, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6247, Train: 28.71%, Valid: 28.53%, Test: 28.82%
Epoch: 25, Loss: 1.5060, Train: 34.46%, Valid: 34.23%, Test: 34.54%
Epoch: 50, Loss: 1.4813, Train: 35.86%, Valid: 35.48%, Test: 35.73%
Epoch: 75, Loss: 1.4458, Train: 37.66%, Valid: 36.67%, Test: 36.85%
Epoch: 100, Loss: 1.4164, Train: 39.13%, Valid: 37.90%, Test: 38.15%
Epoch: 125, Loss: 1.4090, Train: 39.46%, Valid: 38.20%, Test: 38.58%
Epoch: 150, Loss: 1.3839, Train: 40.88%, Valid: 39.49%, Test: 39.73%
Epoch: 175, Loss: 1.3893, Train: 40.19%, Valid: 38.97%, Test: 39.19%
Epoch: 200, Loss: 1.3618, Train: 41.15%, Valid: 39.96%, Test: 40.00%
Epoch: 225, Loss: 1.3502, Train: 41.36%, Valid: 40.48%, Test: 40.18%
Epoch: 250, Loss: 1.3542, Train: 42.57%, Valid: 41.50%, Test: 41.45%
Epoch: 275, Loss: 1.3366, Train: 43.72%, Valid: 42.77%, Test: 42.67%
Epoch: 300, Loss: 1.3512, Train: 36.76%, Valid: 36.27%, Test: 36.73%
Epoch: 325, Loss: 1.3370, Train: 42.88%, Valid: 42.44%, Test: 42.45%
Epoch: 350, Loss: 1.3615, Train: 39.18%, Valid: 38.41%, Test: 38.45%
Epoch: 375, Loss: 1.3253, Train: 43.98%, Valid: 43.02%, Test: 43.27%
Epoch: 400, Loss: 1.3505, Train: 42.31%, Valid: 41.65%, Test: 41.59%
Epoch: 425, Loss: 1.3390, Train: 43.24%, Valid: 42.52%, Test: 42.50%
Epoch: 450, Loss: 1.3294, Train: 43.70%, Valid: 42.96%, Test: 43.28%
Epoch: 475, Loss: 1.3435, Train: 42.19%, Valid: 41.28%, Test: 41.58%
Epoch: 500, Loss: 1.3484, Train: 41.81%, Valid: 41.11%, Test: 41.10%
Epoch: 525, Loss: 1.3315, Train: 42.74%, Valid: 41.97%, Test: 42.14%
Epoch: 550, Loss: 1.3193, Train: 43.25%, Valid: 42.64%, Test: 42.66%
Epoch: 575, Loss: 1.3342, Train: 43.14%, Valid: 42.04%, Test: 42.58%
Epoch: 600, Loss: 1.3400, Train: 42.40%, Valid: 41.41%, Test: 41.93%
Epoch: 625, Loss: 1.3220, Train: 41.92%, Valid: 41.08%, Test: 41.58%
Epoch: 650, Loss: 1.3317, Train: 43.50%, Valid: 42.50%, Test: 42.80%
Epoch: 675, Loss: 1.3362, Train: 42.58%, Valid: 41.96%, Test: 42.23%
Epoch: 700, Loss: 1.3558, Train: 43.20%, Valid: 42.44%, Test: 42.74%
Epoch: 725, Loss: 1.3227, Train: 42.79%, Valid: 42.05%, Test: 42.20%
Epoch: 750, Loss: 1.3194, Train: 40.04%, Valid: 39.52%, Test: 39.79%
Epoch: 775, Loss: 1.3318, Train: 43.00%, Valid: 42.43%, Test: 42.77%
Epoch: 800, Loss: 1.3692, Train: 43.53%, Valid: 43.00%, Test: 43.08%
Epoch: 825, Loss: 1.3085, Train: 43.59%, Valid: 43.05%, Test: 43.01%
Epoch: 850, Loss: 1.3214, Train: 41.37%, Valid: 40.97%, Test: 41.46%
Epoch: 875, Loss: 1.3578, Train: 41.05%, Valid: 40.43%, Test: 40.86%
Epoch: 900, Loss: 1.3469, Train: 43.93%, Valid: 43.07%, Test: 43.40%
Epoch: 925, Loss: 1.3098, Train: 40.47%, Valid: 39.92%, Test: 40.39%
Epoch: 950, Loss: 1.3352, Train: 42.55%, Valid: 41.88%, Test: 41.99%
Epoch: 975, Loss: 1.3070, Train: 39.42%, Valid: 38.89%, Test: 39.09%
Run 01:
Highest Train: 44.64
Highest Valid: 43.98
  Final Train: 44.64
   Final Test: 44.16
All runs:
Highest Train: 44.64, nan
Highest Valid: 43.98, nan
  Final Train: 44.64, nan
   Final Test: 44.16, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6130, Train: 27.31%, Valid: 27.07%, Test: 26.91%
Epoch: 25, Loss: 1.4775, Train: 34.49%, Valid: 34.22%, Test: 34.43%
Epoch: 50, Loss: 1.4518, Train: 35.85%, Valid: 35.45%, Test: 35.69%
Epoch: 75, Loss: 1.4216, Train: 29.13%, Valid: 28.92%, Test: 28.91%
Epoch: 100, Loss: 1.3954, Train: 38.69%, Valid: 38.19%, Test: 38.45%
Epoch: 125, Loss: 1.3782, Train: 38.41%, Valid: 37.93%, Test: 38.30%
Epoch: 150, Loss: 1.3688, Train: 39.93%, Valid: 39.62%, Test: 39.80%
Epoch: 175, Loss: 1.3740, Train: 39.77%, Valid: 39.38%, Test: 39.62%
Epoch: 200, Loss: 1.3753, Train: 40.15%, Valid: 39.74%, Test: 39.78%
Epoch: 225, Loss: 1.3674, Train: 39.56%, Valid: 39.16%, Test: 39.41%
Epoch: 250, Loss: 1.3648, Train: 40.55%, Valid: 40.06%, Test: 40.24%
Epoch: 275, Loss: 1.3613, Train: 41.45%, Valid: 40.95%, Test: 41.17%
Epoch: 300, Loss: 1.3462, Train: 41.33%, Valid: 40.85%, Test: 41.19%
Epoch: 325, Loss: 1.3490, Train: 41.90%, Valid: 41.35%, Test: 41.64%
Epoch: 350, Loss: 1.3489, Train: 41.41%, Valid: 41.04%, Test: 41.24%
Epoch: 375, Loss: 1.3425, Train: 41.92%, Valid: 41.17%, Test: 41.62%
Epoch: 400, Loss: 1.3422, Train: 42.20%, Valid: 41.74%, Test: 41.72%
Epoch: 425, Loss: 1.3847, Train: 38.29%, Valid: 37.80%, Test: 38.01%
Epoch: 450, Loss: 1.3479, Train: 42.23%, Valid: 41.57%, Test: 41.90%
Epoch: 475, Loss: 1.3360, Train: 42.45%, Valid: 41.88%, Test: 42.11%
Epoch: 500, Loss: 1.3425, Train: 41.76%, Valid: 41.20%, Test: 41.73%
Epoch: 525, Loss: 1.3489, Train: 42.17%, Valid: 41.68%, Test: 41.89%
Epoch: 550, Loss: 1.3431, Train: 42.10%, Valid: 41.48%, Test: 41.70%
Epoch: 575, Loss: 1.3379, Train: 42.25%, Valid: 41.73%, Test: 41.77%
Epoch: 600, Loss: 1.3431, Train: 41.32%, Valid: 40.87%, Test: 40.93%
Epoch: 625, Loss: 1.3507, Train: 43.05%, Valid: 42.31%, Test: 42.55%
Epoch: 650, Loss: 1.3455, Train: 42.98%, Valid: 42.19%, Test: 42.44%
Epoch: 675, Loss: 1.3426, Train: 42.10%, Valid: 41.67%, Test: 41.57%
Epoch: 700, Loss: 1.3419, Train: 41.46%, Valid: 40.85%, Test: 41.00%
Epoch: 725, Loss: 1.3343, Train: 42.92%, Valid: 42.42%, Test: 42.52%
Epoch: 750, Loss: 1.3533, Train: 42.52%, Valid: 41.77%, Test: 42.01%
Epoch: 775, Loss: 1.3321, Train: 42.45%, Valid: 41.81%, Test: 42.22%
Epoch: 800, Loss: 1.3834, Train: 41.19%, Valid: 40.60%, Test: 40.89%
Epoch: 825, Loss: 1.3386, Train: 42.48%, Valid: 41.75%, Test: 41.98%
Epoch: 850, Loss: 1.3464, Train: 42.05%, Valid: 41.40%, Test: 41.50%
Epoch: 875, Loss: 1.3322, Train: 43.31%, Valid: 42.65%, Test: 42.74%
Epoch: 900, Loss: 1.3484, Train: 41.13%, Valid: 40.51%, Test: 40.77%
Epoch: 925, Loss: 1.3340, Train: 41.91%, Valid: 41.41%, Test: 41.63%
Epoch: 950, Loss: 1.3339, Train: 42.69%, Valid: 42.21%, Test: 42.37%
Epoch: 975, Loss: 1.3292, Train: 42.36%, Valid: 41.83%, Test: 41.98%
Run 01:
Highest Train: 43.53
Highest Valid: 43.07
  Final Train: 43.50
   Final Test: 43.18
All runs:
Highest Train: 43.53, nan
Highest Valid: 43.07, nan
  Final Train: 43.50, nan
   Final Test: 43.18, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6039, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4972, Train: 34.83%, Valid: 34.57%, Test: 34.69%
Epoch: 50, Loss: 1.4749, Train: 36.39%, Valid: 35.74%, Test: 36.03%
Epoch: 75, Loss: 1.4617, Train: 37.23%, Valid: 36.15%, Test: 36.56%
Epoch: 100, Loss: 1.4532, Train: 37.76%, Valid: 36.35%, Test: 36.76%
Epoch: 125, Loss: 1.4491, Train: 38.25%, Valid: 36.27%, Test: 36.67%
Epoch: 150, Loss: 1.4450, Train: 38.66%, Valid: 36.32%, Test: 36.94%
Epoch: 175, Loss: 1.4396, Train: 38.94%, Valid: 36.39%, Test: 36.93%
Epoch: 200, Loss: 1.4375, Train: 39.07%, Valid: 36.35%, Test: 36.81%
Epoch: 225, Loss: 1.4333, Train: 39.32%, Valid: 36.43%, Test: 36.87%
Epoch: 250, Loss: 1.4290, Train: 39.47%, Valid: 36.53%, Test: 36.87%
Epoch: 275, Loss: 1.4280, Train: 39.78%, Valid: 36.51%, Test: 36.88%
Epoch: 300, Loss: 1.4249, Train: 39.87%, Valid: 36.64%, Test: 36.89%
Epoch: 325, Loss: 1.4195, Train: 40.01%, Valid: 36.79%, Test: 37.08%
Epoch: 350, Loss: 1.4147, Train: 40.06%, Valid: 36.80%, Test: 37.01%
Epoch: 375, Loss: 1.4111, Train: 40.60%, Valid: 37.26%, Test: 37.56%
Epoch: 400, Loss: 1.4106, Train: 40.01%, Valid: 37.10%, Test: 37.26%
Epoch: 425, Loss: 1.4020, Train: 40.99%, Valid: 37.61%, Test: 37.66%
Epoch: 450, Loss: 1.3966, Train: 41.93%, Valid: 38.43%, Test: 38.74%
Epoch: 475, Loss: 1.4004, Train: 41.27%, Valid: 38.18%, Test: 38.27%
Epoch: 500, Loss: 1.3929, Train: 42.03%, Valid: 38.67%, Test: 38.87%
Epoch: 525, Loss: 1.3890, Train: 42.87%, Valid: 39.41%, Test: 39.54%
Epoch: 550, Loss: 1.3866, Train: 41.85%, Valid: 38.87%, Test: 39.07%
Epoch: 575, Loss: 1.3854, Train: 42.49%, Valid: 39.19%, Test: 39.21%
Epoch: 600, Loss: 1.3750, Train: 42.94%, Valid: 39.63%, Test: 39.59%
Epoch: 625, Loss: 1.3786, Train: 42.34%, Valid: 39.17%, Test: 39.29%
Epoch: 650, Loss: 1.3764, Train: 43.19%, Valid: 39.90%, Test: 39.91%
Epoch: 675, Loss: 1.3673, Train: 43.86%, Valid: 40.42%, Test: 40.43%
Epoch: 700, Loss: 1.3747, Train: 42.94%, Valid: 39.82%, Test: 39.85%
Epoch: 725, Loss: 1.3661, Train: 43.18%, Valid: 39.98%, Test: 40.00%
Epoch: 750, Loss: 1.3696, Train: 43.27%, Valid: 40.11%, Test: 40.06%
Epoch: 775, Loss: 1.3625, Train: 43.20%, Valid: 40.03%, Test: 39.97%
Epoch: 800, Loss: 1.3606, Train: 44.14%, Valid: 40.69%, Test: 40.73%
Epoch: 825, Loss: 1.3608, Train: 43.50%, Valid: 40.34%, Test: 40.36%
Epoch: 850, Loss: 1.3619, Train: 43.89%, Valid: 40.61%, Test: 40.53%
Epoch: 875, Loss: 1.3666, Train: 43.67%, Valid: 40.38%, Test: 40.47%
Epoch: 900, Loss: 1.3581, Train: 44.09%, Valid: 40.69%, Test: 40.47%
Epoch: 925, Loss: 1.3693, Train: 44.29%, Valid: 40.66%, Test: 40.73%
Epoch: 950, Loss: 1.3530, Train: 43.92%, Valid: 40.50%, Test: 40.59%
Epoch: 975, Loss: 1.3639, Train: 44.10%, Valid: 40.63%, Test: 40.53%
Run 01:
Highest Train: 44.58
Highest Valid: 41.11
  Final Train: 44.30
   Final Test: 40.86
All runs:
Highest Train: 44.58, nan
Highest Valid: 41.11, nan
  Final Train: 44.30, nan
   Final Test: 40.86, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6120, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4952, Train: 34.82%, Valid: 34.66%, Test: 34.76%
Epoch: 50, Loss: 1.4631, Train: 36.96%, Valid: 36.06%, Test: 36.35%
Epoch: 75, Loss: 1.4272, Train: 38.57%, Valid: 37.36%, Test: 37.86%
Epoch: 100, Loss: 1.4016, Train: 39.46%, Valid: 38.12%, Test: 38.41%
Epoch: 125, Loss: 1.3978, Train: 40.09%, Valid: 38.59%, Test: 39.23%
Epoch: 150, Loss: 1.3851, Train: 40.53%, Valid: 39.28%, Test: 39.75%
Epoch: 175, Loss: 1.3870, Train: 39.95%, Valid: 38.93%, Test: 39.08%
Epoch: 200, Loss: 1.3748, Train: 41.04%, Valid: 39.83%, Test: 40.03%
Epoch: 225, Loss: 1.3884, Train: 41.36%, Valid: 40.30%, Test: 40.38%
Epoch: 250, Loss: 1.3496, Train: 41.96%, Valid: 40.80%, Test: 40.78%
Epoch: 275, Loss: 1.3398, Train: 42.25%, Valid: 41.13%, Test: 41.21%
Epoch: 300, Loss: 1.3458, Train: 40.56%, Valid: 39.34%, Test: 39.62%
Epoch: 325, Loss: 1.3309, Train: 43.01%, Valid: 42.05%, Test: 42.01%
Epoch: 350, Loss: 1.3956, Train: 37.59%, Valid: 37.14%, Test: 37.21%
Epoch: 375, Loss: 1.3513, Train: 42.78%, Valid: 41.61%, Test: 41.93%
Epoch: 400, Loss: 1.3301, Train: 43.66%, Valid: 42.59%, Test: 42.79%
Epoch: 425, Loss: 1.3378, Train: 41.77%, Valid: 41.01%, Test: 41.13%
Epoch: 450, Loss: 1.3245, Train: 42.89%, Valid: 42.23%, Test: 42.18%
Epoch: 475, Loss: 1.3377, Train: 41.59%, Valid: 40.86%, Test: 41.21%
Epoch: 500, Loss: 1.3183, Train: 42.99%, Valid: 42.18%, Test: 42.15%
Epoch: 525, Loss: 1.3341, Train: 43.32%, Valid: 42.45%, Test: 42.61%
Epoch: 550, Loss: 1.3424, Train: 42.59%, Valid: 41.64%, Test: 41.73%
Epoch: 575, Loss: 1.3286, Train: 43.43%, Valid: 42.50%, Test: 42.70%
Epoch: 600, Loss: 1.3580, Train: 42.69%, Valid: 41.73%, Test: 41.98%
Epoch: 625, Loss: 1.3345, Train: 43.50%, Valid: 42.60%, Test: 43.16%
Epoch: 650, Loss: 1.3170, Train: 43.51%, Valid: 42.69%, Test: 42.96%
Epoch: 675, Loss: 1.3118, Train: 43.45%, Valid: 43.02%, Test: 43.27%
Epoch: 700, Loss: 1.3250, Train: 41.80%, Valid: 41.23%, Test: 41.25%
Epoch: 725, Loss: 1.3410, Train: 44.37%, Valid: 43.81%, Test: 43.97%
Epoch: 750, Loss: 1.3240, Train: 38.02%, Valid: 37.47%, Test: 37.97%
Epoch: 775, Loss: 1.3208, Train: 40.50%, Valid: 39.54%, Test: 40.09%
Epoch: 800, Loss: 1.3059, Train: 40.56%, Valid: 39.81%, Test: 40.30%
Epoch: 825, Loss: 1.3114, Train: 44.73%, Valid: 44.10%, Test: 44.31%
Epoch: 850, Loss: 1.3500, Train: 43.27%, Valid: 42.58%, Test: 42.82%
Epoch: 875, Loss: 1.3034, Train: 43.65%, Valid: 42.89%, Test: 43.26%
Epoch: 900, Loss: 1.3284, Train: 42.72%, Valid: 42.00%, Test: 42.43%
Epoch: 925, Loss: 1.3151, Train: 41.61%, Valid: 41.19%, Test: 41.42%
Epoch: 950, Loss: 1.3158, Train: 43.47%, Valid: 42.74%, Test: 43.13%
Epoch: 975, Loss: 1.3098, Train: 42.73%, Valid: 42.26%, Test: 42.59%
Run 01:
Highest Train: 45.01
Highest Valid: 44.21
  Final Train: 44.91
   Final Test: 44.42
All runs:
Highest Train: 45.01, nan
Highest Valid: 44.21, nan
  Final Train: 44.91, nan
   Final Test: 44.42, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6279, Train: 12.53%, Valid: 12.58%, Test: 12.46%
Epoch: 25, Loss: 1.5176, Train: 32.01%, Valid: 31.75%, Test: 32.15%
Epoch: 50, Loss: 1.4646, Train: 33.85%, Valid: 33.70%, Test: 33.91%
Epoch: 75, Loss: 1.4665, Train: 34.83%, Valid: 34.55%, Test: 34.82%
Epoch: 100, Loss: 1.4498, Train: 32.37%, Valid: 32.09%, Test: 32.48%
Epoch: 125, Loss: 1.4388, Train: 33.11%, Valid: 32.80%, Test: 32.96%
Epoch: 150, Loss: 1.4236, Train: 30.01%, Valid: 29.72%, Test: 29.95%
Epoch: 175, Loss: 1.4211, Train: 31.88%, Valid: 31.60%, Test: 31.84%
Epoch: 200, Loss: 1.4237, Train: 34.06%, Valid: 33.45%, Test: 33.91%
Epoch: 225, Loss: 1.4250, Train: 33.06%, Valid: 32.76%, Test: 32.95%
Epoch: 250, Loss: 1.4192, Train: 34.07%, Valid: 33.67%, Test: 33.79%
Epoch: 275, Loss: 1.4120, Train: 31.54%, Valid: 31.09%, Test: 31.54%
Epoch: 300, Loss: 1.4282, Train: 31.83%, Valid: 31.46%, Test: 31.92%
Epoch: 325, Loss: 1.4379, Train: 31.65%, Valid: 31.57%, Test: 31.38%
Epoch: 350, Loss: 1.4257, Train: 30.47%, Valid: 30.21%, Test: 30.26%
Epoch: 375, Loss: 1.4108, Train: 30.88%, Valid: 30.42%, Test: 30.72%
Epoch: 400, Loss: 1.4098, Train: 31.23%, Valid: 31.09%, Test: 31.14%
Epoch: 425, Loss: 1.4108, Train: 33.13%, Valid: 32.79%, Test: 32.86%
Epoch: 450, Loss: 1.4405, Train: 37.66%, Valid: 37.14%, Test: 37.57%
Epoch: 475, Loss: 1.3993, Train: 36.87%, Valid: 36.26%, Test: 36.42%
Epoch: 500, Loss: 1.3868, Train: 32.66%, Valid: 32.42%, Test: 32.31%
Epoch: 525, Loss: 1.3773, Train: 37.91%, Valid: 37.34%, Test: 37.67%
Epoch: 550, Loss: 1.4381, Train: 26.81%, Valid: 26.50%, Test: 26.94%
Epoch: 575, Loss: 1.4251, Train: 27.59%, Valid: 27.50%, Test: 27.90%
Epoch: 600, Loss: 1.3900, Train: 39.65%, Valid: 39.36%, Test: 39.51%
Epoch: 625, Loss: 1.3790, Train: 39.95%, Valid: 39.72%, Test: 39.80%
Epoch: 650, Loss: 1.3630, Train: 40.07%, Valid: 40.00%, Test: 39.98%
Epoch: 675, Loss: 1.3638, Train: 37.54%, Valid: 37.33%, Test: 37.34%
Epoch: 700, Loss: 1.3783, Train: 39.42%, Valid: 39.08%, Test: 39.32%
Epoch: 725, Loss: 1.3718, Train: 39.36%, Valid: 39.02%, Test: 39.32%
Epoch: 750, Loss: 1.3576, Train: 39.77%, Valid: 39.45%, Test: 39.57%
Epoch: 775, Loss: 1.3826, Train: 39.32%, Valid: 39.11%, Test: 39.11%
Epoch: 800, Loss: 1.3680, Train: 39.78%, Valid: 39.41%, Test: 39.55%
Epoch: 825, Loss: 1.3841, Train: 38.00%, Valid: 37.76%, Test: 37.57%
Epoch: 850, Loss: 1.3655, Train: 38.91%, Valid: 38.62%, Test: 38.72%
Epoch: 875, Loss: 1.3631, Train: 37.67%, Valid: 37.27%, Test: 37.52%
Epoch: 900, Loss: 1.3612, Train: 39.20%, Valid: 38.86%, Test: 39.03%
Epoch: 925, Loss: 1.3501, Train: 40.02%, Valid: 39.68%, Test: 40.01%
Epoch: 950, Loss: 1.3606, Train: 37.98%, Valid: 37.93%, Test: 38.06%
Epoch: 975, Loss: 1.3563, Train: 39.52%, Valid: 39.29%, Test: 39.42%
Run 01:
Highest Train: 40.49
Highest Valid: 40.20
  Final Train: 40.49
   Final Test: 40.21
All runs:
Highest Train: 40.49, nan
Highest Valid: 40.20, nan
  Final Train: 40.49, nan
   Final Test: 40.21, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6064, Train: 28.55%, Valid: 28.27%, Test: 28.71%
Epoch: 25, Loss: 1.5073, Train: 29.79%, Valid: 29.59%, Test: 29.95%
Epoch: 50, Loss: 1.4320, Train: 37.23%, Valid: 36.61%, Test: 37.04%
Epoch: 75, Loss: 1.3966, Train: 39.16%, Valid: 38.34%, Test: 38.71%
Epoch: 100, Loss: 1.3673, Train: 40.93%, Valid: 40.06%, Test: 40.16%
Epoch: 125, Loss: 1.3386, Train: 42.40%, Valid: 41.24%, Test: 41.35%
Epoch: 150, Loss: 1.3305, Train: 42.56%, Valid: 41.46%, Test: 41.40%
Epoch: 175, Loss: 1.3034, Train: 44.10%, Valid: 42.61%, Test: 42.67%
Epoch: 200, Loss: 1.2889, Train: 44.98%, Valid: 42.78%, Test: 43.06%
Epoch: 225, Loss: 1.2812, Train: 45.29%, Valid: 43.16%, Test: 43.47%
Epoch: 250, Loss: 1.3194, Train: 46.27%, Valid: 43.69%, Test: 43.92%
Epoch: 275, Loss: 1.2359, Train: 47.45%, Valid: 44.24%, Test: 44.42%
Epoch: 300, Loss: 1.2161, Train: 48.25%, Valid: 44.34%, Test: 44.85%
Epoch: 325, Loss: 1.2107, Train: 48.71%, Valid: 44.76%, Test: 44.92%
Epoch: 350, Loss: 1.1811, Train: 49.67%, Valid: 45.00%, Test: 45.11%
Epoch: 375, Loss: 1.1613, Train: 50.64%, Valid: 45.02%, Test: 45.56%
Epoch: 400, Loss: 1.1502, Train: 51.43%, Valid: 45.62%, Test: 45.44%
Epoch: 425, Loss: 1.1336, Train: 52.38%, Valid: 45.99%, Test: 46.10%
Epoch: 450, Loss: 1.1761, Train: 50.11%, Valid: 44.94%, Test: 45.05%
Epoch: 475, Loss: 1.1178, Train: 52.72%, Valid: 45.86%, Test: 46.43%
Epoch: 500, Loss: 1.1124, Train: 53.09%, Valid: 45.70%, Test: 46.05%
Epoch: 525, Loss: 1.0935, Train: 53.92%, Valid: 45.68%, Test: 46.08%
Epoch: 550, Loss: 1.0810, Train: 54.75%, Valid: 46.38%, Test: 46.54%
Epoch: 575, Loss: 1.0728, Train: 54.83%, Valid: 46.39%, Test: 46.57%
Epoch: 600, Loss: 1.0617, Train: 55.29%, Valid: 46.43%, Test: 46.75%
Epoch: 625, Loss: 1.1287, Train: 52.67%, Valid: 45.15%, Test: 45.48%
Epoch: 650, Loss: 1.0471, Train: 56.04%, Valid: 46.54%, Test: 46.76%
Epoch: 675, Loss: 1.0375, Train: 56.63%, Valid: 46.86%, Test: 46.88%
Epoch: 700, Loss: 1.0313, Train: 57.05%, Valid: 46.51%, Test: 46.89%
Epoch: 725, Loss: 1.0216, Train: 57.07%, Valid: 46.42%, Test: 46.45%
Epoch: 750, Loss: 1.0220, Train: 57.17%, Valid: 46.71%, Test: 46.66%
Epoch: 775, Loss: 1.0123, Train: 57.56%, Valid: 46.55%, Test: 46.56%
Epoch: 800, Loss: 1.0129, Train: 57.78%, Valid: 46.71%, Test: 46.91%
Epoch: 825, Loss: 1.0070, Train: 58.07%, Valid: 46.68%, Test: 46.73%
Epoch: 850, Loss: 0.9963, Train: 58.51%, Valid: 46.97%, Test: 46.91%
Epoch: 875, Loss: 0.9946, Train: 58.10%, Valid: 46.29%, Test: 46.43%
Epoch: 900, Loss: 0.9955, Train: 58.18%, Valid: 46.84%, Test: 46.77%
Epoch: 925, Loss: 0.9975, Train: 58.70%, Valid: 46.66%, Test: 46.66%
Epoch: 950, Loss: 0.9775, Train: 59.33%, Valid: 46.69%, Test: 46.50%
Epoch: 975, Loss: 0.9881, Train: 58.81%, Valid: 47.02%, Test: 46.97%
Run 01:
Highest Train: 59.73
Highest Valid: 47.31
  Final Train: 59.50
   Final Test: 46.79
All runs:
Highest Train: 59.73, nan
Highest Valid: 47.31, nan
  Final Train: 59.50, nan
   Final Test: 46.79, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.0059, Train: 23.50%, Valid: 23.82%, Test: 23.97%
Epoch: 25, Loss: 1.4256, Train: 38.60%, Valid: 38.32%, Test: 38.59%
Epoch: 50, Loss: 1.3879, Train: 40.92%, Valid: 40.54%, Test: 40.63%
Epoch: 75, Loss: 1.3938, Train: 40.25%, Valid: 40.02%, Test: 40.09%
Epoch: 100, Loss: 1.3567, Train: 41.86%, Valid: 41.74%, Test: 41.69%
Epoch: 125, Loss: 1.3499, Train: 42.37%, Valid: 42.38%, Test: 42.37%
Epoch: 150, Loss: 1.3328, Train: 42.97%, Valid: 42.86%, Test: 42.87%
Epoch: 175, Loss: 1.4024, Train: 39.91%, Valid: 39.43%, Test: 39.75%
Epoch: 200, Loss: 1.3510, Train: 41.82%, Valid: 41.72%, Test: 41.79%
Epoch: 225, Loss: 1.3642, Train: 41.57%, Valid: 41.33%, Test: 41.38%
Epoch: 250, Loss: 1.3321, Train: 42.49%, Valid: 42.50%, Test: 42.42%
Epoch: 275, Loss: 1.3438, Train: 41.92%, Valid: 41.85%, Test: 41.89%
Epoch: 300, Loss: 1.3170, Train: 43.19%, Valid: 43.15%, Test: 43.06%
Epoch: 325, Loss: 1.3073, Train: 42.63%, Valid: 42.66%, Test: 42.66%
Epoch: 350, Loss: 1.3060, Train: 43.33%, Valid: 43.20%, Test: 43.17%
Epoch: 375, Loss: 1.3114, Train: 42.77%, Valid: 42.71%, Test: 42.69%
Epoch: 400, Loss: 1.3124, Train: 43.80%, Valid: 43.55%, Test: 43.59%
Epoch: 425, Loss: 1.2865, Train: 44.57%, Valid: 44.51%, Test: 44.23%
Epoch: 450, Loss: 1.2857, Train: 44.42%, Valid: 44.38%, Test: 44.09%
Epoch: 475, Loss: 1.2837, Train: 44.52%, Valid: 44.48%, Test: 44.35%
Epoch: 500, Loss: 1.2870, Train: 44.34%, Valid: 44.34%, Test: 44.02%
Epoch: 525, Loss: 1.2801, Train: 44.79%, Valid: 44.57%, Test: 44.41%
Epoch: 550, Loss: 1.2784, Train: 44.96%, Valid: 44.92%, Test: 44.81%
Epoch: 575, Loss: 1.2631, Train: 45.48%, Valid: 45.26%, Test: 45.02%
Epoch: 600, Loss: 1.2703, Train: 45.30%, Valid: 44.93%, Test: 44.84%
Epoch: 625, Loss: 1.3257, Train: 42.95%, Valid: 42.85%, Test: 42.88%
Epoch: 650, Loss: 1.2763, Train: 45.06%, Valid: 44.91%, Test: 44.80%
Epoch: 675, Loss: 1.2723, Train: 45.57%, Valid: 45.38%, Test: 45.07%
Epoch: 700, Loss: 1.2538, Train: 45.96%, Valid: 45.81%, Test: 45.53%
Epoch: 725, Loss: 1.2485, Train: 46.17%, Valid: 45.94%, Test: 45.84%
Epoch: 750, Loss: 1.2628, Train: 41.92%, Valid: 41.96%, Test: 41.86%
Epoch: 775, Loss: 1.2574, Train: 45.91%, Valid: 45.77%, Test: 45.43%
Epoch: 800, Loss: 1.2401, Train: 46.37%, Valid: 46.38%, Test: 46.00%
Epoch: 825, Loss: 1.2407, Train: 46.12%, Valid: 46.06%, Test: 45.82%
Epoch: 850, Loss: 1.2313, Train: 46.92%, Valid: 46.72%, Test: 46.59%
Epoch: 875, Loss: 1.2501, Train: 46.42%, Valid: 46.13%, Test: 46.00%
Epoch: 900, Loss: 1.2419, Train: 46.65%, Valid: 46.30%, Test: 46.15%
Epoch: 925, Loss: 1.2288, Train: 47.03%, Valid: 46.85%, Test: 46.59%
Epoch: 950, Loss: 1.2880, Train: 45.97%, Valid: 45.64%, Test: 45.56%
Epoch: 975, Loss: 1.2365, Train: 46.85%, Valid: 46.73%, Test: 46.41%
Run 01:
Highest Train: 47.34
Highest Valid: 47.08
  Final Train: 47.34
   Final Test: 46.84
All runs:
Highest Train: 47.34, nan
Highest Valid: 47.08, nan
  Final Train: 47.34, nan
   Final Test: 46.84, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 93.2649, Train: 28.05%, Valid: 27.81%, Test: 28.20%
Epoch: 25, Loss: 2.8858, Train: 27.46%, Valid: 27.36%, Test: 27.57%
Epoch: 50, Loss: 1.5689, Train: 28.71%, Valid: 28.52%, Test: 28.79%
Epoch: 75, Loss: 1.5584, Train: 28.64%, Valid: 28.47%, Test: 28.77%
Epoch: 100, Loss: 1.5338, Train: 28.65%, Valid: 28.48%, Test: 28.77%
Epoch: 125, Loss: 1.5112, Train: 28.59%, Valid: 28.41%, Test: 28.71%
Epoch: 150, Loss: 1.4953, Train: 28.57%, Valid: 28.39%, Test: 28.71%
Epoch: 175, Loss: 1.4816, Train: 28.65%, Valid: 28.45%, Test: 28.79%
Epoch: 200, Loss: 1.4694, Train: 28.71%, Valid: 28.50%, Test: 28.85%
Epoch: 225, Loss: 1.4577, Train: 28.87%, Valid: 28.65%, Test: 29.00%
Epoch: 250, Loss: 1.4453, Train: 29.29%, Valid: 29.06%, Test: 29.39%
Epoch: 275, Loss: 1.4354, Train: 29.58%, Valid: 29.35%, Test: 29.74%
Epoch: 300, Loss: 1.4285, Train: 29.84%, Valid: 29.60%, Test: 30.01%
Epoch: 325, Loss: 1.4222, Train: 30.02%, Valid: 29.85%, Test: 30.19%
Epoch: 350, Loss: 1.4170, Train: 30.21%, Valid: 30.03%, Test: 30.34%
Epoch: 375, Loss: 1.4127, Train: 30.32%, Valid: 30.22%, Test: 30.45%
Epoch: 400, Loss: 1.4092, Train: 30.48%, Valid: 30.32%, Test: 30.61%
Epoch: 425, Loss: 1.4047, Train: 30.56%, Valid: 30.48%, Test: 30.68%
Epoch: 450, Loss: 1.4010, Train: 30.70%, Valid: 30.60%, Test: 30.82%
Epoch: 475, Loss: 1.3976, Train: 30.86%, Valid: 30.67%, Test: 30.92%
Epoch: 500, Loss: 1.3943, Train: 30.94%, Valid: 30.74%, Test: 31.07%
Epoch: 525, Loss: 1.4526, Train: 29.67%, Valid: 29.53%, Test: 29.78%
Epoch: 550, Loss: 1.4068, Train: 29.62%, Valid: 29.32%, Test: 29.70%
Epoch: 575, Loss: 1.3957, Train: 30.39%, Valid: 30.19%, Test: 30.42%
Epoch: 600, Loss: 1.3896, Train: 31.03%, Valid: 30.84%, Test: 31.08%
Epoch: 625, Loss: 1.3860, Train: 31.24%, Valid: 30.96%, Test: 31.31%
Epoch: 650, Loss: 1.3832, Train: 31.38%, Valid: 31.11%, Test: 31.44%
Epoch: 675, Loss: 1.3808, Train: 31.47%, Valid: 31.21%, Test: 31.54%
Epoch: 700, Loss: 1.4113, Train: 29.51%, Valid: 29.19%, Test: 29.56%
Epoch: 725, Loss: 1.3947, Train: 30.58%, Valid: 30.47%, Test: 30.74%
Epoch: 750, Loss: 1.3823, Train: 31.30%, Valid: 31.09%, Test: 31.36%
Epoch: 775, Loss: 1.3785, Train: 31.44%, Valid: 31.16%, Test: 31.52%
Epoch: 800, Loss: 1.3760, Train: 31.57%, Valid: 31.27%, Test: 31.67%
Epoch: 825, Loss: 1.3743, Train: 31.49%, Valid: 31.22%, Test: 31.59%
Epoch: 850, Loss: 1.4264, Train: 29.88%, Valid: 29.53%, Test: 29.75%
Epoch: 875, Loss: 1.3905, Train: 30.59%, Valid: 30.42%, Test: 30.66%
Epoch: 900, Loss: 1.3769, Train: 31.54%, Valid: 31.29%, Test: 31.56%
Epoch: 925, Loss: 1.3729, Train: 31.76%, Valid: 31.40%, Test: 31.86%
Epoch: 950, Loss: 1.3706, Train: 31.86%, Valid: 31.46%, Test: 31.98%
Epoch: 975, Loss: 1.3688, Train: 31.98%, Valid: 31.60%, Test: 32.03%
Run 01:
Highest Train: 31.98
Highest Valid: 31.60
  Final Train: 31.98
   Final Test: 32.03
All runs:
Highest Train: 31.98, nan
Highest Valid: 31.60, nan
  Final Train: 31.98, nan
   Final Test: 32.03, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6098, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.5090, Train: 29.82%, Valid: 29.69%, Test: 30.08%
Epoch: 50, Loss: 1.4297, Train: 39.05%, Valid: 38.51%, Test: 38.76%
Epoch: 75, Loss: 1.3906, Train: 39.54%, Valid: 39.17%, Test: 39.40%
Epoch: 100, Loss: 1.3659, Train: 40.75%, Valid: 40.19%, Test: 40.10%
Epoch: 125, Loss: 1.3599, Train: 41.06%, Valid: 40.33%, Test: 40.54%
Epoch: 150, Loss: 1.3494, Train: 40.34%, Valid: 39.30%, Test: 39.45%
Epoch: 175, Loss: 1.3200, Train: 43.10%, Valid: 41.98%, Test: 42.21%
Epoch: 200, Loss: 1.3033, Train: 44.10%, Valid: 42.70%, Test: 42.83%
Epoch: 225, Loss: 1.3066, Train: 43.78%, Valid: 42.00%, Test: 42.15%
Epoch: 250, Loss: 1.2673, Train: 46.03%, Valid: 43.34%, Test: 43.59%
Epoch: 275, Loss: 1.2515, Train: 46.64%, Valid: 43.66%, Test: 43.88%
Epoch: 300, Loss: 1.2615, Train: 46.02%, Valid: 42.65%, Test: 43.05%
Epoch: 325, Loss: 1.2199, Train: 47.96%, Valid: 44.58%, Test: 44.83%
Epoch: 350, Loss: 1.2023, Train: 49.09%, Valid: 44.97%, Test: 45.25%
Epoch: 375, Loss: 1.2281, Train: 47.99%, Valid: 44.03%, Test: 44.60%
Epoch: 400, Loss: 1.1812, Train: 50.19%, Valid: 45.37%, Test: 45.36%
Epoch: 425, Loss: 1.1823, Train: 50.34%, Valid: 44.77%, Test: 44.97%
Epoch: 450, Loss: 1.1826, Train: 50.25%, Valid: 44.71%, Test: 44.97%
Epoch: 475, Loss: 1.1419, Train: 51.65%, Valid: 45.71%, Test: 45.93%
Epoch: 500, Loss: 1.1304, Train: 52.59%, Valid: 45.95%, Test: 46.00%
Epoch: 525, Loss: 1.1541, Train: 51.24%, Valid: 45.04%, Test: 45.30%
Epoch: 550, Loss: 1.1067, Train: 53.28%, Valid: 45.95%, Test: 45.98%
Epoch: 575, Loss: 1.0992, Train: 53.79%, Valid: 45.94%, Test: 46.20%
Epoch: 600, Loss: 1.0935, Train: 53.89%, Valid: 45.56%, Test: 45.97%
Epoch: 625, Loss: 1.0997, Train: 53.36%, Valid: 45.78%, Test: 45.91%
Epoch: 650, Loss: 1.0730, Train: 54.73%, Valid: 46.27%, Test: 46.52%
Epoch: 675, Loss: 1.0632, Train: 55.45%, Valid: 46.56%, Test: 46.79%
Epoch: 700, Loss: 1.0770, Train: 54.95%, Valid: 46.23%, Test: 46.68%
Epoch: 725, Loss: 1.0550, Train: 55.87%, Valid: 46.59%, Test: 46.82%
Epoch: 750, Loss: 1.0414, Train: 56.61%, Valid: 46.67%, Test: 46.97%
Epoch: 775, Loss: 1.0430, Train: 56.43%, Valid: 46.44%, Test: 46.77%
Epoch: 800, Loss: 1.0293, Train: 56.77%, Valid: 46.78%, Test: 46.89%
Epoch: 825, Loss: 1.0184, Train: 57.14%, Valid: 46.71%, Test: 46.95%
Epoch: 850, Loss: 1.0279, Train: 57.15%, Valid: 46.64%, Test: 46.87%
Epoch: 875, Loss: 1.0309, Train: 56.72%, Valid: 46.56%, Test: 46.72%
Epoch: 900, Loss: 1.0354, Train: 56.45%, Valid: 46.44%, Test: 46.83%
Epoch: 925, Loss: 1.0101, Train: 57.68%, Valid: 46.74%, Test: 47.00%
Epoch: 950, Loss: 1.0272, Train: 57.08%, Valid: 46.70%, Test: 46.98%
Epoch: 975, Loss: 1.0076, Train: 57.31%, Valid: 46.07%, Test: 46.26%
Run 01:
Highest Train: 58.59
Highest Valid: 47.14
  Final Train: 58.48
   Final Test: 47.41
All runs:
Highest Train: 58.59, nan
Highest Valid: 47.14, nan
  Final Train: 58.48, nan
   Final Test: 47.41, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.5458, Train: 27.24%, Valid: 26.92%, Test: 27.24%
Epoch: 25, Loss: 1.4542, Train: 33.88%, Valid: 33.54%, Test: 33.88%
Epoch: 50, Loss: 1.3887, Train: 39.56%, Valid: 39.29%, Test: 39.40%
Epoch: 75, Loss: 1.3546, Train: 41.66%, Valid: 41.38%, Test: 41.42%
Epoch: 100, Loss: 1.3439, Train: 42.63%, Valid: 42.50%, Test: 42.54%
Epoch: 125, Loss: 1.3209, Train: 43.08%, Valid: 42.98%, Test: 43.02%
Epoch: 150, Loss: 1.3203, Train: 43.21%, Valid: 42.85%, Test: 42.85%
Epoch: 175, Loss: 1.2984, Train: 44.03%, Valid: 44.03%, Test: 43.82%
Epoch: 200, Loss: 1.3023, Train: 43.89%, Valid: 43.81%, Test: 43.69%
Epoch: 225, Loss: 1.2807, Train: 44.91%, Valid: 44.70%, Test: 44.47%
Epoch: 250, Loss: 1.2779, Train: 44.92%, Valid: 44.82%, Test: 44.60%
Epoch: 275, Loss: 1.2832, Train: 44.96%, Valid: 44.76%, Test: 44.63%
Epoch: 300, Loss: 1.2753, Train: 45.04%, Valid: 44.91%, Test: 44.68%
Epoch: 325, Loss: 1.3321, Train: 44.37%, Valid: 44.42%, Test: 44.32%
Epoch: 350, Loss: 1.2660, Train: 45.49%, Valid: 45.32%, Test: 45.06%
Epoch: 375, Loss: 1.2511, Train: 46.24%, Valid: 45.98%, Test: 45.72%
Epoch: 400, Loss: 1.2589, Train: 45.59%, Valid: 45.53%, Test: 45.08%
Epoch: 425, Loss: 1.2541, Train: 45.73%, Valid: 45.69%, Test: 45.35%
Epoch: 450, Loss: 1.2368, Train: 46.79%, Valid: 46.67%, Test: 46.36%
Epoch: 475, Loss: 1.2410, Train: 46.38%, Valid: 46.13%, Test: 45.92%
Epoch: 500, Loss: 1.2625, Train: 45.22%, Valid: 45.14%, Test: 45.02%
Epoch: 525, Loss: 1.2563, Train: 45.01%, Valid: 44.81%, Test: 44.70%
Epoch: 550, Loss: 1.2364, Train: 46.52%, Valid: 46.27%, Test: 46.10%
Epoch: 575, Loss: 1.2350, Train: 46.74%, Valid: 46.54%, Test: 46.20%
Epoch: 600, Loss: 1.2612, Train: 45.56%, Valid: 45.48%, Test: 45.31%
Epoch: 625, Loss: 1.2263, Train: 47.04%, Valid: 46.88%, Test: 46.61%
Epoch: 650, Loss: 1.2456, Train: 47.20%, Valid: 46.81%, Test: 46.67%
Epoch: 675, Loss: 1.2199, Train: 47.23%, Valid: 46.88%, Test: 46.71%
Epoch: 700, Loss: 1.2188, Train: 47.18%, Valid: 46.92%, Test: 46.68%
Epoch: 725, Loss: 1.2104, Train: 47.74%, Valid: 47.30%, Test: 47.30%
Epoch: 750, Loss: 4.8243, Train: 29.77%, Valid: 29.81%, Test: 29.42%
Epoch: 775, Loss: 1.4879, Train: 31.14%, Valid: 30.70%, Test: 31.46%
Epoch: 800, Loss: 1.4192, Train: 37.96%, Valid: 37.63%, Test: 37.88%
Epoch: 825, Loss: 1.3998, Train: 38.55%, Valid: 38.19%, Test: 38.42%
Epoch: 850, Loss: 1.3896, Train: 39.13%, Valid: 38.85%, Test: 39.08%
Epoch: 875, Loss: 1.3812, Train: 39.52%, Valid: 39.24%, Test: 39.41%
Epoch: 900, Loss: 1.3726, Train: 39.88%, Valid: 39.64%, Test: 39.75%
Epoch: 925, Loss: 1.3639, Train: 40.41%, Valid: 40.09%, Test: 40.20%
Epoch: 950, Loss: 1.3557, Train: 40.90%, Valid: 40.72%, Test: 40.66%
Epoch: 975, Loss: 1.3607, Train: 40.73%, Valid: 40.51%, Test: 40.47%
Run 01:
Highest Train: 48.05
Highest Valid: 47.63
  Final Train: 48.04
   Final Test: 47.50
All runs:
Highest Train: 48.05, nan
Highest Valid: 47.63, nan
  Final Train: 48.04, nan
   Final Test: 47.50, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 177.2517, Train: 30.76%, Valid: 30.57%, Test: 31.14%
Epoch: 25, Loss: 2.3121, Train: 25.98%, Valid: 26.26%, Test: 26.45%
Epoch: 50, Loss: 1.6527, Train: 33.36%, Valid: 33.26%, Test: 33.71%
Epoch: 75, Loss: 1.5698, Train: 33.39%, Valid: 33.48%, Test: 34.00%
Epoch: 100, Loss: 1.5423, Train: 33.49%, Valid: 33.41%, Test: 33.93%
Epoch: 125, Loss: 1.5234, Train: 33.45%, Valid: 33.34%, Test: 34.00%
Epoch: 150, Loss: 1.5076, Train: 33.50%, Valid: 33.30%, Test: 34.06%
Epoch: 175, Loss: 1.4936, Train: 33.50%, Valid: 33.39%, Test: 34.10%
Epoch: 200, Loss: 1.4812, Train: 33.49%, Valid: 33.46%, Test: 34.11%
Epoch: 225, Loss: 1.4700, Train: 33.57%, Valid: 33.47%, Test: 34.06%
Epoch: 250, Loss: 1.4600, Train: 33.61%, Valid: 33.49%, Test: 34.18%
Epoch: 275, Loss: 1.4507, Train: 33.65%, Valid: 33.55%, Test: 34.23%
Epoch: 300, Loss: 1.4416, Train: 33.71%, Valid: 33.63%, Test: 34.24%
Epoch: 325, Loss: 1.4332, Train: 33.74%, Valid: 33.69%, Test: 34.27%
Epoch: 350, Loss: 1.4275, Train: 33.56%, Valid: 33.52%, Test: 34.09%
Epoch: 375, Loss: 1.4724, Train: 33.86%, Valid: 33.60%, Test: 34.15%
Epoch: 400, Loss: 1.4248, Train: 34.25%, Valid: 34.08%, Test: 34.82%
Epoch: 425, Loss: 1.4146, Train: 34.18%, Valid: 33.99%, Test: 34.74%
Epoch: 450, Loss: 1.4075, Train: 34.16%, Valid: 33.88%, Test: 34.67%
Epoch: 475, Loss: 1.4016, Train: 34.16%, Valid: 33.95%, Test: 34.66%
Epoch: 500, Loss: 1.3962, Train: 34.10%, Valid: 33.98%, Test: 34.52%
Epoch: 525, Loss: 1.3912, Train: 34.05%, Valid: 33.95%, Test: 34.43%
Epoch: 550, Loss: 1.3998, Train: 33.73%, Valid: 33.69%, Test: 34.12%
Epoch: 575, Loss: 1.3862, Train: 34.37%, Valid: 34.24%, Test: 34.72%
Epoch: 600, Loss: 1.3806, Train: 34.27%, Valid: 34.08%, Test: 34.65%
Epoch: 625, Loss: 1.3820, Train: 33.72%, Valid: 33.49%, Test: 34.09%
Epoch: 650, Loss: 1.3779, Train: 34.40%, Valid: 34.19%, Test: 34.70%
Epoch: 675, Loss: 1.3797, Train: 33.88%, Valid: 33.68%, Test: 34.40%
Epoch: 700, Loss: 1.3735, Train: 34.65%, Valid: 34.27%, Test: 35.06%
Epoch: 725, Loss: 1.4341, Train: 34.64%, Valid: 34.37%, Test: 35.10%
Epoch: 750, Loss: 1.4225, Train: 35.40%, Valid: 35.28%, Test: 36.12%
Epoch: 775, Loss: 1.3984, Train: 35.99%, Valid: 35.93%, Test: 36.67%
Epoch: 800, Loss: 1.3875, Train: 35.98%, Valid: 35.91%, Test: 36.70%
Epoch: 825, Loss: 1.3784, Train: 36.08%, Valid: 36.00%, Test: 36.73%
Epoch: 850, Loss: 1.3706, Train: 36.03%, Valid: 35.97%, Test: 36.61%
Epoch: 875, Loss: 1.3643, Train: 35.84%, Valid: 35.77%, Test: 36.48%
Epoch: 900, Loss: 1.9204, Train: 34.66%, Valid: 34.65%, Test: 35.30%
Epoch: 925, Loss: 1.4141, Train: 34.28%, Valid: 34.17%, Test: 34.99%
Epoch: 950, Loss: 1.3862, Train: 35.53%, Valid: 35.54%, Test: 36.24%
Epoch: 975, Loss: 1.3763, Train: 35.62%, Valid: 35.56%, Test: 36.33%
Run 01:
Highest Train: 36.12
Highest Valid: 36.06
  Final Train: 36.06
   Final Test: 36.68
All runs:
Highest Train: 36.12, nan
Highest Valid: 36.06, nan
  Final Train: 36.06, nan
   Final Test: 36.68, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6156, Train: 30.68%, Valid: 30.53%, Test: 31.04%
Epoch: 25, Loss: 1.5016, Train: 34.65%, Valid: 34.34%, Test: 34.65%
Epoch: 50, Loss: 1.4825, Train: 35.68%, Valid: 35.25%, Test: 35.63%
Epoch: 75, Loss: 1.4641, Train: 37.00%, Valid: 36.19%, Test: 36.46%
Epoch: 100, Loss: 1.4388, Train: 38.97%, Valid: 38.03%, Test: 38.13%
Epoch: 125, Loss: 1.4106, Train: 40.67%, Valid: 39.47%, Test: 40.11%
Epoch: 150, Loss: 1.3925, Train: 42.24%, Valid: 40.72%, Test: 40.96%
Epoch: 175, Loss: 1.3615, Train: 42.94%, Valid: 41.63%, Test: 41.68%
Epoch: 200, Loss: 1.3512, Train: 42.33%, Valid: 40.81%, Test: 40.96%
Epoch: 225, Loss: 1.3544, Train: 44.03%, Valid: 42.39%, Test: 42.65%
Epoch: 250, Loss: 1.3253, Train: 44.24%, Valid: 42.29%, Test: 42.75%
Epoch: 275, Loss: 1.3237, Train: 44.62%, Valid: 42.53%, Test: 42.80%
Epoch: 300, Loss: 1.3070, Train: 43.65%, Valid: 41.43%, Test: 41.85%
Epoch: 325, Loss: 1.3131, Train: 45.59%, Valid: 43.47%, Test: 44.05%
Epoch: 350, Loss: 1.2976, Train: 44.14%, Valid: 41.89%, Test: 42.22%
Epoch: 375, Loss: 1.2900, Train: 45.20%, Valid: 42.70%, Test: 42.99%
Epoch: 400, Loss: 1.2864, Train: 45.96%, Valid: 43.58%, Test: 43.77%
Epoch: 425, Loss: 1.2819, Train: 45.83%, Valid: 43.73%, Test: 43.70%
Epoch: 450, Loss: 1.2733, Train: 47.14%, Valid: 44.77%, Test: 44.70%
Epoch: 475, Loss: 1.2673, Train: 46.34%, Valid: 43.90%, Test: 43.81%
Epoch: 500, Loss: 1.2719, Train: 47.80%, Valid: 45.28%, Test: 45.25%
Epoch: 525, Loss: 1.2648, Train: 47.71%, Valid: 44.76%, Test: 45.23%
Epoch: 550, Loss: 1.2648, Train: 47.93%, Valid: 45.44%, Test: 45.55%
Epoch: 575, Loss: 1.2600, Train: 47.60%, Valid: 44.98%, Test: 45.26%
Epoch: 600, Loss: 1.2548, Train: 47.72%, Valid: 45.19%, Test: 45.15%
Epoch: 625, Loss: 1.2456, Train: 48.44%, Valid: 45.83%, Test: 45.98%
Epoch: 650, Loss: 1.2614, Train: 47.22%, Valid: 44.93%, Test: 44.88%
Epoch: 675, Loss: 1.2435, Train: 48.83%, Valid: 45.90%, Test: 46.21%
Epoch: 700, Loss: 1.2505, Train: 46.49%, Valid: 44.09%, Test: 44.24%
Epoch: 725, Loss: 1.2466, Train: 47.17%, Valid: 44.99%, Test: 45.12%
Epoch: 750, Loss: 1.2401, Train: 47.13%, Valid: 44.64%, Test: 44.62%
Epoch: 775, Loss: 1.2392, Train: 48.27%, Valid: 45.75%, Test: 45.87%
Epoch: 800, Loss: 1.2413, Train: 47.01%, Valid: 44.62%, Test: 44.81%
Epoch: 825, Loss: 1.2461, Train: 47.61%, Valid: 45.25%, Test: 45.36%
Epoch: 850, Loss: 1.2438, Train: 48.34%, Valid: 45.59%, Test: 45.61%
Epoch: 875, Loss: 1.2322, Train: 48.85%, Valid: 46.08%, Test: 46.17%
Epoch: 900, Loss: 1.2341, Train: 48.37%, Valid: 45.33%, Test: 45.55%
Epoch: 925, Loss: 1.2487, Train: 48.02%, Valid: 45.67%, Test: 45.57%
Epoch: 950, Loss: 1.2314, Train: 47.84%, Valid: 45.05%, Test: 45.20%
Epoch: 975, Loss: 1.2335, Train: 48.44%, Valid: 45.84%, Test: 45.82%
Run 01:
Highest Train: 49.44
Highest Valid: 46.49
  Final Train: 49.44
   Final Test: 46.61
All runs:
Highest Train: 49.44, nan
Highest Valid: 46.49, nan
  Final Train: 49.44, nan
   Final Test: 46.61, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.7006, Train: 17.24%, Valid: 17.28%, Test: 16.96%
Epoch: 25, Loss: 1.4506, Train: 28.72%, Valid: 28.49%, Test: 28.94%
Epoch: 50, Loss: 1.4176, Train: 26.17%, Valid: 26.73%, Test: 26.73%
Epoch: 75, Loss: 1.4030, Train: 30.09%, Valid: 30.52%, Test: 29.96%
Epoch: 100, Loss: 1.3994, Train: 30.60%, Valid: 30.86%, Test: 30.43%
Epoch: 125, Loss: 1.3773, Train: 29.24%, Valid: 29.40%, Test: 29.26%
Epoch: 150, Loss: 1.3658, Train: 32.50%, Valid: 32.68%, Test: 32.32%
Epoch: 175, Loss: 1.3898, Train: 32.28%, Valid: 32.48%, Test: 32.35%
Epoch: 200, Loss: 1.3684, Train: 31.65%, Valid: 31.98%, Test: 31.34%
Epoch: 225, Loss: 1.3775, Train: 31.87%, Valid: 32.35%, Test: 31.62%
Epoch: 250, Loss: 1.3526, Train: 32.66%, Valid: 32.90%, Test: 32.18%
Epoch: 275, Loss: 1.3380, Train: 31.45%, Valid: 31.64%, Test: 31.37%
Epoch: 300, Loss: 1.3794, Train: 32.57%, Valid: 32.70%, Test: 32.20%
Epoch: 325, Loss: 1.3637, Train: 33.19%, Valid: 33.31%, Test: 32.92%
Epoch: 350, Loss: 1.3545, Train: 32.90%, Valid: 33.09%, Test: 32.52%
Epoch: 375, Loss: 1.3276, Train: 28.77%, Valid: 28.99%, Test: 28.64%
Epoch: 400, Loss: 1.3294, Train: 32.21%, Valid: 32.62%, Test: 32.07%
Epoch: 425, Loss: 1.3277, Train: 32.58%, Valid: 32.81%, Test: 32.41%
Epoch: 450, Loss: 1.3319, Train: 34.80%, Valid: 35.15%, Test: 34.62%
Epoch: 475, Loss: 1.3350, Train: 34.15%, Valid: 34.30%, Test: 33.78%
Epoch: 500, Loss: 1.3399, Train: 31.76%, Valid: 32.08%, Test: 31.66%
Epoch: 525, Loss: 1.3196, Train: 34.88%, Valid: 34.93%, Test: 34.70%
Epoch: 550, Loss: 1.3342, Train: 34.33%, Valid: 34.53%, Test: 34.30%
Epoch: 575, Loss: 1.3358, Train: 39.65%, Valid: 39.68%, Test: 39.50%
Epoch: 600, Loss: 1.3098, Train: 36.03%, Valid: 36.23%, Test: 36.31%
Epoch: 625, Loss: 1.3213, Train: 35.86%, Valid: 35.97%, Test: 35.78%
Epoch: 650, Loss: 1.3183, Train: 34.23%, Valid: 34.18%, Test: 34.03%
Epoch: 675, Loss: 1.3307, Train: 36.26%, Valid: 36.12%, Test: 36.19%
Epoch: 700, Loss: 1.3296, Train: 36.87%, Valid: 36.93%, Test: 36.98%
Epoch: 725, Loss: 1.3211, Train: 36.67%, Valid: 36.67%, Test: 36.54%
Epoch: 750, Loss: 1.3314, Train: 36.87%, Valid: 36.88%, Test: 36.94%
Epoch: 775, Loss: 1.3377, Train: 35.09%, Valid: 35.07%, Test: 34.79%
Epoch: 800, Loss: 1.3086, Train: 37.65%, Valid: 37.72%, Test: 37.44%
Epoch: 825, Loss: 1.3192, Train: 38.93%, Valid: 38.79%, Test: 38.69%
Epoch: 850, Loss: 1.3104, Train: 37.66%, Valid: 37.42%, Test: 37.37%
Epoch: 875, Loss: 1.3112, Train: 42.03%, Valid: 41.82%, Test: 41.89%
Epoch: 900, Loss: 1.3131, Train: 38.81%, Valid: 38.75%, Test: 38.62%
Epoch: 925, Loss: 1.3025, Train: 39.76%, Valid: 39.57%, Test: 39.89%
Epoch: 950, Loss: 1.3045, Train: 36.79%, Valid: 36.51%, Test: 36.48%
Epoch: 975, Loss: 1.3173, Train: 39.76%, Valid: 39.42%, Test: 39.66%
Run 01:
Highest Train: 42.65
Highest Valid: 42.46
  Final Train: 42.65
   Final Test: 42.58
All runs:
Highest Train: 42.65, nan
Highest Valid: 42.46, nan
  Final Train: 42.65, nan
   Final Test: 42.58, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 68.0875, Train: 27.46%, Valid: 27.11%, Test: 27.46%
Epoch: 25, Loss: 1.9483, Train: 26.47%, Valid: 26.24%, Test: 26.60%
Epoch: 50, Loss: 1.7556, Train: 27.17%, Valid: 26.98%, Test: 27.25%
Epoch: 75, Loss: 1.6812, Train: 26.93%, Valid: 26.76%, Test: 27.03%
Epoch: 100, Loss: 1.5520, Train: 26.18%, Valid: 26.06%, Test: 26.27%
Epoch: 125, Loss: 1.6127, Train: 27.43%, Valid: 27.05%, Test: 27.50%
Epoch: 150, Loss: 1.5034, Train: 27.81%, Valid: 27.46%, Test: 27.90%
Epoch: 175, Loss: 1.5068, Train: 27.86%, Valid: 27.52%, Test: 28.00%
Epoch: 200, Loss: 1.5035, Train: 27.80%, Valid: 27.43%, Test: 28.00%
Epoch: 225, Loss: 1.4758, Train: 28.65%, Valid: 28.23%, Test: 28.80%
Epoch: 250, Loss: 1.4993, Train: 35.06%, Valid: 34.90%, Test: 35.00%
Epoch: 275, Loss: 1.4596, Train: 36.01%, Valid: 35.79%, Test: 35.87%
Epoch: 300, Loss: 1.4665, Train: 37.97%, Valid: 37.74%, Test: 37.75%
Epoch: 325, Loss: 1.4682, Train: 37.94%, Valid: 37.75%, Test: 37.78%
Epoch: 350, Loss: 1.4417, Train: 37.78%, Valid: 37.53%, Test: 37.49%
Epoch: 375, Loss: 1.4472, Train: 38.01%, Valid: 37.76%, Test: 37.67%
Epoch: 400, Loss: 1.4414, Train: 38.13%, Valid: 37.88%, Test: 37.78%
Epoch: 425, Loss: 1.4442, Train: 38.58%, Valid: 38.35%, Test: 38.27%
Epoch: 450, Loss: 1.4615, Train: 39.17%, Valid: 39.10%, Test: 38.88%
Epoch: 475, Loss: 1.4289, Train: 39.05%, Valid: 38.79%, Test: 38.69%
Epoch: 500, Loss: 1.4248, Train: 32.41%, Valid: 32.34%, Test: 32.39%
Epoch: 525, Loss: 1.4251, Train: 26.93%, Valid: 26.85%, Test: 26.80%
Epoch: 550, Loss: 1.4218, Train: 26.58%, Valid: 26.53%, Test: 26.44%
Epoch: 575, Loss: 1.4248, Train: 26.57%, Valid: 26.54%, Test: 26.46%
Epoch: 600, Loss: 1.4197, Train: 26.40%, Valid: 26.43%, Test: 26.25%
Epoch: 625, Loss: 1.4152, Train: 26.96%, Valid: 26.91%, Test: 26.79%
Epoch: 650, Loss: 1.4088, Train: 26.12%, Valid: 26.18%, Test: 26.00%
Epoch: 675, Loss: 1.4083, Train: 26.12%, Valid: 26.16%, Test: 25.97%
Epoch: 700, Loss: 1.4064, Train: 26.03%, Valid: 26.08%, Test: 25.90%
Epoch: 725, Loss: 1.4097, Train: 25.89%, Valid: 25.99%, Test: 25.76%
Epoch: 750, Loss: 1.4160, Train: 25.83%, Valid: 25.90%, Test: 25.72%
Epoch: 775, Loss: 1.4047, Train: 25.86%, Valid: 25.96%, Test: 25.75%
Epoch: 800, Loss: 1.4022, Train: 25.79%, Valid: 25.86%, Test: 25.66%
Epoch: 825, Loss: 1.4038, Train: 25.78%, Valid: 25.81%, Test: 25.63%
Epoch: 850, Loss: 1.4053, Train: 25.91%, Valid: 25.95%, Test: 25.73%
Epoch: 875, Loss: 1.4013, Train: 25.82%, Valid: 25.84%, Test: 25.74%
Epoch: 900, Loss: 1.4055, Train: 26.05%, Valid: 26.07%, Test: 25.91%
Epoch: 925, Loss: 1.3993, Train: 25.88%, Valid: 25.95%, Test: 25.77%
Epoch: 950, Loss: 1.4008, Train: 25.96%, Valid: 26.00%, Test: 25.88%
Epoch: 975, Loss: 1.4007, Train: 26.16%, Valid: 26.17%, Test: 25.99%
Run 01:
Highest Train: 39.17
Highest Valid: 39.11
  Final Train: 39.17
   Final Test: 38.91
All runs:
Highest Train: 39.17, nan
Highest Valid: 39.11, nan
  Final Train: 39.17, nan
   Final Test: 38.91, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6131, Train: 28.80%, Valid: 28.58%, Test: 28.90%
Epoch: 25, Loss: 1.5048, Train: 34.56%, Valid: 34.32%, Test: 34.62%
Epoch: 50, Loss: 1.4823, Train: 35.79%, Valid: 35.37%, Test: 35.62%
Epoch: 75, Loss: 1.4550, Train: 37.35%, Valid: 36.59%, Test: 36.83%
Epoch: 100, Loss: 1.4305, Train: 39.56%, Valid: 38.79%, Test: 38.94%
Epoch: 125, Loss: 1.4057, Train: 41.21%, Valid: 40.01%, Test: 40.56%
Epoch: 150, Loss: 1.3928, Train: 42.22%, Valid: 40.99%, Test: 41.37%
Epoch: 175, Loss: 1.3804, Train: 42.01%, Valid: 40.61%, Test: 40.97%
Epoch: 200, Loss: 1.3526, Train: 43.80%, Valid: 42.98%, Test: 42.84%
Epoch: 225, Loss: 1.3579, Train: 43.91%, Valid: 42.44%, Test: 42.79%
Epoch: 250, Loss: 1.3354, Train: 41.80%, Valid: 40.69%, Test: 40.92%
Epoch: 275, Loss: 1.3368, Train: 43.51%, Valid: 42.15%, Test: 42.37%
Epoch: 300, Loss: 1.3362, Train: 44.67%, Valid: 43.14%, Test: 43.27%
Epoch: 325, Loss: 1.3167, Train: 43.99%, Valid: 42.19%, Test: 42.44%
Epoch: 350, Loss: 1.3132, Train: 45.79%, Valid: 43.89%, Test: 44.29%
Epoch: 375, Loss: 1.3027, Train: 45.54%, Valid: 43.79%, Test: 44.17%
Epoch: 400, Loss: 1.3033, Train: 44.76%, Valid: 42.78%, Test: 43.30%
Epoch: 425, Loss: 1.2962, Train: 45.28%, Valid: 43.24%, Test: 43.66%
Epoch: 450, Loss: 1.2893, Train: 46.15%, Valid: 44.03%, Test: 44.29%
Epoch: 475, Loss: 1.2917, Train: 46.56%, Valid: 44.36%, Test: 44.67%
Epoch: 500, Loss: 1.2908, Train: 46.34%, Valid: 44.21%, Test: 44.75%
Epoch: 525, Loss: 1.2885, Train: 46.11%, Valid: 43.91%, Test: 44.14%
Epoch: 550, Loss: 1.2876, Train: 46.85%, Valid: 44.59%, Test: 45.00%
Epoch: 575, Loss: 1.2796, Train: 46.67%, Valid: 44.44%, Test: 44.71%
Epoch: 600, Loss: 1.2731, Train: 45.98%, Valid: 43.69%, Test: 43.77%
Epoch: 625, Loss: 1.2843, Train: 47.51%, Valid: 45.06%, Test: 45.39%
Epoch: 650, Loss: 1.2717, Train: 46.62%, Valid: 44.18%, Test: 44.28%
Epoch: 675, Loss: 1.2640, Train: 47.59%, Valid: 44.68%, Test: 45.16%
Epoch: 700, Loss: 1.2697, Train: 48.15%, Valid: 45.40%, Test: 45.65%
Epoch: 725, Loss: 1.2636, Train: 47.44%, Valid: 44.62%, Test: 44.94%
Epoch: 750, Loss: 1.2642, Train: 48.04%, Valid: 45.38%, Test: 45.52%
Epoch: 775, Loss: 1.2709, Train: 48.25%, Valid: 45.41%, Test: 45.70%
Epoch: 800, Loss: 1.2637, Train: 48.03%, Valid: 45.31%, Test: 45.49%
Epoch: 825, Loss: 1.2654, Train: 47.71%, Valid: 44.80%, Test: 45.22%
Epoch: 850, Loss: 1.2699, Train: 47.30%, Valid: 44.49%, Test: 44.68%
Epoch: 875, Loss: 1.2492, Train: 47.62%, Valid: 44.88%, Test: 45.12%
Epoch: 900, Loss: 1.2479, Train: 48.59%, Valid: 45.77%, Test: 45.83%
Epoch: 925, Loss: 1.2613, Train: 48.46%, Valid: 45.33%, Test: 45.57%
Epoch: 950, Loss: 1.2738, Train: 48.73%, Valid: 45.27%, Test: 45.77%
Epoch: 975, Loss: 1.2588, Train: 48.51%, Valid: 45.50%, Test: 45.60%
Run 01:
Highest Train: 49.07
Highest Valid: 45.85
  Final Train: 48.83
   Final Test: 46.03
All runs:
Highest Train: 49.07, nan
Highest Valid: 45.85, nan
  Final Train: 48.83, nan
   Final Test: 46.03, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.7651, Train: 27.92%, Valid: 27.64%, Test: 28.02%
Epoch: 25, Loss: 1.4964, Train: 26.98%, Valid: 27.10%, Test: 27.42%
Epoch: 50, Loss: 1.4310, Train: 30.77%, Valid: 30.85%, Test: 30.73%
Epoch: 75, Loss: 1.4045, Train: 25.97%, Valid: 25.91%, Test: 25.85%
Epoch: 100, Loss: 1.3917, Train: 30.70%, Valid: 30.81%, Test: 30.32%
Epoch: 125, Loss: 1.3907, Train: 34.20%, Valid: 34.41%, Test: 34.50%
Epoch: 150, Loss: 1.3690, Train: 25.30%, Valid: 25.34%, Test: 24.90%
Epoch: 175, Loss: 1.3990, Train: 29.62%, Valid: 29.93%, Test: 29.50%
Epoch: 200, Loss: 1.3672, Train: 36.05%, Valid: 36.32%, Test: 36.16%
Epoch: 225, Loss: 1.3673, Train: 32.41%, Valid: 32.54%, Test: 32.13%
Epoch: 250, Loss: 1.3974, Train: 33.27%, Valid: 33.42%, Test: 32.93%
Epoch: 275, Loss: 1.3754, Train: 38.62%, Valid: 38.67%, Test: 38.63%
Epoch: 300, Loss: 1.3771, Train: 35.07%, Valid: 35.21%, Test: 34.78%
Epoch: 325, Loss: 1.3419, Train: 34.41%, Valid: 34.40%, Test: 34.24%
Epoch: 350, Loss: 1.3400, Train: 36.24%, Valid: 36.23%, Test: 35.80%
Epoch: 375, Loss: 1.3421, Train: 35.05%, Valid: 34.93%, Test: 34.71%
Epoch: 400, Loss: 1.3322, Train: 36.19%, Valid: 36.32%, Test: 35.75%
Epoch: 425, Loss: 1.3296, Train: 37.13%, Valid: 37.36%, Test: 36.89%
Epoch: 450, Loss: 1.3156, Train: 35.07%, Valid: 35.11%, Test: 35.01%
Epoch: 475, Loss: 1.3494, Train: 39.99%, Valid: 39.91%, Test: 39.72%
Epoch: 500, Loss: 1.3327, Train: 40.23%, Valid: 40.40%, Test: 39.92%
Epoch: 525, Loss: 1.3340, Train: 42.85%, Valid: 42.98%, Test: 42.72%
Epoch: 550, Loss: 1.3162, Train: 37.72%, Valid: 38.03%, Test: 37.54%
Epoch: 575, Loss: 1.3331, Train: 40.36%, Valid: 40.63%, Test: 40.40%
Epoch: 600, Loss: 1.3275, Train: 41.74%, Valid: 41.73%, Test: 41.67%
Epoch: 625, Loss: 1.3070, Train: 39.40%, Valid: 39.57%, Test: 39.52%
Epoch: 650, Loss: 1.3161, Train: 39.95%, Valid: 39.88%, Test: 40.09%
Epoch: 675, Loss: 1.3850, Train: 37.28%, Valid: 37.34%, Test: 37.22%
Epoch: 700, Loss: 1.3378, Train: 44.48%, Valid: 44.62%, Test: 44.31%
Epoch: 725, Loss: 1.3105, Train: 43.40%, Valid: 43.40%, Test: 43.39%
Epoch: 750, Loss: 1.3249, Train: 39.41%, Valid: 39.47%, Test: 39.60%
Epoch: 775, Loss: 1.3212, Train: 43.10%, Valid: 43.17%, Test: 42.85%
Epoch: 800, Loss: 1.3195, Train: 40.42%, Valid: 40.39%, Test: 40.54%
Epoch: 825, Loss: 1.3163, Train: 44.08%, Valid: 43.99%, Test: 43.87%
Epoch: 850, Loss: 1.3070, Train: 36.29%, Valid: 36.39%, Test: 36.38%
Epoch: 875, Loss: 1.3343, Train: 41.20%, Valid: 40.97%, Test: 41.10%
Epoch: 900, Loss: 1.3011, Train: 42.50%, Valid: 42.31%, Test: 42.20%
Epoch: 925, Loss: 1.3226, Train: 43.88%, Valid: 43.94%, Test: 43.53%
Epoch: 950, Loss: 1.3299, Train: 38.42%, Valid: 38.13%, Test: 38.09%
Epoch: 975, Loss: 1.3126, Train: 41.05%, Valid: 41.13%, Test: 41.15%
Run 01:
Highest Train: 44.84
Highest Valid: 44.86
  Final Train: 44.84
   Final Test: 44.63
All runs:
Highest Train: 44.84, nan
Highest Valid: 44.86, nan
  Final Train: 44.84, nan
   Final Test: 44.63, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 89.2404, Train: 28.72%, Valid: 28.34%, Test: 28.53%
Epoch: 25, Loss: 1.9025, Train: 27.83%, Valid: 27.67%, Test: 27.87%
Epoch: 50, Loss: 1.7340, Train: 28.21%, Valid: 27.89%, Test: 28.24%
Epoch: 75, Loss: 1.6826, Train: 28.31%, Valid: 27.97%, Test: 28.37%
Epoch: 100, Loss: 1.6338, Train: 28.48%, Valid: 28.13%, Test: 28.58%
Epoch: 125, Loss: 1.5680, Train: 28.62%, Valid: 28.27%, Test: 28.67%
Epoch: 150, Loss: 1.6353, Train: 28.76%, Valid: 28.37%, Test: 28.85%
Epoch: 175, Loss: 1.5580, Train: 28.83%, Valid: 28.45%, Test: 28.90%
Epoch: 200, Loss: 1.5803, Train: 28.97%, Valid: 28.57%, Test: 29.01%
Epoch: 225, Loss: 1.5185, Train: 29.11%, Valid: 28.74%, Test: 29.23%
Epoch: 250, Loss: 1.5361, Train: 29.19%, Valid: 28.66%, Test: 29.27%
Epoch: 275, Loss: 1.4980, Train: 30.02%, Valid: 29.36%, Test: 30.15%
Epoch: 300, Loss: 1.4729, Train: 32.32%, Valid: 32.01%, Test: 32.73%
Epoch: 325, Loss: 1.4713, Train: 31.92%, Valid: 31.79%, Test: 32.34%
Epoch: 350, Loss: 1.4656, Train: 32.73%, Valid: 32.53%, Test: 33.21%
Epoch: 375, Loss: 1.4525, Train: 38.30%, Valid: 37.99%, Test: 38.13%
Epoch: 400, Loss: 1.4396, Train: 38.25%, Valid: 38.04%, Test: 38.05%
Epoch: 425, Loss: 1.4443, Train: 38.23%, Valid: 38.06%, Test: 38.09%
Epoch: 450, Loss: 1.4366, Train: 38.21%, Valid: 38.04%, Test: 38.00%
Epoch: 475, Loss: 1.4356, Train: 38.26%, Valid: 38.02%, Test: 38.13%
Epoch: 500, Loss: 1.4354, Train: 38.16%, Valid: 37.96%, Test: 37.89%
Epoch: 525, Loss: 1.4279, Train: 38.28%, Valid: 38.10%, Test: 38.03%
Epoch: 550, Loss: 1.4208, Train: 38.29%, Valid: 38.08%, Test: 37.91%
Epoch: 575, Loss: 1.4165, Train: 38.07%, Valid: 37.87%, Test: 37.80%
Epoch: 600, Loss: 1.4208, Train: 37.90%, Valid: 37.78%, Test: 37.71%
Epoch: 625, Loss: 1.4221, Train: 38.13%, Valid: 37.91%, Test: 37.88%
Epoch: 650, Loss: 1.4209, Train: 36.85%, Valid: 36.64%, Test: 36.81%
Epoch: 675, Loss: 1.4132, Train: 37.71%, Valid: 37.36%, Test: 37.53%
Epoch: 700, Loss: 1.4113, Train: 27.87%, Valid: 27.86%, Test: 27.97%
Epoch: 725, Loss: 1.4102, Train: 29.55%, Valid: 29.24%, Test: 29.54%
Epoch: 750, Loss: 1.4046, Train: 21.54%, Valid: 21.48%, Test: 21.24%
Epoch: 775, Loss: 1.4052, Train: 22.03%, Valid: 21.98%, Test: 21.73%
Epoch: 800, Loss: 1.4036, Train: 23.03%, Valid: 23.03%, Test: 22.80%
Epoch: 825, Loss: 1.4021, Train: 23.49%, Valid: 23.56%, Test: 23.32%
Epoch: 850, Loss: 1.4056, Train: 21.83%, Valid: 21.65%, Test: 21.58%
Epoch: 875, Loss: 1.3937, Train: 26.28%, Valid: 26.15%, Test: 26.23%
Epoch: 900, Loss: 1.3909, Train: 23.25%, Valid: 23.05%, Test: 23.00%
Epoch: 925, Loss: 1.3866, Train: 23.25%, Valid: 22.94%, Test: 23.00%
Epoch: 950, Loss: 1.3789, Train: 21.72%, Valid: 21.56%, Test: 21.35%
Epoch: 975, Loss: 1.4151, Train: 25.51%, Valid: 25.26%, Test: 25.25%
Run 01:
Highest Train: 38.68
Highest Valid: 38.50
  Final Train: 38.65
   Final Test: 38.44
All runs:
Highest Train: 38.68, nan
Highest Valid: 38.50, nan
  Final Train: 38.65, nan
   Final Test: 38.44, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6268, Train: 23.21%, Valid: 23.02%, Test: 23.00%
Epoch: 25, Loss: 1.4833, Train: 35.03%, Valid: 34.72%, Test: 34.97%
Epoch: 50, Loss: 1.4529, Train: 37.26%, Valid: 36.77%, Test: 37.19%
Epoch: 75, Loss: 1.4241, Train: 38.97%, Valid: 38.59%, Test: 38.81%
Epoch: 100, Loss: 1.3936, Train: 40.05%, Valid: 39.63%, Test: 39.82%
Epoch: 125, Loss: 1.3670, Train: 41.69%, Valid: 40.97%, Test: 41.12%
Epoch: 150, Loss: 1.3617, Train: 42.22%, Valid: 41.48%, Test: 41.59%
Epoch: 175, Loss: 1.3327, Train: 43.28%, Valid: 42.37%, Test: 42.31%
Epoch: 200, Loss: 1.3228, Train: 43.91%, Valid: 42.67%, Test: 42.70%
Epoch: 225, Loss: 1.3071, Train: 44.47%, Valid: 43.21%, Test: 43.30%
Epoch: 250, Loss: 1.3267, Train: 43.91%, Valid: 42.50%, Test: 42.89%
Epoch: 275, Loss: 1.2869, Train: 45.26%, Valid: 43.74%, Test: 44.05%
Epoch: 300, Loss: 1.2810, Train: 45.90%, Valid: 43.74%, Test: 44.11%
Epoch: 325, Loss: 1.2591, Train: 46.57%, Valid: 44.25%, Test: 44.52%
Epoch: 350, Loss: 1.2417, Train: 47.27%, Valid: 44.47%, Test: 44.96%
Epoch: 375, Loss: 1.2175, Train: 47.77%, Valid: 44.81%, Test: 44.99%
Epoch: 400, Loss: 1.2120, Train: 48.35%, Valid: 45.00%, Test: 45.29%
Epoch: 425, Loss: 1.2172, Train: 49.19%, Valid: 45.20%, Test: 45.37%
Epoch: 450, Loss: 1.2098, Train: 49.08%, Valid: 45.42%, Test: 45.33%
Epoch: 475, Loss: 1.2550, Train: 47.79%, Valid: 44.61%, Test: 44.42%
Epoch: 500, Loss: 1.1573, Train: 51.03%, Valid: 46.35%, Test: 46.43%
Epoch: 525, Loss: 1.1440, Train: 51.70%, Valid: 46.75%, Test: 47.02%
Epoch: 550, Loss: 1.1722, Train: 49.60%, Valid: 45.48%, Test: 45.53%
Epoch: 575, Loss: 1.1283, Train: 52.18%, Valid: 47.01%, Test: 47.23%
Epoch: 600, Loss: 1.1280, Train: 52.20%, Valid: 46.90%, Test: 47.03%
Epoch: 625, Loss: 1.1603, Train: 51.12%, Valid: 46.14%, Test: 46.29%
Epoch: 650, Loss: 1.0987, Train: 53.27%, Valid: 47.47%, Test: 47.62%
Epoch: 675, Loss: 1.1146, Train: 52.50%, Valid: 47.26%, Test: 47.38%
Epoch: 700, Loss: 1.0923, Train: 53.09%, Valid: 47.16%, Test: 47.45%
Epoch: 725, Loss: 1.0943, Train: 53.58%, Valid: 47.33%, Test: 47.54%
Epoch: 750, Loss: 1.1235, Train: 52.22%, Valid: 46.38%, Test: 46.66%
Epoch: 775, Loss: 1.0772, Train: 53.78%, Valid: 47.04%, Test: 47.07%
Epoch: 800, Loss: 1.0829, Train: 53.99%, Valid: 47.61%, Test: 47.88%
Epoch: 825, Loss: 1.0883, Train: 52.43%, Valid: 47.07%, Test: 47.37%
Epoch: 850, Loss: 1.0631, Train: 54.56%, Valid: 47.58%, Test: 47.78%
Epoch: 875, Loss: 1.0607, Train: 54.69%, Valid: 47.62%, Test: 47.67%
Epoch: 900, Loss: 1.0735, Train: 54.06%, Valid: 47.04%, Test: 47.36%
Epoch: 925, Loss: 1.0558, Train: 54.96%, Valid: 47.66%, Test: 47.51%
Epoch: 950, Loss: 1.0769, Train: 54.17%, Valid: 47.08%, Test: 46.95%
Epoch: 975, Loss: 1.0633, Train: 54.85%, Valid: 47.66%, Test: 47.99%
Run 01:
Highest Train: 55.68
Highest Valid: 48.00
  Final Train: 55.25
   Final Test: 48.10
All runs:
Highest Train: 55.68, nan
Highest Valid: 48.00, nan
  Final Train: 55.25, nan
   Final Test: 48.10, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.2493, Train: 22.04%, Valid: 22.40%, Test: 22.01%
Epoch: 25, Loss: 1.4739, Train: 28.79%, Valid: 28.72%, Test: 29.04%
Epoch: 50, Loss: 1.4993, Train: 31.09%, Valid: 31.42%, Test: 31.78%
Epoch: 75, Loss: 1.4514, Train: 33.70%, Valid: 33.49%, Test: 34.33%
Epoch: 100, Loss: 1.4313, Train: 33.53%, Valid: 33.34%, Test: 33.97%
Epoch: 125, Loss: 1.4341, Train: 30.84%, Valid: 30.65%, Test: 31.04%
Epoch: 150, Loss: 1.4421, Train: 34.00%, Valid: 33.56%, Test: 34.49%
Epoch: 175, Loss: 1.4183, Train: 34.50%, Valid: 34.17%, Test: 34.96%
Epoch: 200, Loss: 1.4022, Train: 35.05%, Valid: 34.73%, Test: 35.64%
Epoch: 225, Loss: 1.4640, Train: 34.60%, Valid: 34.43%, Test: 35.34%
Epoch: 250, Loss: 1.4170, Train: 38.78%, Valid: 38.25%, Test: 38.61%
Epoch: 275, Loss: 1.3867, Train: 38.44%, Valid: 38.10%, Test: 38.31%
Epoch: 300, Loss: 1.3858, Train: 39.36%, Valid: 39.17%, Test: 39.22%
Epoch: 325, Loss: 1.3723, Train: 40.18%, Valid: 40.03%, Test: 40.25%
Epoch: 350, Loss: 1.3643, Train: 40.55%, Valid: 40.40%, Test: 40.57%
Epoch: 375, Loss: 1.3567, Train: 40.88%, Valid: 40.75%, Test: 40.94%
Epoch: 400, Loss: 1.4408, Train: 35.49%, Valid: 34.96%, Test: 35.11%
Epoch: 425, Loss: 1.3878, Train: 39.41%, Valid: 39.12%, Test: 39.04%
Epoch: 450, Loss: 1.3648, Train: 40.42%, Valid: 40.02%, Test: 40.20%
Epoch: 475, Loss: 1.3827, Train: 40.68%, Valid: 40.39%, Test: 40.30%
Epoch: 500, Loss: 1.3529, Train: 40.82%, Valid: 40.63%, Test: 40.85%
Epoch: 525, Loss: 1.3407, Train: 41.66%, Valid: 41.32%, Test: 41.54%
Epoch: 550, Loss: 1.3319, Train: 42.15%, Valid: 41.72%, Test: 41.98%
Epoch: 575, Loss: 1.3278, Train: 42.72%, Valid: 42.45%, Test: 42.55%
Epoch: 600, Loss: 1.3368, Train: 41.83%, Valid: 41.26%, Test: 41.68%
Epoch: 625, Loss: 1.3216, Train: 41.32%, Valid: 41.20%, Test: 41.37%
Epoch: 650, Loss: 1.3166, Train: 43.24%, Valid: 42.83%, Test: 43.10%
Epoch: 675, Loss: 1.3034, Train: 43.40%, Valid: 43.25%, Test: 43.54%
Epoch: 700, Loss: 1.3900, Train: 39.66%, Valid: 39.10%, Test: 39.73%
Epoch: 725, Loss: 1.3515, Train: 41.37%, Valid: 40.92%, Test: 41.41%
Epoch: 750, Loss: 1.3321, Train: 42.52%, Valid: 42.23%, Test: 42.44%
Epoch: 775, Loss: 1.3215, Train: 42.98%, Valid: 42.53%, Test: 42.90%
Epoch: 800, Loss: 1.3266, Train: 43.36%, Valid: 42.77%, Test: 43.14%
Epoch: 825, Loss: 1.3079, Train: 43.72%, Valid: 43.09%, Test: 43.40%
Epoch: 850, Loss: 1.3019, Train: 44.09%, Valid: 43.39%, Test: 43.72%
Epoch: 875, Loss: 1.3017, Train: 44.13%, Valid: 43.49%, Test: 43.81%
Epoch: 900, Loss: 1.2884, Train: 44.30%, Valid: 43.73%, Test: 43.99%
Epoch: 925, Loss: 1.2910, Train: 44.54%, Valid: 43.92%, Test: 44.28%
Epoch: 950, Loss: 1.2859, Train: 44.64%, Valid: 44.16%, Test: 44.41%
Epoch: 975, Loss: 1.2900, Train: 44.75%, Valid: 44.04%, Test: 44.19%
Run 01:
Highest Train: 45.04
Highest Valid: 44.40
  Final Train: 45.01
   Final Test: 44.63
All runs:
Highest Train: 45.04, nan
Highest Valid: 44.40, nan
  Final Train: 45.01, nan
   Final Test: 44.63, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.8073, Train: 17.96%, Valid: 18.18%, Test: 17.68%
Epoch: 25, Loss: 1.4709, Train: 28.41%, Valid: 28.17%, Test: 28.50%
Epoch: 50, Loss: 1.4332, Train: 28.74%, Valid: 28.56%, Test: 28.86%
Epoch: 75, Loss: 1.4230, Train: 29.05%, Valid: 28.76%, Test: 29.15%
Epoch: 100, Loss: 1.5038, Train: 24.76%, Valid: 24.78%, Test: 24.62%
Epoch: 125, Loss: 1.4730, Train: 30.20%, Valid: 30.13%, Test: 30.60%
Epoch: 150, Loss: 1.4520, Train: 30.14%, Valid: 29.83%, Test: 30.36%
Epoch: 175, Loss: 1.4395, Train: 30.08%, Valid: 29.82%, Test: 30.22%
Epoch: 200, Loss: 1.4291, Train: 30.12%, Valid: 29.95%, Test: 30.26%
Epoch: 225, Loss: 1.4213, Train: 29.98%, Valid: 29.94%, Test: 29.94%
Epoch: 250, Loss: 1.4128, Train: 30.01%, Valid: 29.96%, Test: 30.02%
Epoch: 275, Loss: 1.4319, Train: 30.36%, Valid: 30.30%, Test: 30.73%
Epoch: 300, Loss: 1.4016, Train: 32.42%, Valid: 32.49%, Test: 32.55%
Epoch: 325, Loss: 1.3919, Train: 40.22%, Valid: 40.22%, Test: 40.08%
Epoch: 350, Loss: 1.3828, Train: 39.93%, Valid: 39.88%, Test: 39.64%
Epoch: 375, Loss: 1.4060, Train: 39.97%, Valid: 40.01%, Test: 39.91%
Epoch: 400, Loss: 1.3865, Train: 40.37%, Valid: 40.30%, Test: 40.19%
Epoch: 425, Loss: 1.3758, Train: 40.16%, Valid: 40.07%, Test: 39.90%
Epoch: 450, Loss: 1.3707, Train: 40.13%, Valid: 40.16%, Test: 40.00%
Epoch: 475, Loss: 1.3641, Train: 40.22%, Valid: 40.21%, Test: 40.04%
Epoch: 500, Loss: 1.3602, Train: 40.55%, Valid: 40.49%, Test: 40.37%
Epoch: 525, Loss: 1.3853, Train: 39.35%, Valid: 39.23%, Test: 39.16%
Epoch: 550, Loss: 1.3713, Train: 40.11%, Valid: 40.05%, Test: 39.86%
Epoch: 575, Loss: 1.3646, Train: 40.07%, Valid: 40.02%, Test: 39.80%
Epoch: 600, Loss: 1.3613, Train: 40.65%, Valid: 40.75%, Test: 40.48%
Epoch: 625, Loss: 1.8291, Train: 37.18%, Valid: 36.96%, Test: 37.04%
Epoch: 650, Loss: 1.5258, Train: 28.70%, Valid: 28.48%, Test: 28.77%
Epoch: 675, Loss: 1.4538, Train: 38.21%, Valid: 37.96%, Test: 37.93%
Epoch: 700, Loss: 1.4113, Train: 38.81%, Valid: 38.40%, Test: 38.52%
Epoch: 725, Loss: 1.3944, Train: 39.03%, Valid: 38.69%, Test: 38.73%
Epoch: 750, Loss: 1.3805, Train: 39.24%, Valid: 39.00%, Test: 38.96%
Epoch: 775, Loss: 1.3708, Train: 40.03%, Valid: 39.75%, Test: 39.65%
Epoch: 800, Loss: 1.3795, Train: 39.69%, Valid: 39.46%, Test: 39.38%
Epoch: 825, Loss: 1.3627, Train: 40.74%, Valid: 40.61%, Test: 40.61%
Epoch: 850, Loss: 1.3616, Train: 40.32%, Valid: 40.01%, Test: 40.08%
Epoch: 875, Loss: 1.3567, Train: 40.56%, Valid: 40.51%, Test: 40.56%
Epoch: 900, Loss: 1.3535, Train: 40.90%, Valid: 40.76%, Test: 40.81%
Epoch: 925, Loss: 1.3612, Train: 41.01%, Valid: 40.94%, Test: 40.91%
Epoch: 950, Loss: 1.3527, Train: 41.10%, Valid: 41.04%, Test: 40.99%
Epoch: 975, Loss: 1.3527, Train: 41.29%, Valid: 41.07%, Test: 41.22%
Run 01:
Highest Train: 41.65
Highest Valid: 41.52
  Final Train: 41.50
   Final Test: 41.58
All runs:
Highest Train: 41.65, nan
Highest Valid: 41.52, nan
  Final Train: 41.50, nan
   Final Test: 41.58, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6256, Train: 28.62%, Valid: 28.42%, Test: 28.70%
Epoch: 25, Loss: 1.4770, Train: 35.16%, Valid: 34.80%, Test: 35.05%
Epoch: 50, Loss: 1.4409, Train: 37.09%, Valid: 36.75%, Test: 36.86%
Epoch: 75, Loss: 1.4215, Train: 38.60%, Valid: 37.93%, Test: 38.32%
Epoch: 100, Loss: 1.3956, Train: 40.39%, Valid: 39.38%, Test: 39.69%
Epoch: 125, Loss: 1.3673, Train: 41.62%, Valid: 40.77%, Test: 40.71%
Epoch: 150, Loss: 1.3550, Train: 42.00%, Valid: 41.23%, Test: 41.02%
Epoch: 175, Loss: 1.3236, Train: 43.50%, Valid: 42.57%, Test: 42.32%
Epoch: 200, Loss: 1.3133, Train: 43.46%, Valid: 42.27%, Test: 42.16%
Epoch: 225, Loss: 1.2730, Train: 45.70%, Valid: 43.94%, Test: 43.95%
Epoch: 250, Loss: 1.2672, Train: 46.07%, Valid: 44.21%, Test: 44.15%
Epoch: 275, Loss: 1.2305, Train: 47.75%, Valid: 45.31%, Test: 45.22%
Epoch: 300, Loss: 1.2282, Train: 47.95%, Valid: 45.35%, Test: 45.53%
Epoch: 325, Loss: 1.2031, Train: 48.81%, Valid: 45.66%, Test: 45.56%
Epoch: 350, Loss: 1.1788, Train: 49.92%, Valid: 46.53%, Test: 46.37%
Epoch: 375, Loss: 1.1776, Train: 50.03%, Valid: 46.53%, Test: 46.48%
Epoch: 400, Loss: 1.1417, Train: 51.34%, Valid: 46.59%, Test: 47.01%
Epoch: 425, Loss: 1.1363, Train: 51.94%, Valid: 46.86%, Test: 47.03%
Epoch: 450, Loss: 1.1148, Train: 52.50%, Valid: 47.01%, Test: 47.12%
Epoch: 475, Loss: 1.2013, Train: 49.15%, Valid: 44.98%, Test: 45.35%
Epoch: 500, Loss: 1.0945, Train: 53.66%, Valid: 47.61%, Test: 47.75%
Epoch: 525, Loss: 1.0774, Train: 54.44%, Valid: 47.56%, Test: 47.77%
Epoch: 550, Loss: 1.0608, Train: 54.82%, Valid: 47.54%, Test: 47.62%
Epoch: 575, Loss: 1.0473, Train: 55.27%, Valid: 47.29%, Test: 47.42%
Epoch: 600, Loss: 1.0508, Train: 55.91%, Valid: 47.42%, Test: 47.60%
Epoch: 625, Loss: 1.0222, Train: 56.97%, Valid: 47.74%, Test: 47.94%
Epoch: 650, Loss: 1.0186, Train: 57.21%, Valid: 48.03%, Test: 48.34%
Epoch: 675, Loss: 1.0180, Train: 56.78%, Valid: 47.37%, Test: 47.96%
Epoch: 700, Loss: 1.0340, Train: 56.07%, Valid: 47.15%, Test: 47.66%
Epoch: 725, Loss: 1.0574, Train: 54.74%, Valid: 45.63%, Test: 46.05%
Epoch: 750, Loss: 0.9930, Train: 58.04%, Valid: 48.30%, Test: 48.44%
Epoch: 775, Loss: 0.9773, Train: 58.78%, Valid: 47.91%, Test: 48.08%
Epoch: 800, Loss: 0.9806, Train: 58.76%, Valid: 47.88%, Test: 48.31%
Epoch: 825, Loss: 0.9734, Train: 59.14%, Valid: 47.71%, Test: 48.06%
Epoch: 850, Loss: 0.9859, Train: 58.98%, Valid: 47.87%, Test: 48.06%
Epoch: 875, Loss: 0.9599, Train: 59.73%, Valid: 48.08%, Test: 48.34%
Epoch: 900, Loss: 0.9713, Train: 59.06%, Valid: 47.93%, Test: 48.18%
Epoch: 925, Loss: 0.9559, Train: 59.80%, Valid: 47.25%, Test: 47.41%
Epoch: 950, Loss: 0.9477, Train: 60.38%, Valid: 48.24%, Test: 48.43%
Epoch: 975, Loss: 0.9609, Train: 59.95%, Valid: 48.05%, Test: 48.19%
Run 01:
Highest Train: 61.10
Highest Valid: 48.39
  Final Train: 58.75
   Final Test: 48.50
All runs:
Highest Train: 61.10, nan
Highest Valid: 48.39, nan
  Final Train: 58.75, nan
   Final Test: 48.50, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.9761, Train: 27.01%, Valid: 26.97%, Test: 27.13%
Epoch: 25, Loss: 1.4655, Train: 32.83%, Valid: 32.29%, Test: 33.46%
Epoch: 50, Loss: 1.4431, Train: 31.96%, Valid: 31.55%, Test: 32.39%
Epoch: 75, Loss: 1.4045, Train: 40.15%, Valid: 40.11%, Test: 40.16%
Epoch: 100, Loss: 1.3711, Train: 41.45%, Valid: 41.26%, Test: 41.40%
Epoch: 125, Loss: 1.3576, Train: 42.13%, Valid: 42.13%, Test: 42.17%
Epoch: 150, Loss: 1.3502, Train: 42.77%, Valid: 42.69%, Test: 42.88%
Epoch: 175, Loss: 1.3272, Train: 43.55%, Valid: 43.42%, Test: 43.60%
Epoch: 200, Loss: 1.3424, Train: 43.44%, Valid: 43.01%, Test: 43.30%
Epoch: 225, Loss: 1.3188, Train: 44.25%, Valid: 43.87%, Test: 43.93%
Epoch: 250, Loss: 1.3098, Train: 44.92%, Valid: 44.51%, Test: 44.70%
Epoch: 275, Loss: 1.3442, Train: 43.72%, Valid: 43.27%, Test: 43.42%
Epoch: 300, Loss: 1.3137, Train: 44.23%, Valid: 43.90%, Test: 44.08%
Epoch: 325, Loss: 1.3071, Train: 45.17%, Valid: 44.61%, Test: 44.71%
Epoch: 350, Loss: 1.2982, Train: 44.92%, Valid: 44.58%, Test: 44.82%
Epoch: 375, Loss: 1.2872, Train: 45.20%, Valid: 44.80%, Test: 44.80%
Epoch: 400, Loss: 1.3214, Train: 43.89%, Valid: 43.53%, Test: 43.59%
Epoch: 425, Loss: 1.2989, Train: 44.87%, Valid: 44.57%, Test: 44.70%
Epoch: 450, Loss: 1.2890, Train: 45.50%, Valid: 44.93%, Test: 45.21%
Epoch: 475, Loss: 1.2811, Train: 45.95%, Valid: 45.44%, Test: 45.54%
Epoch: 500, Loss: 1.2834, Train: 45.79%, Valid: 45.30%, Test: 45.40%
Epoch: 525, Loss: 1.2707, Train: 46.25%, Valid: 45.63%, Test: 45.71%
Epoch: 550, Loss: 1.2879, Train: 45.29%, Valid: 44.83%, Test: 44.97%
Epoch: 575, Loss: 1.2696, Train: 46.18%, Valid: 45.70%, Test: 45.82%
Epoch: 600, Loss: 1.2701, Train: 46.48%, Valid: 45.78%, Test: 45.85%
Epoch: 625, Loss: 1.2721, Train: 46.37%, Valid: 45.86%, Test: 45.99%
Epoch: 650, Loss: 1.2761, Train: 45.75%, Valid: 45.38%, Test: 45.46%
Epoch: 675, Loss: 1.2566, Train: 46.73%, Valid: 46.01%, Test: 46.25%
Epoch: 700, Loss: 1.2861, Train: 46.38%, Valid: 45.71%, Test: 45.81%
Epoch: 725, Loss: 1.2595, Train: 46.43%, Valid: 45.92%, Test: 46.01%
Epoch: 750, Loss: 1.2500, Train: 46.41%, Valid: 45.84%, Test: 46.08%
Epoch: 775, Loss: 1.2612, Train: 46.61%, Valid: 46.03%, Test: 46.12%
Epoch: 800, Loss: 1.2543, Train: 46.26%, Valid: 45.77%, Test: 45.94%
Epoch: 825, Loss: 1.2497, Train: 47.00%, Valid: 46.28%, Test: 46.63%
Epoch: 850, Loss: 1.2416, Train: 47.27%, Valid: 46.42%, Test: 46.75%
Epoch: 875, Loss: 1.2608, Train: 45.64%, Valid: 45.15%, Test: 45.22%
Epoch: 900, Loss: 1.2382, Train: 47.31%, Valid: 46.63%, Test: 46.91%
Epoch: 925, Loss: 1.2388, Train: 47.54%, Valid: 46.69%, Test: 46.91%
Epoch: 950, Loss: 1.2565, Train: 47.40%, Valid: 46.69%, Test: 46.89%
Epoch: 975, Loss: 1.2350, Train: 47.40%, Valid: 46.73%, Test: 47.04%
Run 01:
Highest Train: 47.78
Highest Valid: 46.93
  Final Train: 47.69
   Final Test: 47.06
All runs:
Highest Train: 47.78, nan
Highest Valid: 46.93, nan
  Final Train: 47.69, nan
   Final Test: 47.06, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 5.0843, Train: 26.70%, Valid: 26.58%, Test: 26.49%
Epoch: 25, Loss: 1.5012, Train: 30.70%, Valid: 30.28%, Test: 30.80%
Epoch: 50, Loss: 1.4602, Train: 28.67%, Valid: 28.50%, Test: 28.78%
Epoch: 75, Loss: 1.4476, Train: 28.82%, Valid: 28.64%, Test: 28.90%
Epoch: 100, Loss: 1.4420, Train: 29.78%, Valid: 29.63%, Test: 29.89%
Epoch: 125, Loss: 1.4362, Train: 31.48%, Valid: 31.20%, Test: 31.77%
Epoch: 150, Loss: 1.4701, Train: 28.69%, Valid: 28.50%, Test: 28.78%
Epoch: 175, Loss: 1.4477, Train: 28.82%, Valid: 28.59%, Test: 28.87%
Epoch: 200, Loss: 1.4407, Train: 29.15%, Valid: 28.93%, Test: 29.24%
Epoch: 225, Loss: 1.4374, Train: 29.47%, Valid: 29.26%, Test: 29.59%
Epoch: 250, Loss: 1.4343, Train: 29.76%, Valid: 29.53%, Test: 29.87%
Epoch: 275, Loss: 1.4306, Train: 30.01%, Valid: 29.71%, Test: 30.13%
Epoch: 300, Loss: 1.5739, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 325, Loss: 1.4544, Train: 28.71%, Valid: 28.55%, Test: 28.80%
Epoch: 350, Loss: 1.4465, Train: 28.68%, Valid: 28.51%, Test: 28.79%
Epoch: 375, Loss: 1.4368, Train: 28.71%, Valid: 28.52%, Test: 28.80%
Epoch: 400, Loss: 1.5186, Train: 28.71%, Valid: 28.56%, Test: 28.76%
Epoch: 425, Loss: 1.4393, Train: 30.45%, Valid: 30.09%, Test: 30.75%
Epoch: 450, Loss: 1.4293, Train: 31.15%, Valid: 30.75%, Test: 31.58%
Epoch: 475, Loss: 1.4408, Train: 29.55%, Valid: 29.27%, Test: 29.76%
Epoch: 500, Loss: 1.4207, Train: 30.42%, Valid: 30.11%, Test: 30.50%
Epoch: 525, Loss: 1.4103, Train: 30.84%, Valid: 30.68%, Test: 30.87%
Epoch: 550, Loss: 1.4133, Train: 28.64%, Valid: 28.44%, Test: 28.99%
Epoch: 575, Loss: 1.4299, Train: 33.32%, Valid: 32.84%, Test: 33.45%
Epoch: 600, Loss: 1.4117, Train: 32.36%, Valid: 31.93%, Test: 32.37%
Epoch: 625, Loss: 1.4041, Train: 32.53%, Valid: 32.20%, Test: 32.82%
Epoch: 650, Loss: 1.3989, Train: 32.55%, Valid: 32.32%, Test: 32.85%
Epoch: 675, Loss: 1.3942, Train: 33.26%, Valid: 33.01%, Test: 33.50%
Epoch: 700, Loss: 1.3937, Train: 33.57%, Valid: 33.28%, Test: 34.02%
Epoch: 725, Loss: 1.4176, Train: 31.91%, Valid: 31.77%, Test: 32.13%
Epoch: 750, Loss: 1.3983, Train: 32.55%, Valid: 32.21%, Test: 32.70%
Epoch: 775, Loss: 1.3896, Train: 33.05%, Valid: 32.85%, Test: 33.44%
Epoch: 800, Loss: 1.3843, Train: 34.10%, Valid: 33.83%, Test: 34.47%
Epoch: 825, Loss: 1.3808, Train: 34.46%, Valid: 34.20%, Test: 35.04%
Epoch: 850, Loss: 1.4093, Train: 31.51%, Valid: 31.25%, Test: 31.39%
Epoch: 875, Loss: 1.3881, Train: 32.35%, Valid: 32.17%, Test: 32.65%
Epoch: 900, Loss: 1.3784, Train: 36.45%, Valid: 36.40%, Test: 36.84%
Epoch: 925, Loss: 1.3738, Train: 38.21%, Valid: 38.00%, Test: 38.23%
Epoch: 950, Loss: 1.7013, Train: 28.78%, Valid: 28.73%, Test: 29.42%
Epoch: 975, Loss: 1.4831, Train: 30.76%, Valid: 30.70%, Test: 31.28%
Run 01:
Highest Train: 40.71
Highest Valid: 40.47
  Final Train: 40.71
   Final Test: 40.91
All runs:
Highest Train: 40.71, nan
Highest Valid: 40.47, nan
  Final Train: 40.71, nan
   Final Test: 40.91, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6074, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4951, Train: 34.98%, Valid: 34.52%, Test: 34.76%
Epoch: 50, Loss: 1.4626, Train: 36.90%, Valid: 36.36%, Test: 36.78%
Epoch: 75, Loss: 1.4421, Train: 38.61%, Valid: 37.94%, Test: 38.25%
Epoch: 100, Loss: 1.4260, Train: 40.14%, Valid: 39.35%, Test: 39.53%
Epoch: 125, Loss: 1.3872, Train: 41.12%, Valid: 40.44%, Test: 40.43%
Epoch: 150, Loss: 1.3764, Train: 42.50%, Valid: 41.67%, Test: 41.44%
Epoch: 175, Loss: 1.3605, Train: 42.74%, Valid: 41.50%, Test: 41.67%
Epoch: 200, Loss: 1.3444, Train: 43.77%, Valid: 42.42%, Test: 42.48%
Epoch: 225, Loss: 1.3560, Train: 43.88%, Valid: 42.45%, Test: 42.71%
Epoch: 250, Loss: 1.3301, Train: 44.30%, Valid: 43.01%, Test: 42.93%
Epoch: 275, Loss: 1.3383, Train: 43.74%, Valid: 42.45%, Test: 42.63%
Epoch: 300, Loss: 1.3295, Train: 45.24%, Valid: 44.05%, Test: 44.03%
Epoch: 325, Loss: 1.3125, Train: 45.41%, Valid: 43.90%, Test: 43.95%
Epoch: 350, Loss: 1.3275, Train: 44.35%, Valid: 43.25%, Test: 43.42%
Epoch: 375, Loss: 1.3054, Train: 45.62%, Valid: 44.50%, Test: 44.45%
Epoch: 400, Loss: 1.3050, Train: 45.94%, Valid: 44.39%, Test: 44.74%
Epoch: 425, Loss: 1.2960, Train: 45.55%, Valid: 44.10%, Test: 44.33%
Epoch: 450, Loss: 1.2879, Train: 46.05%, Valid: 44.54%, Test: 44.85%
Epoch: 475, Loss: 1.2954, Train: 45.93%, Valid: 44.63%, Test: 44.61%
Epoch: 500, Loss: 1.3062, Train: 45.52%, Valid: 44.15%, Test: 44.29%
Epoch: 525, Loss: 1.2924, Train: 45.89%, Valid: 44.59%, Test: 44.56%
Epoch: 550, Loss: 1.2800, Train: 45.57%, Valid: 44.21%, Test: 44.43%
Epoch: 575, Loss: 1.2847, Train: 45.77%, Valid: 44.50%, Test: 44.54%
Epoch: 600, Loss: 1.2749, Train: 46.02%, Valid: 44.61%, Test: 44.69%
Epoch: 625, Loss: 1.2735, Train: 46.51%, Valid: 44.87%, Test: 45.00%
Epoch: 650, Loss: 1.2821, Train: 46.13%, Valid: 44.50%, Test: 44.62%
Epoch: 675, Loss: 1.2724, Train: 46.27%, Valid: 44.61%, Test: 45.00%
Epoch: 700, Loss: 1.2723, Train: 45.94%, Valid: 44.35%, Test: 44.59%
Epoch: 725, Loss: 1.2698, Train: 45.84%, Valid: 44.21%, Test: 44.36%
Epoch: 750, Loss: 1.2809, Train: 45.97%, Valid: 44.30%, Test: 44.59%
Epoch: 775, Loss: 1.2643, Train: 46.12%, Valid: 44.56%, Test: 44.76%
Epoch: 800, Loss: 1.2729, Train: 45.98%, Valid: 44.22%, Test: 44.42%
Epoch: 825, Loss: 1.2619, Train: 46.35%, Valid: 44.54%, Test: 44.82%
Epoch: 850, Loss: 1.2541, Train: 46.97%, Valid: 45.26%, Test: 45.47%
Epoch: 875, Loss: 1.2624, Train: 46.12%, Valid: 44.70%, Test: 44.62%
Epoch: 900, Loss: 1.2622, Train: 46.57%, Valid: 44.94%, Test: 44.93%
Epoch: 925, Loss: 1.2709, Train: 46.91%, Valid: 45.03%, Test: 45.25%
Epoch: 950, Loss: 1.2663, Train: 46.71%, Valid: 44.88%, Test: 45.15%
Epoch: 975, Loss: 1.2589, Train: 47.30%, Valid: 45.14%, Test: 45.63%
Run 01:
Highest Train: 47.54
Highest Valid: 45.57
  Final Train: 47.53
   Final Test: 45.86
All runs:
Highest Train: 47.54, nan
Highest Valid: 45.57, nan
  Final Train: 47.53, nan
   Final Test: 45.86, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.8654, Train: 24.60%, Valid: 24.47%, Test: 24.33%
Epoch: 25, Loss: 1.5418, Train: 28.70%, Valid: 28.52%, Test: 28.80%
Epoch: 50, Loss: 1.5106, Train: 28.73%, Valid: 28.54%, Test: 28.82%
Epoch: 75, Loss: 1.4777, Train: 28.78%, Valid: 28.61%, Test: 28.89%
Epoch: 100, Loss: 1.4670, Train: 28.74%, Valid: 28.53%, Test: 28.81%
Epoch: 125, Loss: 1.4573, Train: 28.60%, Valid: 28.47%, Test: 28.79%
Epoch: 150, Loss: 1.4545, Train: 29.32%, Valid: 29.28%, Test: 29.45%
Epoch: 175, Loss: 1.4395, Train: 29.96%, Valid: 29.82%, Test: 29.92%
Epoch: 200, Loss: 1.4497, Train: 28.54%, Valid: 28.48%, Test: 28.65%
Epoch: 225, Loss: 1.4429, Train: 29.19%, Valid: 29.09%, Test: 29.25%
Epoch: 250, Loss: 1.4267, Train: 37.63%, Valid: 37.43%, Test: 37.43%
Epoch: 275, Loss: 1.4179, Train: 38.35%, Valid: 38.34%, Test: 38.15%
Epoch: 300, Loss: 1.4056, Train: 39.62%, Valid: 39.41%, Test: 39.44%
Epoch: 325, Loss: 1.3964, Train: 39.67%, Valid: 39.41%, Test: 39.61%
Epoch: 350, Loss: 1.3854, Train: 40.23%, Valid: 40.14%, Test: 40.39%
Epoch: 375, Loss: 1.3772, Train: 40.51%, Valid: 40.42%, Test: 40.41%
Epoch: 400, Loss: 1.3713, Train: 40.19%, Valid: 40.30%, Test: 40.35%
Epoch: 425, Loss: 1.3629, Train: 40.92%, Valid: 40.80%, Test: 40.99%
Epoch: 450, Loss: 1.3573, Train: 40.94%, Valid: 40.81%, Test: 41.06%
Epoch: 475, Loss: 1.3565, Train: 41.58%, Valid: 41.52%, Test: 41.76%
Epoch: 500, Loss: 1.3517, Train: 41.90%, Valid: 41.95%, Test: 42.09%
Epoch: 525, Loss: 1.3598, Train: 40.21%, Valid: 40.22%, Test: 40.83%
Epoch: 550, Loss: 1.3568, Train: 41.58%, Valid: 41.66%, Test: 42.03%
Epoch: 575, Loss: 1.3485, Train: 42.46%, Valid: 42.52%, Test: 42.62%
Epoch: 600, Loss: 1.3544, Train: 42.22%, Valid: 42.07%, Test: 42.39%
Epoch: 625, Loss: 1.3433, Train: 42.35%, Valid: 42.06%, Test: 42.57%
Epoch: 650, Loss: 1.3473, Train: 42.96%, Valid: 42.79%, Test: 43.07%
Epoch: 675, Loss: 1.3559, Train: 42.57%, Valid: 42.35%, Test: 42.65%
Epoch: 700, Loss: 1.3390, Train: 42.56%, Valid: 42.48%, Test: 42.71%
Epoch: 725, Loss: 1.3335, Train: 42.81%, Valid: 42.68%, Test: 42.99%
Epoch: 750, Loss: 1.3457, Train: 43.05%, Valid: 42.98%, Test: 43.24%
Epoch: 775, Loss: 1.3402, Train: 42.49%, Valid: 42.33%, Test: 42.52%
Epoch: 800, Loss: 1.3618, Train: 41.82%, Valid: 41.54%, Test: 41.85%
Epoch: 825, Loss: 1.3347, Train: 42.31%, Valid: 42.06%, Test: 42.49%
Epoch: 850, Loss: 1.3365, Train: 42.45%, Valid: 42.42%, Test: 42.68%
Epoch: 875, Loss: 1.3425, Train: 42.03%, Valid: 42.01%, Test: 42.37%
Epoch: 900, Loss: 1.3304, Train: 43.59%, Valid: 43.47%, Test: 43.75%
Epoch: 925, Loss: 1.3282, Train: 43.72%, Valid: 43.46%, Test: 43.80%
Epoch: 950, Loss: 1.3313, Train: 43.36%, Valid: 43.25%, Test: 43.45%
Epoch: 975, Loss: 1.3403, Train: 43.03%, Valid: 42.80%, Test: 43.11%
Run 01:
Highest Train: 44.17
Highest Valid: 43.92
  Final Train: 44.07
   Final Test: 44.16
All runs:
Highest Train: 44.17, nan
Highest Valid: 43.92, nan
  Final Train: 44.07, nan
   Final Test: 44.16, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 4.2065, Train: 23.98%, Valid: 23.44%, Test: 23.85%
Epoch: 25, Loss: 1.8910, Train: 17.32%, Valid: 17.45%, Test: 17.46%
Epoch: 50, Loss: 1.8599, Train: 18.01%, Valid: 18.12%, Test: 18.19%
Epoch: 75, Loss: 1.6865, Train: 18.63%, Valid: 18.72%, Test: 18.62%
Epoch: 100, Loss: 1.5042, Train: 18.54%, Valid: 18.72%, Test: 18.57%
Epoch: 125, Loss: 1.4858, Train: 17.77%, Valid: 17.92%, Test: 17.73%
Epoch: 150, Loss: 1.4665, Train: 18.04%, Valid: 18.25%, Test: 17.93%
Epoch: 175, Loss: 1.4656, Train: 18.13%, Valid: 18.50%, Test: 18.08%
Epoch: 200, Loss: 1.4606, Train: 18.83%, Valid: 19.14%, Test: 18.51%
Epoch: 225, Loss: 1.4671, Train: 19.39%, Valid: 19.67%, Test: 19.18%
Epoch: 250, Loss: 1.4645, Train: 19.18%, Valid: 19.39%, Test: 19.00%
Epoch: 275, Loss: 1.4627, Train: 19.75%, Valid: 19.83%, Test: 19.78%
Epoch: 300, Loss: 1.4544, Train: 21.73%, Valid: 21.76%, Test: 21.82%
Epoch: 325, Loss: 1.4597, Train: 29.86%, Valid: 29.70%, Test: 30.07%
Epoch: 350, Loss: 1.4546, Train: 17.67%, Valid: 17.95%, Test: 17.50%
Epoch: 375, Loss: 1.4505, Train: 21.15%, Valid: 21.11%, Test: 21.09%
Epoch: 400, Loss: 1.4479, Train: 24.53%, Valid: 24.45%, Test: 24.22%
Epoch: 425, Loss: 1.4495, Train: 19.07%, Valid: 19.37%, Test: 18.90%
Epoch: 450, Loss: 1.4465, Train: 19.95%, Valid: 20.23%, Test: 20.22%
Epoch: 475, Loss: 1.4480, Train: 25.24%, Valid: 25.02%, Test: 25.04%
Epoch: 500, Loss: 1.4445, Train: 24.32%, Valid: 24.41%, Test: 24.72%
Epoch: 525, Loss: 1.4467, Train: 24.37%, Valid: 24.57%, Test: 24.82%
Epoch: 550, Loss: 1.4441, Train: 18.90%, Valid: 19.25%, Test: 18.95%
Epoch: 575, Loss: 1.4437, Train: 24.16%, Valid: 24.32%, Test: 24.60%
Epoch: 600, Loss: 1.4449, Train: 20.87%, Valid: 21.00%, Test: 21.12%
Epoch: 625, Loss: 1.4497, Train: 24.19%, Valid: 24.17%, Test: 24.62%
Epoch: 650, Loss: 1.4503, Train: 18.60%, Valid: 18.87%, Test: 18.72%
Epoch: 675, Loss: 1.4411, Train: 26.83%, Valid: 26.85%, Test: 27.17%
Epoch: 700, Loss: 1.4477, Train: 28.09%, Valid: 27.70%, Test: 28.49%
Epoch: 725, Loss: 1.4422, Train: 23.24%, Valid: 23.42%, Test: 23.74%
Epoch: 750, Loss: 1.4358, Train: 24.78%, Valid: 24.68%, Test: 25.25%
Epoch: 775, Loss: 1.4559, Train: 26.31%, Valid: 26.18%, Test: 26.81%
Epoch: 800, Loss: 1.4323, Train: 23.91%, Valid: 23.93%, Test: 24.29%
Epoch: 825, Loss: 1.4308, Train: 26.46%, Valid: 26.52%, Test: 26.84%
Epoch: 850, Loss: 1.4331, Train: 29.71%, Valid: 29.55%, Test: 30.16%
Epoch: 875, Loss: 1.4326, Train: 27.83%, Valid: 27.75%, Test: 28.14%
Epoch: 900, Loss: 1.4328, Train: 31.90%, Valid: 31.50%, Test: 32.11%
Epoch: 925, Loss: 1.4276, Train: 29.25%, Valid: 29.05%, Test: 29.54%
Epoch: 950, Loss: 1.4316, Train: 31.57%, Valid: 31.23%, Test: 31.67%
Epoch: 975, Loss: 1.4271, Train: 34.60%, Valid: 34.27%, Test: 34.60%
Run 01:
Highest Train: 35.97
Highest Valid: 35.59
  Final Train: 35.97
   Final Test: 35.94
All runs:
Highest Train: 35.97, nan
Highest Valid: 35.59, nan
  Final Train: 35.97, nan
   Final Test: 35.94, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6135, Train: 28.78%, Valid: 28.48%, Test: 28.88%
Epoch: 25, Loss: 1.4949, Train: 34.63%, Valid: 34.39%, Test: 34.49%
Epoch: 50, Loss: 1.4694, Train: 36.32%, Valid: 35.96%, Test: 36.28%
Epoch: 75, Loss: 1.4483, Train: 37.93%, Valid: 37.22%, Test: 37.65%
Epoch: 100, Loss: 1.4244, Train: 39.73%, Valid: 39.06%, Test: 39.25%
Epoch: 125, Loss: 1.4071, Train: 40.59%, Valid: 39.79%, Test: 39.90%
Epoch: 150, Loss: 1.4044, Train: 41.54%, Valid: 40.76%, Test: 40.91%
Epoch: 175, Loss: 1.3981, Train: 42.24%, Valid: 41.40%, Test: 41.37%
Epoch: 200, Loss: 1.3862, Train: 42.46%, Valid: 41.67%, Test: 41.63%
Epoch: 225, Loss: 1.3695, Train: 42.73%, Valid: 41.66%, Test: 41.66%
Epoch: 250, Loss: 1.3736, Train: 43.10%, Valid: 41.87%, Test: 41.93%
Epoch: 275, Loss: 1.3600, Train: 42.72%, Valid: 40.97%, Test: 41.33%
Epoch: 300, Loss: 1.3686, Train: 43.57%, Valid: 42.32%, Test: 42.46%
Epoch: 325, Loss: 1.3581, Train: 43.71%, Valid: 42.66%, Test: 42.61%
Epoch: 350, Loss: 1.3450, Train: 44.56%, Valid: 43.21%, Test: 43.36%
Epoch: 375, Loss: 1.3325, Train: 44.69%, Valid: 43.35%, Test: 43.81%
Epoch: 400, Loss: 1.3378, Train: 44.59%, Valid: 43.05%, Test: 43.11%
Epoch: 425, Loss: 1.3300, Train: 44.91%, Valid: 43.70%, Test: 43.83%
Epoch: 450, Loss: 1.3213, Train: 45.30%, Valid: 43.95%, Test: 44.09%
Epoch: 475, Loss: 1.3155, Train: 45.59%, Valid: 43.97%, Test: 44.17%
Epoch: 500, Loss: 1.3214, Train: 45.34%, Valid: 44.09%, Test: 44.07%
Epoch: 525, Loss: 1.3117, Train: 45.17%, Valid: 43.92%, Test: 43.90%
Epoch: 550, Loss: 1.3049, Train: 45.92%, Valid: 44.23%, Test: 44.54%
Epoch: 575, Loss: 1.3081, Train: 45.45%, Valid: 44.21%, Test: 44.18%
Epoch: 600, Loss: 1.3126, Train: 45.04%, Valid: 44.25%, Test: 44.07%
Epoch: 625, Loss: 1.3077, Train: 44.67%, Valid: 43.55%, Test: 43.53%
Epoch: 650, Loss: 1.3006, Train: 45.56%, Valid: 44.25%, Test: 44.20%
Epoch: 675, Loss: 1.2820, Train: 46.32%, Valid: 44.93%, Test: 45.20%
Epoch: 700, Loss: 1.2967, Train: 45.98%, Valid: 44.48%, Test: 44.61%
Epoch: 725, Loss: 1.3009, Train: 45.55%, Valid: 44.15%, Test: 44.21%
Epoch: 750, Loss: 1.2803, Train: 46.26%, Valid: 44.89%, Test: 44.83%
Epoch: 775, Loss: 1.2844, Train: 45.69%, Valid: 44.32%, Test: 44.30%
Epoch: 800, Loss: 1.2845, Train: 45.56%, Valid: 44.41%, Test: 44.28%
Epoch: 825, Loss: 1.2875, Train: 45.42%, Valid: 43.99%, Test: 44.05%
Epoch: 850, Loss: 1.2795, Train: 46.03%, Valid: 44.72%, Test: 44.62%
Epoch: 875, Loss: 1.2850, Train: 46.20%, Valid: 44.63%, Test: 44.58%
Epoch: 900, Loss: 1.2843, Train: 46.03%, Valid: 44.68%, Test: 44.78%
Epoch: 925, Loss: 1.2780, Train: 44.20%, Valid: 42.91%, Test: 42.91%
Epoch: 950, Loss: 1.2844, Train: 45.96%, Valid: 44.45%, Test: 44.66%
Epoch: 975, Loss: 1.2755, Train: 46.58%, Valid: 44.94%, Test: 45.21%
Run 01:
Highest Train: 46.90
Highest Valid: 45.46
  Final Train: 46.90
   Final Test: 45.40
All runs:
Highest Train: 46.90, nan
Highest Valid: 45.46, nan
  Final Train: 46.90, nan
   Final Test: 45.40, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.0245, Train: 22.20%, Valid: 22.58%, Test: 22.55%
Epoch: 25, Loss: 1.5089, Train: 29.97%, Valid: 29.82%, Test: 30.15%
Epoch: 50, Loss: 1.4743, Train: 29.51%, Valid: 29.38%, Test: 29.77%
Epoch: 75, Loss: 1.4589, Train: 29.93%, Valid: 29.78%, Test: 30.16%
Epoch: 100, Loss: 1.4440, Train: 30.04%, Valid: 29.88%, Test: 30.38%
Epoch: 125, Loss: 1.4416, Train: 32.84%, Valid: 32.41%, Test: 33.47%
Epoch: 150, Loss: 1.4365, Train: 31.09%, Valid: 30.76%, Test: 31.55%
Epoch: 175, Loss: 1.4407, Train: 32.39%, Valid: 32.01%, Test: 32.92%
Epoch: 200, Loss: 1.4351, Train: 34.02%, Valid: 33.64%, Test: 34.64%
Epoch: 225, Loss: 1.4415, Train: 30.68%, Valid: 30.59%, Test: 30.97%
Epoch: 250, Loss: 1.4237, Train: 32.58%, Valid: 32.45%, Test: 32.94%
Epoch: 275, Loss: 1.4533, Train: 34.41%, Valid: 34.03%, Test: 34.95%
Epoch: 300, Loss: 1.4236, Train: 33.17%, Valid: 33.05%, Test: 33.60%
Epoch: 325, Loss: 1.4302, Train: 33.71%, Valid: 33.39%, Test: 34.16%
Epoch: 350, Loss: 1.4373, Train: 34.57%, Valid: 34.39%, Test: 35.45%
Epoch: 375, Loss: 1.4446, Train: 35.47%, Valid: 35.16%, Test: 35.89%
Epoch: 400, Loss: 1.4550, Train: 35.05%, Valid: 34.98%, Test: 35.75%
Epoch: 425, Loss: 1.4138, Train: 35.00%, Valid: 34.59%, Test: 35.24%
Epoch: 450, Loss: 1.4169, Train: 34.99%, Valid: 34.71%, Test: 35.39%
Epoch: 475, Loss: 1.4136, Train: 35.06%, Valid: 34.51%, Test: 35.71%
Epoch: 500, Loss: 1.4090, Train: 35.41%, Valid: 35.08%, Test: 36.19%
Epoch: 525, Loss: 1.4139, Train: 35.92%, Valid: 35.54%, Test: 36.38%
Epoch: 550, Loss: 1.4100, Train: 32.87%, Valid: 32.76%, Test: 33.71%
Epoch: 575, Loss: 1.4148, Train: 36.98%, Valid: 36.38%, Test: 37.56%
Epoch: 600, Loss: 1.4063, Train: 37.03%, Valid: 36.76%, Test: 37.64%
Epoch: 625, Loss: 1.4103, Train: 36.77%, Valid: 36.33%, Test: 37.28%
Epoch: 650, Loss: 1.4097, Train: 36.90%, Valid: 36.47%, Test: 37.47%
Epoch: 675, Loss: 1.4052, Train: 38.44%, Valid: 38.16%, Test: 38.78%
Epoch: 700, Loss: 1.3985, Train: 36.56%, Valid: 36.17%, Test: 37.09%
Epoch: 725, Loss: 1.4032, Train: 38.06%, Valid: 37.71%, Test: 38.32%
Epoch: 750, Loss: 1.3938, Train: 38.44%, Valid: 37.85%, Test: 38.64%
Epoch: 775, Loss: 1.3932, Train: 37.00%, Valid: 36.55%, Test: 37.39%
Epoch: 800, Loss: 1.3934, Train: 37.56%, Valid: 37.06%, Test: 37.64%
Epoch: 825, Loss: 1.4187, Train: 37.29%, Valid: 36.86%, Test: 37.43%
Epoch: 850, Loss: 1.3916, Train: 38.44%, Valid: 37.96%, Test: 38.72%
Epoch: 875, Loss: 1.3969, Train: 36.00%, Valid: 35.62%, Test: 36.18%
Epoch: 900, Loss: 1.4114, Train: 37.91%, Valid: 37.37%, Test: 38.01%
Epoch: 925, Loss: 1.4001, Train: 38.33%, Valid: 37.98%, Test: 38.50%
Epoch: 950, Loss: 1.4160, Train: 36.92%, Valid: 36.58%, Test: 36.95%
Epoch: 975, Loss: 1.3974, Train: 37.63%, Valid: 37.27%, Test: 37.72%
Run 01:
Highest Train: 39.31
Highest Valid: 38.91
  Final Train: 39.31
   Final Test: 39.31
All runs:
Highest Train: 39.31, nan
Highest Valid: 38.91, nan
  Final Train: 39.31, nan
   Final Test: 39.31, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 6.6591, Train: 28.67%, Valid: 28.61%, Test: 28.99%
Epoch: 25, Loss: 1.8319, Train: 27.45%, Valid: 27.08%, Test: 27.51%
Epoch: 50, Loss: 1.7793, Train: 28.17%, Valid: 27.81%, Test: 28.20%
Epoch: 75, Loss: 1.5247, Train: 27.23%, Valid: 27.03%, Test: 27.38%
Epoch: 100, Loss: 1.5572, Train: 26.26%, Valid: 26.14%, Test: 26.51%
Epoch: 125, Loss: 1.4866, Train: 28.28%, Valid: 28.00%, Test: 28.19%
Epoch: 150, Loss: 1.4666, Train: 27.57%, Valid: 27.22%, Test: 27.53%
Epoch: 175, Loss: 1.4605, Train: 27.61%, Valid: 27.24%, Test: 27.52%
Epoch: 200, Loss: 1.4582, Train: 27.41%, Valid: 27.09%, Test: 27.39%
Epoch: 225, Loss: 1.4290, Train: 27.61%, Valid: 27.24%, Test: 27.51%
Epoch: 250, Loss: 1.4234, Train: 27.92%, Valid: 27.44%, Test: 27.75%
Epoch: 275, Loss: 1.4212, Train: 28.15%, Valid: 27.80%, Test: 28.07%
Epoch: 300, Loss: 1.4204, Train: 28.14%, Valid: 27.83%, Test: 28.11%
Epoch: 325, Loss: 1.4311, Train: 29.30%, Valid: 28.88%, Test: 29.18%
Epoch: 350, Loss: 1.4139, Train: 31.03%, Valid: 30.65%, Test: 30.78%
Epoch: 375, Loss: 1.4153, Train: 33.56%, Valid: 33.42%, Test: 33.56%
Epoch: 400, Loss: 1.4086, Train: 35.69%, Valid: 35.54%, Test: 35.64%
Epoch: 425, Loss: 1.4099, Train: 36.58%, Valid: 36.39%, Test: 36.42%
Epoch: 450, Loss: 1.4075, Train: 38.62%, Valid: 38.67%, Test: 38.72%
Epoch: 475, Loss: 1.4008, Train: 38.31%, Valid: 38.29%, Test: 38.19%
Epoch: 500, Loss: 1.4030, Train: 39.27%, Valid: 39.23%, Test: 39.16%
Epoch: 525, Loss: 1.4156, Train: 38.87%, Valid: 38.93%, Test: 38.78%
Epoch: 550, Loss: 1.3961, Train: 38.56%, Valid: 38.44%, Test: 38.42%
Epoch: 575, Loss: 1.3953, Train: 38.57%, Valid: 38.44%, Test: 38.33%
Epoch: 600, Loss: 1.3941, Train: 38.39%, Valid: 38.18%, Test: 38.21%
Epoch: 625, Loss: 1.3889, Train: 38.58%, Valid: 38.44%, Test: 38.47%
Epoch: 650, Loss: 1.3953, Train: 38.64%, Valid: 38.44%, Test: 38.48%
Epoch: 675, Loss: 1.3895, Train: 38.79%, Valid: 38.65%, Test: 38.53%
Epoch: 700, Loss: 1.3870, Train: 38.81%, Valid: 38.70%, Test: 38.62%
Epoch: 725, Loss: 1.3872, Train: 38.97%, Valid: 38.78%, Test: 38.90%
Epoch: 750, Loss: 1.3803, Train: 39.27%, Valid: 39.22%, Test: 39.02%
Epoch: 775, Loss: 1.3801, Train: 38.82%, Valid: 38.64%, Test: 38.60%
Epoch: 800, Loss: 1.3805, Train: 39.29%, Valid: 38.99%, Test: 39.15%
Epoch: 825, Loss: 1.3810, Train: 37.46%, Valid: 37.23%, Test: 37.34%
Epoch: 850, Loss: 1.3919, Train: 39.65%, Valid: 39.53%, Test: 39.34%
Epoch: 875, Loss: 1.3714, Train: 39.07%, Valid: 38.81%, Test: 38.97%
Epoch: 900, Loss: 1.3791, Train: 39.61%, Valid: 39.52%, Test: 39.40%
Epoch: 925, Loss: 1.3739, Train: 39.67%, Valid: 39.70%, Test: 39.47%
Epoch: 950, Loss: 1.3734, Train: 39.08%, Valid: 38.94%, Test: 39.07%
Epoch: 975, Loss: 1.3744, Train: 39.30%, Valid: 39.09%, Test: 39.11%
Run 01:
Highest Train: 40.86
Highest Valid: 40.81
  Final Train: 40.86
   Final Test: 40.71
All runs:
Highest Train: 40.86, nan
Highest Valid: 40.81, nan
  Final Train: 40.86, nan
   Final Test: 40.71, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6063, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4715, Train: 35.57%, Valid: 35.21%, Test: 35.53%
Epoch: 50, Loss: 1.4365, Train: 37.57%, Valid: 36.67%, Test: 37.01%
Epoch: 75, Loss: 1.4004, Train: 39.19%, Valid: 38.36%, Test: 38.48%
Epoch: 100, Loss: 1.3727, Train: 40.59%, Valid: 39.27%, Test: 39.31%
Epoch: 125, Loss: 1.3485, Train: 41.72%, Valid: 40.42%, Test: 40.52%
Epoch: 150, Loss: 1.3208, Train: 42.52%, Valid: 41.10%, Test: 41.18%
Epoch: 175, Loss: 1.3059, Train: 43.83%, Valid: 42.05%, Test: 42.15%
Epoch: 200, Loss: 1.2844, Train: 44.61%, Valid: 42.22%, Test: 42.49%
Epoch: 225, Loss: 1.2693, Train: 45.71%, Valid: 42.96%, Test: 43.27%
Epoch: 250, Loss: 1.2580, Train: 45.45%, Valid: 42.97%, Test: 42.92%
Epoch: 275, Loss: 1.2409, Train: 46.38%, Valid: 42.82%, Test: 43.30%
Epoch: 300, Loss: 1.2231, Train: 47.42%, Valid: 43.75%, Test: 43.90%
Epoch: 325, Loss: 1.2256, Train: 47.07%, Valid: 42.74%, Test: 43.09%
Epoch: 350, Loss: 1.2094, Train: 48.28%, Valid: 44.21%, Test: 44.59%
Epoch: 375, Loss: 1.1922, Train: 49.39%, Valid: 44.42%, Test: 44.79%
Epoch: 400, Loss: 1.1783, Train: 49.84%, Valid: 44.38%, Test: 44.53%
Epoch: 425, Loss: 1.1715, Train: 49.85%, Valid: 44.18%, Test: 44.44%
Epoch: 450, Loss: 1.1726, Train: 47.96%, Valid: 43.06%, Test: 43.42%
Epoch: 475, Loss: 1.1526, Train: 50.94%, Valid: 45.29%, Test: 45.57%
Epoch: 500, Loss: 1.1610, Train: 50.95%, Valid: 45.38%, Test: 45.67%
Epoch: 525, Loss: 1.1413, Train: 51.66%, Valid: 45.31%, Test: 45.38%
Epoch: 550, Loss: 1.1309, Train: 51.97%, Valid: 45.41%, Test: 45.75%
Epoch: 575, Loss: 1.1299, Train: 51.80%, Valid: 45.70%, Test: 45.96%
Epoch: 600, Loss: 1.1135, Train: 52.81%, Valid: 45.51%, Test: 45.85%
Epoch: 625, Loss: 1.1349, Train: 52.10%, Valid: 45.62%, Test: 45.80%
Epoch: 650, Loss: 1.1037, Train: 53.28%, Valid: 45.92%, Test: 46.20%
Epoch: 675, Loss: 1.1235, Train: 52.29%, Valid: 43.92%, Test: 44.40%
Epoch: 700, Loss: 1.0929, Train: 53.89%, Valid: 45.69%, Test: 46.17%
Epoch: 725, Loss: 1.0969, Train: 53.53%, Valid: 45.24%, Test: 45.69%
Epoch: 750, Loss: 1.0844, Train: 54.02%, Valid: 46.22%, Test: 46.43%
Epoch: 775, Loss: 1.0899, Train: 53.24%, Valid: 45.35%, Test: 45.49%
Epoch: 800, Loss: 1.0763, Train: 54.63%, Valid: 46.18%, Test: 46.56%
Epoch: 825, Loss: 1.0773, Train: 54.51%, Valid: 46.05%, Test: 46.30%
Epoch: 850, Loss: 1.0746, Train: 54.01%, Valid: 45.74%, Test: 45.99%
Epoch: 875, Loss: 1.0681, Train: 55.11%, Valid: 46.37%, Test: 46.55%
Epoch: 900, Loss: 1.0958, Train: 52.10%, Valid: 44.81%, Test: 44.93%
Epoch: 925, Loss: 1.0731, Train: 54.90%, Valid: 46.22%, Test: 46.43%
Epoch: 950, Loss: 1.0551, Train: 55.55%, Valid: 46.51%, Test: 46.78%
Epoch: 975, Loss: 1.0548, Train: 55.68%, Valid: 45.96%, Test: 46.23%
Run 01:
Highest Train: 56.05
Highest Valid: 46.65
  Final Train: 55.48
   Final Test: 46.82
All runs:
Highest Train: 56.05, nan
Highest Valid: 46.65, nan
  Final Train: 55.48, nan
   Final Test: 46.82, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6332, Train: 17.64%, Valid: 17.78%, Test: 17.32%
Epoch: 25, Loss: 1.4433, Train: 29.69%, Valid: 29.47%, Test: 29.95%
Epoch: 50, Loss: 1.4018, Train: 37.82%, Valid: 37.51%, Test: 37.85%
Epoch: 75, Loss: 1.3599, Train: 39.94%, Valid: 39.54%, Test: 39.71%
Epoch: 100, Loss: 1.3363, Train: 41.52%, Valid: 41.17%, Test: 41.34%
Epoch: 125, Loss: 1.3160, Train: 42.13%, Valid: 41.66%, Test: 41.90%
Epoch: 150, Loss: 1.4019, Train: 42.32%, Valid: 42.06%, Test: 42.13%
Epoch: 175, Loss: 1.3075, Train: 42.76%, Valid: 42.40%, Test: 42.59%
Epoch: 200, Loss: 1.2919, Train: 43.36%, Valid: 42.94%, Test: 43.32%
Epoch: 225, Loss: 1.2918, Train: 43.93%, Valid: 43.36%, Test: 43.72%
Epoch: 250, Loss: 1.2703, Train: 44.34%, Valid: 43.76%, Test: 43.98%
Epoch: 275, Loss: 1.2869, Train: 44.44%, Valid: 43.99%, Test: 44.10%
Epoch: 300, Loss: 1.2632, Train: 45.00%, Valid: 44.49%, Test: 44.71%
Epoch: 325, Loss: 1.2742, Train: 39.56%, Valid: 38.81%, Test: 39.15%
Epoch: 350, Loss: 1.2646, Train: 44.97%, Valid: 44.43%, Test: 44.60%
Epoch: 375, Loss: 1.2501, Train: 45.58%, Valid: 45.11%, Test: 45.09%
Epoch: 400, Loss: 1.2401, Train: 46.06%, Valid: 45.66%, Test: 45.43%
Epoch: 425, Loss: 1.2343, Train: 46.36%, Valid: 45.86%, Test: 45.69%
Epoch: 450, Loss: 1.3043, Train: 45.44%, Valid: 44.88%, Test: 45.02%
Epoch: 475, Loss: 1.2407, Train: 46.11%, Valid: 45.76%, Test: 45.78%
Epoch: 500, Loss: 1.2299, Train: 46.55%, Valid: 46.07%, Test: 46.03%
Epoch: 525, Loss: 1.2284, Train: 45.41%, Valid: 44.68%, Test: 44.84%
Epoch: 550, Loss: 1.2238, Train: 47.05%, Valid: 46.51%, Test: 46.46%
Epoch: 575, Loss: 1.2170, Train: 47.39%, Valid: 46.81%, Test: 46.66%
Epoch: 600, Loss: 1.2333, Train: 46.30%, Valid: 45.88%, Test: 45.82%
Epoch: 625, Loss: 1.2162, Train: 47.30%, Valid: 46.65%, Test: 46.72%
Epoch: 650, Loss: 1.2112, Train: 47.47%, Valid: 46.90%, Test: 46.85%
Epoch: 675, Loss: 1.2335, Train: 47.26%, Valid: 46.69%, Test: 46.64%
Epoch: 700, Loss: 1.2073, Train: 47.73%, Valid: 47.06%, Test: 47.05%
Epoch: 725, Loss: 1.2091, Train: 47.97%, Valid: 47.34%, Test: 47.23%
Epoch: 750, Loss: 1.2006, Train: 48.38%, Valid: 47.72%, Test: 47.59%
Epoch: 775, Loss: 1.2127, Train: 47.75%, Valid: 47.14%, Test: 47.06%
Epoch: 800, Loss: 1.1996, Train: 48.30%, Valid: 47.74%, Test: 47.63%
Epoch: 825, Loss: 1.1968, Train: 48.44%, Valid: 47.75%, Test: 47.74%
Epoch: 850, Loss: 1.2099, Train: 48.39%, Valid: 47.75%, Test: 47.76%
Epoch: 875, Loss: 1.1974, Train: 48.67%, Valid: 48.08%, Test: 48.01%
Epoch: 900, Loss: 1.2030, Train: 48.29%, Valid: 47.40%, Test: 47.34%
Epoch: 925, Loss: 1.1989, Train: 48.48%, Valid: 47.83%, Test: 47.68%
Epoch: 950, Loss: 1.1887, Train: 48.80%, Valid: 48.21%, Test: 48.07%
Epoch: 975, Loss: 1.1883, Train: 48.90%, Valid: 48.17%, Test: 48.14%
Run 01:
Highest Train: 48.99
Highest Valid: 48.41
  Final Train: 48.97
   Final Test: 48.17
All runs:
Highest Train: 48.99, nan
Highest Valid: 48.41, nan
  Final Train: 48.97, nan
   Final Test: 48.17, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 6.5713, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 5.8324, Train: 33.36%, Valid: 33.16%, Test: 34.07%
Epoch: 50, Loss: 1.9454, Train: 18.74%, Valid: 18.90%, Test: 18.39%
Epoch: 75, Loss: 2.1893, Train: 17.85%, Valid: 18.10%, Test: 17.57%
Epoch: 100, Loss: 1.6843, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 125, Loss: 1.5225, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 150, Loss: 1.4470, Train: 28.53%, Valid: 28.48%, Test: 28.52%
Epoch: 175, Loss: 1.4141, Train: 39.26%, Valid: 39.04%, Test: 38.93%
Epoch: 200, Loss: 1.4018, Train: 39.02%, Valid: 38.58%, Test: 38.63%
Epoch: 225, Loss: 1.3964, Train: 38.98%, Valid: 38.68%, Test: 38.58%
Epoch: 250, Loss: 1.3961, Train: 38.67%, Valid: 38.37%, Test: 38.35%
Epoch: 275, Loss: 1.3916, Train: 38.87%, Valid: 38.54%, Test: 38.59%
Epoch: 300, Loss: 1.4427, Train: 30.49%, Valid: 30.47%, Test: 30.22%
Epoch: 325, Loss: 1.4370, Train: 37.60%, Valid: 37.30%, Test: 37.33%
Epoch: 350, Loss: 1.3934, Train: 39.72%, Valid: 39.52%, Test: 39.30%
Epoch: 375, Loss: 1.3864, Train: 39.90%, Valid: 39.44%, Test: 39.47%
Epoch: 400, Loss: 1.3824, Train: 39.89%, Valid: 39.47%, Test: 39.54%
Epoch: 425, Loss: 1.3788, Train: 40.06%, Valid: 39.61%, Test: 39.58%
Epoch: 450, Loss: 1.3753, Train: 40.21%, Valid: 39.91%, Test: 39.82%
Epoch: 475, Loss: 1.3720, Train: 40.47%, Valid: 40.26%, Test: 40.08%
Epoch: 500, Loss: 1.3690, Train: 40.73%, Valid: 40.43%, Test: 40.27%
Epoch: 525, Loss: 1.3692, Train: 40.58%, Valid: 40.28%, Test: 40.28%
Epoch: 550, Loss: 1.3777, Train: 40.29%, Valid: 39.99%, Test: 40.02%
Epoch: 575, Loss: 1.3651, Train: 40.63%, Valid: 40.38%, Test: 40.39%
Epoch: 600, Loss: 1.3686, Train: 40.21%, Valid: 39.90%, Test: 40.07%
Epoch: 625, Loss: 1.4368, Train: 35.72%, Valid: 35.22%, Test: 35.56%
Epoch: 650, Loss: 1.3712, Train: 40.75%, Valid: 40.33%, Test: 40.48%
Epoch: 675, Loss: 1.3609, Train: 40.82%, Valid: 40.52%, Test: 40.73%
Epoch: 700, Loss: 1.3572, Train: 41.29%, Valid: 41.02%, Test: 41.01%
Epoch: 725, Loss: 1.3550, Train: 41.28%, Valid: 41.05%, Test: 41.08%
Epoch: 750, Loss: 1.3530, Train: 41.34%, Valid: 41.14%, Test: 41.16%
Epoch: 775, Loss: 1.3513, Train: 41.46%, Valid: 41.25%, Test: 41.26%
Epoch: 800, Loss: 1.3513, Train: 41.42%, Valid: 41.08%, Test: 41.20%
Epoch: 825, Loss: 1.3569, Train: 41.08%, Valid: 40.80%, Test: 40.91%
Epoch: 850, Loss: 1.3561, Train: 41.62%, Valid: 41.29%, Test: 41.26%
Epoch: 875, Loss: 1.3912, Train: 39.20%, Valid: 38.90%, Test: 39.15%
Epoch: 900, Loss: 1.3538, Train: 41.29%, Valid: 40.96%, Test: 41.21%
Epoch: 925, Loss: 1.3471, Train: 41.63%, Valid: 41.41%, Test: 41.46%
Epoch: 950, Loss: 1.3429, Train: 41.70%, Valid: 41.54%, Test: 41.62%
Epoch: 975, Loss: 1.3391, Train: 41.88%, Valid: 41.65%, Test: 41.78%
Run 01:
Highest Train: 42.08
Highest Valid: 41.81
  Final Train: 42.08
   Final Test: 41.96
All runs:
Highest Train: 42.08, nan
Highest Valid: 41.81, nan
  Final Train: 42.08, nan
   Final Test: 41.96, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.5964, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4708, Train: 35.48%, Valid: 35.05%, Test: 35.30%
Epoch: 50, Loss: 1.4384, Train: 37.16%, Valid: 36.39%, Test: 36.74%
Epoch: 75, Loss: 1.3996, Train: 39.11%, Valid: 38.10%, Test: 38.25%
Epoch: 100, Loss: 1.3719, Train: 41.12%, Valid: 39.82%, Test: 39.98%
Epoch: 125, Loss: 1.3417, Train: 42.15%, Valid: 40.42%, Test: 40.61%
Epoch: 150, Loss: 1.3093, Train: 43.79%, Valid: 41.77%, Test: 41.81%
Epoch: 175, Loss: 1.3161, Train: 42.30%, Valid: 40.04%, Test: 40.40%
Epoch: 200, Loss: 1.2763, Train: 44.90%, Valid: 42.15%, Test: 42.41%
Epoch: 225, Loss: 1.2582, Train: 45.80%, Valid: 42.59%, Test: 42.88%
Epoch: 250, Loss: 1.2403, Train: 46.94%, Valid: 43.23%, Test: 43.62%
Epoch: 275, Loss: 1.2195, Train: 47.76%, Valid: 43.57%, Test: 44.01%
Epoch: 300, Loss: 1.2133, Train: 48.05%, Valid: 43.77%, Test: 44.22%
Epoch: 325, Loss: 1.2072, Train: 46.36%, Valid: 41.88%, Test: 42.29%
Epoch: 350, Loss: 1.1863, Train: 49.45%, Valid: 43.96%, Test: 44.45%
Epoch: 375, Loss: 1.1780, Train: 49.21%, Valid: 43.66%, Test: 44.07%
Epoch: 400, Loss: 1.1632, Train: 50.34%, Valid: 44.63%, Test: 45.00%
Epoch: 425, Loss: 1.1517, Train: 50.38%, Valid: 44.74%, Test: 45.06%
Epoch: 450, Loss: 1.1381, Train: 51.39%, Valid: 44.68%, Test: 45.15%
Epoch: 475, Loss: 1.1308, Train: 52.01%, Valid: 44.87%, Test: 45.31%
Epoch: 500, Loss: 1.1264, Train: 51.99%, Valid: 44.47%, Test: 44.71%
Epoch: 525, Loss: 1.1286, Train: 52.28%, Valid: 44.72%, Test: 44.99%
Epoch: 550, Loss: 1.1222, Train: 52.32%, Valid: 45.25%, Test: 45.70%
Epoch: 575, Loss: 1.0994, Train: 53.24%, Valid: 45.33%, Test: 45.70%
Epoch: 600, Loss: 1.1159, Train: 52.89%, Valid: 45.44%, Test: 45.74%
Epoch: 625, Loss: 1.0850, Train: 54.16%, Valid: 45.30%, Test: 45.75%
Epoch: 650, Loss: 1.1022, Train: 53.59%, Valid: 45.11%, Test: 45.41%
Epoch: 675, Loss: 1.0717, Train: 54.73%, Valid: 45.87%, Test: 46.18%
Epoch: 700, Loss: 1.0719, Train: 54.74%, Valid: 45.78%, Test: 46.03%
Epoch: 725, Loss: 1.0684, Train: 54.77%, Valid: 44.86%, Test: 45.50%
Epoch: 750, Loss: 1.0622, Train: 55.42%, Valid: 45.98%, Test: 46.13%
Epoch: 775, Loss: 1.0623, Train: 54.80%, Valid: 46.02%, Test: 46.15%
Epoch: 800, Loss: 1.0474, Train: 55.83%, Valid: 45.83%, Test: 46.32%
Epoch: 825, Loss: 1.0556, Train: 55.60%, Valid: 45.60%, Test: 45.95%
Epoch: 850, Loss: 1.0419, Train: 55.30%, Valid: 44.97%, Test: 45.31%
Epoch: 875, Loss: 1.0524, Train: 55.32%, Valid: 44.31%, Test: 45.11%
Epoch: 900, Loss: 1.0335, Train: 56.74%, Valid: 46.05%, Test: 46.57%
Epoch: 925, Loss: 1.0517, Train: 55.00%, Valid: 44.58%, Test: 45.03%
Epoch: 950, Loss: 1.0260, Train: 57.00%, Valid: 46.16%, Test: 46.48%
Epoch: 975, Loss: 1.0410, Train: 56.55%, Valid: 45.25%, Test: 46.03%
Run 01:
Highest Train: 57.34
Highest Valid: 46.55
  Final Train: 56.97
   Final Test: 46.75
All runs:
Highest Train: 57.34, nan
Highest Valid: 46.55, nan
  Final Train: 56.97, nan
   Final Test: 46.75, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.4905, Train: 22.94%, Valid: 22.85%, Test: 23.40%
Epoch: 25, Loss: 1.4349, Train: 29.67%, Valid: 29.53%, Test: 29.94%
Epoch: 50, Loss: 1.3831, Train: 38.81%, Valid: 38.23%, Test: 38.67%
Epoch: 75, Loss: 1.3485, Train: 39.86%, Valid: 39.49%, Test: 39.65%
Epoch: 100, Loss: 1.3262, Train: 39.76%, Valid: 39.44%, Test: 39.68%
Epoch: 125, Loss: 1.3218, Train: 41.51%, Valid: 41.23%, Test: 41.27%
Epoch: 150, Loss: 1.3012, Train: 42.41%, Valid: 42.07%, Test: 42.03%
Epoch: 175, Loss: 1.2856, Train: 43.49%, Valid: 43.15%, Test: 43.08%
Epoch: 200, Loss: 1.3164, Train: 42.48%, Valid: 42.10%, Test: 42.17%
Epoch: 225, Loss: 1.2842, Train: 43.94%, Valid: 43.49%, Test: 43.49%
Epoch: 250, Loss: 1.2668, Train: 44.73%, Valid: 43.92%, Test: 44.16%
Epoch: 275, Loss: 1.2596, Train: 44.95%, Valid: 44.33%, Test: 44.59%
Epoch: 300, Loss: 1.2814, Train: 44.78%, Valid: 44.16%, Test: 44.45%
Epoch: 325, Loss: 1.2504, Train: 45.54%, Valid: 44.93%, Test: 45.15%
Epoch: 350, Loss: 1.2399, Train: 46.03%, Valid: 45.45%, Test: 45.55%
Epoch: 375, Loss: 1.2333, Train: 46.38%, Valid: 45.69%, Test: 45.83%
Epoch: 400, Loss: 1.2765, Train: 44.82%, Valid: 44.31%, Test: 44.43%
Epoch: 425, Loss: 1.2394, Train: 46.19%, Valid: 45.47%, Test: 45.90%
Epoch: 450, Loss: 1.2277, Train: 46.64%, Valid: 46.04%, Test: 46.25%
Epoch: 475, Loss: 1.2201, Train: 47.04%, Valid: 46.40%, Test: 46.56%
Epoch: 500, Loss: 1.2266, Train: 47.27%, Valid: 46.65%, Test: 46.56%
Epoch: 525, Loss: 1.2117, Train: 47.51%, Valid: 46.94%, Test: 47.01%
Epoch: 550, Loss: 1.2109, Train: 47.55%, Valid: 46.66%, Test: 46.89%
Epoch: 575, Loss: 1.2020, Train: 48.09%, Valid: 47.32%, Test: 47.28%
Epoch: 600, Loss: 1.2061, Train: 48.00%, Valid: 47.21%, Test: 47.31%
Epoch: 625, Loss: 1.1955, Train: 48.27%, Valid: 47.53%, Test: 47.57%
Epoch: 650, Loss: 1.2025, Train: 47.87%, Valid: 47.36%, Test: 47.21%
Epoch: 675, Loss: 1.2106, Train: 48.19%, Valid: 47.14%, Test: 47.36%
Epoch: 700, Loss: 1.1907, Train: 48.66%, Valid: 47.70%, Test: 47.90%
Epoch: 725, Loss: 1.1861, Train: 48.93%, Valid: 47.93%, Test: 48.21%
Epoch: 750, Loss: 1.4425, Train: 44.31%, Valid: 43.90%, Test: 43.87%
Epoch: 775, Loss: 1.2412, Train: 46.32%, Valid: 45.78%, Test: 45.86%
Epoch: 800, Loss: 1.2172, Train: 47.39%, Valid: 46.72%, Test: 46.75%
Epoch: 825, Loss: 1.2034, Train: 48.00%, Valid: 47.14%, Test: 47.21%
Epoch: 850, Loss: 1.1946, Train: 48.44%, Valid: 47.41%, Test: 47.58%
Epoch: 875, Loss: 1.1896, Train: 48.81%, Valid: 47.67%, Test: 47.85%
Epoch: 900, Loss: 1.2155, Train: 45.61%, Valid: 44.69%, Test: 44.84%
Epoch: 925, Loss: 1.2131, Train: 47.68%, Valid: 47.07%, Test: 47.00%
Epoch: 950, Loss: 1.1965, Train: 48.40%, Valid: 47.49%, Test: 47.52%
Epoch: 975, Loss: 1.1892, Train: 48.68%, Valid: 47.72%, Test: 47.73%
Run 01:
Highest Train: 49.04
Highest Valid: 47.97
  Final Train: 48.95
   Final Test: 48.20
All runs:
Highest Train: 49.04, nan
Highest Valid: 47.97, nan
  Final Train: 48.95, nan
   Final Test: 48.20, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 34.3529, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 7.0666, Train: 28.71%, Valid: 28.54%, Test: 28.80%
Epoch: 50, Loss: 2.3464, Train: 28.72%, Valid: 28.54%, Test: 28.81%
Epoch: 75, Loss: 1.5103, Train: 27.92%, Valid: 27.58%, Test: 28.01%
Epoch: 100, Loss: 1.4536, Train: 28.63%, Valid: 28.40%, Test: 28.77%
Epoch: 125, Loss: 1.4336, Train: 31.84%, Valid: 31.73%, Test: 31.85%
Epoch: 150, Loss: 1.4759, Train: 32.50%, Valid: 32.32%, Test: 32.52%
Epoch: 175, Loss: 1.4439, Train: 30.53%, Valid: 30.44%, Test: 30.62%
Epoch: 200, Loss: 1.4066, Train: 36.74%, Valid: 36.67%, Test: 36.84%
Epoch: 225, Loss: 1.3946, Train: 38.04%, Valid: 37.75%, Test: 37.81%
Epoch: 250, Loss: 1.3858, Train: 38.62%, Valid: 38.52%, Test: 38.44%
Epoch: 275, Loss: 1.3782, Train: 39.14%, Valid: 38.99%, Test: 38.94%
Epoch: 300, Loss: 1.3715, Train: 39.61%, Valid: 39.48%, Test: 39.47%
Epoch: 325, Loss: 1.3662, Train: 39.97%, Valid: 39.78%, Test: 39.91%
Epoch: 350, Loss: 1.3622, Train: 40.33%, Valid: 40.07%, Test: 40.21%
Epoch: 375, Loss: 1.3587, Train: 40.61%, Valid: 40.34%, Test: 40.51%
Epoch: 400, Loss: 1.3556, Train: 40.84%, Valid: 40.50%, Test: 40.75%
Epoch: 425, Loss: 1.3527, Train: 41.04%, Valid: 40.75%, Test: 41.03%
Epoch: 450, Loss: 1.3500, Train: 41.24%, Valid: 40.91%, Test: 41.26%
Epoch: 475, Loss: 1.3470, Train: 41.41%, Valid: 41.09%, Test: 41.44%
Epoch: 500, Loss: 1.3455, Train: 41.55%, Valid: 41.40%, Test: 41.54%
Epoch: 525, Loss: 1.3423, Train: 41.60%, Valid: 41.51%, Test: 41.51%
Epoch: 550, Loss: 1.4877, Train: 33.60%, Valid: 33.27%, Test: 33.57%
Epoch: 575, Loss: 1.3946, Train: 38.13%, Valid: 37.98%, Test: 38.05%
Epoch: 600, Loss: 1.3606, Train: 40.30%, Valid: 40.15%, Test: 40.17%
Epoch: 625, Loss: 1.3507, Train: 40.74%, Valid: 40.62%, Test: 40.66%
Epoch: 650, Loss: 1.3442, Train: 41.07%, Valid: 40.92%, Test: 41.04%
Epoch: 675, Loss: 1.3388, Train: 41.37%, Valid: 41.13%, Test: 41.43%
Epoch: 700, Loss: 1.3340, Train: 41.61%, Valid: 41.39%, Test: 41.73%
Epoch: 725, Loss: 1.3296, Train: 41.83%, Valid: 41.64%, Test: 42.02%
Epoch: 750, Loss: 1.3254, Train: 42.01%, Valid: 41.87%, Test: 42.29%
Epoch: 775, Loss: 1.3214, Train: 42.18%, Valid: 42.05%, Test: 42.42%
Epoch: 800, Loss: 1.3180, Train: 42.35%, Valid: 42.19%, Test: 42.51%
Epoch: 825, Loss: 1.3151, Train: 42.51%, Valid: 42.31%, Test: 42.62%
Epoch: 850, Loss: 1.3126, Train: 42.67%, Valid: 42.44%, Test: 42.69%
Epoch: 875, Loss: 1.3103, Train: 42.86%, Valid: 42.55%, Test: 42.79%
Epoch: 900, Loss: 1.3086, Train: 42.77%, Valid: 42.49%, Test: 42.75%
Epoch: 925, Loss: 1.3066, Train: 42.81%, Valid: 42.56%, Test: 42.82%
Epoch: 950, Loss: 1.3065, Train: 43.02%, Valid: 42.76%, Test: 42.98%
Epoch: 975, Loss: 1.4191, Train: 39.64%, Valid: 39.35%, Test: 39.81%
Run 01:
Highest Train: 43.34
Highest Valid: 43.10
  Final Train: 43.34
   Final Test: 43.22
All runs:
Highest Train: 43.34, nan
Highest Valid: 43.10, nan
  Final Train: 43.34, nan
   Final Test: 43.22, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6222, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4828, Train: 35.05%, Valid: 34.77%, Test: 35.11%
Epoch: 50, Loss: 1.4493, Train: 37.13%, Valid: 36.35%, Test: 36.69%
Epoch: 75, Loss: 1.4212, Train: 38.85%, Valid: 37.63%, Test: 38.16%
Epoch: 100, Loss: 1.3944, Train: 40.01%, Valid: 38.73%, Test: 39.38%
Epoch: 125, Loss: 1.3760, Train: 41.04%, Valid: 39.79%, Test: 40.31%
Epoch: 150, Loss: 1.3634, Train: 42.10%, Valid: 40.81%, Test: 41.04%
Epoch: 175, Loss: 1.3516, Train: 43.07%, Valid: 41.63%, Test: 42.03%
Epoch: 200, Loss: 1.3422, Train: 43.96%, Valid: 42.32%, Test: 42.70%
Epoch: 225, Loss: 1.3318, Train: 44.38%, Valid: 42.70%, Test: 43.28%
Epoch: 250, Loss: 1.3246, Train: 44.77%, Valid: 43.21%, Test: 43.49%
Epoch: 275, Loss: 1.3224, Train: 45.19%, Valid: 43.26%, Test: 43.89%
Epoch: 300, Loss: 1.3162, Train: 45.61%, Valid: 43.85%, Test: 44.40%
Epoch: 325, Loss: 1.3010, Train: 46.08%, Valid: 44.21%, Test: 44.49%
Epoch: 350, Loss: 1.3070, Train: 46.06%, Valid: 44.24%, Test: 44.63%
Epoch: 375, Loss: 1.3097, Train: 45.69%, Valid: 43.71%, Test: 44.08%
Epoch: 400, Loss: 1.2963, Train: 46.40%, Valid: 44.31%, Test: 44.89%
Epoch: 425, Loss: 1.2916, Train: 46.69%, Valid: 44.63%, Test: 44.98%
Epoch: 450, Loss: 1.2839, Train: 46.61%, Valid: 44.48%, Test: 44.82%
Epoch: 475, Loss: 1.2798, Train: 46.17%, Valid: 44.15%, Test: 44.48%
Epoch: 500, Loss: 1.2851, Train: 47.12%, Valid: 44.86%, Test: 45.23%
Epoch: 525, Loss: 1.2818, Train: 46.96%, Valid: 44.92%, Test: 45.12%
Epoch: 550, Loss: 1.2921, Train: 46.86%, Valid: 44.73%, Test: 44.89%
Epoch: 575, Loss: 1.2777, Train: 46.63%, Valid: 44.41%, Test: 44.80%
Epoch: 600, Loss: 1.2975, Train: 45.01%, Valid: 42.83%, Test: 43.00%
Epoch: 625, Loss: 1.2725, Train: 46.82%, Valid: 44.54%, Test: 44.84%
Epoch: 650, Loss: 1.2769, Train: 45.80%, Valid: 43.54%, Test: 43.86%
Epoch: 675, Loss: 1.2676, Train: 47.27%, Valid: 44.96%, Test: 45.28%
Epoch: 700, Loss: 1.2708, Train: 45.76%, Valid: 43.38%, Test: 43.75%
Epoch: 725, Loss: 1.2654, Train: 46.45%, Valid: 43.93%, Test: 44.34%
Epoch: 750, Loss: 1.2651, Train: 47.25%, Valid: 44.79%, Test: 45.18%
Epoch: 775, Loss: 1.2595, Train: 46.45%, Valid: 44.10%, Test: 44.67%
Epoch: 800, Loss: 1.2568, Train: 47.57%, Valid: 45.11%, Test: 45.27%
Epoch: 825, Loss: 1.2644, Train: 45.75%, Valid: 43.64%, Test: 43.73%
Epoch: 850, Loss: 1.2614, Train: 47.36%, Valid: 44.75%, Test: 45.07%
Epoch: 875, Loss: 1.2563, Train: 47.05%, Valid: 44.73%, Test: 44.98%
Epoch: 900, Loss: 1.2646, Train: 47.95%, Valid: 45.38%, Test: 45.68%
Epoch: 925, Loss: 1.2510, Train: 47.27%, Valid: 44.89%, Test: 45.23%
Epoch: 950, Loss: 1.2545, Train: 45.98%, Valid: 43.65%, Test: 43.72%
Epoch: 975, Loss: 1.2739, Train: 47.44%, Valid: 44.87%, Test: 45.13%
Run 01:
Highest Train: 48.16
Highest Valid: 45.82
  Final Train: 48.16
   Final Test: 45.96
All runs:
Highest Train: 48.16, nan
Highest Valid: 45.82, nan
  Final Train: 48.16, nan
   Final Test: 45.96, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.8628, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4672, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 50, Loss: 1.4369, Train: 32.31%, Valid: 31.81%, Test: 32.46%
Epoch: 75, Loss: 1.4196, Train: 37.85%, Valid: 37.55%, Test: 37.71%
Epoch: 100, Loss: 1.3952, Train: 39.99%, Valid: 39.64%, Test: 40.14%
Epoch: 125, Loss: 1.3789, Train: 40.06%, Valid: 39.73%, Test: 39.89%
Epoch: 150, Loss: 1.3738, Train: 41.20%, Valid: 40.89%, Test: 41.03%
Epoch: 175, Loss: 1.3584, Train: 41.32%, Valid: 41.19%, Test: 41.31%
Epoch: 200, Loss: 1.3572, Train: 41.96%, Valid: 41.82%, Test: 41.76%
Epoch: 225, Loss: 1.3470, Train: 42.30%, Valid: 42.00%, Test: 42.21%
Epoch: 250, Loss: 1.3375, Train: 42.56%, Valid: 42.39%, Test: 42.51%
Epoch: 275, Loss: 1.3371, Train: 42.61%, Valid: 42.37%, Test: 42.35%
Epoch: 300, Loss: 1.3268, Train: 42.70%, Valid: 42.47%, Test: 42.45%
Epoch: 325, Loss: 1.3343, Train: 43.38%, Valid: 43.17%, Test: 43.35%
Epoch: 350, Loss: 1.3228, Train: 42.77%, Valid: 42.63%, Test: 42.64%
Epoch: 375, Loss: 1.3600, Train: 43.04%, Valid: 43.01%, Test: 43.15%
Epoch: 400, Loss: 1.3323, Train: 43.35%, Valid: 43.09%, Test: 43.00%
Epoch: 425, Loss: 1.3185, Train: 44.26%, Valid: 44.11%, Test: 44.11%
Epoch: 450, Loss: 1.3174, Train: 43.36%, Valid: 42.92%, Test: 43.12%
Epoch: 475, Loss: 1.3065, Train: 43.27%, Valid: 43.05%, Test: 43.33%
Epoch: 500, Loss: 1.3137, Train: 44.32%, Valid: 43.98%, Test: 44.05%
Epoch: 525, Loss: 1.3036, Train: 44.51%, Valid: 44.38%, Test: 44.54%
Epoch: 550, Loss: 1.3103, Train: 43.58%, Valid: 43.09%, Test: 43.53%
Epoch: 575, Loss: 1.3101, Train: 40.72%, Valid: 40.21%, Test: 40.43%
Epoch: 600, Loss: 1.3039, Train: 44.17%, Valid: 43.82%, Test: 43.92%
Epoch: 625, Loss: 1.3033, Train: 44.72%, Valid: 44.45%, Test: 44.59%
Epoch: 650, Loss: 1.3055, Train: 44.47%, Valid: 44.27%, Test: 44.44%
Epoch: 675, Loss: 1.2921, Train: 43.86%, Valid: 43.78%, Test: 43.82%
Epoch: 700, Loss: 1.2964, Train: 44.76%, Valid: 44.60%, Test: 44.54%
Epoch: 725, Loss: 1.3018, Train: 44.81%, Valid: 44.51%, Test: 44.58%
Epoch: 750, Loss: 1.2947, Train: 45.33%, Valid: 45.05%, Test: 44.94%
Epoch: 775, Loss: 1.2895, Train: 43.80%, Valid: 43.50%, Test: 43.63%
Epoch: 800, Loss: 1.2990, Train: 43.88%, Valid: 43.39%, Test: 43.67%
Epoch: 825, Loss: 1.2918, Train: 44.57%, Valid: 44.39%, Test: 44.24%
Epoch: 850, Loss: 1.2826, Train: 44.56%, Valid: 44.34%, Test: 44.57%
Epoch: 875, Loss: 1.2753, Train: 44.55%, Valid: 44.27%, Test: 44.55%
Epoch: 900, Loss: 1.2810, Train: 43.56%, Valid: 43.10%, Test: 43.15%
Epoch: 925, Loss: 1.2980, Train: 41.99%, Valid: 41.67%, Test: 41.75%
Epoch: 950, Loss: 1.2819, Train: 44.94%, Valid: 44.50%, Test: 44.56%
Epoch: 975, Loss: 1.3131, Train: 44.52%, Valid: 44.13%, Test: 44.24%
Run 01:
Highest Train: 45.91
Highest Valid: 45.70
  Final Train: 45.73
   Final Test: 45.36
All runs:
Highest Train: 45.91, nan
Highest Valid: 45.70, nan
  Final Train: 45.73, nan
   Final Test: 45.36, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 20.8972, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 4.5636, Train: 18.42%, Valid: 18.34%, Test: 18.42%
Epoch: 50, Loss: 2.7724, Train: 17.46%, Valid: 17.68%, Test: 17.13%
Epoch: 75, Loss: 1.7556, Train: 28.73%, Valid: 28.56%, Test: 28.82%
Epoch: 100, Loss: 1.8388, Train: 28.25%, Valid: 28.15%, Test: 27.97%
Epoch: 125, Loss: 1.5428, Train: 28.73%, Valid: 28.56%, Test: 28.83%
Epoch: 150, Loss: 1.7204, Train: 29.97%, Valid: 30.04%, Test: 30.22%
Epoch: 175, Loss: 1.9313, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 200, Loss: 1.5170, Train: 32.82%, Valid: 32.71%, Test: 33.56%
Epoch: 225, Loss: 1.5602, Train: 28.73%, Valid: 28.54%, Test: 28.84%
Epoch: 250, Loss: 1.5121, Train: 33.02%, Valid: 32.86%, Test: 33.76%
Epoch: 275, Loss: 1.4867, Train: 32.81%, Valid: 32.80%, Test: 33.60%
Epoch: 300, Loss: 1.5112, Train: 33.90%, Valid: 33.80%, Test: 34.59%
Epoch: 325, Loss: 1.5002, Train: 35.24%, Valid: 35.05%, Test: 35.51%
Epoch: 350, Loss: 1.5083, Train: 32.02%, Valid: 31.75%, Test: 32.79%
Epoch: 375, Loss: 1.4496, Train: 32.76%, Valid: 32.71%, Test: 33.44%
Epoch: 400, Loss: 1.4562, Train: 33.38%, Valid: 33.24%, Test: 33.94%
Epoch: 425, Loss: 1.4536, Train: 36.18%, Valid: 35.99%, Test: 36.47%
Epoch: 450, Loss: 1.4510, Train: 35.69%, Valid: 35.63%, Test: 36.22%
Epoch: 475, Loss: 1.4494, Train: 37.11%, Valid: 36.98%, Test: 37.47%
Epoch: 500, Loss: 1.4417, Train: 36.94%, Valid: 36.68%, Test: 37.37%
Epoch: 525, Loss: 1.4555, Train: 37.40%, Valid: 37.24%, Test: 37.79%
Epoch: 550, Loss: 1.4763, Train: 37.23%, Valid: 37.18%, Test: 37.73%
Epoch: 575, Loss: 1.4410, Train: 34.87%, Valid: 34.65%, Test: 35.36%
Epoch: 600, Loss: 1.4383, Train: 33.92%, Valid: 33.95%, Test: 34.47%
Epoch: 625, Loss: 1.4350, Train: 34.75%, Valid: 34.45%, Test: 35.08%
Epoch: 650, Loss: 1.4370, Train: 37.86%, Valid: 37.65%, Test: 38.25%
Epoch: 675, Loss: 1.4303, Train: 37.91%, Valid: 37.79%, Test: 38.35%
Epoch: 700, Loss: 1.4299, Train: 38.28%, Valid: 37.99%, Test: 38.50%
Epoch: 725, Loss: 1.4257, Train: 38.44%, Valid: 38.15%, Test: 38.63%
Epoch: 750, Loss: 1.4267, Train: 38.48%, Valid: 38.12%, Test: 38.63%
Epoch: 775, Loss: 1.4479, Train: 38.10%, Valid: 37.74%, Test: 38.29%
Epoch: 800, Loss: 1.4344, Train: 38.00%, Valid: 37.65%, Test: 37.96%
Epoch: 825, Loss: 1.4330, Train: 38.59%, Valid: 38.31%, Test: 38.89%
Epoch: 850, Loss: 1.4161, Train: 38.76%, Valid: 38.39%, Test: 38.94%
Epoch: 875, Loss: 1.4267, Train: 38.37%, Valid: 38.02%, Test: 38.47%
Epoch: 900, Loss: 1.4172, Train: 38.50%, Valid: 38.23%, Test: 38.84%
Epoch: 925, Loss: 1.4272, Train: 38.90%, Valid: 38.57%, Test: 39.11%
Epoch: 950, Loss: 1.4128, Train: 38.72%, Valid: 38.36%, Test: 39.09%
Epoch: 975, Loss: 1.4287, Train: 38.80%, Valid: 38.49%, Test: 38.83%
Run 01:
Highest Train: 39.53
Highest Valid: 39.08
  Final Train: 39.53
   Final Test: 39.67
All runs:
Highest Train: 39.53, nan
Highest Valid: 39.08, nan
  Final Train: 39.53, nan
   Final Test: 39.67, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6145, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4814, Train: 35.15%, Valid: 34.85%, Test: 35.09%
Epoch: 50, Loss: 1.4478, Train: 37.36%, Valid: 36.48%, Test: 36.84%
Epoch: 75, Loss: 1.4190, Train: 38.52%, Valid: 37.62%, Test: 37.93%
Epoch: 100, Loss: 1.3998, Train: 40.01%, Valid: 38.72%, Test: 39.23%
Epoch: 125, Loss: 1.3828, Train: 41.69%, Valid: 40.49%, Test: 40.89%
Epoch: 150, Loss: 1.3682, Train: 42.44%, Valid: 41.15%, Test: 41.45%
Epoch: 175, Loss: 1.3439, Train: 43.38%, Valid: 41.97%, Test: 42.26%
Epoch: 200, Loss: 1.3371, Train: 44.08%, Valid: 42.58%, Test: 42.66%
Epoch: 225, Loss: 1.3234, Train: 44.67%, Valid: 43.15%, Test: 43.37%
Epoch: 250, Loss: 1.3198, Train: 44.87%, Valid: 43.55%, Test: 43.64%
Epoch: 275, Loss: 1.3135, Train: 45.34%, Valid: 43.63%, Test: 43.99%
Epoch: 300, Loss: 1.3064, Train: 45.37%, Valid: 43.82%, Test: 43.83%
Epoch: 325, Loss: 1.3041, Train: 46.02%, Valid: 44.21%, Test: 44.41%
Epoch: 350, Loss: 1.2990, Train: 46.39%, Valid: 44.40%, Test: 44.69%
Epoch: 375, Loss: 1.2937, Train: 46.59%, Valid: 44.66%, Test: 44.78%
Epoch: 400, Loss: 1.2994, Train: 46.55%, Valid: 44.62%, Test: 44.77%
Epoch: 425, Loss: 1.2858, Train: 46.07%, Valid: 44.18%, Test: 44.50%
Epoch: 450, Loss: 1.2851, Train: 46.83%, Valid: 44.71%, Test: 45.01%
Epoch: 475, Loss: 1.2899, Train: 45.71%, Valid: 43.54%, Test: 43.86%
Epoch: 500, Loss: 1.2902, Train: 46.34%, Valid: 44.34%, Test: 44.62%
Epoch: 525, Loss: 1.2950, Train: 46.79%, Valid: 44.66%, Test: 44.89%
Epoch: 550, Loss: 1.2813, Train: 45.99%, Valid: 43.79%, Test: 44.12%
Epoch: 575, Loss: 1.2712, Train: 47.04%, Valid: 44.73%, Test: 44.99%
Epoch: 600, Loss: 1.2860, Train: 46.29%, Valid: 44.20%, Test: 44.28%
Epoch: 625, Loss: 1.2709, Train: 45.85%, Valid: 43.87%, Test: 43.84%
Epoch: 650, Loss: 1.2833, Train: 45.82%, Valid: 43.71%, Test: 43.76%
Epoch: 675, Loss: 1.2761, Train: 45.60%, Valid: 43.67%, Test: 43.67%
Epoch: 700, Loss: 1.2679, Train: 47.58%, Valid: 45.02%, Test: 45.12%
Epoch: 725, Loss: 1.2635, Train: 47.39%, Valid: 44.98%, Test: 45.03%
Epoch: 750, Loss: 1.2601, Train: 47.27%, Valid: 44.89%, Test: 45.17%
Epoch: 775, Loss: 1.2771, Train: 46.38%, Valid: 44.20%, Test: 44.17%
Epoch: 800, Loss: 1.2661, Train: 45.05%, Valid: 42.87%, Test: 42.83%
Epoch: 825, Loss: 1.2685, Train: 45.72%, Valid: 43.73%, Test: 43.45%
Epoch: 850, Loss: 1.2750, Train: 45.09%, Valid: 42.93%, Test: 42.67%
Epoch: 875, Loss: 1.2695, Train: 47.05%, Valid: 44.97%, Test: 45.03%
Epoch: 900, Loss: 1.2594, Train: 44.75%, Valid: 42.66%, Test: 42.65%
Epoch: 925, Loss: 1.2603, Train: 45.48%, Valid: 43.20%, Test: 43.23%
Epoch: 950, Loss: 1.2668, Train: 46.58%, Valid: 44.16%, Test: 44.16%
Epoch: 975, Loss: 1.2483, Train: 46.49%, Valid: 44.01%, Test: 44.22%
Run 01:
Highest Train: 48.56
Highest Valid: 46.21
  Final Train: 48.56
   Final Test: 45.93
All runs:
Highest Train: 48.56, nan
Highest Valid: 46.21, nan
  Final Train: 48.56, nan
   Final Test: 45.93, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.1773, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4644, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 50, Loss: 1.4356, Train: 28.73%, Valid: 28.55%, Test: 28.82%
Epoch: 75, Loss: 1.4196, Train: 33.77%, Valid: 33.45%, Test: 33.91%
Epoch: 100, Loss: 1.3945, Train: 38.61%, Valid: 38.35%, Test: 38.62%
Epoch: 125, Loss: 1.3706, Train: 40.40%, Valid: 40.20%, Test: 40.44%
Epoch: 150, Loss: 1.3739, Train: 40.64%, Valid: 40.52%, Test: 40.69%
Epoch: 175, Loss: 1.3553, Train: 42.28%, Valid: 42.20%, Test: 42.23%
Epoch: 200, Loss: 1.3515, Train: 42.07%, Valid: 41.98%, Test: 41.96%
Epoch: 225, Loss: 1.3847, Train: 40.88%, Valid: 40.71%, Test: 40.75%
Epoch: 250, Loss: 1.3406, Train: 42.93%, Valid: 42.53%, Test: 42.74%
Epoch: 275, Loss: 1.3295, Train: 42.75%, Valid: 42.80%, Test: 42.91%
Epoch: 300, Loss: 1.3305, Train: 43.23%, Valid: 42.94%, Test: 43.22%
Epoch: 325, Loss: 1.3267, Train: 43.30%, Valid: 42.95%, Test: 42.98%
Epoch: 350, Loss: 1.3212, Train: 42.95%, Valid: 42.67%, Test: 42.79%
Epoch: 375, Loss: 1.3241, Train: 43.61%, Valid: 43.34%, Test: 43.58%
Epoch: 400, Loss: 1.3200, Train: 43.17%, Valid: 42.85%, Test: 43.03%
Epoch: 425, Loss: 1.3326, Train: 42.48%, Valid: 42.15%, Test: 42.37%
Epoch: 450, Loss: 1.3257, Train: 42.82%, Valid: 42.34%, Test: 42.66%
Epoch: 475, Loss: 1.3279, Train: 42.65%, Valid: 42.35%, Test: 42.50%
Epoch: 500, Loss: 1.3246, Train: 43.34%, Valid: 42.94%, Test: 43.40%
Epoch: 525, Loss: 1.3299, Train: 42.81%, Valid: 42.44%, Test: 42.67%
Epoch: 550, Loss: 1.3115, Train: 44.12%, Valid: 43.73%, Test: 44.03%
Epoch: 575, Loss: 1.3440, Train: 41.63%, Valid: 41.28%, Test: 41.39%
Epoch: 600, Loss: 1.3184, Train: 42.89%, Valid: 42.47%, Test: 42.81%
Epoch: 625, Loss: 1.3085, Train: 43.98%, Valid: 43.58%, Test: 43.88%
Epoch: 650, Loss: 1.3164, Train: 42.95%, Valid: 42.57%, Test: 43.08%
Epoch: 675, Loss: 1.3144, Train: 44.19%, Valid: 43.80%, Test: 43.93%
Epoch: 700, Loss: 1.3224, Train: 43.37%, Valid: 42.96%, Test: 43.05%
Epoch: 725, Loss: 1.3189, Train: 42.53%, Valid: 42.36%, Test: 42.34%
Epoch: 750, Loss: 1.3226, Train: 43.15%, Valid: 42.99%, Test: 43.26%
Epoch: 775, Loss: 1.3168, Train: 44.36%, Valid: 43.90%, Test: 44.11%
Epoch: 800, Loss: 1.3063, Train: 44.42%, Valid: 44.10%, Test: 44.36%
Epoch: 825, Loss: 1.3023, Train: 43.72%, Valid: 43.64%, Test: 43.82%
Epoch: 850, Loss: 1.3003, Train: 44.42%, Valid: 44.11%, Test: 44.11%
Epoch: 875, Loss: 1.3112, Train: 44.14%, Valid: 43.81%, Test: 44.08%
Epoch: 900, Loss: 1.3026, Train: 43.70%, Valid: 43.43%, Test: 43.65%
Epoch: 925, Loss: 1.3182, Train: 43.84%, Valid: 43.43%, Test: 43.75%
Epoch: 950, Loss: 1.3266, Train: 43.45%, Valid: 43.00%, Test: 43.32%
Epoch: 975, Loss: 1.2927, Train: 44.36%, Valid: 43.98%, Test: 44.14%
Run 01:
Highest Train: 44.79
Highest Valid: 44.46
  Final Train: 44.79
   Final Test: 44.56
All runs:
Highest Train: 44.79, nan
Highest Valid: 44.46, nan
  Final Train: 44.79, nan
   Final Test: 44.56, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 48.9405, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 7.8198, Train: 26.00%, Valid: 25.84%, Test: 26.19%
Epoch: 50, Loss: 2.7114, Train: 28.45%, Valid: 28.18%, Test: 28.55%
Epoch: 75, Loss: 1.7266, Train: 28.62%, Valid: 28.51%, Test: 28.74%
Epoch: 100, Loss: 1.5659, Train: 26.44%, Valid: 26.16%, Test: 26.21%
Epoch: 125, Loss: 1.5466, Train: 26.90%, Valid: 26.56%, Test: 26.74%
Epoch: 150, Loss: 1.5264, Train: 26.18%, Valid: 25.86%, Test: 25.97%
Epoch: 175, Loss: 1.5482, Train: 17.64%, Valid: 17.76%, Test: 17.33%
Epoch: 200, Loss: 1.5294, Train: 28.70%, Valid: 28.55%, Test: 28.80%
Epoch: 225, Loss: 1.5110, Train: 28.33%, Valid: 28.18%, Test: 28.43%
Epoch: 250, Loss: 1.4757, Train: 18.49%, Valid: 18.75%, Test: 18.32%
Epoch: 275, Loss: 1.4826, Train: 26.39%, Valid: 26.01%, Test: 26.26%
Epoch: 300, Loss: 1.4731, Train: 17.63%, Valid: 17.77%, Test: 17.35%
Epoch: 325, Loss: 1.4691, Train: 26.86%, Valid: 26.37%, Test: 26.72%
Epoch: 350, Loss: 1.4680, Train: 22.08%, Valid: 21.92%, Test: 21.89%
Epoch: 375, Loss: 1.5467, Train: 31.14%, Valid: 30.99%, Test: 31.77%
Epoch: 400, Loss: 1.4668, Train: 17.63%, Valid: 17.72%, Test: 17.28%
Epoch: 425, Loss: 1.4555, Train: 17.65%, Valid: 17.78%, Test: 17.32%
Epoch: 450, Loss: 1.4492, Train: 17.80%, Valid: 17.98%, Test: 17.49%
Epoch: 475, Loss: 1.4461, Train: 18.16%, Valid: 18.26%, Test: 17.88%
Epoch: 500, Loss: 1.4401, Train: 18.04%, Valid: 18.16%, Test: 17.81%
Epoch: 525, Loss: 1.4390, Train: 18.54%, Valid: 18.72%, Test: 18.32%
Epoch: 550, Loss: 1.4555, Train: 19.52%, Valid: 19.67%, Test: 19.40%
Epoch: 575, Loss: 1.4337, Train: 20.91%, Valid: 21.31%, Test: 21.28%
Epoch: 600, Loss: 1.4351, Train: 22.35%, Valid: 22.46%, Test: 22.74%
Epoch: 625, Loss: 1.4379, Train: 23.65%, Valid: 23.81%, Test: 24.16%
Epoch: 650, Loss: 1.4303, Train: 23.68%, Valid: 23.82%, Test: 24.17%
Epoch: 675, Loss: 1.4264, Train: 24.63%, Valid: 24.80%, Test: 25.10%
Epoch: 700, Loss: 1.4227, Train: 24.76%, Valid: 24.93%, Test: 25.40%
Epoch: 725, Loss: 1.4318, Train: 33.49%, Valid: 33.26%, Test: 34.20%
Epoch: 750, Loss: 1.4334, Train: 32.77%, Valid: 32.44%, Test: 33.22%
Epoch: 775, Loss: 1.4293, Train: 33.58%, Valid: 33.35%, Test: 34.35%
Epoch: 800, Loss: 1.4210, Train: 32.04%, Valid: 31.68%, Test: 32.74%
Epoch: 825, Loss: 1.4185, Train: 28.78%, Valid: 28.83%, Test: 29.62%
Epoch: 850, Loss: 1.4231, Train: 33.95%, Valid: 33.63%, Test: 34.49%
Epoch: 875, Loss: 1.4189, Train: 33.65%, Valid: 33.28%, Test: 34.31%
Epoch: 900, Loss: 1.4259, Train: 24.99%, Valid: 25.06%, Test: 25.66%
Epoch: 925, Loss: 1.4149, Train: 34.18%, Valid: 33.73%, Test: 34.62%
Epoch: 950, Loss: 1.4138, Train: 29.17%, Valid: 28.97%, Test: 30.03%
Epoch: 975, Loss: 1.4242, Train: 30.51%, Valid: 30.10%, Test: 31.25%
Run 01:
Highest Train: 34.30
Highest Valid: 33.86
  Final Train: 34.16
   Final Test: 34.70
All runs:
Highest Train: 34.30, nan
Highest Valid: 33.86, nan
  Final Train: 34.16, nan
   Final Test: 34.70, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6251, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4711, Train: 35.20%, Valid: 34.96%, Test: 35.15%
Epoch: 50, Loss: 1.4275, Train: 37.83%, Valid: 37.16%, Test: 37.45%
Epoch: 75, Loss: 1.3897, Train: 39.26%, Valid: 38.33%, Test: 38.55%
Epoch: 100, Loss: 1.3519, Train: 40.67%, Valid: 39.83%, Test: 39.95%
Epoch: 125, Loss: 1.3229, Train: 42.48%, Valid: 41.20%, Test: 41.59%
Epoch: 150, Loss: 1.2931, Train: 44.02%, Valid: 42.52%, Test: 42.85%
Epoch: 175, Loss: 1.2873, Train: 44.38%, Valid: 42.86%, Test: 43.27%
Epoch: 200, Loss: 1.2775, Train: 44.74%, Valid: 42.94%, Test: 43.29%
Epoch: 225, Loss: 1.2502, Train: 45.20%, Valid: 43.21%, Test: 43.39%
Epoch: 250, Loss: 1.2368, Train: 46.63%, Valid: 44.59%, Test: 44.91%
Epoch: 275, Loss: 1.2169, Train: 47.60%, Valid: 45.01%, Test: 45.40%
Epoch: 300, Loss: 1.2121, Train: 48.29%, Valid: 45.62%, Test: 45.73%
Epoch: 325, Loss: 1.2602, Train: 47.86%, Valid: 44.98%, Test: 45.47%
Epoch: 350, Loss: 1.1883, Train: 49.10%, Valid: 46.25%, Test: 46.30%
Epoch: 375, Loss: 1.1738, Train: 49.57%, Valid: 46.28%, Test: 46.42%
Epoch: 400, Loss: 1.1538, Train: 50.74%, Valid: 47.03%, Test: 47.07%
Epoch: 425, Loss: 1.1556, Train: 50.61%, Valid: 47.19%, Test: 47.12%
Epoch: 450, Loss: 1.1708, Train: 50.20%, Valid: 46.38%, Test: 46.59%
Epoch: 475, Loss: 1.1305, Train: 51.86%, Valid: 47.44%, Test: 47.52%
Epoch: 500, Loss: 1.1259, Train: 51.32%, Valid: 46.82%, Test: 46.60%
Epoch: 525, Loss: 1.1109, Train: 52.56%, Valid: 47.76%, Test: 47.88%
Epoch: 550, Loss: 1.1268, Train: 52.19%, Valid: 47.44%, Test: 47.63%
Epoch: 575, Loss: 1.0967, Train: 53.23%, Valid: 47.91%, Test: 48.11%
Epoch: 600, Loss: 1.0963, Train: 53.14%, Valid: 47.92%, Test: 48.11%
Epoch: 625, Loss: 1.0903, Train: 53.59%, Valid: 47.88%, Test: 48.02%
Epoch: 650, Loss: 1.1712, Train: 49.92%, Valid: 45.84%, Test: 46.21%
Epoch: 675, Loss: 1.0957, Train: 53.29%, Valid: 48.10%, Test: 48.25%
Epoch: 700, Loss: 1.0876, Train: 54.09%, Valid: 48.28%, Test: 48.37%
Epoch: 725, Loss: 1.0834, Train: 54.41%, Valid: 48.13%, Test: 48.39%
Epoch: 750, Loss: 1.0789, Train: 54.24%, Valid: 47.96%, Test: 47.98%
Epoch: 775, Loss: 1.0644, Train: 54.34%, Valid: 47.97%, Test: 48.01%
Epoch: 800, Loss: 1.0850, Train: 53.81%, Valid: 47.80%, Test: 47.71%
Epoch: 825, Loss: 1.0490, Train: 55.46%, Valid: 48.65%, Test: 48.81%
Epoch: 850, Loss: 1.0499, Train: 55.08%, Valid: 48.64%, Test: 48.61%
Epoch: 875, Loss: 1.0382, Train: 55.94%, Valid: 48.87%, Test: 48.91%
Epoch: 900, Loss: 1.0454, Train: 55.45%, Valid: 48.74%, Test: 48.66%
Epoch: 925, Loss: 1.0482, Train: 55.84%, Valid: 48.60%, Test: 48.67%
Epoch: 950, Loss: 1.0358, Train: 56.11%, Valid: 48.78%, Test: 48.78%
Epoch: 975, Loss: 1.0284, Train: 56.48%, Valid: 48.86%, Test: 48.88%
Run 01:
Highest Train: 56.64
Highest Valid: 49.01
  Final Train: 56.14
   Final Test: 48.87
All runs:
Highest Train: 56.64, nan
Highest Valid: 49.01, nan
  Final Train: 56.14, nan
   Final Test: 48.87, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.1253, Train: 26.15%, Valid: 26.09%, Test: 26.05%
Epoch: 25, Loss: 1.4472, Train: 34.79%, Valid: 34.32%, Test: 34.68%
Epoch: 50, Loss: 1.4246, Train: 39.40%, Valid: 39.03%, Test: 39.17%
Epoch: 75, Loss: 1.3767, Train: 40.50%, Valid: 40.23%, Test: 40.55%
Epoch: 100, Loss: 1.3971, Train: 40.17%, Valid: 39.72%, Test: 39.75%
Epoch: 125, Loss: 1.3638, Train: 41.06%, Valid: 40.75%, Test: 41.04%
Epoch: 150, Loss: 1.3709, Train: 40.98%, Valid: 40.63%, Test: 40.99%
Epoch: 175, Loss: 1.3477, Train: 41.63%, Valid: 41.24%, Test: 41.57%
Epoch: 200, Loss: 1.3816, Train: 39.80%, Valid: 39.47%, Test: 39.83%
Epoch: 225, Loss: 1.3455, Train: 41.35%, Valid: 40.96%, Test: 41.31%
Epoch: 250, Loss: 1.3552, Train: 41.95%, Valid: 41.53%, Test: 41.93%
Epoch: 275, Loss: 1.3422, Train: 41.63%, Valid: 41.20%, Test: 41.55%
Epoch: 300, Loss: 1.3297, Train: 42.28%, Valid: 41.88%, Test: 42.23%
Epoch: 325, Loss: 1.3528, Train: 42.09%, Valid: 41.76%, Test: 42.02%
Epoch: 350, Loss: 1.3466, Train: 41.90%, Valid: 41.46%, Test: 42.02%
Epoch: 375, Loss: 1.3301, Train: 42.28%, Valid: 41.87%, Test: 42.21%
Epoch: 400, Loss: 1.3222, Train: 42.64%, Valid: 42.33%, Test: 42.61%
Epoch: 425, Loss: 1.5607, Train: 38.84%, Valid: 38.60%, Test: 38.70%
Epoch: 450, Loss: 1.3861, Train: 39.45%, Valid: 39.09%, Test: 39.22%
Epoch: 475, Loss: 1.3547, Train: 41.71%, Valid: 41.10%, Test: 41.43%
Epoch: 500, Loss: 1.3618, Train: 41.80%, Valid: 41.42%, Test: 41.85%
Epoch: 525, Loss: 1.3427, Train: 41.78%, Valid: 41.26%, Test: 41.69%
Epoch: 550, Loss: 1.3305, Train: 42.75%, Valid: 42.21%, Test: 42.66%
Epoch: 575, Loss: 1.3238, Train: 43.00%, Valid: 42.41%, Test: 42.95%
Epoch: 600, Loss: 1.3278, Train: 43.00%, Valid: 42.48%, Test: 42.82%
Epoch: 625, Loss: 1.3167, Train: 43.46%, Valid: 42.80%, Test: 43.20%
Epoch: 650, Loss: 1.3346, Train: 43.18%, Valid: 42.57%, Test: 42.92%
Epoch: 675, Loss: 1.3187, Train: 43.36%, Valid: 42.71%, Test: 43.11%
Epoch: 700, Loss: 1.3162, Train: 42.69%, Valid: 42.23%, Test: 42.56%
Epoch: 725, Loss: 1.3548, Train: 38.15%, Valid: 38.06%, Test: 38.14%
Epoch: 750, Loss: 1.3297, Train: 42.55%, Valid: 42.11%, Test: 42.34%
Epoch: 775, Loss: 1.3133, Train: 43.31%, Valid: 42.82%, Test: 43.06%
Epoch: 800, Loss: 1.3148, Train: 43.30%, Valid: 42.75%, Test: 43.03%
Epoch: 825, Loss: 1.3030, Train: 43.71%, Valid: 43.34%, Test: 43.43%
Epoch: 850, Loss: 1.3262, Train: 42.92%, Valid: 42.57%, Test: 42.61%
Epoch: 875, Loss: 1.3195, Train: 43.10%, Valid: 42.71%, Test: 42.89%
Epoch: 900, Loss: 1.3100, Train: 43.87%, Valid: 43.34%, Test: 43.54%
Epoch: 925, Loss: 1.3125, Train: 44.01%, Valid: 43.57%, Test: 43.72%
Epoch: 950, Loss: 1.3080, Train: 43.59%, Valid: 43.10%, Test: 43.33%
Epoch: 975, Loss: 1.2992, Train: 43.50%, Valid: 43.24%, Test: 43.14%
Run 01:
Highest Train: 44.46
Highest Valid: 43.94
  Final Train: 44.37
   Final Test: 43.94
All runs:
Highest Train: 44.46, nan
Highest Valid: 43.94, nan
  Final Train: 44.37, nan
   Final Test: 43.94, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 11.5121, Train: 25.00%, Valid: 24.82%, Test: 24.79%
Epoch: 25, Loss: 2.0322, Train: 28.81%, Valid: 28.57%, Test: 28.94%
Epoch: 50, Loss: 1.5817, Train: 29.08%, Valid: 29.26%, Test: 29.44%
Epoch: 75, Loss: 1.5235, Train: 31.21%, Valid: 31.06%, Test: 31.75%
Epoch: 100, Loss: 1.5326, Train: 30.60%, Valid: 30.51%, Test: 31.01%
Epoch: 125, Loss: 1.4512, Train: 35.55%, Valid: 35.19%, Test: 35.46%
Epoch: 150, Loss: 1.4285, Train: 37.99%, Valid: 37.54%, Test: 37.97%
Epoch: 175, Loss: 1.4186, Train: 38.08%, Valid: 37.63%, Test: 38.09%
Epoch: 200, Loss: 1.4180, Train: 37.83%, Valid: 37.30%, Test: 37.90%
Epoch: 225, Loss: 1.4159, Train: 38.10%, Valid: 37.74%, Test: 38.28%
Epoch: 250, Loss: 1.4141, Train: 38.82%, Valid: 38.36%, Test: 38.85%
Epoch: 275, Loss: 1.4449, Train: 34.54%, Valid: 34.23%, Test: 35.13%
Epoch: 300, Loss: 1.4091, Train: 38.34%, Valid: 37.99%, Test: 38.52%
Epoch: 325, Loss: 1.4002, Train: 38.84%, Valid: 38.25%, Test: 38.77%
Epoch: 350, Loss: 1.4087, Train: 38.03%, Valid: 37.53%, Test: 37.89%
Epoch: 375, Loss: 1.4851, Train: 39.11%, Valid: 38.60%, Test: 38.93%
Epoch: 400, Loss: 1.4274, Train: 33.57%, Valid: 33.24%, Test: 34.18%
Epoch: 425, Loss: 1.3994, Train: 38.66%, Valid: 38.16%, Test: 38.45%
Epoch: 450, Loss: 1.3900, Train: 39.14%, Valid: 38.49%, Test: 38.96%
Epoch: 475, Loss: 1.4016, Train: 38.31%, Valid: 37.78%, Test: 38.29%
Epoch: 500, Loss: 1.4556, Train: 33.17%, Valid: 32.81%, Test: 33.35%
Epoch: 525, Loss: 1.4241, Train: 36.52%, Valid: 35.88%, Test: 36.54%
Epoch: 550, Loss: 1.4075, Train: 38.36%, Valid: 37.87%, Test: 38.19%
Epoch: 575, Loss: 1.3992, Train: 39.06%, Valid: 38.52%, Test: 38.90%
Epoch: 600, Loss: 1.4241, Train: 35.23%, Valid: 34.75%, Test: 35.81%
Epoch: 625, Loss: 1.4363, Train: 39.54%, Valid: 39.07%, Test: 39.40%
Epoch: 650, Loss: 1.4093, Train: 39.43%, Valid: 38.91%, Test: 39.05%
Epoch: 675, Loss: 1.3984, Train: 39.42%, Valid: 38.99%, Test: 39.29%
Epoch: 700, Loss: 1.4058, Train: 39.27%, Valid: 38.65%, Test: 39.04%
Epoch: 725, Loss: 1.3886, Train: 39.39%, Valid: 39.04%, Test: 39.15%
Epoch: 750, Loss: 1.3801, Train: 39.51%, Valid: 39.14%, Test: 39.30%
Epoch: 775, Loss: 1.4333, Train: 38.04%, Valid: 37.34%, Test: 37.59%
Epoch: 800, Loss: 1.4011, Train: 38.43%, Valid: 37.81%, Test: 38.16%
Epoch: 825, Loss: 1.3865, Train: 39.22%, Valid: 38.81%, Test: 38.90%
Epoch: 850, Loss: 1.3790, Train: 39.47%, Valid: 39.06%, Test: 39.20%
Epoch: 875, Loss: 1.4598, Train: 38.79%, Valid: 38.41%, Test: 38.73%
Epoch: 900, Loss: 1.4289, Train: 36.67%, Valid: 36.21%, Test: 36.73%
Epoch: 925, Loss: 1.3984, Train: 39.15%, Valid: 38.73%, Test: 38.84%
Epoch: 950, Loss: 1.3868, Train: 39.37%, Valid: 39.08%, Test: 39.20%
Epoch: 975, Loss: 1.3813, Train: 39.67%, Valid: 39.40%, Test: 39.50%
Run 01:
Highest Train: 39.86
Highest Valid: 39.57
  Final Train: 39.78
   Final Test: 39.63
All runs:
Highest Train: 39.86, nan
Highest Valid: 39.57, nan
  Final Train: 39.78, nan
   Final Test: 39.63, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6269, Train: 28.69%, Valid: 28.53%, Test: 28.84%
Epoch: 25, Loss: 1.4772, Train: 35.01%, Valid: 34.74%, Test: 34.99%
Epoch: 50, Loss: 1.4315, Train: 37.66%, Valid: 37.02%, Test: 37.22%
Epoch: 75, Loss: 1.3894, Train: 39.26%, Valid: 38.30%, Test: 38.47%
Epoch: 100, Loss: 1.3584, Train: 40.85%, Valid: 39.84%, Test: 39.91%
Epoch: 125, Loss: 1.3278, Train: 42.35%, Valid: 41.28%, Test: 41.49%
Epoch: 150, Loss: 1.3237, Train: 42.72%, Valid: 41.76%, Test: 41.88%
Epoch: 175, Loss: 1.3051, Train: 40.31%, Valid: 39.24%, Test: 39.58%
Epoch: 200, Loss: 1.2740, Train: 45.08%, Valid: 43.56%, Test: 43.65%
Epoch: 225, Loss: 1.2549, Train: 44.95%, Valid: 43.03%, Test: 43.18%
Epoch: 250, Loss: 1.2351, Train: 47.07%, Valid: 44.78%, Test: 44.94%
Epoch: 275, Loss: 1.2572, Train: 45.78%, Valid: 43.81%, Test: 44.24%
Epoch: 300, Loss: 1.2204, Train: 47.55%, Valid: 44.95%, Test: 45.31%
Epoch: 325, Loss: 1.2251, Train: 47.46%, Valid: 44.71%, Test: 45.01%
Epoch: 350, Loss: 1.1971, Train: 48.74%, Valid: 45.59%, Test: 45.98%
Epoch: 375, Loss: 1.1877, Train: 49.39%, Valid: 45.92%, Test: 46.27%
Epoch: 400, Loss: 1.1942, Train: 49.22%, Valid: 46.04%, Test: 46.33%
Epoch: 425, Loss: 1.1679, Train: 50.42%, Valid: 46.43%, Test: 46.79%
Epoch: 450, Loss: 1.2329, Train: 46.67%, Valid: 44.28%, Test: 44.65%
Epoch: 475, Loss: 1.1735, Train: 50.15%, Valid: 46.56%, Test: 46.60%
Epoch: 500, Loss: 1.1523, Train: 51.09%, Valid: 46.90%, Test: 46.98%
Epoch: 525, Loss: 1.1709, Train: 50.66%, Valid: 46.28%, Test: 46.53%
Epoch: 550, Loss: 1.1379, Train: 51.69%, Valid: 47.07%, Test: 47.11%
Epoch: 575, Loss: 1.1424, Train: 51.85%, Valid: 47.19%, Test: 47.20%
Epoch: 600, Loss: 1.1386, Train: 51.55%, Valid: 46.86%, Test: 46.81%
Epoch: 625, Loss: 1.1207, Train: 52.43%, Valid: 47.51%, Test: 47.49%
Epoch: 650, Loss: 1.1182, Train: 52.43%, Valid: 47.32%, Test: 47.41%
Epoch: 675, Loss: 1.1213, Train: 52.64%, Valid: 47.56%, Test: 47.59%
Epoch: 700, Loss: 1.0974, Train: 53.45%, Valid: 47.71%, Test: 47.95%
Epoch: 725, Loss: 1.4361, Train: 40.04%, Valid: 39.94%, Test: 39.91%
Epoch: 750, Loss: 1.3257, Train: 42.89%, Valid: 42.46%, Test: 42.78%
Epoch: 775, Loss: 1.2982, Train: 44.52%, Valid: 43.50%, Test: 44.01%
Epoch: 800, Loss: 1.2844, Train: 45.18%, Valid: 44.17%, Test: 44.47%
Epoch: 825, Loss: 1.2736, Train: 45.30%, Valid: 43.99%, Test: 44.64%
Epoch: 850, Loss: 1.2622, Train: 46.03%, Valid: 44.67%, Test: 45.15%
Epoch: 875, Loss: 1.2584, Train: 46.06%, Valid: 44.71%, Test: 45.22%
Epoch: 900, Loss: 1.2491, Train: 46.51%, Valid: 45.18%, Test: 45.56%
Epoch: 925, Loss: 1.2467, Train: 46.69%, Valid: 45.30%, Test: 45.74%
Epoch: 950, Loss: 1.2371, Train: 46.93%, Valid: 45.48%, Test: 45.75%
Epoch: 975, Loss: 1.2337, Train: 47.02%, Valid: 45.46%, Test: 45.90%
Run 01:
Highest Train: 53.56
Highest Valid: 47.80
  Final Train: 53.44
   Final Test: 47.88
All runs:
Highest Train: 53.56, nan
Highest Valid: 47.80, nan
  Final Train: 53.44, nan
   Final Test: 47.88, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.8175, Train: 28.71%, Valid: 28.54%, Test: 28.80%
Epoch: 25, Loss: 1.4252, Train: 38.09%, Valid: 37.78%, Test: 38.37%
Epoch: 50, Loss: 1.3773, Train: 40.98%, Valid: 40.60%, Test: 40.83%
Epoch: 75, Loss: 1.3767, Train: 41.14%, Valid: 40.58%, Test: 40.99%
Epoch: 100, Loss: 1.3743, Train: 39.56%, Valid: 39.36%, Test: 39.73%
Epoch: 125, Loss: 1.3492, Train: 42.04%, Valid: 41.61%, Test: 41.81%
Epoch: 150, Loss: 1.3708, Train: 41.76%, Valid: 41.35%, Test: 41.51%
Epoch: 175, Loss: 1.3499, Train: 41.00%, Valid: 40.74%, Test: 41.16%
Epoch: 200, Loss: 1.3425, Train: 41.30%, Valid: 41.05%, Test: 41.29%
Epoch: 225, Loss: 1.3484, Train: 41.18%, Valid: 40.81%, Test: 41.20%
Epoch: 250, Loss: 1.3348, Train: 41.52%, Valid: 41.22%, Test: 41.44%
Epoch: 275, Loss: 1.3532, Train: 40.87%, Valid: 40.56%, Test: 40.90%
Epoch: 300, Loss: 1.3276, Train: 42.77%, Valid: 42.40%, Test: 42.70%
Epoch: 325, Loss: 1.3456, Train: 42.46%, Valid: 42.12%, Test: 42.09%
Epoch: 350, Loss: 1.3277, Train: 42.82%, Valid: 42.29%, Test: 42.57%
Epoch: 375, Loss: 1.3376, Train: 42.45%, Valid: 42.06%, Test: 42.40%
Epoch: 400, Loss: 1.3439, Train: 42.16%, Valid: 41.54%, Test: 41.88%
Epoch: 425, Loss: 1.3276, Train: 42.64%, Valid: 42.04%, Test: 42.45%
Epoch: 450, Loss: 1.5385, Train: 41.15%, Valid: 40.65%, Test: 40.82%
Epoch: 475, Loss: 1.3543, Train: 41.48%, Valid: 40.99%, Test: 41.40%
Epoch: 500, Loss: 1.3400, Train: 41.98%, Valid: 41.52%, Test: 41.86%
Epoch: 525, Loss: 1.3296, Train: 42.52%, Valid: 42.03%, Test: 42.47%
Epoch: 550, Loss: 1.3487, Train: 42.54%, Valid: 42.12%, Test: 42.21%
Epoch: 575, Loss: 1.3291, Train: 42.54%, Valid: 42.02%, Test: 42.38%
Epoch: 600, Loss: 1.3183, Train: 43.29%, Valid: 42.81%, Test: 42.95%
Epoch: 625, Loss: 1.3431, Train: 41.86%, Valid: 41.23%, Test: 41.62%
Epoch: 650, Loss: 1.3239, Train: 42.80%, Valid: 42.14%, Test: 42.51%
Epoch: 675, Loss: 1.3263, Train: 42.53%, Valid: 42.09%, Test: 42.47%
Epoch: 700, Loss: 1.3126, Train: 43.37%, Valid: 42.75%, Test: 43.09%
Epoch: 725, Loss: 1.3439, Train: 41.78%, Valid: 41.37%, Test: 41.58%
Epoch: 750, Loss: 1.3162, Train: 43.07%, Valid: 42.44%, Test: 42.73%
Epoch: 775, Loss: 1.3115, Train: 42.78%, Valid: 42.26%, Test: 42.62%
Epoch: 800, Loss: 1.3083, Train: 43.46%, Valid: 43.05%, Test: 43.30%
Epoch: 825, Loss: 1.3144, Train: 43.32%, Valid: 42.73%, Test: 43.05%
Epoch: 850, Loss: 1.3020, Train: 42.92%, Valid: 42.50%, Test: 42.72%
Epoch: 875, Loss: 1.3127, Train: 43.67%, Valid: 43.07%, Test: 43.40%
Epoch: 900, Loss: 1.2954, Train: 43.59%, Valid: 43.25%, Test: 43.38%
Epoch: 925, Loss: 1.3540, Train: 42.08%, Valid: 41.37%, Test: 41.73%
Epoch: 950, Loss: 1.3219, Train: 42.86%, Valid: 42.41%, Test: 42.67%
Epoch: 975, Loss: 1.3062, Train: 43.64%, Valid: 43.15%, Test: 43.42%
Run 01:
Highest Train: 44.28
Highest Valid: 43.79
  Final Train: 44.28
   Final Test: 43.86
All runs:
Highest Train: 44.28, nan
Highest Valid: 43.79, nan
  Final Train: 44.28, nan
   Final Test: 43.86, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 13.0762, Train: 27.87%, Valid: 27.67%, Test: 27.74%
Epoch: 25, Loss: 1.8200, Train: 27.67%, Valid: 27.45%, Test: 27.60%
Epoch: 50, Loss: 1.5185, Train: 31.64%, Valid: 31.51%, Test: 31.87%
Epoch: 75, Loss: 1.4700, Train: 32.94%, Valid: 32.64%, Test: 32.80%
Epoch: 100, Loss: 1.4473, Train: 35.75%, Valid: 35.44%, Test: 35.39%
Epoch: 125, Loss: 1.4346, Train: 36.34%, Valid: 35.93%, Test: 36.07%
Epoch: 150, Loss: 1.4965, Train: 32.05%, Valid: 31.84%, Test: 32.35%
Epoch: 175, Loss: 1.4388, Train: 35.31%, Valid: 35.16%, Test: 35.13%
Epoch: 200, Loss: 1.4222, Train: 36.71%, Valid: 36.36%, Test: 36.25%
Epoch: 225, Loss: 1.4359, Train: 37.96%, Valid: 37.68%, Test: 37.75%
Epoch: 250, Loss: 1.4267, Train: 38.04%, Valid: 37.82%, Test: 37.71%
Epoch: 275, Loss: 1.4091, Train: 37.19%, Valid: 36.92%, Test: 36.84%
Epoch: 300, Loss: 1.4083, Train: 38.35%, Valid: 38.02%, Test: 37.87%
Epoch: 325, Loss: 1.4223, Train: 37.16%, Valid: 36.84%, Test: 36.80%
Epoch: 350, Loss: 1.4116, Train: 37.79%, Valid: 37.60%, Test: 37.33%
Epoch: 375, Loss: 1.4872, Train: 34.36%, Valid: 34.10%, Test: 34.11%
Epoch: 400, Loss: 1.4424, Train: 34.39%, Valid: 34.05%, Test: 34.32%
Epoch: 425, Loss: 1.4161, Train: 36.80%, Valid: 36.53%, Test: 36.43%
Epoch: 450, Loss: 1.4037, Train: 38.02%, Valid: 37.74%, Test: 37.63%
Epoch: 475, Loss: 1.3961, Train: 38.64%, Valid: 38.25%, Test: 38.28%
Epoch: 500, Loss: 1.4580, Train: 33.47%, Valid: 33.35%, Test: 33.88%
Epoch: 525, Loss: 1.4147, Train: 35.97%, Valid: 35.44%, Test: 35.81%
Epoch: 550, Loss: 1.4034, Train: 38.22%, Valid: 37.76%, Test: 37.79%
Epoch: 575, Loss: 1.3960, Train: 38.70%, Valid: 38.35%, Test: 38.31%
Epoch: 600, Loss: 1.3916, Train: 38.29%, Valid: 37.88%, Test: 38.06%
Epoch: 625, Loss: 1.8489, Train: 31.28%, Valid: 30.95%, Test: 31.73%
Epoch: 650, Loss: 1.5525, Train: 28.40%, Valid: 28.14%, Test: 28.71%
Epoch: 675, Loss: 1.4916, Train: 32.77%, Valid: 32.37%, Test: 32.79%
Epoch: 700, Loss: 1.4548, Train: 35.61%, Valid: 35.03%, Test: 35.45%
Epoch: 725, Loss: 1.4343, Train: 37.36%, Valid: 36.76%, Test: 36.92%
Epoch: 750, Loss: 1.4194, Train: 37.99%, Valid: 37.46%, Test: 37.63%
Epoch: 775, Loss: 1.4490, Train: 30.87%, Valid: 30.46%, Test: 30.95%
Epoch: 800, Loss: 1.4086, Train: 38.39%, Valid: 37.81%, Test: 38.03%
Epoch: 825, Loss: 1.4039, Train: 38.72%, Valid: 38.14%, Test: 38.49%
Epoch: 850, Loss: 1.4029, Train: 38.65%, Valid: 38.04%, Test: 38.35%
Epoch: 875, Loss: 1.3903, Train: 38.46%, Valid: 37.96%, Test: 38.20%
Epoch: 900, Loss: 1.3870, Train: 39.74%, Valid: 39.09%, Test: 39.44%
Epoch: 925, Loss: 1.4539, Train: 34.83%, Valid: 34.49%, Test: 34.60%
Epoch: 950, Loss: 1.4022, Train: 39.66%, Valid: 39.15%, Test: 39.39%
Epoch: 975, Loss: 1.3831, Train: 39.79%, Valid: 39.30%, Test: 39.58%
Run 01:
Highest Train: 40.35
Highest Valid: 39.89
  Final Train: 40.28
   Final Test: 40.07
All runs:
Highest Train: 40.35, nan
Highest Valid: 39.89, nan
  Final Train: 40.28, nan
   Final Test: 40.07, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.5984, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4737, Train: 35.48%, Valid: 35.16%, Test: 35.36%
Epoch: 50, Loss: 1.4344, Train: 37.94%, Valid: 37.23%, Test: 37.63%
Epoch: 75, Loss: 1.3961, Train: 39.54%, Valid: 38.72%, Test: 39.00%
Epoch: 100, Loss: 1.3699, Train: 41.55%, Valid: 40.41%, Test: 40.96%
Epoch: 125, Loss: 1.3504, Train: 42.64%, Valid: 41.83%, Test: 41.91%
Epoch: 150, Loss: 1.3415, Train: 43.66%, Valid: 42.58%, Test: 42.96%
Epoch: 175, Loss: 1.3237, Train: 44.26%, Valid: 43.21%, Test: 43.69%
Epoch: 200, Loss: 1.3325, Train: 43.92%, Valid: 42.91%, Test: 43.05%
Epoch: 225, Loss: 1.3132, Train: 43.84%, Valid: 42.92%, Test: 43.04%
Epoch: 250, Loss: 1.3110, Train: 45.24%, Valid: 44.20%, Test: 44.59%
Epoch: 275, Loss: 1.3040, Train: 43.93%, Valid: 43.09%, Test: 43.19%
Epoch: 300, Loss: 1.3119, Train: 43.54%, Valid: 42.75%, Test: 42.84%
Epoch: 325, Loss: 1.2943, Train: 44.68%, Valid: 43.89%, Test: 43.93%
Epoch: 350, Loss: 1.3043, Train: 44.46%, Valid: 43.57%, Test: 43.61%
Epoch: 375, Loss: 1.3305, Train: 44.34%, Valid: 43.34%, Test: 43.63%
Epoch: 400, Loss: 1.3006, Train: 43.77%, Valid: 42.82%, Test: 42.87%
Epoch: 425, Loss: 1.3092, Train: 44.62%, Valid: 43.65%, Test: 43.84%
Epoch: 450, Loss: 1.2776, Train: 45.17%, Valid: 43.94%, Test: 44.20%
Epoch: 475, Loss: 1.3029, Train: 43.06%, Valid: 41.85%, Test: 41.97%
Epoch: 500, Loss: 1.2847, Train: 44.83%, Valid: 43.71%, Test: 44.01%
Epoch: 525, Loss: 1.2907, Train: 44.20%, Valid: 43.19%, Test: 43.20%
Epoch: 550, Loss: 1.2962, Train: 44.54%, Valid: 43.52%, Test: 43.69%
Epoch: 575, Loss: 1.2930, Train: 44.72%, Valid: 43.77%, Test: 43.85%
Epoch: 600, Loss: 1.3059, Train: 44.93%, Valid: 43.85%, Test: 43.94%
Epoch: 625, Loss: 1.2870, Train: 45.25%, Valid: 44.02%, Test: 44.16%
Epoch: 650, Loss: 1.2781, Train: 45.00%, Valid: 43.73%, Test: 43.99%
Epoch: 675, Loss: 1.2847, Train: 44.48%, Valid: 42.97%, Test: 43.31%
Epoch: 700, Loss: 1.2708, Train: 45.60%, Valid: 44.42%, Test: 44.65%
Epoch: 725, Loss: 1.2718, Train: 44.64%, Valid: 43.58%, Test: 43.70%
Epoch: 750, Loss: 1.2871, Train: 44.78%, Valid: 43.56%, Test: 43.47%
Epoch: 775, Loss: 1.2674, Train: 44.94%, Valid: 44.00%, Test: 44.03%
Epoch: 800, Loss: 1.2682, Train: 44.62%, Valid: 43.44%, Test: 43.37%
Epoch: 825, Loss: 1.2735, Train: 43.59%, Valid: 42.60%, Test: 42.95%
Epoch: 850, Loss: 1.2813, Train: 45.64%, Valid: 44.33%, Test: 44.91%
Epoch: 875, Loss: 1.2632, Train: 45.38%, Valid: 44.04%, Test: 44.24%
Epoch: 900, Loss: 1.2744, Train: 44.80%, Valid: 43.66%, Test: 43.73%
Epoch: 925, Loss: 1.2769, Train: 45.88%, Valid: 44.65%, Test: 44.86%
Epoch: 950, Loss: 1.2747, Train: 45.81%, Valid: 44.65%, Test: 44.68%
Epoch: 975, Loss: 1.2660, Train: 45.05%, Valid: 43.90%, Test: 44.03%
Run 01:
Highest Train: 46.42
Highest Valid: 45.10
  Final Train: 46.36
   Final Test: 45.26
All runs:
Highest Train: 46.42, nan
Highest Valid: 45.10, nan
  Final Train: 46.36, nan
   Final Test: 45.26, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.0076, Train: 28.06%, Valid: 27.78%, Test: 28.04%
Epoch: 25, Loss: 1.5107, Train: 29.09%, Valid: 28.92%, Test: 29.07%
Epoch: 50, Loss: 1.4904, Train: 29.39%, Valid: 29.03%, Test: 29.43%
Epoch: 75, Loss: 1.4771, Train: 29.93%, Valid: 29.84%, Test: 29.89%
Epoch: 100, Loss: 1.4924, Train: 30.05%, Valid: 29.92%, Test: 29.97%
Epoch: 125, Loss: 1.4715, Train: 29.17%, Valid: 29.09%, Test: 29.18%
Epoch: 150, Loss: 1.4623, Train: 30.41%, Valid: 30.35%, Test: 30.45%
Epoch: 175, Loss: 1.4484, Train: 33.04%, Valid: 32.80%, Test: 32.93%
Epoch: 200, Loss: 1.4435, Train: 35.98%, Valid: 35.57%, Test: 35.79%
Epoch: 225, Loss: 1.4349, Train: 36.71%, Valid: 36.43%, Test: 36.62%
Epoch: 250, Loss: 1.4205, Train: 39.91%, Valid: 39.63%, Test: 39.64%
Epoch: 275, Loss: 1.4241, Train: 38.87%, Valid: 38.60%, Test: 38.55%
Epoch: 300, Loss: 1.4024, Train: 39.60%, Valid: 39.22%, Test: 39.24%
Epoch: 325, Loss: 1.4061, Train: 39.77%, Valid: 39.40%, Test: 39.45%
Epoch: 350, Loss: 1.3973, Train: 39.96%, Valid: 39.42%, Test: 39.50%
Epoch: 375, Loss: 1.3929, Train: 40.13%, Valid: 39.72%, Test: 39.67%
Epoch: 400, Loss: 1.3961, Train: 39.52%, Valid: 39.03%, Test: 39.22%
Epoch: 425, Loss: 1.3930, Train: 39.73%, Valid: 39.26%, Test: 39.47%
Epoch: 450, Loss: 1.3850, Train: 39.93%, Valid: 39.35%, Test: 39.71%
Epoch: 475, Loss: 1.3912, Train: 39.49%, Valid: 39.01%, Test: 39.24%
Epoch: 500, Loss: 1.4042, Train: 40.00%, Valid: 39.50%, Test: 39.61%
Epoch: 525, Loss: 1.3902, Train: 39.89%, Valid: 39.57%, Test: 39.59%
Epoch: 550, Loss: 1.3931, Train: 39.13%, Valid: 38.73%, Test: 39.04%
Epoch: 575, Loss: 1.4077, Train: 39.75%, Valid: 39.35%, Test: 39.47%
Epoch: 600, Loss: 1.3807, Train: 40.15%, Valid: 39.75%, Test: 39.78%
Epoch: 625, Loss: 1.3871, Train: 39.32%, Valid: 38.92%, Test: 39.14%
Epoch: 650, Loss: 1.3909, Train: 39.95%, Valid: 39.60%, Test: 39.73%
Epoch: 675, Loss: 1.3900, Train: 40.01%, Valid: 39.59%, Test: 39.78%
Epoch: 700, Loss: 1.3812, Train: 39.52%, Valid: 39.01%, Test: 39.34%
Epoch: 725, Loss: 1.3859, Train: 39.94%, Valid: 39.55%, Test: 39.82%
Epoch: 750, Loss: 1.3788, Train: 39.85%, Valid: 39.36%, Test: 39.74%
Epoch: 775, Loss: 1.3787, Train: 39.63%, Valid: 39.20%, Test: 39.51%
Epoch: 800, Loss: 1.3725, Train: 40.06%, Valid: 39.68%, Test: 40.00%
Epoch: 825, Loss: 1.3760, Train: 40.35%, Valid: 39.90%, Test: 40.06%
Epoch: 850, Loss: 1.3737, Train: 40.02%, Valid: 39.53%, Test: 39.94%
Epoch: 875, Loss: 1.3697, Train: 40.47%, Valid: 40.01%, Test: 40.19%
Epoch: 900, Loss: 1.3662, Train: 40.73%, Valid: 40.26%, Test: 40.42%
Epoch: 925, Loss: 1.3702, Train: 40.33%, Valid: 39.95%, Test: 40.11%
Epoch: 950, Loss: 1.3726, Train: 40.41%, Valid: 39.84%, Test: 40.15%
Epoch: 975, Loss: 1.3757, Train: 40.67%, Valid: 40.09%, Test: 40.26%
Run 01:
Highest Train: 40.95
Highest Valid: 40.40
  Final Train: 40.95
   Final Test: 40.63
All runs:
Highest Train: 40.95, nan
Highest Valid: 40.40, nan
  Final Train: 40.95, nan
   Final Test: 40.63, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 41.2177, Train: 19.11%, Valid: 19.31%, Test: 19.07%
Epoch: 25, Loss: 3.8478, Train: 25.74%, Valid: 25.17%, Test: 25.48%
Epoch: 50, Loss: 3.9698, Train: 28.58%, Valid: 28.09%, Test: 28.37%
Epoch: 75, Loss: 2.9747, Train: 29.33%, Valid: 29.20%, Test: 29.37%
Epoch: 100, Loss: 2.2857, Train: 23.62%, Valid: 23.42%, Test: 23.53%
Epoch: 125, Loss: 1.7071, Train: 28.49%, Valid: 28.34%, Test: 28.57%
Epoch: 150, Loss: 1.8438, Train: 26.14%, Valid: 26.18%, Test: 25.98%
Epoch: 175, Loss: 1.7118, Train: 30.62%, Valid: 30.36%, Test: 30.85%
Epoch: 200, Loss: 1.5992, Train: 31.58%, Valid: 31.36%, Test: 31.56%
Epoch: 225, Loss: 1.5714, Train: 30.67%, Valid: 30.15%, Test: 30.30%
Epoch: 250, Loss: 1.5322, Train: 30.80%, Valid: 30.09%, Test: 30.46%
Epoch: 275, Loss: 1.5243, Train: 31.47%, Valid: 30.80%, Test: 31.25%
Epoch: 300, Loss: 1.5348, Train: 32.30%, Valid: 31.90%, Test: 32.31%
Epoch: 325, Loss: 1.5124, Train: 32.01%, Valid: 31.56%, Test: 31.94%
Epoch: 350, Loss: 1.4904, Train: 32.80%, Valid: 32.34%, Test: 32.80%
Epoch: 375, Loss: 1.4834, Train: 32.87%, Valid: 32.41%, Test: 32.91%
Epoch: 400, Loss: 1.4895, Train: 33.27%, Valid: 32.82%, Test: 33.31%
Epoch: 425, Loss: 1.4941, Train: 32.85%, Valid: 32.52%, Test: 33.07%
Epoch: 450, Loss: 1.4946, Train: 33.64%, Valid: 33.11%, Test: 33.77%
Epoch: 475, Loss: 1.4775, Train: 34.66%, Valid: 34.20%, Test: 34.76%
Epoch: 500, Loss: 1.4956, Train: 35.20%, Valid: 34.78%, Test: 35.35%
Epoch: 525, Loss: 1.4746, Train: 34.57%, Valid: 34.14%, Test: 34.77%
Epoch: 550, Loss: 1.4655, Train: 35.39%, Valid: 34.95%, Test: 35.54%
Epoch: 575, Loss: 1.4609, Train: 35.09%, Valid: 34.66%, Test: 35.45%
Epoch: 600, Loss: 1.4600, Train: 36.65%, Valid: 36.14%, Test: 36.73%
Epoch: 625, Loss: 1.4741, Train: 36.29%, Valid: 35.88%, Test: 36.42%
Epoch: 650, Loss: 1.4515, Train: 36.50%, Valid: 36.11%, Test: 36.78%
Epoch: 675, Loss: 1.4743, Train: 37.05%, Valid: 36.69%, Test: 37.15%
Epoch: 700, Loss: 1.4749, Train: 37.94%, Valid: 37.51%, Test: 38.03%
Epoch: 725, Loss: 1.4523, Train: 37.63%, Valid: 37.15%, Test: 37.59%
Epoch: 750, Loss: 1.4986, Train: 38.33%, Valid: 37.90%, Test: 38.40%
Epoch: 775, Loss: 1.4495, Train: 37.79%, Valid: 37.39%, Test: 37.95%
Epoch: 800, Loss: 1.4666, Train: 37.77%, Valid: 37.34%, Test: 37.85%
Epoch: 825, Loss: 1.4374, Train: 37.65%, Valid: 37.27%, Test: 37.93%
Epoch: 850, Loss: 1.4439, Train: 37.26%, Valid: 36.74%, Test: 37.45%
Epoch: 875, Loss: 1.4343, Train: 38.09%, Valid: 37.66%, Test: 38.19%
Epoch: 900, Loss: 1.4423, Train: 36.88%, Valid: 36.56%, Test: 37.12%
Epoch: 925, Loss: 1.4460, Train: 38.46%, Valid: 37.93%, Test: 38.49%
Epoch: 950, Loss: 1.4447, Train: 38.61%, Valid: 38.10%, Test: 38.44%
Epoch: 975, Loss: 1.4298, Train: 38.07%, Valid: 37.60%, Test: 38.06%
Run 01:
Highest Train: 39.20
Highest Valid: 38.82
  Final Train: 39.16
   Final Test: 39.14
All runs:
Highest Train: 39.20, nan
Highest Valid: 38.82, nan
  Final Train: 39.16, nan
   Final Test: 39.14, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.6041, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4715, Train: 35.61%, Valid: 35.26%, Test: 35.47%
Epoch: 50, Loss: 1.4326, Train: 38.01%, Valid: 37.17%, Test: 37.60%
Epoch: 75, Loss: 1.3984, Train: 39.67%, Valid: 38.57%, Test: 39.15%
Epoch: 100, Loss: 1.3703, Train: 42.10%, Valid: 40.96%, Test: 41.43%
Epoch: 125, Loss: 1.3365, Train: 42.37%, Valid: 41.59%, Test: 41.82%
Epoch: 150, Loss: 1.3386, Train: 43.65%, Valid: 42.70%, Test: 42.89%
Epoch: 175, Loss: 1.3120, Train: 42.20%, Valid: 41.30%, Test: 41.67%
Epoch: 200, Loss: 1.3123, Train: 43.84%, Valid: 42.86%, Test: 43.02%
Epoch: 225, Loss: 1.3059, Train: 44.74%, Valid: 43.66%, Test: 44.09%
Epoch: 250, Loss: 1.3011, Train: 44.68%, Valid: 43.38%, Test: 43.67%
Epoch: 275, Loss: 1.3224, Train: 43.05%, Valid: 42.06%, Test: 42.37%
Epoch: 300, Loss: 1.3052, Train: 43.22%, Valid: 42.43%, Test: 42.63%
Epoch: 325, Loss: 1.3099, Train: 43.81%, Valid: 42.69%, Test: 42.88%
Epoch: 350, Loss: 1.2946, Train: 44.00%, Valid: 42.92%, Test: 42.97%
Epoch: 375, Loss: 1.2902, Train: 44.40%, Valid: 43.15%, Test: 43.31%
Epoch: 400, Loss: 1.2833, Train: 42.77%, Valid: 42.03%, Test: 42.11%
Epoch: 425, Loss: 1.2800, Train: 44.44%, Valid: 43.51%, Test: 43.53%
Epoch: 450, Loss: 1.2998, Train: 43.37%, Valid: 42.34%, Test: 42.40%
Epoch: 475, Loss: 1.2776, Train: 45.17%, Valid: 44.09%, Test: 44.30%
Epoch: 500, Loss: 1.2746, Train: 44.43%, Valid: 43.43%, Test: 43.56%
Epoch: 525, Loss: 1.2854, Train: 44.78%, Valid: 43.82%, Test: 43.90%
Epoch: 550, Loss: 1.2739, Train: 45.43%, Valid: 44.55%, Test: 44.73%
Epoch: 575, Loss: 1.2803, Train: 43.13%, Valid: 42.25%, Test: 42.32%
Epoch: 600, Loss: 1.2799, Train: 44.81%, Valid: 43.79%, Test: 43.87%
Epoch: 625, Loss: 1.3027, Train: 43.51%, Valid: 42.59%, Test: 42.94%
Epoch: 650, Loss: 1.2809, Train: 44.92%, Valid: 43.92%, Test: 43.98%
Epoch: 675, Loss: 1.2786, Train: 44.82%, Valid: 43.72%, Test: 43.91%
Epoch: 700, Loss: 1.2789, Train: 45.58%, Valid: 44.40%, Test: 44.61%
Epoch: 725, Loss: 1.2788, Train: 45.35%, Valid: 44.35%, Test: 44.27%
Epoch: 750, Loss: 1.2781, Train: 44.73%, Valid: 43.82%, Test: 43.54%
Epoch: 775, Loss: 1.2809, Train: 45.10%, Valid: 44.00%, Test: 44.15%
Epoch: 800, Loss: 1.2740, Train: 45.52%, Valid: 44.14%, Test: 44.39%
Epoch: 825, Loss: 1.2783, Train: 45.09%, Valid: 44.28%, Test: 44.25%
Epoch: 850, Loss: 1.2979, Train: 44.59%, Valid: 43.58%, Test: 43.68%
Epoch: 875, Loss: 1.2650, Train: 45.69%, Valid: 44.58%, Test: 44.76%
Epoch: 900, Loss: 1.2562, Train: 45.15%, Valid: 44.18%, Test: 44.19%
Epoch: 925, Loss: 1.2606, Train: 46.04%, Valid: 44.71%, Test: 44.97%
Epoch: 950, Loss: 1.2832, Train: 44.99%, Valid: 43.95%, Test: 44.19%
Epoch: 975, Loss: 1.2640, Train: 45.44%, Valid: 44.42%, Test: 44.43%
Run 01:
Highest Train: 46.64
Highest Valid: 45.29
  Final Train: 46.49
   Final Test: 45.53
All runs:
Highest Train: 46.64, nan
Highest Valid: 45.29, nan
  Final Train: 46.49, nan
   Final Test: 45.53, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.1769, Train: 23.98%, Valid: 23.82%, Test: 23.83%
Epoch: 25, Loss: 1.5190, Train: 28.66%, Valid: 28.45%, Test: 28.80%
Epoch: 50, Loss: 1.4882, Train: 29.05%, Valid: 28.79%, Test: 29.08%
Epoch: 75, Loss: 1.4683, Train: 29.14%, Valid: 28.85%, Test: 29.10%
Epoch: 100, Loss: 1.4624, Train: 29.22%, Valid: 28.91%, Test: 29.25%
Epoch: 125, Loss: 1.4515, Train: 35.85%, Valid: 35.20%, Test: 35.65%
Epoch: 150, Loss: 1.4540, Train: 34.24%, Valid: 33.90%, Test: 34.26%
Epoch: 175, Loss: 1.4393, Train: 34.36%, Valid: 33.89%, Test: 34.30%
Epoch: 200, Loss: 1.4996, Train: 32.48%, Valid: 32.02%, Test: 32.42%
Epoch: 225, Loss: 1.4447, Train: 37.67%, Valid: 37.13%, Test: 37.38%
Epoch: 250, Loss: 1.4289, Train: 37.65%, Valid: 37.12%, Test: 37.26%
Epoch: 275, Loss: 1.4240, Train: 37.70%, Valid: 37.24%, Test: 37.42%
Epoch: 300, Loss: 1.4204, Train: 37.95%, Valid: 37.30%, Test: 37.54%
Epoch: 325, Loss: 1.4127, Train: 38.35%, Valid: 37.75%, Test: 38.04%
Epoch: 350, Loss: 1.4173, Train: 38.51%, Valid: 37.90%, Test: 38.14%
Epoch: 375, Loss: 1.4009, Train: 38.30%, Valid: 37.75%, Test: 37.93%
Epoch: 400, Loss: 1.4147, Train: 38.24%, Valid: 37.76%, Test: 37.85%
Epoch: 425, Loss: 1.4218, Train: 38.75%, Valid: 38.08%, Test: 38.35%
Epoch: 450, Loss: 1.3993, Train: 38.99%, Valid: 38.39%, Test: 38.61%
Epoch: 475, Loss: 1.4282, Train: 37.08%, Valid: 36.54%, Test: 36.89%
Epoch: 500, Loss: 1.4215, Train: 38.22%, Valid: 37.70%, Test: 37.89%
Epoch: 525, Loss: 1.4045, Train: 39.06%, Valid: 38.36%, Test: 38.58%
Epoch: 550, Loss: 1.3930, Train: 39.38%, Valid: 38.59%, Test: 38.99%
Epoch: 575, Loss: 1.3797, Train: 39.76%, Valid: 38.88%, Test: 39.40%
Epoch: 600, Loss: 1.3814, Train: 40.16%, Valid: 39.74%, Test: 40.18%
Epoch: 625, Loss: 1.3935, Train: 39.71%, Valid: 39.18%, Test: 39.47%
Epoch: 650, Loss: 1.3814, Train: 40.46%, Valid: 39.75%, Test: 40.52%
Epoch: 675, Loss: 1.3679, Train: 40.93%, Valid: 40.36%, Test: 41.13%
Epoch: 700, Loss: 1.3724, Train: 41.78%, Valid: 41.24%, Test: 41.83%
Epoch: 725, Loss: 1.3739, Train: 41.02%, Valid: 40.65%, Test: 40.97%
Epoch: 750, Loss: 1.3740, Train: 41.33%, Valid: 41.03%, Test: 41.27%
Epoch: 775, Loss: 1.3622, Train: 41.61%, Valid: 41.24%, Test: 41.58%
Epoch: 800, Loss: 1.4005, Train: 38.96%, Valid: 38.54%, Test: 38.89%
Epoch: 825, Loss: 1.3683, Train: 41.69%, Valid: 41.30%, Test: 41.67%
Epoch: 850, Loss: 1.3499, Train: 41.90%, Valid: 41.69%, Test: 42.05%
Epoch: 875, Loss: 1.3588, Train: 41.83%, Valid: 41.56%, Test: 42.03%
Epoch: 900, Loss: 1.3443, Train: 41.38%, Valid: 41.18%, Test: 41.54%
Epoch: 925, Loss: 1.3545, Train: 42.09%, Valid: 41.86%, Test: 42.07%
Epoch: 950, Loss: 1.3406, Train: 42.45%, Valid: 42.06%, Test: 42.48%
Epoch: 975, Loss: 1.3622, Train: 41.24%, Valid: 41.02%, Test: 41.18%
Run 01:
Highest Train: 42.90
Highest Valid: 42.52
  Final Train: 42.90
   Final Test: 42.78
All runs:
Highest Train: 42.90, nan
Highest Valid: 42.52, nan
  Final Train: 42.90, nan
   Final Test: 42.78, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=5, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 9.1856, Train: 14.27%, Valid: 14.24%, Test: 14.54%
Epoch: 25, Loss: 4.1332, Train: 20.99%, Valid: 20.87%, Test: 21.18%
Epoch: 50, Loss: 2.9866, Train: 25.11%, Valid: 24.86%, Test: 24.73%
Epoch: 75, Loss: 1.7968, Train: 22.58%, Valid: 22.27%, Test: 22.46%
Epoch: 100, Loss: 1.6658, Train: 29.62%, Valid: 29.42%, Test: 29.76%
Epoch: 125, Loss: 1.7003, Train: 21.59%, Valid: 21.70%, Test: 21.87%
Epoch: 150, Loss: 1.9523, Train: 29.74%, Valid: 29.58%, Test: 29.78%
Epoch: 175, Loss: 1.6114, Train: 25.62%, Valid: 25.46%, Test: 25.52%
Epoch: 200, Loss: 1.5320, Train: 29.96%, Valid: 29.67%, Test: 30.09%
Epoch: 225, Loss: 1.5066, Train: 30.03%, Valid: 29.81%, Test: 30.03%
Epoch: 250, Loss: 1.4894, Train: 29.91%, Valid: 29.64%, Test: 29.96%
Epoch: 275, Loss: 1.5033, Train: 30.28%, Valid: 30.00%, Test: 30.36%
Epoch: 300, Loss: 1.4711, Train: 29.79%, Valid: 29.61%, Test: 29.82%
Epoch: 325, Loss: 1.4785, Train: 30.09%, Valid: 29.87%, Test: 30.10%
Epoch: 350, Loss: 1.4802, Train: 30.50%, Valid: 30.26%, Test: 30.56%
Epoch: 375, Loss: 1.4559, Train: 31.69%, Valid: 31.43%, Test: 31.85%
Epoch: 400, Loss: 1.4827, Train: 32.17%, Valid: 31.88%, Test: 32.44%
Epoch: 425, Loss: 1.4574, Train: 31.83%, Valid: 31.50%, Test: 32.05%
Epoch: 450, Loss: 1.4448, Train: 33.22%, Valid: 32.93%, Test: 33.61%
Epoch: 475, Loss: 1.4421, Train: 36.06%, Valid: 35.81%, Test: 36.13%
Epoch: 500, Loss: 1.4363, Train: 36.10%, Valid: 35.80%, Test: 36.34%
Epoch: 525, Loss: 1.4528, Train: 35.24%, Valid: 34.75%, Test: 35.35%
Epoch: 550, Loss: 1.4549, Train: 35.84%, Valid: 35.72%, Test: 36.08%
Epoch: 575, Loss: 1.4326, Train: 37.18%, Valid: 36.59%, Test: 37.28%
Epoch: 600, Loss: 1.4733, Train: 37.32%, Valid: 36.80%, Test: 37.29%
Epoch: 625, Loss: 1.4201, Train: 37.95%, Valid: 37.50%, Test: 37.88%
Epoch: 650, Loss: 1.4422, Train: 38.55%, Valid: 38.14%, Test: 38.45%
Epoch: 675, Loss: 1.4309, Train: 36.32%, Valid: 35.98%, Test: 36.72%
Epoch: 700, Loss: 1.4236, Train: 38.13%, Valid: 37.58%, Test: 38.15%
Epoch: 725, Loss: 1.4084, Train: 38.75%, Valid: 38.18%, Test: 38.63%
Epoch: 750, Loss: 1.4203, Train: 39.03%, Valid: 38.43%, Test: 38.92%
Epoch: 775, Loss: 1.4365, Train: 37.69%, Valid: 37.19%, Test: 37.55%
Epoch: 800, Loss: 1.4205, Train: 39.08%, Valid: 38.60%, Test: 38.96%
Epoch: 825, Loss: 1.4121, Train: 38.72%, Valid: 38.08%, Test: 38.40%
Epoch: 850, Loss: 1.4148, Train: 39.33%, Valid: 38.69%, Test: 39.04%
Epoch: 875, Loss: 1.4086, Train: 39.42%, Valid: 38.77%, Test: 39.21%
Epoch: 900, Loss: 1.4084, Train: 39.16%, Valid: 38.60%, Test: 38.85%
Epoch: 925, Loss: 1.4054, Train: 39.29%, Valid: 38.59%, Test: 39.12%
Epoch: 950, Loss: 1.4050, Train: 39.18%, Valid: 38.43%, Test: 38.82%
Epoch: 975, Loss: 1.4067, Train: 39.29%, Valid: 38.62%, Test: 39.02%
Run 01:
Highest Train: 39.68
Highest Valid: 38.96
  Final Train: 39.64
   Final Test: 39.41
All runs:
Highest Train: 39.68, nan
Highest Valid: 38.96, nan
  Final Train: 39.64, nan
   Final Test: 39.41, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6100, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4940, Train: 34.55%, Valid: 34.30%, Test: 34.50%
Epoch: 50, Loss: 1.4696, Train: 35.88%, Valid: 35.12%, Test: 35.54%
Epoch: 75, Loss: 1.4479, Train: 36.97%, Valid: 35.75%, Test: 36.11%
Epoch: 100, Loss: 1.4307, Train: 38.07%, Valid: 35.97%, Test: 36.36%
Epoch: 125, Loss: 1.4175, Train: 38.44%, Valid: 36.07%, Test: 36.44%
Epoch: 150, Loss: 1.4033, Train: 39.22%, Valid: 35.97%, Test: 36.42%
Epoch: 175, Loss: 1.3898, Train: 39.65%, Valid: 35.56%, Test: 35.97%
Epoch: 200, Loss: 1.4071, Train: 38.92%, Valid: 34.82%, Test: 35.60%
Epoch: 225, Loss: 1.3613, Train: 41.42%, Valid: 35.25%, Test: 35.65%
Epoch: 250, Loss: 1.3637, Train: 41.41%, Valid: 35.00%, Test: 35.48%
Epoch: 275, Loss: 1.3352, Train: 42.83%, Valid: 34.81%, Test: 35.29%
Epoch: 300, Loss: 1.3448, Train: 42.05%, Valid: 34.93%, Test: 35.61%
Epoch: 325, Loss: 1.3267, Train: 43.03%, Valid: 35.35%, Test: 35.77%
Epoch: 350, Loss: 1.3016, Train: 44.59%, Valid: 34.13%, Test: 34.35%
Epoch: 375, Loss: 1.2967, Train: 44.22%, Valid: 34.07%, Test: 34.58%
Epoch: 400, Loss: 1.2761, Train: 45.55%, Valid: 34.72%, Test: 34.92%
Epoch: 425, Loss: 1.2697, Train: 46.14%, Valid: 32.48%, Test: 32.85%
Epoch: 450, Loss: 1.2623, Train: 47.15%, Valid: 33.36%, Test: 33.98%
Epoch: 475, Loss: 1.2473, Train: 47.09%, Valid: 32.53%, Test: 33.03%
Epoch: 500, Loss: 1.2264, Train: 48.35%, Valid: 32.79%, Test: 33.09%
Epoch: 525, Loss: 1.2388, Train: 48.01%, Valid: 33.28%, Test: 33.61%
Epoch: 550, Loss: 1.2227, Train: 48.25%, Valid: 32.78%, Test: 33.76%
Epoch: 575, Loss: 1.2072, Train: 49.66%, Valid: 31.95%, Test: 32.73%
Epoch: 600, Loss: 1.2060, Train: 49.68%, Valid: 32.87%, Test: 33.34%
Epoch: 625, Loss: 1.1949, Train: 50.07%, Valid: 31.39%, Test: 32.14%
Epoch: 650, Loss: 1.1776, Train: 50.78%, Valid: 32.45%, Test: 32.79%
Epoch: 675, Loss: 1.1573, Train: 51.08%, Valid: 32.38%, Test: 33.05%
Epoch: 700, Loss: 1.1533, Train: 51.77%, Valid: 31.39%, Test: 31.73%
Epoch: 725, Loss: 1.1424, Train: 52.39%, Valid: 31.87%, Test: 32.15%
Epoch: 750, Loss: 1.1149, Train: 53.37%, Valid: 31.88%, Test: 32.34%
Epoch: 775, Loss: 1.1498, Train: 52.72%, Valid: 32.49%, Test: 32.72%
Epoch: 800, Loss: 1.1205, Train: 53.06%, Valid: 31.99%, Test: 32.14%
Epoch: 825, Loss: 1.1735, Train: 50.87%, Valid: 30.82%, Test: 31.04%
Epoch: 850, Loss: 1.0920, Train: 55.34%, Valid: 31.72%, Test: 32.03%
Epoch: 875, Loss: 1.1016, Train: 52.30%, Valid: 31.61%, Test: 32.23%
Epoch: 900, Loss: 1.0739, Train: 56.26%, Valid: 30.84%, Test: 30.92%
Epoch: 925, Loss: 1.0867, Train: 55.15%, Valid: 32.56%, Test: 32.76%
Epoch: 950, Loss: 1.0688, Train: 55.90%, Valid: 30.79%, Test: 31.22%
Epoch: 975, Loss: 1.0879, Train: 55.27%, Valid: 30.90%, Test: 30.68%
Run 01:
Highest Train: 57.61
Highest Valid: 36.17
  Final Train: 39.51
   Final Test: 36.50
All runs:
Highest Train: 57.61, nan
Highest Valid: 36.17, nan
  Final Train: 39.51, nan
   Final Test: 36.50, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6095, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4919, Train: 34.57%, Valid: 34.28%, Test: 34.52%
Epoch: 50, Loss: 1.4631, Train: 35.64%, Valid: 35.22%, Test: 35.46%
Epoch: 75, Loss: 1.4606, Train: 35.58%, Valid: 35.27%, Test: 35.43%
Epoch: 100, Loss: 1.4310, Train: 37.97%, Valid: 36.85%, Test: 37.38%
Epoch: 125, Loss: 1.4110, Train: 39.11%, Valid: 37.69%, Test: 38.20%
Epoch: 150, Loss: 1.3969, Train: 40.16%, Valid: 38.36%, Test: 38.82%
Epoch: 175, Loss: 1.3721, Train: 41.36%, Valid: 39.13%, Test: 39.26%
Epoch: 200, Loss: 1.3692, Train: 41.31%, Valid: 38.49%, Test: 38.93%
Epoch: 225, Loss: 1.3592, Train: 42.09%, Valid: 38.60%, Test: 39.02%
Epoch: 250, Loss: 1.3410, Train: 42.78%, Valid: 38.70%, Test: 39.08%
Epoch: 275, Loss: 1.3453, Train: 42.83%, Valid: 39.24%, Test: 39.07%
Epoch: 300, Loss: 1.3293, Train: 43.30%, Valid: 38.52%, Test: 39.03%
Epoch: 325, Loss: 1.3137, Train: 43.29%, Valid: 37.90%, Test: 38.54%
Epoch: 350, Loss: 1.3107, Train: 45.00%, Valid: 39.48%, Test: 39.72%
Epoch: 375, Loss: 1.2926, Train: 45.68%, Valid: 38.71%, Test: 39.28%
Epoch: 400, Loss: 1.3010, Train: 44.72%, Valid: 39.01%, Test: 39.18%
Epoch: 425, Loss: 1.2862, Train: 45.96%, Valid: 38.34%, Test: 38.99%
Epoch: 450, Loss: 1.3431, Train: 42.09%, Valid: 36.57%, Test: 36.98%
Epoch: 475, Loss: 1.2772, Train: 46.07%, Valid: 38.33%, Test: 38.68%
Epoch: 500, Loss: 1.2595, Train: 46.98%, Valid: 37.99%, Test: 38.85%
Epoch: 525, Loss: 1.2568, Train: 47.11%, Valid: 37.32%, Test: 38.14%
Epoch: 550, Loss: 1.2448, Train: 48.02%, Valid: 37.73%, Test: 38.43%
Epoch: 575, Loss: 1.2241, Train: 47.81%, Valid: 37.84%, Test: 38.37%
Epoch: 600, Loss: 1.2215, Train: 48.85%, Valid: 37.65%, Test: 38.36%
Epoch: 625, Loss: 1.2326, Train: 49.00%, Valid: 38.04%, Test: 38.60%
Epoch: 650, Loss: 1.1734, Train: 51.30%, Valid: 37.64%, Test: 38.41%
Epoch: 675, Loss: 1.1973, Train: 48.99%, Valid: 37.73%, Test: 38.27%
Epoch: 700, Loss: 1.1589, Train: 50.26%, Valid: 37.33%, Test: 37.91%
Epoch: 725, Loss: 1.1252, Train: 52.79%, Valid: 36.93%, Test: 37.71%
Epoch: 750, Loss: 1.1329, Train: 53.62%, Valid: 37.67%, Test: 38.16%
Epoch: 775, Loss: 1.1433, Train: 52.72%, Valid: 36.94%, Test: 37.65%
Epoch: 800, Loss: 1.0842, Train: 53.48%, Valid: 36.59%, Test: 36.80%
Epoch: 825, Loss: 1.1091, Train: 53.82%, Valid: 36.79%, Test: 37.27%
Epoch: 850, Loss: 1.0642, Train: 55.99%, Valid: 36.71%, Test: 37.17%
Epoch: 875, Loss: 1.0766, Train: 55.09%, Valid: 37.08%, Test: 37.53%
Epoch: 900, Loss: 1.0259, Train: 57.67%, Valid: 37.05%, Test: 37.42%
Epoch: 925, Loss: 1.0504, Train: 56.69%, Valid: 35.71%, Test: 36.11%
Epoch: 950, Loss: 1.0128, Train: 58.88%, Valid: 37.01%, Test: 37.40%
Epoch: 975, Loss: 1.0610, Train: 55.47%, Valid: 35.64%, Test: 36.25%
Run 01:
Highest Train: 59.86
Highest Valid: 39.79
  Final Train: 43.20
   Final Test: 39.65
All runs:
Highest Train: 59.86, nan
Highest Valid: 39.79, nan
  Final Train: 43.20, nan
   Final Test: 39.65, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6748, Train: 34.18%, Valid: 34.11%, Test: 34.14%
Epoch: 25, Loss: 1.4956, Train: 33.68%, Valid: 33.69%, Test: 34.21%
Epoch: 50, Loss: 1.4609, Train: 35.65%, Valid: 35.74%, Test: 35.91%
Epoch: 75, Loss: 1.4578, Train: 36.09%, Valid: 36.04%, Test: 36.57%
Epoch: 100, Loss: 1.4656, Train: 35.77%, Valid: 35.76%, Test: 36.18%
Epoch: 125, Loss: 1.4507, Train: 36.51%, Valid: 36.22%, Test: 36.45%
Epoch: 150, Loss: 1.4685, Train: 35.75%, Valid: 35.93%, Test: 36.20%
Epoch: 175, Loss: 1.4506, Train: 35.73%, Valid: 35.47%, Test: 35.62%
Epoch: 200, Loss: 1.4437, Train: 36.44%, Valid: 35.95%, Test: 36.44%
Epoch: 225, Loss: 1.4177, Train: 38.90%, Valid: 38.50%, Test: 39.03%
Epoch: 250, Loss: 1.4074, Train: 38.97%, Valid: 38.71%, Test: 39.08%
Epoch: 275, Loss: 1.4075, Train: 37.91%, Valid: 37.65%, Test: 37.84%
Epoch: 300, Loss: 1.4029, Train: 39.66%, Valid: 39.14%, Test: 39.60%
Epoch: 325, Loss: 1.3929, Train: 39.76%, Valid: 39.00%, Test: 39.41%
Epoch: 350, Loss: 1.3809, Train: 39.60%, Valid: 38.74%, Test: 39.05%
Epoch: 375, Loss: 1.3924, Train: 39.55%, Valid: 38.73%, Test: 39.06%
Epoch: 400, Loss: 1.3818, Train: 38.59%, Valid: 37.84%, Test: 38.33%
Epoch: 425, Loss: 1.3549, Train: 41.30%, Valid: 40.50%, Test: 41.00%
Epoch: 450, Loss: 1.3437, Train: 41.17%, Valid: 40.19%, Test: 40.69%
Epoch: 475, Loss: 1.3361, Train: 42.02%, Valid: 41.01%, Test: 41.26%
Epoch: 500, Loss: 1.3201, Train: 42.72%, Valid: 41.55%, Test: 41.97%
Epoch: 525, Loss: 1.3104, Train: 43.24%, Valid: 41.84%, Test: 42.06%
Epoch: 550, Loss: 1.4259, Train: 41.14%, Valid: 40.02%, Test: 40.43%
Epoch: 575, Loss: 1.3499, Train: 41.37%, Valid: 40.57%, Test: 40.68%
Epoch: 600, Loss: 1.3727, Train: 39.67%, Valid: 39.04%, Test: 39.37%
Epoch: 625, Loss: 1.3501, Train: 41.10%, Valid: 40.09%, Test: 40.60%
Epoch: 650, Loss: 1.3175, Train: 42.80%, Valid: 41.56%, Test: 41.94%
Epoch: 675, Loss: 1.3203, Train: 42.12%, Valid: 41.20%, Test: 41.39%
Epoch: 700, Loss: 1.3025, Train: 43.18%, Valid: 42.10%, Test: 42.35%
Epoch: 725, Loss: 1.2965, Train: 43.63%, Valid: 42.64%, Test: 42.72%
Epoch: 750, Loss: 1.2924, Train: 43.72%, Valid: 42.68%, Test: 42.92%
Epoch: 775, Loss: 1.2958, Train: 43.83%, Valid: 42.66%, Test: 42.83%
Epoch: 800, Loss: 1.3283, Train: 42.27%, Valid: 41.13%, Test: 41.37%
Epoch: 825, Loss: 1.2924, Train: 43.42%, Valid: 42.60%, Test: 42.71%
Epoch: 850, Loss: 1.2861, Train: 44.15%, Valid: 42.92%, Test: 42.94%
Epoch: 875, Loss: 1.2798, Train: 44.14%, Valid: 43.08%, Test: 43.08%
Epoch: 900, Loss: 1.3660, Train: 39.14%, Valid: 38.00%, Test: 38.29%
Epoch: 925, Loss: 1.2983, Train: 43.63%, Valid: 42.71%, Test: 42.94%
Epoch: 950, Loss: 1.2860, Train: 44.33%, Valid: 43.24%, Test: 43.38%
Epoch: 975, Loss: 1.2804, Train: 44.40%, Valid: 43.23%, Test: 43.40%
Run 01:
Highest Train: 44.73
Highest Valid: 43.50
  Final Train: 44.52
   Final Test: 43.63
All runs:
Highest Train: 44.73, nan
Highest Valid: 43.50, nan
  Final Train: 44.52, nan
   Final Test: 43.63, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6101, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4942, Train: 34.49%, Valid: 34.20%, Test: 34.41%
Epoch: 50, Loss: 1.4743, Train: 35.65%, Valid: 35.03%, Test: 35.25%
Epoch: 75, Loss: 1.4508, Train: 36.93%, Valid: 35.80%, Test: 35.98%
Epoch: 100, Loss: 1.4362, Train: 37.81%, Valid: 35.92%, Test: 36.22%
Epoch: 125, Loss: 1.4182, Train: 38.61%, Valid: 36.21%, Test: 36.62%
Epoch: 150, Loss: 1.4027, Train: 39.64%, Valid: 35.88%, Test: 36.42%
Epoch: 175, Loss: 1.4097, Train: 38.55%, Valid: 35.51%, Test: 36.19%
Epoch: 200, Loss: 1.3770, Train: 40.76%, Valid: 36.05%, Test: 36.49%
Epoch: 225, Loss: 1.3618, Train: 41.47%, Valid: 35.19%, Test: 35.56%
Epoch: 250, Loss: 1.3704, Train: 41.28%, Valid: 35.27%, Test: 35.60%
Epoch: 275, Loss: 1.3432, Train: 42.26%, Valid: 35.76%, Test: 36.17%
Epoch: 300, Loss: 1.3275, Train: 43.35%, Valid: 34.35%, Test: 34.67%
Epoch: 325, Loss: 1.3338, Train: 42.32%, Valid: 33.14%, Test: 33.59%
Epoch: 350, Loss: 1.2964, Train: 45.17%, Valid: 34.68%, Test: 35.10%
Epoch: 375, Loss: 1.2937, Train: 44.76%, Valid: 35.41%, Test: 35.79%
Epoch: 400, Loss: 1.3155, Train: 46.21%, Valid: 33.82%, Test: 34.36%
Epoch: 425, Loss: 1.2607, Train: 46.94%, Valid: 34.01%, Test: 34.57%
Epoch: 450, Loss: 1.2530, Train: 46.51%, Valid: 34.16%, Test: 34.78%
Epoch: 475, Loss: 1.2287, Train: 47.88%, Valid: 32.27%, Test: 32.65%
Epoch: 500, Loss: 1.2222, Train: 48.81%, Valid: 32.97%, Test: 33.37%
Epoch: 525, Loss: 1.2211, Train: 47.97%, Valid: 32.13%, Test: 32.72%
Epoch: 550, Loss: 1.2034, Train: 49.83%, Valid: 33.05%, Test: 33.27%
Epoch: 575, Loss: 1.2509, Train: 48.49%, Valid: 31.87%, Test: 31.88%
Epoch: 600, Loss: 1.1908, Train: 50.46%, Valid: 33.23%, Test: 33.22%
Epoch: 625, Loss: 1.1607, Train: 50.73%, Valid: 33.54%, Test: 33.48%
Epoch: 650, Loss: 1.1604, Train: 52.11%, Valid: 32.29%, Test: 32.42%
Epoch: 675, Loss: 1.1908, Train: 50.83%, Valid: 32.65%, Test: 32.76%
Epoch: 700, Loss: 1.1241, Train: 53.42%, Valid: 32.72%, Test: 33.03%
Epoch: 725, Loss: 1.2083, Train: 48.46%, Valid: 28.54%, Test: 28.70%
Epoch: 750, Loss: 1.1183, Train: 53.86%, Valid: 31.53%, Test: 31.59%
Epoch: 775, Loss: 1.1097, Train: 54.06%, Valid: 31.85%, Test: 32.20%
Epoch: 800, Loss: 1.1173, Train: 54.78%, Valid: 32.06%, Test: 32.21%
Epoch: 825, Loss: 1.0789, Train: 56.30%, Valid: 31.47%, Test: 31.80%
Epoch: 850, Loss: 1.0878, Train: 55.82%, Valid: 32.15%, Test: 32.64%
Epoch: 875, Loss: 1.0475, Train: 56.99%, Valid: 31.12%, Test: 31.15%
Epoch: 900, Loss: 1.1195, Train: 53.69%, Valid: 31.74%, Test: 32.14%
Epoch: 925, Loss: 1.0343, Train: 57.69%, Valid: 31.33%, Test: 31.64%
Epoch: 950, Loss: 1.0501, Train: 56.00%, Valid: 29.60%, Test: 29.73%
Epoch: 975, Loss: 1.0411, Train: 57.19%, Valid: 31.96%, Test: 32.32%
Run 01:
Highest Train: 58.52
Highest Valid: 36.37
  Final Train: 38.44
   Final Test: 36.60
All runs:
Highest Train: 58.52, nan
Highest Valid: 36.37, nan
  Final Train: 38.44, nan
   Final Test: 36.60, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6084, Train: 28.72%, Valid: 28.53%, Test: 28.81%
Epoch: 25, Loss: 1.5041, Train: 34.23%, Valid: 34.09%, Test: 34.22%
Epoch: 50, Loss: 1.4751, Train: 35.14%, Valid: 34.89%, Test: 35.02%
Epoch: 75, Loss: 1.4550, Train: 36.37%, Valid: 35.73%, Test: 36.00%
Epoch: 100, Loss: 1.4363, Train: 36.91%, Valid: 36.19%, Test: 36.37%
Epoch: 125, Loss: 1.4147, Train: 38.36%, Valid: 37.30%, Test: 37.43%
Epoch: 150, Loss: 1.4042, Train: 39.23%, Valid: 37.66%, Test: 38.12%
Epoch: 175, Loss: 1.3613, Train: 41.75%, Valid: 39.73%, Test: 40.09%
Epoch: 200, Loss: 1.3422, Train: 42.62%, Valid: 39.90%, Test: 40.25%
Epoch: 225, Loss: 1.3397, Train: 42.52%, Valid: 39.87%, Test: 40.22%
Epoch: 250, Loss: 1.3164, Train: 43.58%, Valid: 40.15%, Test: 40.44%
Epoch: 275, Loss: 1.3119, Train: 42.59%, Valid: 38.58%, Test: 38.97%
Epoch: 300, Loss: 1.3168, Train: 43.33%, Valid: 39.24%, Test: 39.38%
Epoch: 325, Loss: 1.2894, Train: 44.30%, Valid: 39.57%, Test: 39.88%
Epoch: 350, Loss: 1.2938, Train: 44.45%, Valid: 40.30%, Test: 40.62%
Epoch: 375, Loss: 1.2955, Train: 44.76%, Valid: 39.29%, Test: 39.95%
Epoch: 400, Loss: 1.2571, Train: 46.00%, Valid: 40.02%, Test: 40.30%
Epoch: 425, Loss: 1.3646, Train: 41.30%, Valid: 38.51%, Test: 38.96%
Epoch: 450, Loss: 1.3068, Train: 44.24%, Valid: 39.49%, Test: 39.80%
Epoch: 475, Loss: 1.2718, Train: 45.49%, Valid: 40.11%, Test: 40.29%
Epoch: 500, Loss: 1.2734, Train: 45.38%, Valid: 39.44%, Test: 39.86%
Epoch: 525, Loss: 1.2463, Train: 46.28%, Valid: 39.98%, Test: 40.42%
Epoch: 550, Loss: 1.2371, Train: 46.73%, Valid: 39.85%, Test: 40.46%
Epoch: 575, Loss: 1.2379, Train: 46.76%, Valid: 39.95%, Test: 40.43%
Epoch: 600, Loss: 1.3285, Train: 43.11%, Valid: 37.82%, Test: 38.36%
Epoch: 625, Loss: 1.2627, Train: 45.69%, Valid: 39.40%, Test: 39.81%
Epoch: 650, Loss: 1.2369, Train: 46.79%, Valid: 39.80%, Test: 40.30%
Epoch: 675, Loss: 1.2533, Train: 45.88%, Valid: 39.25%, Test: 39.71%
Epoch: 700, Loss: 1.2199, Train: 47.45%, Valid: 39.80%, Test: 40.45%
Epoch: 725, Loss: 1.2521, Train: 45.93%, Valid: 39.31%, Test: 39.67%
Epoch: 750, Loss: 1.2120, Train: 47.76%, Valid: 39.98%, Test: 40.36%
Epoch: 775, Loss: 1.2407, Train: 46.44%, Valid: 38.85%, Test: 39.51%
Epoch: 800, Loss: 1.2008, Train: 48.20%, Valid: 39.92%, Test: 40.36%
Epoch: 825, Loss: 1.1974, Train: 48.47%, Valid: 39.91%, Test: 40.24%
Epoch: 850, Loss: 1.1875, Train: 49.05%, Valid: 40.12%, Test: 40.43%
Epoch: 875, Loss: 1.2074, Train: 48.11%, Valid: 39.96%, Test: 40.19%
Epoch: 900, Loss: 1.1945, Train: 48.96%, Valid: 40.11%, Test: 40.43%
Epoch: 925, Loss: 1.1661, Train: 49.95%, Valid: 40.39%, Test: 40.86%
Epoch: 950, Loss: 1.1907, Train: 49.02%, Valid: 40.26%, Test: 40.60%
Epoch: 975, Loss: 1.2098, Train: 48.07%, Valid: 39.53%, Test: 40.14%
Run 01:
Highest Train: 50.25
Highest Valid: 40.57
  Final Train: 49.66
   Final Test: 40.81
All runs:
Highest Train: 50.25, nan
Highest Valid: 40.57, nan
  Final Train: 49.66, nan
   Final Test: 40.81, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6260, Train: 24.78%, Valid: 24.64%, Test: 24.98%
Epoch: 25, Loss: 1.4985, Train: 30.26%, Valid: 30.05%, Test: 30.30%
Epoch: 50, Loss: 1.4444, Train: 35.69%, Valid: 35.49%, Test: 36.28%
Epoch: 75, Loss: 1.4691, Train: 36.31%, Valid: 36.13%, Test: 36.33%
Epoch: 100, Loss: 1.4403, Train: 37.22%, Valid: 37.13%, Test: 37.63%
Epoch: 125, Loss: 1.5314, Train: 32.52%, Valid: 32.20%, Test: 32.52%
Epoch: 150, Loss: 1.4909, Train: 35.02%, Valid: 34.59%, Test: 35.04%
Epoch: 175, Loss: 1.4338, Train: 36.88%, Valid: 36.31%, Test: 36.68%
Epoch: 200, Loss: 1.4145, Train: 38.51%, Valid: 37.98%, Test: 38.36%
Epoch: 225, Loss: 1.3962, Train: 38.63%, Valid: 37.76%, Test: 38.40%
Epoch: 250, Loss: 1.3787, Train: 40.45%, Valid: 39.77%, Test: 40.16%
Epoch: 275, Loss: 1.3483, Train: 41.16%, Valid: 40.58%, Test: 41.05%
Epoch: 300, Loss: 1.3429, Train: 41.61%, Valid: 41.07%, Test: 41.27%
Epoch: 325, Loss: 1.3521, Train: 41.32%, Valid: 40.79%, Test: 41.17%
Epoch: 350, Loss: 1.3395, Train: 41.94%, Valid: 41.35%, Test: 41.60%
Epoch: 375, Loss: 1.3274, Train: 42.27%, Valid: 41.39%, Test: 41.72%
Epoch: 400, Loss: 1.3376, Train: 41.76%, Valid: 40.94%, Test: 41.30%
Epoch: 425, Loss: 1.3292, Train: 42.14%, Valid: 41.32%, Test: 41.45%
Epoch: 450, Loss: 1.3269, Train: 42.17%, Valid: 41.18%, Test: 41.46%
Epoch: 475, Loss: 1.3044, Train: 43.09%, Valid: 42.35%, Test: 42.56%
Epoch: 500, Loss: 1.3298, Train: 42.12%, Valid: 41.13%, Test: 41.47%
Epoch: 525, Loss: 1.3526, Train: 41.57%, Valid: 40.90%, Test: 41.01%
Epoch: 550, Loss: 1.3099, Train: 42.96%, Valid: 42.10%, Test: 42.32%
Epoch: 575, Loss: 1.3175, Train: 42.47%, Valid: 41.63%, Test: 41.95%
Epoch: 600, Loss: 1.3914, Train: 38.38%, Valid: 37.80%, Test: 38.25%
Epoch: 625, Loss: 1.3870, Train: 40.16%, Valid: 39.65%, Test: 39.71%
Epoch: 650, Loss: 1.3280, Train: 41.73%, Valid: 41.17%, Test: 41.25%
Epoch: 675, Loss: 1.3249, Train: 41.87%, Valid: 41.33%, Test: 41.31%
Epoch: 700, Loss: 1.3122, Train: 42.16%, Valid: 41.24%, Test: 41.71%
Epoch: 725, Loss: 1.3186, Train: 42.32%, Valid: 41.60%, Test: 41.65%
Epoch: 750, Loss: 1.2993, Train: 43.22%, Valid: 42.49%, Test: 42.55%
Epoch: 775, Loss: 1.3361, Train: 41.91%, Valid: 40.81%, Test: 41.33%
Epoch: 800, Loss: 1.3088, Train: 42.48%, Valid: 41.82%, Test: 42.03%
Epoch: 825, Loss: 1.3038, Train: 43.11%, Valid: 42.24%, Test: 42.49%
Epoch: 850, Loss: 1.3081, Train: 42.78%, Valid: 41.77%, Test: 42.08%
Epoch: 875, Loss: 1.4146, Train: 38.18%, Valid: 37.18%, Test: 37.70%
Epoch: 900, Loss: 1.3424, Train: 39.92%, Valid: 39.51%, Test: 39.40%
Epoch: 925, Loss: 1.3174, Train: 41.64%, Valid: 41.07%, Test: 41.15%
Epoch: 950, Loss: 1.3187, Train: 42.24%, Valid: 41.42%, Test: 41.53%
Epoch: 975, Loss: 1.3018, Train: 42.81%, Valid: 41.93%, Test: 42.15%
Run 01:
Highest Train: 43.41
Highest Valid: 42.50
  Final Train: 43.37
   Final Test: 42.59
All runs:
Highest Train: 43.41, nan
Highest Valid: 42.50, nan
  Final Train: 43.37, nan
   Final Test: 42.59, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6085, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4990, Train: 34.60%, Valid: 34.30%, Test: 34.43%
Epoch: 50, Loss: 1.4766, Train: 36.02%, Valid: 35.28%, Test: 35.58%
Epoch: 75, Loss: 1.4608, Train: 37.27%, Valid: 35.98%, Test: 36.37%
Epoch: 100, Loss: 1.4506, Train: 37.97%, Valid: 36.32%, Test: 36.51%
Epoch: 125, Loss: 1.4373, Train: 38.83%, Valid: 36.54%, Test: 36.67%
Epoch: 150, Loss: 1.4293, Train: 39.27%, Valid: 36.52%, Test: 36.78%
Epoch: 175, Loss: 1.4215, Train: 39.71%, Valid: 36.51%, Test: 36.57%
Epoch: 200, Loss: 1.4151, Train: 40.66%, Valid: 36.47%, Test: 36.95%
Epoch: 225, Loss: 1.4060, Train: 40.88%, Valid: 36.39%, Test: 36.88%
Epoch: 250, Loss: 1.4024, Train: 41.30%, Valid: 36.51%, Test: 36.80%
Epoch: 275, Loss: 1.3987, Train: 41.27%, Valid: 36.18%, Test: 36.51%
Epoch: 300, Loss: 1.3900, Train: 42.26%, Valid: 36.47%, Test: 36.75%
Epoch: 325, Loss: 1.3825, Train: 42.63%, Valid: 36.52%, Test: 36.76%
Epoch: 350, Loss: 1.3850, Train: 42.60%, Valid: 36.19%, Test: 36.52%
Epoch: 375, Loss: 1.3760, Train: 43.19%, Valid: 36.43%, Test: 36.66%
Epoch: 400, Loss: 1.3705, Train: 43.44%, Valid: 36.38%, Test: 36.63%
Epoch: 425, Loss: 1.3709, Train: 43.92%, Valid: 36.29%, Test: 36.60%
Epoch: 450, Loss: 1.3668, Train: 43.97%, Valid: 36.31%, Test: 36.62%
Epoch: 475, Loss: 1.3644, Train: 44.13%, Valid: 36.22%, Test: 36.65%
Epoch: 500, Loss: 1.3609, Train: 44.51%, Valid: 36.27%, Test: 36.56%
Epoch: 525, Loss: 1.3607, Train: 44.46%, Valid: 36.27%, Test: 36.70%
Epoch: 550, Loss: 1.3565, Train: 44.88%, Valid: 36.18%, Test: 36.58%
Epoch: 575, Loss: 1.3509, Train: 44.92%, Valid: 36.10%, Test: 36.57%
Epoch: 600, Loss: 1.3518, Train: 45.08%, Valid: 36.21%, Test: 36.55%
Epoch: 625, Loss: 1.3521, Train: 45.28%, Valid: 36.18%, Test: 36.62%
Epoch: 650, Loss: 1.3495, Train: 45.52%, Valid: 36.15%, Test: 36.54%
Epoch: 675, Loss: 1.3472, Train: 45.71%, Valid: 36.07%, Test: 36.51%
Epoch: 700, Loss: 1.3464, Train: 45.94%, Valid: 35.85%, Test: 36.49%
Epoch: 725, Loss: 1.3460, Train: 45.68%, Valid: 36.15%, Test: 36.59%
Epoch: 750, Loss: 1.3446, Train: 46.09%, Valid: 35.92%, Test: 36.48%
Epoch: 775, Loss: 1.3416, Train: 46.19%, Valid: 35.98%, Test: 36.37%
Epoch: 800, Loss: 1.3418, Train: 46.18%, Valid: 36.07%, Test: 36.43%
Epoch: 825, Loss: 1.3403, Train: 46.29%, Valid: 35.95%, Test: 36.45%
Epoch: 850, Loss: 1.3403, Train: 46.38%, Valid: 36.04%, Test: 36.46%
Epoch: 875, Loss: 1.3379, Train: 46.46%, Valid: 36.13%, Test: 36.28%
Epoch: 900, Loss: 1.3377, Train: 46.54%, Valid: 36.06%, Test: 36.46%
Epoch: 925, Loss: 1.3350, Train: 46.68%, Valid: 35.97%, Test: 36.46%
Epoch: 950, Loss: 1.3380, Train: 46.78%, Valid: 35.97%, Test: 36.26%
Epoch: 975, Loss: 1.3358, Train: 46.77%, Valid: 35.90%, Test: 36.43%
Run 01:
Highest Train: 46.96
Highest Valid: 36.69
  Final Train: 38.94
   Final Test: 36.76
All runs:
Highest Train: 46.96, nan
Highest Valid: 36.69, nan
  Final Train: 38.94, nan
   Final Test: 36.76, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6110, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4960, Train: 34.65%, Valid: 34.35%, Test: 34.65%
Epoch: 50, Loss: 1.4734, Train: 36.16%, Valid: 35.37%, Test: 35.81%
Epoch: 75, Loss: 1.4568, Train: 37.44%, Valid: 36.13%, Test: 36.53%
Epoch: 100, Loss: 1.4433, Train: 38.39%, Valid: 36.55%, Test: 36.96%
Epoch: 125, Loss: 1.4293, Train: 39.11%, Valid: 36.69%, Test: 36.85%
Epoch: 150, Loss: 1.4239, Train: 39.73%, Valid: 36.78%, Test: 36.94%
Epoch: 175, Loss: 1.4114, Train: 40.37%, Valid: 36.66%, Test: 36.67%
Epoch: 200, Loss: 1.4052, Train: 41.02%, Valid: 36.72%, Test: 37.02%
Epoch: 225, Loss: 1.3963, Train: 41.28%, Valid: 36.61%, Test: 36.98%
Epoch: 250, Loss: 1.3917, Train: 41.99%, Valid: 37.00%, Test: 37.24%
Epoch: 275, Loss: 1.3765, Train: 42.62%, Valid: 37.31%, Test: 37.68%
Epoch: 300, Loss: 1.3730, Train: 42.76%, Valid: 37.36%, Test: 37.55%
Epoch: 325, Loss: 1.3700, Train: 42.87%, Valid: 37.43%, Test: 37.64%
Epoch: 350, Loss: 1.3720, Train: 43.10%, Valid: 37.29%, Test: 37.45%
Epoch: 375, Loss: 1.3594, Train: 43.57%, Valid: 37.91%, Test: 38.18%
Epoch: 400, Loss: 1.3518, Train: 44.62%, Valid: 38.82%, Test: 39.00%
Epoch: 425, Loss: 1.3591, Train: 43.54%, Valid: 37.81%, Test: 38.00%
Epoch: 450, Loss: 1.3375, Train: 45.97%, Valid: 39.14%, Test: 39.41%
Epoch: 475, Loss: 1.3422, Train: 46.16%, Valid: 39.23%, Test: 39.60%
Epoch: 500, Loss: 1.3536, Train: 45.75%, Valid: 38.92%, Test: 39.39%
Epoch: 525, Loss: 1.3436, Train: 46.06%, Valid: 39.12%, Test: 39.79%
Epoch: 550, Loss: 1.3223, Train: 46.48%, Valid: 39.69%, Test: 40.05%
Epoch: 575, Loss: 1.3192, Train: 46.75%, Valid: 39.88%, Test: 40.22%
Epoch: 600, Loss: 1.3365, Train: 46.28%, Valid: 39.61%, Test: 40.13%
Epoch: 625, Loss: 1.3526, Train: 45.47%, Valid: 39.64%, Test: 40.04%
Epoch: 650, Loss: 1.3423, Train: 45.63%, Valid: 39.73%, Test: 40.26%
Epoch: 675, Loss: 1.3527, Train: 44.07%, Valid: 39.11%, Test: 39.17%
Epoch: 700, Loss: 1.3361, Train: 45.63%, Valid: 39.99%, Test: 40.26%
Epoch: 725, Loss: 1.3504, Train: 45.25%, Valid: 39.85%, Test: 39.89%
Epoch: 750, Loss: 1.3433, Train: 45.16%, Valid: 39.45%, Test: 39.90%
Epoch: 775, Loss: 1.3348, Train: 42.61%, Valid: 36.76%, Test: 37.17%
Epoch: 800, Loss: 1.3385, Train: 45.24%, Valid: 39.76%, Test: 40.36%
Epoch: 825, Loss: 1.3327, Train: 45.14%, Valid: 39.48%, Test: 40.01%
Epoch: 850, Loss: 1.3415, Train: 45.47%, Valid: 40.23%, Test: 40.66%
Epoch: 875, Loss: 1.3399, Train: 45.04%, Valid: 40.21%, Test: 40.68%
Epoch: 900, Loss: 1.3775, Train: 43.86%, Valid: 39.57%, Test: 39.82%
Epoch: 925, Loss: 1.3603, Train: 44.40%, Valid: 40.19%, Test: 40.48%
Epoch: 950, Loss: 1.3541, Train: 44.30%, Valid: 40.43%, Test: 40.82%
Epoch: 975, Loss: 1.3789, Train: 43.41%, Valid: 39.16%, Test: 39.38%
Run 01:
Highest Train: 47.34
Highest Valid: 40.80
  Final Train: 45.07
   Final Test: 41.16
All runs:
Highest Train: 47.34, nan
Highest Valid: 40.80, nan
  Final Train: 45.07, nan
   Final Test: 41.16, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.5939, Train: 33.49%, Valid: 33.26%, Test: 33.33%
Epoch: 25, Loss: 1.4621, Train: 26.97%, Valid: 26.76%, Test: 27.05%
Epoch: 50, Loss: 1.4333, Train: 38.09%, Valid: 37.75%, Test: 37.85%
Epoch: 75, Loss: 1.4263, Train: 37.78%, Valid: 37.36%, Test: 37.53%
Epoch: 100, Loss: 1.4515, Train: 38.64%, Valid: 38.17%, Test: 38.46%
Epoch: 125, Loss: 1.4168, Train: 38.22%, Valid: 37.68%, Test: 37.84%
Epoch: 150, Loss: 1.4028, Train: 38.97%, Valid: 38.53%, Test: 38.68%
Epoch: 175, Loss: 1.3915, Train: 39.60%, Valid: 39.39%, Test: 39.48%
Epoch: 200, Loss: 1.3874, Train: 39.82%, Valid: 39.64%, Test: 39.73%
Epoch: 225, Loss: 1.4045, Train: 39.63%, Valid: 39.38%, Test: 39.42%
Epoch: 250, Loss: 1.3830, Train: 39.79%, Valid: 39.50%, Test: 39.37%
Epoch: 275, Loss: 1.3834, Train: 33.16%, Valid: 33.15%, Test: 32.71%
Epoch: 300, Loss: 1.4894, Train: 37.37%, Valid: 37.03%, Test: 37.28%
Epoch: 325, Loss: 1.4214, Train: 37.21%, Valid: 36.92%, Test: 37.11%
Epoch: 350, Loss: 1.3993, Train: 40.12%, Valid: 39.81%, Test: 39.80%
Epoch: 375, Loss: 1.3941, Train: 35.97%, Valid: 35.65%, Test: 35.81%
Epoch: 400, Loss: 1.3898, Train: 34.11%, Valid: 33.95%, Test: 33.85%
Epoch: 425, Loss: 1.3811, Train: 39.33%, Valid: 39.10%, Test: 38.90%
Epoch: 450, Loss: 1.3714, Train: 39.23%, Valid: 39.08%, Test: 38.99%
Epoch: 475, Loss: 1.3680, Train: 41.23%, Valid: 40.92%, Test: 41.03%
Epoch: 500, Loss: 1.3792, Train: 33.69%, Valid: 33.63%, Test: 33.37%
Epoch: 525, Loss: 1.3705, Train: 40.57%, Valid: 40.32%, Test: 40.35%
Epoch: 550, Loss: 1.3823, Train: 39.37%, Valid: 39.29%, Test: 39.38%
Epoch: 575, Loss: 1.3803, Train: 31.82%, Valid: 31.92%, Test: 31.52%
Epoch: 600, Loss: 1.4000, Train: 36.67%, Valid: 36.81%, Test: 36.75%
Epoch: 625, Loss: 1.3751, Train: 40.36%, Valid: 40.16%, Test: 40.16%
Epoch: 650, Loss: 1.3829, Train: 32.03%, Valid: 32.19%, Test: 31.58%
Epoch: 675, Loss: 1.3635, Train: 31.58%, Valid: 31.64%, Test: 31.19%
Epoch: 700, Loss: 1.3728, Train: 40.15%, Valid: 39.99%, Test: 39.99%
Epoch: 725, Loss: 1.3835, Train: 40.20%, Valid: 40.03%, Test: 40.13%
Epoch: 750, Loss: 1.3668, Train: 40.77%, Valid: 40.54%, Test: 40.75%
Epoch: 775, Loss: 1.3761, Train: 40.59%, Valid: 40.43%, Test: 40.46%
Epoch: 800, Loss: 1.3621, Train: 41.25%, Valid: 41.03%, Test: 41.20%
Epoch: 825, Loss: 1.3736, Train: 38.16%, Valid: 38.00%, Test: 38.02%
Epoch: 850, Loss: 1.3869, Train: 40.25%, Valid: 40.09%, Test: 40.09%
Epoch: 875, Loss: 1.3688, Train: 40.99%, Valid: 40.74%, Test: 40.90%
Epoch: 900, Loss: 1.5153, Train: 33.96%, Valid: 33.56%, Test: 33.93%
Epoch: 925, Loss: 1.4621, Train: 30.97%, Valid: 30.84%, Test: 30.51%
Epoch: 950, Loss: 1.4223, Train: 40.43%, Valid: 40.03%, Test: 40.13%
Epoch: 975, Loss: 1.3984, Train: 35.77%, Valid: 35.35%, Test: 35.30%
Run 01:
Highest Train: 42.12
Highest Valid: 41.87
  Final Train: 42.12
   Final Test: 41.85
All runs:
Highest Train: 42.12, nan
Highest Valid: 41.87, nan
  Final Train: 42.12, nan
   Final Test: 41.85, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6083, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4991, Train: 34.59%, Valid: 34.27%, Test: 34.51%
Epoch: 50, Loss: 1.4766, Train: 36.14%, Valid: 35.34%, Test: 35.70%
Epoch: 75, Loss: 1.4605, Train: 37.31%, Valid: 36.08%, Test: 36.11%
Epoch: 100, Loss: 1.4489, Train: 37.94%, Valid: 36.51%, Test: 36.80%
Epoch: 125, Loss: 1.4366, Train: 38.71%, Valid: 36.81%, Test: 36.79%
Epoch: 150, Loss: 1.4260, Train: 39.24%, Valid: 36.70%, Test: 37.05%
Epoch: 175, Loss: 1.4165, Train: 39.95%, Valid: 36.75%, Test: 36.93%
Epoch: 200, Loss: 1.4128, Train: 40.51%, Valid: 36.58%, Test: 37.07%
Epoch: 225, Loss: 1.4054, Train: 40.85%, Valid: 36.54%, Test: 36.79%
Epoch: 250, Loss: 1.3976, Train: 41.37%, Valid: 36.71%, Test: 37.06%
Epoch: 275, Loss: 1.3964, Train: 41.64%, Valid: 36.69%, Test: 37.04%
Epoch: 300, Loss: 1.3884, Train: 42.18%, Valid: 36.64%, Test: 37.16%
Epoch: 325, Loss: 1.3838, Train: 42.50%, Valid: 36.65%, Test: 37.08%
Epoch: 350, Loss: 1.3783, Train: 42.92%, Valid: 36.55%, Test: 37.09%
Epoch: 375, Loss: 1.3770, Train: 43.35%, Valid: 36.51%, Test: 37.20%
Epoch: 400, Loss: 1.3704, Train: 43.41%, Valid: 36.55%, Test: 37.13%
Epoch: 425, Loss: 1.3655, Train: 43.63%, Valid: 36.63%, Test: 37.14%
Epoch: 450, Loss: 1.3690, Train: 43.83%, Valid: 36.40%, Test: 36.99%
Epoch: 475, Loss: 1.3652, Train: 44.10%, Valid: 36.46%, Test: 37.01%
Epoch: 500, Loss: 1.3593, Train: 44.24%, Valid: 36.58%, Test: 37.05%
Epoch: 525, Loss: 1.3587, Train: 44.40%, Valid: 36.50%, Test: 36.84%
Epoch: 550, Loss: 1.3559, Train: 44.65%, Valid: 36.44%, Test: 36.96%
Epoch: 575, Loss: 1.3560, Train: 44.87%, Valid: 36.34%, Test: 36.78%
Epoch: 600, Loss: 1.3545, Train: 44.83%, Valid: 36.52%, Test: 37.00%
Epoch: 625, Loss: 1.3517, Train: 45.09%, Valid: 36.27%, Test: 36.75%
Epoch: 650, Loss: 1.3494, Train: 45.18%, Valid: 36.31%, Test: 36.90%
Epoch: 675, Loss: 1.3488, Train: 45.30%, Valid: 36.25%, Test: 36.86%
Epoch: 700, Loss: 1.3472, Train: 45.52%, Valid: 36.28%, Test: 36.73%
Epoch: 725, Loss: 1.3464, Train: 45.56%, Valid: 36.27%, Test: 36.91%
Epoch: 750, Loss: 1.3426, Train: 45.66%, Valid: 36.23%, Test: 36.88%
Epoch: 775, Loss: 1.3422, Train: 45.97%, Valid: 36.05%, Test: 36.63%
Epoch: 800, Loss: 1.3410, Train: 46.06%, Valid: 36.22%, Test: 36.71%
Epoch: 825, Loss: 1.3408, Train: 46.28%, Valid: 36.04%, Test: 36.82%
Epoch: 850, Loss: 1.3403, Train: 46.00%, Valid: 36.15%, Test: 36.71%
Epoch: 875, Loss: 1.3382, Train: 46.08%, Valid: 36.26%, Test: 36.86%
Epoch: 900, Loss: 1.3379, Train: 46.50%, Valid: 36.09%, Test: 36.63%
Epoch: 925, Loss: 1.3396, Train: 46.54%, Valid: 35.94%, Test: 36.53%
Epoch: 950, Loss: 1.3372, Train: 46.78%, Valid: 36.08%, Test: 36.54%
Epoch: 975, Loss: 1.3330, Train: 46.80%, Valid: 35.98%, Test: 36.64%
Run 01:
Highest Train: 46.89
Highest Valid: 36.84
  Final Train: 40.98
   Final Test: 37.03
All runs:
Highest Train: 46.89, nan
Highest Valid: 36.84, nan
  Final Train: 40.98, nan
   Final Test: 37.03, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6091, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4962, Train: 34.61%, Valid: 34.41%, Test: 34.55%
Epoch: 50, Loss: 1.4738, Train: 36.25%, Valid: 35.58%, Test: 35.84%
Epoch: 75, Loss: 1.4564, Train: 37.44%, Valid: 36.18%, Test: 36.60%
Epoch: 100, Loss: 1.4439, Train: 38.19%, Valid: 36.46%, Test: 36.70%
Epoch: 125, Loss: 1.4340, Train: 38.92%, Valid: 36.71%, Test: 36.89%
Epoch: 150, Loss: 1.4260, Train: 39.55%, Valid: 36.85%, Test: 36.79%
Epoch: 175, Loss: 1.4144, Train: 40.23%, Valid: 36.69%, Test: 37.00%
Epoch: 200, Loss: 1.4139, Train: 40.84%, Valid: 36.73%, Test: 36.73%
Epoch: 225, Loss: 1.3974, Train: 41.58%, Valid: 36.78%, Test: 37.01%
Epoch: 250, Loss: 1.3943, Train: 42.04%, Valid: 36.88%, Test: 37.04%
Epoch: 275, Loss: 1.3845, Train: 42.69%, Valid: 37.04%, Test: 37.24%
Epoch: 300, Loss: 1.3768, Train: 43.15%, Valid: 37.38%, Test: 37.63%
Epoch: 325, Loss: 1.3690, Train: 43.38%, Valid: 37.70%, Test: 37.95%
Epoch: 350, Loss: 1.3521, Train: 44.50%, Valid: 38.12%, Test: 38.36%
Epoch: 375, Loss: 1.3635, Train: 43.74%, Valid: 37.43%, Test: 37.67%
Epoch: 400, Loss: 1.3423, Train: 45.08%, Valid: 38.65%, Test: 38.94%
Epoch: 425, Loss: 1.3523, Train: 44.17%, Valid: 37.82%, Test: 38.19%
Epoch: 450, Loss: 1.3480, Train: 45.13%, Valid: 38.58%, Test: 38.85%
Epoch: 475, Loss: 1.3461, Train: 44.94%, Valid: 38.10%, Test: 38.64%
Epoch: 500, Loss: 1.3364, Train: 46.73%, Valid: 39.31%, Test: 39.74%
Epoch: 525, Loss: 1.3482, Train: 45.15%, Valid: 38.12%, Test: 38.62%
Epoch: 550, Loss: 1.3354, Train: 45.87%, Valid: 39.09%, Test: 39.43%
Epoch: 575, Loss: 1.3365, Train: 46.44%, Valid: 39.44%, Test: 39.91%
Epoch: 600, Loss: 1.3428, Train: 45.99%, Valid: 39.55%, Test: 40.11%
Epoch: 625, Loss: 1.3322, Train: 45.70%, Valid: 39.46%, Test: 40.02%
Epoch: 650, Loss: 1.3576, Train: 44.64%, Valid: 38.86%, Test: 39.53%
Epoch: 675, Loss: 1.3442, Train: 44.26%, Valid: 38.92%, Test: 39.41%
Epoch: 700, Loss: 1.3357, Train: 45.75%, Valid: 40.20%, Test: 40.54%
Epoch: 725, Loss: 1.3498, Train: 45.27%, Valid: 39.77%, Test: 40.31%
Epoch: 750, Loss: 1.3447, Train: 45.17%, Valid: 39.80%, Test: 40.33%
Epoch: 775, Loss: 1.3559, Train: 45.80%, Valid: 40.42%, Test: 40.68%
Epoch: 800, Loss: 1.3720, Train: 44.01%, Valid: 39.46%, Test: 39.87%
Epoch: 825, Loss: 1.3693, Train: 43.61%, Valid: 39.10%, Test: 39.71%
Epoch: 850, Loss: 1.3567, Train: 44.55%, Valid: 39.80%, Test: 40.29%
Epoch: 875, Loss: 1.3465, Train: 44.48%, Valid: 39.92%, Test: 40.11%
Epoch: 900, Loss: 1.3514, Train: 45.06%, Valid: 40.63%, Test: 40.91%
Epoch: 925, Loss: 1.3569, Train: 44.43%, Valid: 39.94%, Test: 40.14%
Epoch: 950, Loss: 1.3510, Train: 44.95%, Valid: 40.65%, Test: 40.80%
Epoch: 975, Loss: 1.3707, Train: 44.62%, Valid: 40.21%, Test: 40.66%
Run 01:
Highest Train: 47.09
Highest Valid: 40.83
  Final Train: 45.23
   Final Test: 40.91
All runs:
Highest Train: 47.09, nan
Highest Valid: 40.83, nan
  Final Train: 45.23, nan
   Final Test: 40.91, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.5977, Train: 13.70%, Valid: 13.72%, Test: 13.68%
Epoch: 25, Loss: 1.4829, Train: 37.05%, Valid: 36.82%, Test: 37.25%
Epoch: 50, Loss: 1.4550, Train: 36.32%, Valid: 35.86%, Test: 36.16%
Epoch: 75, Loss: 1.4221, Train: 36.59%, Valid: 36.00%, Test: 36.30%
Epoch: 100, Loss: 1.4503, Train: 37.90%, Valid: 37.38%, Test: 37.42%
Epoch: 125, Loss: 1.3933, Train: 39.57%, Valid: 38.94%, Test: 38.95%
Epoch: 150, Loss: 1.3976, Train: 39.67%, Valid: 38.89%, Test: 39.11%
Epoch: 175, Loss: 1.3863, Train: 37.27%, Valid: 36.35%, Test: 36.61%
Epoch: 200, Loss: 1.3712, Train: 41.47%, Valid: 40.38%, Test: 40.73%
Epoch: 225, Loss: 1.3686, Train: 41.83%, Valid: 40.88%, Test: 40.93%
Epoch: 250, Loss: 1.3644, Train: 41.58%, Valid: 40.55%, Test: 40.78%
Epoch: 275, Loss: 1.3616, Train: 41.04%, Valid: 40.05%, Test: 40.32%
Epoch: 300, Loss: 1.3547, Train: 42.45%, Valid: 41.12%, Test: 41.20%
Epoch: 325, Loss: 1.5117, Train: 28.53%, Valid: 28.36%, Test: 28.69%
Epoch: 350, Loss: 1.4487, Train: 37.50%, Valid: 37.13%, Test: 37.38%
Epoch: 375, Loss: 1.4154, Train: 29.08%, Valid: 28.90%, Test: 29.15%
Epoch: 400, Loss: 1.4651, Train: 37.81%, Valid: 37.45%, Test: 37.79%
Epoch: 425, Loss: 1.4570, Train: 30.75%, Valid: 30.60%, Test: 31.03%
Epoch: 450, Loss: 1.4214, Train: 33.18%, Valid: 32.73%, Test: 33.69%
Epoch: 475, Loss: 1.4267, Train: 38.32%, Valid: 38.06%, Test: 38.41%
Epoch: 500, Loss: 1.4070, Train: 32.61%, Valid: 32.14%, Test: 33.00%
Epoch: 525, Loss: 1.4162, Train: 31.84%, Valid: 31.35%, Test: 32.21%
Epoch: 550, Loss: 1.3963, Train: 32.65%, Valid: 32.20%, Test: 33.08%
Epoch: 575, Loss: 1.3950, Train: 33.01%, Valid: 32.83%, Test: 33.15%
Epoch: 600, Loss: 1.4211, Train: 29.43%, Valid: 29.42%, Test: 29.53%
Epoch: 625, Loss: 1.6550, Train: 24.66%, Valid: 24.76%, Test: 25.02%
Epoch: 650, Loss: 1.5054, Train: 14.56%, Valid: 14.63%, Test: 14.59%
Epoch: 675, Loss: 1.4806, Train: 21.50%, Valid: 21.72%, Test: 21.48%
Epoch: 700, Loss: 1.4567, Train: 24.01%, Valid: 23.51%, Test: 23.97%
Epoch: 725, Loss: 1.4376, Train: 36.76%, Valid: 36.69%, Test: 36.64%
Epoch: 750, Loss: 1.4237, Train: 29.81%, Valid: 29.72%, Test: 29.48%
Epoch: 775, Loss: 1.4350, Train: 38.86%, Valid: 38.32%, Test: 38.51%
Epoch: 800, Loss: 1.4171, Train: 39.66%, Valid: 39.05%, Test: 39.49%
Epoch: 825, Loss: 1.4066, Train: 30.80%, Valid: 30.84%, Test: 30.67%
Epoch: 850, Loss: 1.4109, Train: 34.35%, Valid: 34.39%, Test: 34.34%
Epoch: 875, Loss: 1.3993, Train: 31.46%, Valid: 31.35%, Test: 31.48%
Epoch: 900, Loss: 1.4177, Train: 32.29%, Valid: 32.27%, Test: 32.47%
Epoch: 925, Loss: 1.3919, Train: 33.91%, Valid: 33.96%, Test: 34.02%
Epoch: 950, Loss: 1.3839, Train: 37.40%, Valid: 37.27%, Test: 37.28%
Epoch: 975, Loss: 1.3877, Train: 31.52%, Valid: 31.41%, Test: 31.62%
Run 01:
Highest Train: 42.54
Highest Valid: 41.19
  Final Train: 42.54
   Final Test: 41.37
All runs:
Highest Train: 42.54, nan
Highest Valid: 41.19, nan
  Final Train: 42.54, nan
   Final Test: 41.37, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6089, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4944, Train: 34.53%, Valid: 34.32%, Test: 34.51%
Epoch: 50, Loss: 1.4708, Train: 35.93%, Valid: 35.24%, Test: 35.51%
Epoch: 75, Loss: 1.4546, Train: 36.78%, Valid: 35.70%, Test: 35.88%
Epoch: 100, Loss: 1.4324, Train: 37.85%, Valid: 36.14%, Test: 36.27%
Epoch: 125, Loss: 1.4239, Train: 38.16%, Valid: 36.00%, Test: 36.39%
Epoch: 150, Loss: 1.4098, Train: 38.90%, Valid: 35.83%, Test: 36.34%
Epoch: 175, Loss: 1.3938, Train: 39.91%, Valid: 35.51%, Test: 36.04%
Epoch: 200, Loss: 1.3864, Train: 40.86%, Valid: 35.98%, Test: 36.56%
Epoch: 225, Loss: 1.3721, Train: 41.56%, Valid: 35.87%, Test: 36.03%
Epoch: 250, Loss: 1.3522, Train: 42.14%, Valid: 35.35%, Test: 35.79%
Epoch: 275, Loss: 1.3552, Train: 39.71%, Valid: 32.06%, Test: 32.32%
Epoch: 300, Loss: 1.3167, Train: 43.35%, Valid: 35.26%, Test: 35.63%
Epoch: 325, Loss: 1.2957, Train: 45.03%, Valid: 34.26%, Test: 34.44%
Epoch: 350, Loss: 1.2786, Train: 45.66%, Valid: 33.44%, Test: 33.44%
Epoch: 375, Loss: 1.2589, Train: 46.69%, Valid: 34.37%, Test: 34.66%
Epoch: 400, Loss: 1.2789, Train: 46.85%, Valid: 34.22%, Test: 34.32%
Epoch: 425, Loss: 1.2243, Train: 48.57%, Valid: 34.22%, Test: 34.11%
Epoch: 450, Loss: 1.2124, Train: 49.50%, Valid: 32.65%, Test: 32.92%
Epoch: 475, Loss: 1.2080, Train: 48.39%, Valid: 32.39%, Test: 32.63%
Epoch: 500, Loss: 1.1745, Train: 51.08%, Valid: 33.30%, Test: 33.79%
Epoch: 525, Loss: 1.1600, Train: 51.66%, Valid: 33.37%, Test: 33.57%
Epoch: 550, Loss: 1.1910, Train: 48.30%, Valid: 32.75%, Test: 32.92%
Epoch: 575, Loss: 1.1117, Train: 54.01%, Valid: 31.98%, Test: 32.35%
Epoch: 600, Loss: 1.1012, Train: 54.30%, Valid: 31.64%, Test: 32.03%
Epoch: 625, Loss: 1.0851, Train: 55.22%, Valid: 31.35%, Test: 31.47%
Epoch: 650, Loss: 1.0619, Train: 55.55%, Valid: 31.31%, Test: 31.65%
Epoch: 675, Loss: 1.0453, Train: 57.35%, Valid: 31.26%, Test: 31.76%
Epoch: 700, Loss: 1.0445, Train: 57.65%, Valid: 31.14%, Test: 31.44%
Epoch: 725, Loss: 1.0359, Train: 56.96%, Valid: 30.99%, Test: 31.27%
Epoch: 750, Loss: 1.0462, Train: 56.75%, Valid: 31.18%, Test: 31.18%
Epoch: 775, Loss: 1.0741, Train: 54.86%, Valid: 31.78%, Test: 31.68%
Epoch: 800, Loss: 1.0431, Train: 56.69%, Valid: 30.41%, Test: 30.64%
Epoch: 825, Loss: 0.9715, Train: 60.68%, Valid: 30.91%, Test: 31.32%
Epoch: 850, Loss: 0.9899, Train: 60.79%, Valid: 30.64%, Test: 30.51%
Epoch: 875, Loss: 1.0086, Train: 56.46%, Valid: 30.62%, Test: 30.57%
Epoch: 900, Loss: 0.9320, Train: 62.20%, Valid: 30.65%, Test: 30.54%
Epoch: 925, Loss: 0.9612, Train: 60.35%, Valid: 31.52%, Test: 31.74%
Epoch: 950, Loss: 0.9790, Train: 51.39%, Valid: 26.43%, Test: 26.29%
Epoch: 975, Loss: 0.9069, Train: 63.99%, Valid: 30.19%, Test: 30.46%
Run 01:
Highest Train: 64.37
Highest Valid: 36.33
  Final Train: 39.36
   Final Test: 36.54
All runs:
Highest Train: 64.37, nan
Highest Valid: 36.33, nan
  Final Train: 39.36, nan
   Final Test: 36.54, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6071, Train: 21.96%, Valid: 22.04%, Test: 21.83%
Epoch: 25, Loss: 1.4962, Train: 34.33%, Valid: 34.18%, Test: 34.37%
Epoch: 50, Loss: 1.4723, Train: 35.52%, Valid: 35.08%, Test: 35.44%
Epoch: 75, Loss: 1.4577, Train: 36.19%, Valid: 35.53%, Test: 35.97%
Epoch: 100, Loss: 1.4256, Train: 37.41%, Valid: 36.65%, Test: 36.92%
Epoch: 125, Loss: 1.4037, Train: 39.38%, Valid: 38.30%, Test: 38.72%
Epoch: 150, Loss: 1.3921, Train: 39.90%, Valid: 38.44%, Test: 38.79%
Epoch: 175, Loss: 1.3709, Train: 41.02%, Valid: 39.27%, Test: 39.63%
Epoch: 200, Loss: 1.3716, Train: 41.13%, Valid: 39.03%, Test: 39.27%
Epoch: 225, Loss: 1.3577, Train: 41.65%, Valid: 39.19%, Test: 39.69%
Epoch: 250, Loss: 1.3281, Train: 43.38%, Valid: 40.34%, Test: 40.83%
Epoch: 275, Loss: 1.3262, Train: 43.33%, Valid: 40.23%, Test: 40.57%
Epoch: 300, Loss: 1.3178, Train: 43.95%, Valid: 40.32%, Test: 40.54%
Epoch: 325, Loss: 1.3049, Train: 44.26%, Valid: 40.00%, Test: 40.26%
Epoch: 350, Loss: 1.2996, Train: 45.07%, Valid: 40.56%, Test: 40.64%
Epoch: 375, Loss: 1.2869, Train: 45.67%, Valid: 40.35%, Test: 40.55%
Epoch: 400, Loss: 1.2874, Train: 45.08%, Valid: 39.29%, Test: 39.71%
Epoch: 425, Loss: 1.2890, Train: 44.76%, Valid: 37.90%, Test: 38.22%
Epoch: 450, Loss: 1.2595, Train: 46.27%, Valid: 39.21%, Test: 39.50%
Epoch: 475, Loss: 1.2489, Train: 46.05%, Valid: 38.21%, Test: 38.52%
Epoch: 500, Loss: 1.2436, Train: 47.10%, Valid: 38.99%, Test: 39.42%
Epoch: 525, Loss: 1.2739, Train: 44.13%, Valid: 37.28%, Test: 37.88%
Epoch: 550, Loss: 1.2195, Train: 48.38%, Valid: 39.81%, Test: 39.89%
Epoch: 575, Loss: 1.4136, Train: 39.93%, Valid: 34.46%, Test: 34.76%
Epoch: 600, Loss: 1.2809, Train: 44.58%, Valid: 38.48%, Test: 38.80%
Epoch: 625, Loss: 1.2442, Train: 47.17%, Valid: 39.44%, Test: 39.48%
Epoch: 650, Loss: 1.2776, Train: 45.94%, Valid: 38.52%, Test: 38.64%
Epoch: 675, Loss: 1.2042, Train: 48.67%, Valid: 40.31%, Test: 40.44%
Epoch: 700, Loss: 1.1957, Train: 48.66%, Valid: 39.62%, Test: 39.85%
Epoch: 725, Loss: 1.2446, Train: 47.11%, Valid: 38.89%, Test: 39.29%
Epoch: 750, Loss: 1.1961, Train: 48.88%, Valid: 39.50%, Test: 39.79%
Epoch: 775, Loss: 1.1922, Train: 49.11%, Valid: 39.60%, Test: 39.82%
Epoch: 800, Loss: 1.2183, Train: 48.18%, Valid: 38.71%, Test: 38.67%
Epoch: 825, Loss: 1.1613, Train: 50.27%, Valid: 39.70%, Test: 39.79%
Epoch: 850, Loss: 1.1893, Train: 49.11%, Valid: 38.57%, Test: 38.89%
Epoch: 875, Loss: 1.1264, Train: 51.49%, Valid: 39.89%, Test: 39.97%
Epoch: 900, Loss: 1.1523, Train: 48.06%, Valid: 37.90%, Test: 38.34%
Epoch: 925, Loss: 1.1735, Train: 49.70%, Valid: 39.88%, Test: 40.18%
Epoch: 950, Loss: 1.1226, Train: 52.27%, Valid: 40.43%, Test: 40.66%
Epoch: 975, Loss: 1.1055, Train: 52.55%, Valid: 40.24%, Test: 40.28%
Run 01:
Highest Train: 53.49
Highest Valid: 41.08
  Final Train: 52.19
   Final Test: 41.35
All runs:
Highest Train: 53.49, nan
Highest Valid: 41.08, nan
  Final Train: 52.19, nan
   Final Test: 41.35, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6372, Train: 28.17%, Valid: 28.01%, Test: 28.30%
Epoch: 25, Loss: 1.5085, Train: 28.85%, Valid: 28.70%, Test: 28.98%
Epoch: 50, Loss: 1.4622, Train: 33.18%, Valid: 32.95%, Test: 33.86%
Epoch: 75, Loss: 1.4761, Train: 32.29%, Valid: 31.81%, Test: 32.93%
Epoch: 100, Loss: 1.4678, Train: 32.34%, Valid: 32.17%, Test: 33.05%
Epoch: 125, Loss: 1.5132, Train: 33.25%, Valid: 33.09%, Test: 34.00%
Epoch: 150, Loss: 1.4572, Train: 32.83%, Valid: 32.58%, Test: 33.52%
Epoch: 175, Loss: 1.4836, Train: 32.15%, Valid: 32.15%, Test: 32.42%
Epoch: 200, Loss: 1.4470, Train: 32.70%, Valid: 32.44%, Test: 33.25%
Epoch: 225, Loss: 1.4530, Train: 32.79%, Valid: 32.51%, Test: 33.33%
Epoch: 250, Loss: 1.4351, Train: 33.75%, Valid: 33.42%, Test: 34.30%
Epoch: 275, Loss: 1.4389, Train: 33.53%, Valid: 33.29%, Test: 34.12%
Epoch: 300, Loss: 1.4191, Train: 33.81%, Valid: 33.51%, Test: 34.32%
Epoch: 325, Loss: 1.4508, Train: 33.02%, Valid: 32.53%, Test: 32.98%
Epoch: 350, Loss: 1.5406, Train: 33.30%, Valid: 33.10%, Test: 34.02%
Epoch: 375, Loss: 1.4490, Train: 33.18%, Valid: 32.90%, Test: 33.81%
Epoch: 400, Loss: 1.4539, Train: 32.81%, Valid: 32.51%, Test: 33.54%
Epoch: 425, Loss: 1.4333, Train: 35.80%, Valid: 35.64%, Test: 36.40%
Epoch: 450, Loss: 1.4381, Train: 35.27%, Valid: 35.16%, Test: 35.75%
Epoch: 475, Loss: 1.4265, Train: 34.00%, Valid: 33.80%, Test: 34.23%
Epoch: 500, Loss: 1.4290, Train: 34.83%, Valid: 34.50%, Test: 35.33%
Epoch: 525, Loss: 1.4155, Train: 37.71%, Valid: 37.36%, Test: 37.73%
Epoch: 550, Loss: 1.4294, Train: 34.29%, Valid: 33.96%, Test: 34.76%
Epoch: 575, Loss: 1.4141, Train: 38.63%, Valid: 38.32%, Test: 38.34%
Epoch: 600, Loss: 1.4274, Train: 37.53%, Valid: 37.13%, Test: 37.71%
Epoch: 625, Loss: 1.4714, Train: 36.94%, Valid: 36.87%, Test: 37.07%
Epoch: 650, Loss: 1.4426, Train: 33.10%, Valid: 32.92%, Test: 33.30%
Epoch: 675, Loss: 1.4151, Train: 38.49%, Valid: 38.34%, Test: 38.34%
Epoch: 700, Loss: 1.4366, Train: 33.71%, Valid: 33.45%, Test: 34.26%
Epoch: 725, Loss: 1.4088, Train: 38.62%, Valid: 38.32%, Test: 38.57%
Epoch: 750, Loss: 1.4698, Train: 31.16%, Valid: 30.80%, Test: 31.07%
Epoch: 775, Loss: 1.4184, Train: 38.31%, Valid: 37.83%, Test: 38.19%
Epoch: 800, Loss: 1.4102, Train: 38.48%, Valid: 38.24%, Test: 38.37%
Epoch: 825, Loss: 1.3882, Train: 39.51%, Valid: 39.30%, Test: 39.45%
Epoch: 850, Loss: 1.4151, Train: 37.58%, Valid: 37.00%, Test: 37.58%
Epoch: 875, Loss: 1.3904, Train: 39.28%, Valid: 39.11%, Test: 39.24%
Epoch: 900, Loss: 1.3912, Train: 39.04%, Valid: 38.60%, Test: 38.92%
Epoch: 925, Loss: 1.5155, Train: 37.25%, Valid: 36.84%, Test: 37.34%
Epoch: 950, Loss: 1.4010, Train: 38.86%, Valid: 38.44%, Test: 38.84%
Epoch: 975, Loss: 1.3980, Train: 38.86%, Valid: 38.43%, Test: 38.87%
Run 01:
Highest Train: 39.92
Highest Valid: 39.55
  Final Train: 39.92
   Final Test: 40.02
All runs:
Highest Train: 39.92, nan
Highest Valid: 39.55, nan
  Final Train: 39.92, nan
   Final Test: 40.02, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6102, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.5045, Train: 34.11%, Valid: 33.89%, Test: 34.02%
Epoch: 50, Loss: 1.4793, Train: 35.47%, Valid: 34.73%, Test: 35.24%
Epoch: 75, Loss: 1.4599, Train: 36.72%, Valid: 35.61%, Test: 35.90%
Epoch: 100, Loss: 1.4428, Train: 37.34%, Valid: 35.70%, Test: 36.04%
Epoch: 125, Loss: 1.4258, Train: 38.41%, Valid: 35.97%, Test: 36.41%
Epoch: 150, Loss: 1.4103, Train: 39.17%, Valid: 36.18%, Test: 36.44%
Epoch: 175, Loss: 1.4109, Train: 39.16%, Valid: 35.11%, Test: 35.48%
Epoch: 200, Loss: 1.3792, Train: 41.02%, Valid: 35.53%, Test: 36.10%
Epoch: 225, Loss: 1.3693, Train: 41.55%, Valid: 35.59%, Test: 36.22%
Epoch: 250, Loss: 1.3546, Train: 42.12%, Valid: 35.77%, Test: 36.26%
Epoch: 275, Loss: 1.3432, Train: 42.24%, Valid: 35.07%, Test: 35.42%
Epoch: 300, Loss: 1.3381, Train: 43.42%, Valid: 34.95%, Test: 35.56%
Epoch: 325, Loss: 1.3203, Train: 44.24%, Valid: 34.64%, Test: 35.30%
Epoch: 350, Loss: 1.3015, Train: 44.39%, Valid: 34.66%, Test: 35.12%
Epoch: 375, Loss: 1.2772, Train: 45.72%, Valid: 34.30%, Test: 34.99%
Epoch: 400, Loss: 1.2749, Train: 46.03%, Valid: 34.24%, Test: 34.51%
Epoch: 425, Loss: 1.2681, Train: 46.75%, Valid: 33.23%, Test: 33.69%
Epoch: 450, Loss: 1.2532, Train: 46.94%, Valid: 33.70%, Test: 34.29%
Epoch: 475, Loss: 1.2702, Train: 47.79%, Valid: 33.39%, Test: 33.89%
Epoch: 500, Loss: 1.2204, Train: 48.77%, Valid: 32.74%, Test: 33.44%
Epoch: 525, Loss: 1.2109, Train: 49.42%, Valid: 33.26%, Test: 33.60%
Epoch: 550, Loss: 1.2037, Train: 49.11%, Valid: 33.11%, Test: 33.39%
Epoch: 575, Loss: 1.2288, Train: 47.41%, Valid: 31.93%, Test: 32.55%
Epoch: 600, Loss: 1.1669, Train: 51.60%, Valid: 32.60%, Test: 32.92%
Epoch: 625, Loss: 1.1685, Train: 51.44%, Valid: 32.05%, Test: 32.46%
Epoch: 650, Loss: 1.1720, Train: 52.11%, Valid: 32.49%, Test: 33.00%
Epoch: 675, Loss: 1.1611, Train: 51.35%, Valid: 32.50%, Test: 32.92%
Epoch: 700, Loss: 1.1381, Train: 52.13%, Valid: 31.58%, Test: 31.78%
Epoch: 725, Loss: 1.1186, Train: 53.63%, Valid: 31.00%, Test: 31.14%
Epoch: 750, Loss: 1.1043, Train: 53.90%, Valid: 30.45%, Test: 30.60%
Epoch: 775, Loss: 1.1482, Train: 53.10%, Valid: 33.15%, Test: 33.71%
Epoch: 800, Loss: 1.1575, Train: 53.33%, Valid: 31.60%, Test: 31.69%
Epoch: 825, Loss: 1.1065, Train: 55.75%, Valid: 31.15%, Test: 31.20%
Epoch: 850, Loss: 1.0647, Train: 55.81%, Valid: 30.99%, Test: 31.10%
Epoch: 875, Loss: 1.0858, Train: 54.83%, Valid: 31.56%, Test: 31.62%
Epoch: 900, Loss: 1.0668, Train: 55.14%, Valid: 31.16%, Test: 31.28%
Epoch: 925, Loss: 1.0270, Train: 58.44%, Valid: 30.68%, Test: 30.92%
Epoch: 950, Loss: 1.0687, Train: 56.52%, Valid: 31.67%, Test: 31.90%
Epoch: 975, Loss: 1.1007, Train: 51.42%, Valid: 27.08%, Test: 26.93%
Run 01:
Highest Train: 59.38
Highest Valid: 36.29
  Final Train: 39.30
   Final Test: 36.55
All runs:
Highest Train: 59.38, nan
Highest Valid: 36.29, nan
  Final Train: 39.30, nan
   Final Test: 36.55, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6088, Train: 26.73%, Valid: 26.53%, Test: 26.80%
Epoch: 25, Loss: 1.4980, Train: 34.14%, Valid: 34.05%, Test: 34.19%
Epoch: 50, Loss: 1.4757, Train: 35.49%, Valid: 34.97%, Test: 35.26%
Epoch: 75, Loss: 1.4503, Train: 37.04%, Valid: 36.23%, Test: 36.45%
Epoch: 100, Loss: 1.4222, Train: 38.27%, Valid: 37.34%, Test: 37.41%
Epoch: 125, Loss: 1.4292, Train: 37.86%, Valid: 36.84%, Test: 37.17%
Epoch: 150, Loss: 1.3941, Train: 39.33%, Valid: 37.90%, Test: 38.27%
Epoch: 175, Loss: 1.3695, Train: 40.76%, Valid: 39.13%, Test: 39.47%
Epoch: 200, Loss: 1.3815, Train: 40.84%, Valid: 38.59%, Test: 39.29%
Epoch: 225, Loss: 1.3486, Train: 42.17%, Valid: 40.00%, Test: 40.23%
Epoch: 250, Loss: 1.3334, Train: 43.06%, Valid: 40.69%, Test: 40.84%
Epoch: 275, Loss: 1.3262, Train: 43.42%, Valid: 40.56%, Test: 40.80%
Epoch: 300, Loss: 1.3318, Train: 43.52%, Valid: 40.38%, Test: 40.47%
Epoch: 325, Loss: 1.3063, Train: 44.16%, Valid: 40.39%, Test: 40.80%
Epoch: 350, Loss: 1.2975, Train: 44.62%, Valid: 40.20%, Test: 40.62%
Epoch: 375, Loss: 1.3195, Train: 43.71%, Valid: 39.48%, Test: 39.44%
Epoch: 400, Loss: 1.2893, Train: 45.42%, Valid: 39.86%, Test: 39.94%
Epoch: 425, Loss: 1.2643, Train: 46.19%, Valid: 39.98%, Test: 40.16%
Epoch: 450, Loss: 1.2762, Train: 45.84%, Valid: 40.01%, Test: 40.11%
Epoch: 475, Loss: 1.2554, Train: 46.06%, Valid: 39.14%, Test: 39.33%
Epoch: 500, Loss: 1.2348, Train: 47.58%, Valid: 40.00%, Test: 40.38%
Epoch: 525, Loss: 1.2386, Train: 47.32%, Valid: 39.34%, Test: 39.74%
Epoch: 550, Loss: 1.2879, Train: 46.41%, Valid: 39.02%, Test: 39.42%
Epoch: 575, Loss: 1.2113, Train: 48.58%, Valid: 39.56%, Test: 39.99%
Epoch: 600, Loss: 1.2024, Train: 48.68%, Valid: 39.82%, Test: 40.16%
Epoch: 625, Loss: 1.2027, Train: 49.02%, Valid: 39.00%, Test: 39.40%
Epoch: 650, Loss: 1.2341, Train: 46.93%, Valid: 38.26%, Test: 38.77%
Epoch: 675, Loss: 1.1722, Train: 50.10%, Valid: 39.93%, Test: 40.17%
Epoch: 700, Loss: 1.3022, Train: 43.38%, Valid: 38.81%, Test: 39.16%
Epoch: 725, Loss: 1.2631, Train: 46.55%, Valid: 39.67%, Test: 39.75%
Epoch: 750, Loss: 1.2544, Train: 46.26%, Valid: 38.62%, Test: 39.36%
Epoch: 775, Loss: 1.1897, Train: 49.24%, Valid: 40.18%, Test: 40.48%
Epoch: 800, Loss: 1.1855, Train: 49.39%, Valid: 40.27%, Test: 40.57%
Epoch: 825, Loss: 1.1585, Train: 50.82%, Valid: 41.33%, Test: 41.29%
Epoch: 850, Loss: 1.3281, Train: 42.86%, Valid: 39.73%, Test: 40.13%
Epoch: 875, Loss: 1.2447, Train: 47.25%, Valid: 41.36%, Test: 41.38%
Epoch: 900, Loss: 1.1962, Train: 48.95%, Valid: 41.15%, Test: 41.20%
Epoch: 925, Loss: 1.2799, Train: 44.64%, Valid: 37.24%, Test: 37.41%
Epoch: 950, Loss: 1.1895, Train: 49.15%, Valid: 40.98%, Test: 41.29%
Epoch: 975, Loss: 1.1652, Train: 50.27%, Valid: 41.18%, Test: 41.33%
Run 01:
Highest Train: 51.43
Highest Valid: 42.06
  Final Train: 49.15
   Final Test: 41.94
All runs:
Highest Train: 51.43, nan
Highest Valid: 42.06, nan
  Final Train: 49.15, nan
   Final Test: 41.94, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6226, Train: 28.63%, Valid: 28.45%, Test: 28.71%
Epoch: 25, Loss: 1.5151, Train: 32.04%, Valid: 31.64%, Test: 32.53%
Epoch: 50, Loss: 1.4812, Train: 31.88%, Valid: 31.33%, Test: 32.36%
Epoch: 75, Loss: 1.4566, Train: 32.92%, Valid: 32.61%, Test: 33.61%
Epoch: 100, Loss: 1.4507, Train: 33.20%, Valid: 32.85%, Test: 33.79%
Epoch: 125, Loss: 1.4579, Train: 32.87%, Valid: 32.25%, Test: 33.19%
Epoch: 150, Loss: 1.4735, Train: 30.84%, Valid: 30.52%, Test: 30.94%
Epoch: 175, Loss: 1.5282, Train: 34.64%, Valid: 34.44%, Test: 34.60%
Epoch: 200, Loss: 1.4166, Train: 31.94%, Valid: 31.98%, Test: 32.54%
Epoch: 225, Loss: 1.4115, Train: 38.37%, Valid: 38.18%, Test: 38.59%
Epoch: 250, Loss: 1.4262, Train: 38.25%, Valid: 38.16%, Test: 38.42%
Epoch: 275, Loss: 1.3842, Train: 37.13%, Valid: 37.00%, Test: 37.43%
Epoch: 300, Loss: 1.3936, Train: 39.50%, Valid: 39.08%, Test: 39.34%
Epoch: 325, Loss: 1.4425, Train: 31.30%, Valid: 30.96%, Test: 31.49%
Epoch: 350, Loss: 1.3950, Train: 39.20%, Valid: 38.94%, Test: 39.56%
Epoch: 375, Loss: 1.6048, Train: 29.43%, Valid: 29.30%, Test: 29.36%
Epoch: 400, Loss: 1.4411, Train: 35.91%, Valid: 35.76%, Test: 36.03%
Epoch: 425, Loss: 1.4401, Train: 35.76%, Valid: 35.76%, Test: 36.01%
Epoch: 450, Loss: 1.4145, Train: 38.45%, Valid: 38.33%, Test: 38.84%
Epoch: 475, Loss: 1.4311, Train: 36.26%, Valid: 35.82%, Test: 36.52%
Epoch: 500, Loss: 1.3969, Train: 38.94%, Valid: 38.67%, Test: 39.18%
Epoch: 525, Loss: 1.4296, Train: 37.44%, Valid: 36.98%, Test: 37.40%
Epoch: 550, Loss: 1.4790, Train: 35.06%, Valid: 34.69%, Test: 35.16%
Epoch: 575, Loss: 1.4346, Train: 37.13%, Valid: 37.00%, Test: 37.54%
Epoch: 600, Loss: 1.4083, Train: 39.32%, Valid: 38.90%, Test: 39.31%
Epoch: 625, Loss: 1.3831, Train: 39.47%, Valid: 39.33%, Test: 39.97%
Epoch: 650, Loss: 1.3805, Train: 39.99%, Valid: 39.67%, Test: 40.11%
Epoch: 675, Loss: 1.3932, Train: 37.97%, Valid: 38.00%, Test: 38.24%
Epoch: 700, Loss: 1.3846, Train: 38.62%, Valid: 38.39%, Test: 38.78%
Epoch: 725, Loss: 1.3716, Train: 39.68%, Valid: 39.50%, Test: 39.97%
Epoch: 750, Loss: 1.3665, Train: 39.83%, Valid: 39.38%, Test: 39.88%
Epoch: 775, Loss: 1.5195, Train: 33.55%, Valid: 33.04%, Test: 33.41%
Epoch: 800, Loss: 1.4280, Train: 38.12%, Valid: 37.88%, Test: 38.18%
Epoch: 825, Loss: 1.3937, Train: 38.37%, Valid: 38.16%, Test: 38.50%
Epoch: 850, Loss: 1.4324, Train: 37.04%, Valid: 36.61%, Test: 36.89%
Epoch: 875, Loss: 1.3948, Train: 39.48%, Valid: 39.10%, Test: 39.40%
Epoch: 900, Loss: 1.3863, Train: 39.09%, Valid: 38.62%, Test: 38.90%
Epoch: 925, Loss: 1.3871, Train: 35.30%, Valid: 34.95%, Test: 35.71%
Epoch: 950, Loss: 1.3781, Train: 40.10%, Valid: 39.70%, Test: 40.38%
Epoch: 975, Loss: 1.4163, Train: 37.80%, Valid: 36.86%, Test: 37.67%
Run 01:
Highest Train: 40.45
Highest Valid: 39.97
  Final Train: 40.43
   Final Test: 40.41
All runs:
Highest Train: 40.45, nan
Highest Valid: 39.97, nan
  Final Train: 40.43, nan
   Final Test: 40.41, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6092, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4966, Train: 34.62%, Valid: 34.39%, Test: 34.59%
Epoch: 50, Loss: 1.4747, Train: 36.14%, Valid: 35.41%, Test: 35.78%
Epoch: 75, Loss: 1.4614, Train: 37.05%, Valid: 35.93%, Test: 36.47%
Epoch: 100, Loss: 1.4475, Train: 38.01%, Valid: 36.21%, Test: 36.69%
Epoch: 125, Loss: 1.4382, Train: 38.46%, Valid: 36.42%, Test: 36.88%
Epoch: 150, Loss: 1.4288, Train: 39.27%, Valid: 36.57%, Test: 36.87%
Epoch: 175, Loss: 1.4206, Train: 39.69%, Valid: 36.59%, Test: 36.86%
Epoch: 200, Loss: 1.4135, Train: 40.14%, Valid: 36.45%, Test: 36.67%
Epoch: 225, Loss: 1.4012, Train: 41.06%, Valid: 36.58%, Test: 36.96%
Epoch: 250, Loss: 1.3992, Train: 41.50%, Valid: 36.47%, Test: 37.03%
Epoch: 275, Loss: 1.3902, Train: 41.98%, Valid: 36.54%, Test: 36.96%
Epoch: 300, Loss: 1.3899, Train: 42.01%, Valid: 36.48%, Test: 36.97%
Epoch: 325, Loss: 1.3845, Train: 42.65%, Valid: 36.27%, Test: 36.93%
Epoch: 350, Loss: 1.3747, Train: 43.22%, Valid: 36.46%, Test: 36.96%
Epoch: 375, Loss: 1.3741, Train: 43.23%, Valid: 36.43%, Test: 36.91%
Epoch: 400, Loss: 1.3679, Train: 43.78%, Valid: 36.26%, Test: 36.78%
Epoch: 425, Loss: 1.3641, Train: 43.79%, Valid: 36.37%, Test: 37.01%
Epoch: 450, Loss: 1.3628, Train: 44.00%, Valid: 36.29%, Test: 36.91%
Epoch: 475, Loss: 1.3587, Train: 44.37%, Valid: 36.30%, Test: 36.80%
Epoch: 500, Loss: 1.3570, Train: 44.43%, Valid: 36.37%, Test: 36.63%
Epoch: 525, Loss: 1.3544, Train: 44.80%, Valid: 36.32%, Test: 36.81%
Epoch: 550, Loss: 1.3530, Train: 45.01%, Valid: 36.33%, Test: 36.71%
Epoch: 575, Loss: 1.3512, Train: 45.06%, Valid: 36.23%, Test: 36.68%
Epoch: 600, Loss: 1.3496, Train: 45.48%, Valid: 36.18%, Test: 36.54%
Epoch: 625, Loss: 1.3440, Train: 45.48%, Valid: 36.16%, Test: 36.57%
Epoch: 650, Loss: 1.3447, Train: 45.57%, Valid: 35.98%, Test: 36.61%
Epoch: 675, Loss: 1.3424, Train: 45.73%, Valid: 36.15%, Test: 36.42%
Epoch: 700, Loss: 1.3412, Train: 46.01%, Valid: 36.04%, Test: 36.59%
Epoch: 725, Loss: 1.3421, Train: 46.26%, Valid: 36.09%, Test: 36.33%
Epoch: 750, Loss: 1.3402, Train: 46.08%, Valid: 36.03%, Test: 36.56%
Epoch: 775, Loss: 1.3377, Train: 46.40%, Valid: 35.87%, Test: 36.54%
Epoch: 800, Loss: 1.3369, Train: 46.57%, Valid: 36.07%, Test: 36.43%
Epoch: 825, Loss: 1.3352, Train: 46.49%, Valid: 35.97%, Test: 36.26%
Epoch: 850, Loss: 1.3311, Train: 46.64%, Valid: 35.88%, Test: 36.31%
Epoch: 875, Loss: 1.3325, Train: 46.84%, Valid: 35.86%, Test: 36.28%
Epoch: 900, Loss: 1.3334, Train: 46.82%, Valid: 35.87%, Test: 36.30%
Epoch: 925, Loss: 1.3311, Train: 46.78%, Valid: 35.92%, Test: 36.29%
Epoch: 950, Loss: 1.3307, Train: 46.75%, Valid: 35.73%, Test: 36.03%
Epoch: 975, Loss: 1.3299, Train: 47.14%, Valid: 35.86%, Test: 36.16%
Run 01:
Highest Train: 47.38
Highest Valid: 36.71
  Final Train: 41.52
   Final Test: 36.96
All runs:
Highest Train: 47.38, nan
Highest Valid: 36.71, nan
  Final Train: 41.52, nan
   Final Test: 36.96, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6113, Train: 31.01%, Valid: 30.76%, Test: 31.54%
Epoch: 25, Loss: 1.4980, Train: 34.60%, Valid: 34.31%, Test: 34.60%
Epoch: 50, Loss: 1.4763, Train: 36.09%, Valid: 35.27%, Test: 35.80%
Epoch: 75, Loss: 1.4593, Train: 37.24%, Valid: 36.07%, Test: 36.33%
Epoch: 100, Loss: 1.4435, Train: 38.29%, Valid: 36.58%, Test: 36.83%
Epoch: 125, Loss: 1.4184, Train: 39.01%, Valid: 36.95%, Test: 37.13%
Epoch: 150, Loss: 1.4081, Train: 40.16%, Valid: 37.41%, Test: 37.65%
Epoch: 175, Loss: 1.4039, Train: 40.83%, Valid: 37.58%, Test: 37.85%
Epoch: 200, Loss: 1.3931, Train: 41.66%, Valid: 38.57%, Test: 38.89%
Epoch: 225, Loss: 1.3764, Train: 42.46%, Valid: 38.89%, Test: 39.48%
Epoch: 250, Loss: 1.3878, Train: 42.06%, Valid: 38.53%, Test: 39.23%
Epoch: 275, Loss: 1.3728, Train: 42.64%, Valid: 38.87%, Test: 39.49%
Epoch: 300, Loss: 1.3689, Train: 42.65%, Valid: 38.93%, Test: 39.42%
Epoch: 325, Loss: 1.3766, Train: 42.03%, Valid: 38.63%, Test: 38.97%
Epoch: 350, Loss: 1.3603, Train: 42.93%, Valid: 39.66%, Test: 39.76%
Epoch: 375, Loss: 1.3805, Train: 42.73%, Valid: 39.38%, Test: 39.53%
Epoch: 400, Loss: 1.3653, Train: 42.78%, Valid: 40.23%, Test: 40.61%
Epoch: 425, Loss: 1.3514, Train: 43.20%, Valid: 40.87%, Test: 41.03%
Epoch: 450, Loss: 1.4204, Train: 42.31%, Valid: 40.84%, Test: 40.83%
Epoch: 475, Loss: 1.5917, Train: 28.92%, Valid: 28.75%, Test: 29.00%
Epoch: 500, Loss: 1.4558, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 525, Loss: 1.4313, Train: 28.68%, Valid: 28.50%, Test: 28.81%
Epoch: 550, Loss: 1.5962, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 575, Loss: 1.4925, Train: 19.52%, Valid: 19.65%, Test: 19.48%
Epoch: 600, Loss: 1.4928, Train: 28.72%, Valid: 28.54%, Test: 28.81%
Epoch: 625, Loss: 1.5003, Train: 28.63%, Valid: 28.45%, Test: 28.75%
Epoch: 650, Loss: 1.5767, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 675, Loss: 1.5851, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 700, Loss: 1.5799, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 725, Loss: 1.5514, Train: 21.56%, Valid: 21.53%, Test: 21.32%
Epoch: 750, Loss: 1.4885, Train: 23.54%, Valid: 23.36%, Test: 23.42%
Epoch: 775, Loss: 1.5649, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 800, Loss: 1.5774, Train: 27.61%, Valid: 27.31%, Test: 27.70%
Epoch: 825, Loss: 1.5280, Train: 21.17%, Valid: 21.18%, Test: 21.03%
Epoch: 850, Loss: 1.4960, Train: 25.12%, Valid: 24.83%, Test: 25.23%
Epoch: 875, Loss: 1.4967, Train: 28.73%, Valid: 28.54%, Test: 28.82%
Epoch: 900, Loss: 1.5156, Train: 28.30%, Valid: 28.08%, Test: 28.39%
Epoch: 925, Loss: 1.5825, Train: 25.15%, Valid: 24.92%, Test: 25.14%
Epoch: 950, Loss: 1.5874, Train: 24.83%, Valid: 24.69%, Test: 24.86%
Epoch: 975, Loss: 1.5796, Train: 26.89%, Valid: 26.62%, Test: 26.95%
Run 01:
Highest Train: 43.89
Highest Valid: 41.59
  Final Train: 43.89
   Final Test: 41.96
All runs:
Highest Train: 43.89, nan
Highest Valid: 41.59, nan
  Final Train: 43.89, nan
   Final Test: 41.96, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6144, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.5254, Train: 32.89%, Valid: 32.81%, Test: 33.09%
Epoch: 50, Loss: 1.4806, Train: 35.31%, Valid: 34.96%, Test: 35.16%
Epoch: 75, Loss: 1.4629, Train: 38.62%, Valid: 38.19%, Test: 38.71%
Epoch: 100, Loss: 1.4133, Train: 39.49%, Valid: 38.89%, Test: 39.16%
Epoch: 125, Loss: 1.4181, Train: 38.99%, Valid: 38.31%, Test: 38.83%
Epoch: 150, Loss: 1.4007, Train: 40.70%, Valid: 39.76%, Test: 40.23%
Epoch: 175, Loss: 1.3890, Train: 40.22%, Valid: 39.29%, Test: 39.85%
Epoch: 200, Loss: 1.4614, Train: 37.40%, Valid: 36.79%, Test: 37.18%
Epoch: 225, Loss: 1.4305, Train: 37.62%, Valid: 36.99%, Test: 37.32%
Epoch: 250, Loss: 1.4217, Train: 39.02%, Valid: 38.19%, Test: 38.50%
Epoch: 275, Loss: 1.4170, Train: 39.17%, Valid: 38.32%, Test: 38.76%
Epoch: 300, Loss: 1.3983, Train: 39.79%, Valid: 38.89%, Test: 39.17%
Epoch: 325, Loss: 1.3983, Train: 39.54%, Valid: 38.73%, Test: 39.10%
Epoch: 350, Loss: 1.4012, Train: 35.99%, Valid: 35.36%, Test: 35.90%
Epoch: 375, Loss: 1.4080, Train: 38.95%, Valid: 38.35%, Test: 38.35%
Epoch: 400, Loss: 1.3795, Train: 41.10%, Valid: 40.27%, Test: 40.48%
Epoch: 425, Loss: 1.3718, Train: 41.04%, Valid: 40.15%, Test: 40.35%
Epoch: 450, Loss: 1.4032, Train: 40.82%, Valid: 39.86%, Test: 40.05%
Epoch: 475, Loss: 1.3766, Train: 41.30%, Valid: 40.42%, Test: 40.49%
Epoch: 500, Loss: 1.3715, Train: 41.53%, Valid: 40.47%, Test: 40.76%
Epoch: 525, Loss: 1.3828, Train: 41.96%, Valid: 40.74%, Test: 40.98%
Epoch: 550, Loss: 1.3687, Train: 41.86%, Valid: 40.96%, Test: 41.21%
Epoch: 575, Loss: 1.3687, Train: 42.26%, Valid: 41.22%, Test: 41.33%
Epoch: 600, Loss: 1.3914, Train: 39.13%, Valid: 38.47%, Test: 38.84%
Epoch: 625, Loss: 1.3706, Train: 40.57%, Valid: 39.51%, Test: 39.67%
Epoch: 650, Loss: 1.3625, Train: 42.55%, Valid: 41.36%, Test: 41.79%
Epoch: 675, Loss: 1.3705, Train: 42.18%, Valid: 41.08%, Test: 41.12%
Epoch: 700, Loss: 1.3620, Train: 41.94%, Valid: 40.83%, Test: 40.85%
Epoch: 725, Loss: 1.4080, Train: 41.53%, Valid: 40.34%, Test: 40.69%
Epoch: 750, Loss: 1.3605, Train: 42.23%, Valid: 40.83%, Test: 41.02%
Epoch: 775, Loss: 1.3875, Train: 41.67%, Valid: 40.33%, Test: 40.56%
Epoch: 800, Loss: 1.3714, Train: 41.92%, Valid: 40.75%, Test: 40.87%
Epoch: 825, Loss: 1.3576, Train: 42.83%, Valid: 41.69%, Test: 41.60%
Epoch: 850, Loss: 1.3866, Train: 41.84%, Valid: 40.51%, Test: 40.91%
Epoch: 875, Loss: 1.3627, Train: 41.63%, Valid: 40.43%, Test: 40.42%
Epoch: 900, Loss: 1.3729, Train: 42.00%, Valid: 40.80%, Test: 41.05%
Epoch: 925, Loss: 1.3613, Train: 41.83%, Valid: 40.54%, Test: 40.77%
Epoch: 950, Loss: 1.3611, Train: 42.32%, Valid: 40.77%, Test: 41.01%
Epoch: 975, Loss: 1.3496, Train: 38.63%, Valid: 37.47%, Test: 37.61%
Run 01:
Highest Train: 43.38
Highest Valid: 41.98
  Final Train: 43.38
   Final Test: 42.03
All runs:
Highest Train: 43.38, nan
Highest Valid: 41.98, nan
  Final Train: 43.38, nan
   Final Test: 42.03, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6097, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.5007, Train: 34.34%, Valid: 34.11%, Test: 34.37%
Epoch: 50, Loss: 1.4774, Train: 36.04%, Valid: 35.28%, Test: 35.62%
Epoch: 75, Loss: 1.4625, Train: 37.06%, Valid: 36.13%, Test: 36.20%
Epoch: 100, Loss: 1.4495, Train: 37.99%, Valid: 36.33%, Test: 36.66%
Epoch: 125, Loss: 1.4403, Train: 38.63%, Valid: 36.41%, Test: 36.74%
Epoch: 150, Loss: 1.4310, Train: 39.22%, Valid: 36.48%, Test: 36.85%
Epoch: 175, Loss: 1.4236, Train: 39.55%, Valid: 36.63%, Test: 36.88%
Epoch: 200, Loss: 1.4134, Train: 40.52%, Valid: 36.62%, Test: 36.89%
Epoch: 225, Loss: 1.4074, Train: 40.81%, Valid: 36.51%, Test: 36.86%
Epoch: 250, Loss: 1.3982, Train: 41.47%, Valid: 36.62%, Test: 36.90%
Epoch: 275, Loss: 1.3966, Train: 41.94%, Valid: 36.52%, Test: 36.85%
Epoch: 300, Loss: 1.3899, Train: 42.01%, Valid: 36.34%, Test: 36.70%
Epoch: 325, Loss: 1.3817, Train: 42.76%, Valid: 36.52%, Test: 36.90%
Epoch: 350, Loss: 1.3784, Train: 43.11%, Valid: 36.55%, Test: 36.85%
Epoch: 375, Loss: 1.3740, Train: 43.35%, Valid: 36.52%, Test: 36.79%
Epoch: 400, Loss: 1.3707, Train: 43.71%, Valid: 36.40%, Test: 36.87%
Epoch: 425, Loss: 1.3721, Train: 43.47%, Valid: 36.38%, Test: 36.79%
Epoch: 450, Loss: 1.3616, Train: 44.04%, Valid: 36.38%, Test: 36.83%
Epoch: 475, Loss: 1.3600, Train: 44.37%, Valid: 36.32%, Test: 36.78%
Epoch: 500, Loss: 1.3583, Train: 44.79%, Valid: 36.24%, Test: 36.73%
Epoch: 525, Loss: 1.3571, Train: 44.54%, Valid: 36.34%, Test: 36.97%
Epoch: 550, Loss: 1.3518, Train: 45.04%, Valid: 36.18%, Test: 36.75%
Epoch: 575, Loss: 1.3490, Train: 45.04%, Valid: 36.31%, Test: 36.78%
Epoch: 600, Loss: 1.3497, Train: 45.27%, Valid: 36.11%, Test: 36.70%
Epoch: 625, Loss: 1.3507, Train: 45.52%, Valid: 36.01%, Test: 36.58%
Epoch: 650, Loss: 1.3436, Train: 45.61%, Valid: 36.21%, Test: 36.81%
Epoch: 675, Loss: 1.3445, Train: 45.59%, Valid: 36.27%, Test: 36.75%
Epoch: 700, Loss: 1.3447, Train: 45.59%, Valid: 36.19%, Test: 36.48%
Epoch: 725, Loss: 1.3406, Train: 45.83%, Valid: 36.12%, Test: 36.51%
Epoch: 750, Loss: 1.3425, Train: 46.15%, Valid: 36.11%, Test: 36.43%
Epoch: 775, Loss: 1.3399, Train: 46.03%, Valid: 36.11%, Test: 36.59%
Epoch: 800, Loss: 1.3383, Train: 45.92%, Valid: 36.10%, Test: 36.57%
Epoch: 825, Loss: 1.3369, Train: 46.27%, Valid: 36.27%, Test: 36.48%
Epoch: 850, Loss: 1.3370, Train: 46.30%, Valid: 36.02%, Test: 36.38%
Epoch: 875, Loss: 1.3389, Train: 46.51%, Valid: 36.06%, Test: 36.54%
Epoch: 900, Loss: 1.3311, Train: 46.57%, Valid: 36.10%, Test: 36.46%
Epoch: 925, Loss: 1.3327, Train: 46.51%, Valid: 36.05%, Test: 36.43%
Epoch: 950, Loss: 1.3342, Train: 46.49%, Valid: 36.06%, Test: 36.33%
Epoch: 975, Loss: 1.3302, Train: 46.70%, Valid: 36.08%, Test: 36.53%
Run 01:
Highest Train: 47.05
Highest Valid: 36.79
  Final Train: 42.26
   Final Test: 36.99
All runs:
Highest Train: 47.05, nan
Highest Valid: 36.79, nan
  Final Train: 42.26, nan
   Final Test: 36.99, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6109, Train: 28.68%, Valid: 28.51%, Test: 28.81%
Epoch: 25, Loss: 1.5015, Train: 34.47%, Valid: 34.28%, Test: 34.51%
Epoch: 50, Loss: 1.4803, Train: 35.78%, Valid: 35.11%, Test: 35.60%
Epoch: 75, Loss: 1.4629, Train: 36.94%, Valid: 35.85%, Test: 36.30%
Epoch: 100, Loss: 1.4510, Train: 37.98%, Valid: 36.39%, Test: 36.58%
Epoch: 125, Loss: 1.4387, Train: 38.83%, Valid: 36.58%, Test: 36.84%
Epoch: 150, Loss: 1.4281, Train: 39.55%, Valid: 36.89%, Test: 37.16%
Epoch: 175, Loss: 1.4128, Train: 40.29%, Valid: 37.09%, Test: 37.32%
Epoch: 200, Loss: 1.4086, Train: 41.08%, Valid: 37.20%, Test: 37.64%
Epoch: 225, Loss: 1.3877, Train: 41.81%, Valid: 37.86%, Test: 38.32%
Epoch: 250, Loss: 1.3822, Train: 42.14%, Valid: 38.32%, Test: 38.67%
Epoch: 275, Loss: 1.3736, Train: 43.08%, Valid: 38.74%, Test: 39.14%
Epoch: 300, Loss: 1.3689, Train: 43.48%, Valid: 39.03%, Test: 39.37%
Epoch: 325, Loss: 1.3661, Train: 44.34%, Valid: 39.66%, Test: 40.03%
Epoch: 350, Loss: 1.3665, Train: 43.81%, Valid: 39.16%, Test: 39.59%
Epoch: 375, Loss: 1.3744, Train: 43.41%, Valid: 39.13%, Test: 39.44%
Epoch: 400, Loss: 1.4139, Train: 42.82%, Valid: 38.62%, Test: 39.18%
Epoch: 425, Loss: 1.3680, Train: 44.31%, Valid: 39.69%, Test: 40.21%
Epoch: 450, Loss: 1.3740, Train: 43.32%, Valid: 39.21%, Test: 39.84%
Epoch: 475, Loss: 1.3764, Train: 43.35%, Valid: 39.24%, Test: 39.84%
Epoch: 500, Loss: 1.4221, Train: 43.76%, Valid: 39.86%, Test: 40.30%
Epoch: 525, Loss: 1.3641, Train: 43.74%, Valid: 40.03%, Test: 40.39%
Epoch: 550, Loss: 1.3665, Train: 43.76%, Valid: 39.78%, Test: 40.11%
Epoch: 575, Loss: 1.3647, Train: 42.04%, Valid: 39.06%, Test: 39.40%
Epoch: 600, Loss: 1.4037, Train: 41.00%, Valid: 39.51%, Test: 40.06%
Epoch: 625, Loss: 1.3801, Train: 41.21%, Valid: 39.73%, Test: 39.99%
Epoch: 650, Loss: 1.3711, Train: 42.69%, Valid: 41.02%, Test: 41.54%
Epoch: 675, Loss: 1.3715, Train: 42.10%, Valid: 40.89%, Test: 40.71%
Epoch: 700, Loss: 1.3929, Train: 41.04%, Valid: 39.74%, Test: 40.10%
Epoch: 725, Loss: 1.3645, Train: 35.63%, Valid: 34.68%, Test: 34.73%
Epoch: 750, Loss: 1.3668, Train: 40.95%, Valid: 39.88%, Test: 39.96%
Epoch: 775, Loss: 1.4221, Train: 39.24%, Valid: 38.81%, Test: 38.99%
Epoch: 800, Loss: 1.5884, Train: 33.95%, Valid: 33.72%, Test: 34.49%
Epoch: 825, Loss: 1.4417, Train: 37.92%, Valid: 37.55%, Test: 37.84%
Epoch: 850, Loss: 1.7602, Train: 35.79%, Valid: 35.59%, Test: 35.65%
Epoch: 875, Loss: 1.5731, Train: 28.02%, Valid: 27.81%, Test: 28.08%
Epoch: 900, Loss: 1.5488, Train: 32.73%, Valid: 32.51%, Test: 32.30%
Epoch: 925, Loss: 1.5397, Train: 35.12%, Valid: 34.57%, Test: 34.73%
Epoch: 950, Loss: 1.4954, Train: 27.05%, Valid: 26.65%, Test: 26.62%
Epoch: 975, Loss: 1.5063, Train: 25.26%, Valid: 25.19%, Test: 25.17%
Run 01:
Highest Train: 44.43
Highest Valid: 41.39
  Final Train: 42.68
   Final Test: 41.58
All runs:
Highest Train: 44.43, nan
Highest Valid: 41.39, nan
  Final Train: 42.68, nan
   Final Test: 41.58, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6482, Train: 28.73%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.5251, Train: 32.10%, Valid: 32.00%, Test: 32.23%
Epoch: 50, Loss: 1.4820, Train: 32.77%, Valid: 32.51%, Test: 32.73%
Epoch: 75, Loss: 1.4380, Train: 37.33%, Valid: 36.88%, Test: 37.03%
Epoch: 100, Loss: 1.4244, Train: 37.81%, Valid: 37.28%, Test: 37.58%
Epoch: 125, Loss: 1.4204, Train: 39.12%, Valid: 38.44%, Test: 38.78%
Epoch: 150, Loss: 1.4094, Train: 39.45%, Valid: 38.61%, Test: 39.18%
Epoch: 175, Loss: 1.4042, Train: 39.70%, Valid: 38.93%, Test: 39.33%
Epoch: 200, Loss: 1.4161, Train: 40.24%, Valid: 39.12%, Test: 39.75%
Epoch: 225, Loss: 1.3813, Train: 40.29%, Valid: 39.37%, Test: 39.67%
Epoch: 250, Loss: 1.3767, Train: 41.23%, Valid: 40.13%, Test: 40.57%
Epoch: 275, Loss: 1.3944, Train: 40.41%, Valid: 39.57%, Test: 39.86%
Epoch: 300, Loss: 1.3998, Train: 39.86%, Valid: 38.90%, Test: 39.28%
Epoch: 325, Loss: 1.3820, Train: 40.34%, Valid: 39.38%, Test: 39.71%
Epoch: 350, Loss: 1.3737, Train: 41.30%, Valid: 40.07%, Test: 40.60%
Epoch: 375, Loss: 1.3793, Train: 41.39%, Valid: 40.35%, Test: 40.99%
Epoch: 400, Loss: 1.3810, Train: 40.52%, Valid: 39.72%, Test: 39.94%
Epoch: 425, Loss: 1.3869, Train: 41.39%, Valid: 40.45%, Test: 40.92%
Epoch: 450, Loss: 1.3913, Train: 40.37%, Valid: 39.62%, Test: 39.99%
Epoch: 475, Loss: 1.3775, Train: 40.76%, Valid: 39.93%, Test: 40.39%
Epoch: 500, Loss: 1.3678, Train: 41.43%, Valid: 40.66%, Test: 40.88%
Epoch: 525, Loss: 1.3877, Train: 41.36%, Valid: 40.35%, Test: 40.80%
Epoch: 550, Loss: 1.3820, Train: 40.04%, Valid: 39.25%, Test: 39.54%
Epoch: 575, Loss: 1.3861, Train: 40.79%, Valid: 39.88%, Test: 40.51%
Epoch: 600, Loss: 1.4159, Train: 37.40%, Valid: 36.44%, Test: 37.27%
Epoch: 625, Loss: 1.3796, Train: 41.90%, Valid: 41.36%, Test: 41.50%
Epoch: 650, Loss: 1.3819, Train: 39.79%, Valid: 39.16%, Test: 39.35%
Epoch: 675, Loss: 1.3637, Train: 37.75%, Valid: 37.21%, Test: 37.60%
Epoch: 700, Loss: 1.3665, Train: 42.09%, Valid: 41.02%, Test: 41.58%
Epoch: 725, Loss: 1.3657, Train: 41.88%, Valid: 41.02%, Test: 41.66%
Epoch: 750, Loss: 1.3951, Train: 41.22%, Valid: 40.56%, Test: 41.07%
Epoch: 775, Loss: 1.3718, Train: 41.75%, Valid: 40.95%, Test: 41.33%
Epoch: 800, Loss: 1.4317, Train: 40.93%, Valid: 40.19%, Test: 40.56%
Epoch: 825, Loss: 1.3581, Train: 41.79%, Valid: 40.92%, Test: 41.47%
Epoch: 850, Loss: 1.3562, Train: 42.35%, Valid: 41.51%, Test: 41.91%
Epoch: 875, Loss: 1.4219, Train: 38.07%, Valid: 37.46%, Test: 37.85%
Epoch: 900, Loss: 1.3823, Train: 40.96%, Valid: 40.20%, Test: 40.59%
Epoch: 925, Loss: 1.3813, Train: 41.46%, Valid: 40.69%, Test: 40.98%
Epoch: 950, Loss: 1.3786, Train: 41.55%, Valid: 40.73%, Test: 41.14%
Epoch: 975, Loss: 1.3703, Train: 36.46%, Valid: 35.49%, Test: 35.86%
Run 01:
Highest Train: 42.87
Highest Valid: 42.23
  Final Train: 42.84
   Final Test: 42.50
All runs:
Highest Train: 42.87, nan
Highest Valid: 42.23, nan
  Final Train: 42.84, nan
   Final Test: 42.50, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6125, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4922, Train: 34.43%, Valid: 34.29%, Test: 34.48%
Epoch: 50, Loss: 1.4691, Train: 35.95%, Valid: 35.24%, Test: 35.52%
Epoch: 75, Loss: 1.4479, Train: 37.03%, Valid: 35.84%, Test: 36.05%
Epoch: 100, Loss: 1.4376, Train: 37.66%, Valid: 36.02%, Test: 36.28%
Epoch: 125, Loss: 1.4202, Train: 38.38%, Valid: 35.96%, Test: 36.30%
Epoch: 150, Loss: 1.4091, Train: 39.02%, Valid: 35.63%, Test: 35.93%
Epoch: 175, Loss: 1.3950, Train: 39.83%, Valid: 35.54%, Test: 35.91%
Epoch: 200, Loss: 1.3836, Train: 40.43%, Valid: 35.63%, Test: 35.89%
Epoch: 225, Loss: 1.3721, Train: 41.19%, Valid: 35.33%, Test: 35.60%
Epoch: 250, Loss: 1.3562, Train: 41.78%, Valid: 35.12%, Test: 35.50%
Epoch: 275, Loss: 1.3465, Train: 42.37%, Valid: 35.34%, Test: 35.61%
Epoch: 300, Loss: 1.3375, Train: 42.94%, Valid: 34.77%, Test: 35.02%
Epoch: 325, Loss: 1.3305, Train: 43.46%, Valid: 34.52%, Test: 34.73%
Epoch: 350, Loss: 1.3228, Train: 43.47%, Valid: 34.46%, Test: 34.61%
Epoch: 375, Loss: 1.3155, Train: 43.82%, Valid: 34.16%, Test: 34.58%
Epoch: 400, Loss: 1.3012, Train: 44.62%, Valid: 34.13%, Test: 34.38%
Epoch: 425, Loss: 1.3013, Train: 44.62%, Valid: 34.08%, Test: 34.57%
Epoch: 450, Loss: 1.2919, Train: 45.12%, Valid: 33.67%, Test: 34.00%
Epoch: 475, Loss: 1.2803, Train: 45.97%, Valid: 33.66%, Test: 33.99%
Epoch: 500, Loss: 1.2794, Train: 46.00%, Valid: 33.17%, Test: 33.79%
Epoch: 525, Loss: 1.2797, Train: 45.86%, Valid: 34.32%, Test: 34.60%
Epoch: 550, Loss: 1.2730, Train: 46.51%, Valid: 33.13%, Test: 33.69%
Epoch: 575, Loss: 1.2567, Train: 47.12%, Valid: 33.32%, Test: 33.91%
Epoch: 600, Loss: 1.2518, Train: 47.27%, Valid: 33.60%, Test: 33.99%
Epoch: 625, Loss: 1.2506, Train: 47.43%, Valid: 32.60%, Test: 33.15%
Epoch: 650, Loss: 1.2410, Train: 48.05%, Valid: 32.92%, Test: 33.48%
Epoch: 675, Loss: 1.2418, Train: 47.80%, Valid: 33.44%, Test: 33.83%
Epoch: 700, Loss: 1.2346, Train: 47.79%, Valid: 32.57%, Test: 33.32%
Epoch: 725, Loss: 1.2245, Train: 48.50%, Valid: 32.87%, Test: 33.29%
Epoch: 750, Loss: 1.2240, Train: 48.97%, Valid: 33.25%, Test: 33.64%
Epoch: 775, Loss: 1.2203, Train: 49.03%, Valid: 32.69%, Test: 32.92%
Epoch: 800, Loss: 1.2196, Train: 48.82%, Valid: 32.64%, Test: 33.17%
Epoch: 825, Loss: 1.2130, Train: 48.79%, Valid: 33.05%, Test: 33.50%
Epoch: 850, Loss: 1.2085, Train: 49.59%, Valid: 32.46%, Test: 32.91%
Epoch: 875, Loss: 1.2065, Train: 49.66%, Valid: 33.03%, Test: 33.33%
Epoch: 900, Loss: 1.2041, Train: 49.26%, Valid: 32.28%, Test: 32.60%
Epoch: 925, Loss: 1.1941, Train: 50.06%, Valid: 32.40%, Test: 32.89%
Epoch: 950, Loss: 1.1971, Train: 49.96%, Valid: 32.28%, Test: 32.56%
Epoch: 975, Loss: 1.1863, Train: 50.26%, Valid: 32.39%, Test: 32.65%
Run 01:
Highest Train: 50.53
Highest Valid: 36.09
  Final Train: 38.17
   Final Test: 36.38
All runs:
Highest Train: 50.53, nan
Highest Valid: 36.09, nan
  Final Train: 38.17, nan
   Final Test: 36.38, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6116, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4822, Train: 34.96%, Valid: 34.72%, Test: 34.88%
Epoch: 50, Loss: 1.4377, Train: 37.19%, Valid: 36.34%, Test: 36.75%
Epoch: 75, Loss: 1.3911, Train: 40.15%, Valid: 39.14%, Test: 39.30%
Epoch: 100, Loss: 1.3578, Train: 41.27%, Valid: 39.94%, Test: 40.01%
Epoch: 125, Loss: 1.3443, Train: 41.95%, Valid: 40.49%, Test: 40.92%
Epoch: 150, Loss: 1.3357, Train: 41.96%, Valid: 40.14%, Test: 40.74%
Epoch: 175, Loss: 1.3135, Train: 43.21%, Valid: 41.14%, Test: 41.75%
Epoch: 200, Loss: 1.2935, Train: 43.97%, Valid: 41.41%, Test: 41.86%
Epoch: 225, Loss: 1.3299, Train: 42.15%, Valid: 40.29%, Test: 40.74%
Epoch: 250, Loss: 1.2981, Train: 44.00%, Valid: 41.64%, Test: 42.18%
Epoch: 275, Loss: 1.2808, Train: 44.76%, Valid: 41.89%, Test: 42.48%
Epoch: 300, Loss: 1.3253, Train: 42.70%, Valid: 40.13%, Test: 40.68%
Epoch: 325, Loss: 1.2767, Train: 44.91%, Valid: 41.86%, Test: 42.50%
Epoch: 350, Loss: 1.2616, Train: 45.65%, Valid: 42.27%, Test: 43.00%
Epoch: 375, Loss: 1.2788, Train: 44.19%, Valid: 40.78%, Test: 41.19%
Epoch: 400, Loss: 1.2619, Train: 45.25%, Valid: 41.94%, Test: 42.61%
Epoch: 425, Loss: 1.2410, Train: 46.50%, Valid: 42.59%, Test: 43.28%
Epoch: 450, Loss: 1.2332, Train: 46.88%, Valid: 42.82%, Test: 43.11%
Epoch: 475, Loss: 1.2583, Train: 45.09%, Valid: 41.23%, Test: 41.54%
Epoch: 500, Loss: 1.2255, Train: 47.13%, Valid: 42.71%, Test: 43.22%
Epoch: 525, Loss: 1.2186, Train: 47.33%, Valid: 42.70%, Test: 43.03%
Epoch: 550, Loss: 1.2109, Train: 47.68%, Valid: 42.99%, Test: 43.39%
Epoch: 575, Loss: 1.2097, Train: 47.93%, Valid: 42.89%, Test: 43.30%
Epoch: 600, Loss: 1.2003, Train: 48.20%, Valid: 42.87%, Test: 43.15%
Epoch: 625, Loss: 1.1917, Train: 48.25%, Valid: 42.66%, Test: 42.96%
Epoch: 650, Loss: 1.2144, Train: 46.90%, Valid: 41.51%, Test: 41.64%
Epoch: 675, Loss: 1.2277, Train: 47.48%, Valid: 42.87%, Test: 43.08%
Epoch: 700, Loss: 1.1988, Train: 48.43%, Valid: 43.08%, Test: 43.34%
Epoch: 725, Loss: 1.1853, Train: 48.96%, Valid: 43.19%, Test: 43.31%
Epoch: 750, Loss: 1.1847, Train: 49.29%, Valid: 42.91%, Test: 43.21%
Epoch: 775, Loss: 1.1746, Train: 49.02%, Valid: 43.11%, Test: 43.19%
Epoch: 800, Loss: 1.1744, Train: 49.65%, Valid: 43.33%, Test: 43.28%
Epoch: 825, Loss: 1.1680, Train: 49.04%, Valid: 42.68%, Test: 42.68%
Epoch: 850, Loss: 1.1580, Train: 49.97%, Valid: 42.99%, Test: 43.28%
Epoch: 875, Loss: 1.1675, Train: 49.76%, Valid: 42.78%, Test: 42.84%
Epoch: 900, Loss: 1.1489, Train: 50.50%, Valid: 43.28%, Test: 43.23%
Epoch: 925, Loss: 1.1495, Train: 50.73%, Valid: 43.29%, Test: 43.39%
Epoch: 950, Loss: 1.1432, Train: 51.10%, Valid: 42.94%, Test: 43.30%
Epoch: 975, Loss: 1.2032, Train: 48.61%, Valid: 42.55%, Test: 42.59%
Run 01:
Highest Train: 51.28
Highest Valid: 43.53
  Final Train: 50.37
   Final Test: 43.39
All runs:
Highest Train: 51.28, nan
Highest Valid: 43.53, nan
  Final Train: 50.37, nan
   Final Test: 43.39, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.7202, Train: 24.23%, Valid: 24.24%, Test: 23.96%
Epoch: 25, Loss: 1.4465, Train: 35.42%, Valid: 34.97%, Test: 35.29%
Epoch: 50, Loss: 1.4190, Train: 38.27%, Valid: 37.80%, Test: 38.18%
Epoch: 75, Loss: 1.4620, Train: 35.21%, Valid: 34.76%, Test: 35.24%
Epoch: 100, Loss: 1.4270, Train: 37.40%, Valid: 36.99%, Test: 37.61%
Epoch: 125, Loss: 1.4013, Train: 35.49%, Valid: 34.94%, Test: 35.46%
Epoch: 150, Loss: 1.3987, Train: 38.44%, Valid: 37.86%, Test: 38.46%
Epoch: 175, Loss: 1.3711, Train: 40.77%, Valid: 40.37%, Test: 40.69%
Epoch: 200, Loss: 1.3589, Train: 40.71%, Valid: 40.21%, Test: 40.51%
Epoch: 225, Loss: 1.3444, Train: 41.49%, Valid: 40.91%, Test: 41.52%
Epoch: 250, Loss: 1.3386, Train: 41.83%, Valid: 41.34%, Test: 41.79%
Epoch: 275, Loss: 1.4726, Train: 40.47%, Valid: 39.71%, Test: 40.01%
Epoch: 300, Loss: 1.3502, Train: 41.35%, Valid: 40.36%, Test: 40.72%
Epoch: 325, Loss: 1.3364, Train: 41.89%, Valid: 41.29%, Test: 41.62%
Epoch: 350, Loss: 1.3268, Train: 42.40%, Valid: 41.64%, Test: 42.11%
Epoch: 375, Loss: 1.3233, Train: 42.68%, Valid: 41.86%, Test: 42.31%
Epoch: 400, Loss: 1.3164, Train: 42.78%, Valid: 41.94%, Test: 42.38%
Epoch: 425, Loss: 1.3197, Train: 42.62%, Valid: 41.57%, Test: 42.03%
Epoch: 450, Loss: 1.3114, Train: 43.08%, Valid: 42.20%, Test: 42.65%
Epoch: 475, Loss: 1.3038, Train: 43.41%, Valid: 42.55%, Test: 43.03%
Epoch: 500, Loss: 1.3119, Train: 43.21%, Valid: 42.31%, Test: 42.74%
Epoch: 525, Loss: 1.2989, Train: 43.66%, Valid: 42.67%, Test: 43.19%
Epoch: 550, Loss: 1.2986, Train: 43.82%, Valid: 42.91%, Test: 43.39%
Epoch: 575, Loss: 1.2873, Train: 44.06%, Valid: 42.86%, Test: 43.45%
Epoch: 600, Loss: 1.3578, Train: 40.64%, Valid: 39.58%, Test: 40.08%
Epoch: 625, Loss: 1.3106, Train: 42.51%, Valid: 41.54%, Test: 41.80%
Epoch: 650, Loss: 1.2966, Train: 43.71%, Valid: 42.66%, Test: 43.12%
Epoch: 675, Loss: 1.3007, Train: 43.32%, Valid: 42.10%, Test: 42.65%
Epoch: 700, Loss: 1.2873, Train: 44.16%, Valid: 42.77%, Test: 43.37%
Epoch: 725, Loss: 1.3093, Train: 43.42%, Valid: 42.00%, Test: 42.40%
Epoch: 750, Loss: 1.2908, Train: 43.88%, Valid: 42.54%, Test: 43.12%
Epoch: 775, Loss: 1.2804, Train: 44.38%, Valid: 42.93%, Test: 43.47%
Epoch: 800, Loss: 1.2767, Train: 44.65%, Valid: 43.25%, Test: 43.74%
Epoch: 825, Loss: 1.2714, Train: 44.56%, Valid: 43.12%, Test: 43.69%
Epoch: 850, Loss: 1.2780, Train: 44.59%, Valid: 42.95%, Test: 43.65%
Epoch: 875, Loss: 1.2635, Train: 45.15%, Valid: 43.57%, Test: 44.23%
Epoch: 900, Loss: 1.2575, Train: 45.15%, Valid: 43.59%, Test: 44.18%
Epoch: 925, Loss: 1.3010, Train: 44.07%, Valid: 42.89%, Test: 43.15%
Epoch: 950, Loss: 1.2714, Train: 44.56%, Valid: 43.00%, Test: 43.40%
Epoch: 975, Loss: 1.2589, Train: 45.43%, Valid: 43.70%, Test: 44.19%
Run 01:
Highest Train: 45.77
Highest Valid: 43.93
  Final Train: 45.71
   Final Test: 44.53
All runs:
Highest Train: 45.77, nan
Highest Valid: 43.93, nan
  Final Train: 45.71, nan
   Final Test: 44.53, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6074, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4928, Train: 34.66%, Valid: 34.33%, Test: 34.64%
Epoch: 50, Loss: 1.4703, Train: 35.75%, Valid: 35.13%, Test: 35.52%
Epoch: 75, Loss: 1.4566, Train: 36.55%, Valid: 35.76%, Test: 36.06%
Epoch: 100, Loss: 1.4456, Train: 37.13%, Valid: 35.97%, Test: 36.12%
Epoch: 125, Loss: 1.4331, Train: 37.75%, Valid: 36.14%, Test: 36.45%
Epoch: 150, Loss: 1.4256, Train: 38.15%, Valid: 35.89%, Test: 36.47%
Epoch: 175, Loss: 1.4116, Train: 38.86%, Valid: 36.03%, Test: 36.43%
Epoch: 200, Loss: 1.4015, Train: 39.29%, Valid: 36.02%, Test: 36.44%
Epoch: 225, Loss: 1.3887, Train: 39.84%, Valid: 35.63%, Test: 36.03%
Epoch: 250, Loss: 1.3801, Train: 40.20%, Valid: 35.58%, Test: 35.85%
Epoch: 275, Loss: 1.3716, Train: 40.75%, Valid: 35.33%, Test: 35.76%
Epoch: 300, Loss: 1.3664, Train: 41.05%, Valid: 35.24%, Test: 35.54%
Epoch: 325, Loss: 1.3587, Train: 41.37%, Valid: 35.41%, Test: 35.89%
Epoch: 350, Loss: 1.3499, Train: 41.76%, Valid: 34.88%, Test: 35.06%
Epoch: 375, Loss: 1.3445, Train: 41.95%, Valid: 34.84%, Test: 34.87%
Epoch: 400, Loss: 1.3366, Train: 42.63%, Valid: 35.13%, Test: 35.31%
Epoch: 425, Loss: 1.3345, Train: 42.43%, Valid: 35.19%, Test: 35.38%
Epoch: 450, Loss: 1.3285, Train: 43.07%, Valid: 34.97%, Test: 35.08%
Epoch: 475, Loss: 1.3246, Train: 42.87%, Valid: 34.28%, Test: 34.10%
Epoch: 500, Loss: 1.3264, Train: 43.08%, Valid: 34.65%, Test: 34.98%
Epoch: 525, Loss: 1.3101, Train: 44.04%, Valid: 34.55%, Test: 34.48%
Epoch: 550, Loss: 1.3112, Train: 43.86%, Valid: 34.66%, Test: 34.79%
Epoch: 575, Loss: 1.3060, Train: 44.22%, Valid: 34.51%, Test: 34.56%
Epoch: 600, Loss: 1.2961, Train: 44.49%, Valid: 34.07%, Test: 33.86%
Epoch: 625, Loss: 1.2914, Train: 44.62%, Valid: 33.86%, Test: 33.70%
Epoch: 650, Loss: 1.2885, Train: 44.94%, Valid: 34.22%, Test: 34.06%
Epoch: 675, Loss: 1.2998, Train: 44.51%, Valid: 33.65%, Test: 33.47%
Epoch: 700, Loss: 1.2810, Train: 45.16%, Valid: 33.39%, Test: 33.29%
Epoch: 725, Loss: 1.2807, Train: 45.25%, Valid: 34.16%, Test: 34.19%
Epoch: 750, Loss: 1.2869, Train: 44.80%, Valid: 32.50%, Test: 32.35%
Epoch: 775, Loss: 1.3095, Train: 45.17%, Valid: 34.29%, Test: 34.26%
Epoch: 800, Loss: 1.2643, Train: 46.09%, Valid: 33.76%, Test: 33.85%
Epoch: 825, Loss: 1.2616, Train: 46.47%, Valid: 33.85%, Test: 33.90%
Epoch: 850, Loss: 1.2564, Train: 46.37%, Valid: 33.75%, Test: 33.62%
Epoch: 875, Loss: 1.2662, Train: 46.24%, Valid: 33.76%, Test: 33.84%
Epoch: 900, Loss: 1.2587, Train: 46.31%, Valid: 33.40%, Test: 33.48%
Epoch: 925, Loss: 1.2684, Train: 46.09%, Valid: 32.47%, Test: 32.12%
Epoch: 950, Loss: 1.2492, Train: 46.96%, Valid: 32.88%, Test: 32.79%
Epoch: 975, Loss: 1.2489, Train: 46.97%, Valid: 32.66%, Test: 32.44%
Run 01:
Highest Train: 47.41
Highest Valid: 36.23
  Final Train: 37.32
   Final Test: 36.27
All runs:
Highest Train: 47.41, nan
Highest Valid: 36.23, nan
  Final Train: 37.32, nan
   Final Test: 36.27, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6102, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4738, Train: 35.36%, Valid: 35.02%, Test: 35.24%
Epoch: 50, Loss: 1.4196, Train: 38.40%, Valid: 37.54%, Test: 37.94%
Epoch: 75, Loss: 1.3963, Train: 39.64%, Valid: 38.17%, Test: 38.52%
Epoch: 100, Loss: 1.3654, Train: 40.85%, Valid: 39.23%, Test: 39.50%
Epoch: 125, Loss: 1.3645, Train: 40.61%, Valid: 38.57%, Test: 39.02%
Epoch: 150, Loss: 1.3309, Train: 42.27%, Valid: 40.28%, Test: 40.62%
Epoch: 175, Loss: 1.3145, Train: 42.91%, Valid: 40.34%, Test: 41.10%
Epoch: 200, Loss: 1.3013, Train: 43.73%, Valid: 41.26%, Test: 41.66%
Epoch: 225, Loss: 1.2804, Train: 44.71%, Valid: 41.79%, Test: 42.19%
Epoch: 250, Loss: 1.2939, Train: 44.01%, Valid: 41.25%, Test: 41.91%
Epoch: 275, Loss: 1.2692, Train: 45.10%, Valid: 41.76%, Test: 42.28%
Epoch: 300, Loss: 1.3089, Train: 43.10%, Valid: 39.65%, Test: 39.96%
Epoch: 325, Loss: 1.2599, Train: 45.89%, Valid: 42.15%, Test: 42.71%
Epoch: 350, Loss: 1.4079, Train: 37.18%, Valid: 35.83%, Test: 36.24%
Epoch: 375, Loss: 1.3031, Train: 44.20%, Valid: 42.18%, Test: 42.68%
Epoch: 400, Loss: 1.2858, Train: 45.00%, Valid: 42.41%, Test: 42.73%
Epoch: 425, Loss: 1.2607, Train: 45.48%, Valid: 42.64%, Test: 42.98%
Epoch: 450, Loss: 1.2870, Train: 44.63%, Valid: 41.25%, Test: 41.74%
Epoch: 475, Loss: 1.2716, Train: 43.97%, Valid: 40.89%, Test: 41.33%
Epoch: 500, Loss: 1.2388, Train: 46.35%, Valid: 42.56%, Test: 43.21%
Epoch: 525, Loss: 1.2275, Train: 46.80%, Valid: 42.88%, Test: 43.45%
Epoch: 550, Loss: 1.2273, Train: 46.50%, Valid: 42.36%, Test: 42.73%
Epoch: 575, Loss: 1.2148, Train: 47.32%, Valid: 42.63%, Test: 43.13%
Epoch: 600, Loss: 1.2099, Train: 47.59%, Valid: 42.96%, Test: 43.46%
Epoch: 625, Loss: 1.2093, Train: 46.97%, Valid: 41.83%, Test: 42.46%
Epoch: 650, Loss: 1.2063, Train: 47.83%, Valid: 42.92%, Test: 43.25%
Epoch: 675, Loss: 1.1931, Train: 48.49%, Valid: 42.96%, Test: 43.42%
Epoch: 700, Loss: 1.2235, Train: 47.79%, Valid: 42.57%, Test: 43.21%
Epoch: 725, Loss: 1.1892, Train: 48.71%, Valid: 42.88%, Test: 43.29%
Epoch: 750, Loss: 1.1868, Train: 48.62%, Valid: 43.10%, Test: 43.55%
Epoch: 775, Loss: 1.1786, Train: 49.07%, Valid: 42.74%, Test: 43.23%
Epoch: 800, Loss: 1.1732, Train: 48.94%, Valid: 42.36%, Test: 42.70%
Epoch: 825, Loss: 1.1690, Train: 49.20%, Valid: 43.17%, Test: 43.32%
Epoch: 850, Loss: 1.1652, Train: 49.80%, Valid: 43.39%, Test: 43.67%
Epoch: 875, Loss: 1.1786, Train: 48.78%, Valid: 43.33%, Test: 43.56%
Epoch: 900, Loss: 1.1622, Train: 49.78%, Valid: 43.01%, Test: 43.20%
Epoch: 925, Loss: 1.1622, Train: 49.91%, Valid: 42.65%, Test: 42.91%
Epoch: 950, Loss: 1.1481, Train: 50.55%, Valid: 43.29%, Test: 43.36%
Epoch: 975, Loss: 1.1485, Train: 50.66%, Valid: 43.45%, Test: 43.60%
Run 01:
Highest Train: 51.10
Highest Valid: 43.71
  Final Train: 50.10
   Final Test: 43.84
All runs:
Highest Train: 51.10, nan
Highest Valid: 43.71, nan
  Final Train: 50.10, nan
   Final Test: 43.84, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6124, Train: 29.55%, Valid: 29.23%, Test: 29.88%
Epoch: 25, Loss: 1.4636, Train: 36.45%, Valid: 36.43%, Test: 36.68%
Epoch: 50, Loss: 1.4543, Train: 36.33%, Valid: 36.24%, Test: 36.93%
Epoch: 75, Loss: 1.4500, Train: 36.41%, Valid: 36.16%, Test: 36.46%
Epoch: 100, Loss: 1.4048, Train: 37.98%, Valid: 37.46%, Test: 38.10%
Epoch: 125, Loss: 1.4082, Train: 39.50%, Valid: 38.92%, Test: 39.60%
Epoch: 150, Loss: 1.5039, Train: 30.44%, Valid: 30.58%, Test: 30.60%
Epoch: 175, Loss: 1.4747, Train: 36.14%, Valid: 36.06%, Test: 36.46%
Epoch: 200, Loss: 1.3860, Train: 39.42%, Valid: 38.80%, Test: 39.31%
Epoch: 225, Loss: 1.3677, Train: 40.53%, Valid: 39.99%, Test: 40.44%
Epoch: 250, Loss: 1.3452, Train: 41.71%, Valid: 41.24%, Test: 41.56%
Epoch: 275, Loss: 1.3444, Train: 41.47%, Valid: 40.79%, Test: 41.25%
Epoch: 300, Loss: 1.3379, Train: 42.02%, Valid: 41.24%, Test: 41.79%
Epoch: 325, Loss: 1.3221, Train: 42.74%, Valid: 41.90%, Test: 42.42%
Epoch: 350, Loss: 1.3496, Train: 41.14%, Valid: 40.05%, Test: 40.66%
Epoch: 375, Loss: 1.3240, Train: 42.41%, Valid: 41.61%, Test: 42.09%
Epoch: 400, Loss: 1.3108, Train: 43.00%, Valid: 42.19%, Test: 42.63%
Epoch: 425, Loss: 1.3137, Train: 42.88%, Valid: 41.95%, Test: 42.48%
Epoch: 450, Loss: 1.3017, Train: 43.43%, Valid: 42.34%, Test: 42.86%
Epoch: 475, Loss: 1.3006, Train: 43.67%, Valid: 42.44%, Test: 43.03%
Epoch: 500, Loss: 1.2925, Train: 44.05%, Valid: 42.84%, Test: 43.30%
Epoch: 525, Loss: 1.2885, Train: 43.97%, Valid: 42.75%, Test: 43.31%
Epoch: 550, Loss: 1.2841, Train: 44.30%, Valid: 42.96%, Test: 43.39%
Epoch: 575, Loss: 1.2812, Train: 44.54%, Valid: 43.11%, Test: 43.59%
Epoch: 600, Loss: 1.2823, Train: 44.15%, Valid: 42.78%, Test: 43.23%
Epoch: 625, Loss: 1.2765, Train: 44.79%, Valid: 43.18%, Test: 43.56%
Epoch: 650, Loss: 1.2733, Train: 44.87%, Valid: 43.34%, Test: 43.65%
Epoch: 675, Loss: 1.2730, Train: 44.35%, Valid: 42.98%, Test: 43.38%
Epoch: 700, Loss: 1.2654, Train: 45.02%, Valid: 43.30%, Test: 43.79%
Epoch: 725, Loss: 1.3375, Train: 40.19%, Valid: 38.27%, Test: 38.98%
Epoch: 750, Loss: 1.2855, Train: 44.35%, Valid: 43.08%, Test: 43.45%
Epoch: 775, Loss: 1.2686, Train: 44.85%, Valid: 43.47%, Test: 43.75%
Epoch: 800, Loss: 1.2651, Train: 44.88%, Valid: 43.42%, Test: 43.85%
Epoch: 825, Loss: 1.2827, Train: 44.35%, Valid: 42.52%, Test: 42.73%
Epoch: 850, Loss: 1.2532, Train: 45.56%, Valid: 43.89%, Test: 44.10%
Epoch: 875, Loss: 1.2657, Train: 45.79%, Valid: 43.80%, Test: 44.12%
Epoch: 900, Loss: 1.2443, Train: 46.06%, Valid: 43.87%, Test: 44.20%
Epoch: 925, Loss: 1.2445, Train: 45.83%, Valid: 43.60%, Test: 43.80%
Epoch: 950, Loss: 1.2367, Train: 46.18%, Valid: 43.94%, Test: 44.16%
Epoch: 975, Loss: 1.2759, Train: 44.84%, Valid: 42.93%, Test: 43.06%
Run 01:
Highest Train: 46.66
Highest Valid: 44.42
  Final Train: 46.66
   Final Test: 44.54
All runs:
Highest Train: 46.66, nan
Highest Valid: 44.42, nan
  Final Train: 46.66, nan
   Final Test: 44.54, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6143, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4961, Train: 34.78%, Valid: 34.60%, Test: 34.77%
Epoch: 50, Loss: 1.4744, Train: 36.53%, Valid: 35.67%, Test: 36.03%
Epoch: 75, Loss: 1.4615, Train: 37.34%, Valid: 36.01%, Test: 36.29%
Epoch: 100, Loss: 1.4539, Train: 37.85%, Valid: 36.11%, Test: 36.45%
Epoch: 125, Loss: 1.4485, Train: 38.33%, Valid: 36.14%, Test: 36.57%
Epoch: 150, Loss: 1.4442, Train: 38.57%, Valid: 36.22%, Test: 36.60%
Epoch: 175, Loss: 1.4399, Train: 38.88%, Valid: 36.39%, Test: 36.68%
Epoch: 200, Loss: 1.4367, Train: 39.19%, Valid: 36.37%, Test: 36.73%
Epoch: 225, Loss: 1.4323, Train: 39.42%, Valid: 36.42%, Test: 36.63%
Epoch: 250, Loss: 1.4292, Train: 39.64%, Valid: 36.39%, Test: 36.54%
Epoch: 275, Loss: 1.4276, Train: 39.85%, Valid: 36.27%, Test: 36.73%
Epoch: 300, Loss: 1.4240, Train: 39.99%, Valid: 36.36%, Test: 36.60%
Epoch: 325, Loss: 1.4211, Train: 40.10%, Valid: 36.34%, Test: 36.68%
Epoch: 350, Loss: 1.4189, Train: 40.26%, Valid: 36.30%, Test: 36.61%
Epoch: 375, Loss: 1.4175, Train: 40.45%, Valid: 36.31%, Test: 36.69%
Epoch: 400, Loss: 1.4167, Train: 40.45%, Valid: 36.29%, Test: 36.77%
Epoch: 425, Loss: 1.4156, Train: 40.80%, Valid: 36.23%, Test: 36.65%
Epoch: 450, Loss: 1.4133, Train: 40.60%, Valid: 36.24%, Test: 36.60%
Epoch: 475, Loss: 1.4144, Train: 40.82%, Valid: 36.27%, Test: 36.59%
Epoch: 500, Loss: 1.4112, Train: 40.82%, Valid: 36.25%, Test: 36.60%
Epoch: 525, Loss: 1.4085, Train: 41.00%, Valid: 36.39%, Test: 36.62%
Epoch: 550, Loss: 1.4062, Train: 41.29%, Valid: 36.23%, Test: 36.60%
Epoch: 575, Loss: 1.4066, Train: 41.42%, Valid: 36.23%, Test: 36.67%
Epoch: 600, Loss: 1.4056, Train: 41.32%, Valid: 36.28%, Test: 36.67%
Epoch: 625, Loss: 1.4021, Train: 41.58%, Valid: 36.32%, Test: 36.66%
Epoch: 650, Loss: 1.3996, Train: 41.70%, Valid: 36.30%, Test: 36.64%
Epoch: 675, Loss: 1.3982, Train: 41.56%, Valid: 36.35%, Test: 36.68%
Epoch: 700, Loss: 1.3978, Train: 41.84%, Valid: 36.45%, Test: 36.72%
Epoch: 725, Loss: 1.3989, Train: 42.11%, Valid: 36.51%, Test: 36.81%
Epoch: 750, Loss: 1.3990, Train: 41.95%, Valid: 36.43%, Test: 36.81%
Epoch: 775, Loss: 1.3937, Train: 42.24%, Valid: 36.52%, Test: 36.86%
Epoch: 800, Loss: 1.3948, Train: 42.18%, Valid: 36.48%, Test: 36.97%
Epoch: 825, Loss: 1.3935, Train: 42.39%, Valid: 36.61%, Test: 36.87%
Epoch: 850, Loss: 1.3907, Train: 42.76%, Valid: 36.72%, Test: 37.07%
Epoch: 875, Loss: 1.3912, Train: 42.54%, Valid: 36.75%, Test: 37.06%
Epoch: 900, Loss: 1.3888, Train: 42.86%, Valid: 36.75%, Test: 37.04%
Epoch: 925, Loss: 1.3854, Train: 42.97%, Valid: 36.83%, Test: 37.18%
Epoch: 950, Loss: 1.3877, Train: 43.16%, Valid: 36.92%, Test: 37.28%
Epoch: 975, Loss: 1.3843, Train: 43.30%, Valid: 37.16%, Test: 37.34%
Run 01:
Highest Train: 43.58
Highest Valid: 37.22
  Final Train: 43.56
   Final Test: 37.44
All runs:
Highest Train: 43.58, nan
Highest Valid: 37.22, nan
  Final Train: 43.56, nan
   Final Test: 37.44, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6209, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.5037, Train: 34.65%, Valid: 34.37%, Test: 34.66%
Epoch: 50, Loss: 1.4705, Train: 36.27%, Valid: 35.75%, Test: 35.87%
Epoch: 75, Loss: 1.4413, Train: 37.66%, Valid: 36.93%, Test: 37.13%
Epoch: 100, Loss: 1.4300, Train: 38.81%, Valid: 37.84%, Test: 38.08%
Epoch: 125, Loss: 1.4136, Train: 39.83%, Valid: 38.61%, Test: 38.98%
Epoch: 150, Loss: 1.4058, Train: 39.80%, Valid: 38.75%, Test: 39.03%
Epoch: 175, Loss: 1.4135, Train: 39.32%, Valid: 38.21%, Test: 38.64%
Epoch: 200, Loss: 1.3918, Train: 41.16%, Valid: 39.93%, Test: 40.31%
Epoch: 225, Loss: 1.3878, Train: 40.96%, Valid: 39.87%, Test: 40.23%
Epoch: 250, Loss: 1.3887, Train: 41.84%, Valid: 40.88%, Test: 41.33%
Epoch: 275, Loss: 1.3898, Train: 42.11%, Valid: 41.11%, Test: 41.31%
Epoch: 300, Loss: 1.4057, Train: 41.28%, Valid: 40.51%, Test: 40.80%
Epoch: 325, Loss: 1.3624, Train: 40.86%, Valid: 40.04%, Test: 40.77%
Epoch: 350, Loss: 1.3711, Train: 41.68%, Valid: 40.77%, Test: 41.13%
Epoch: 375, Loss: 1.3815, Train: 41.71%, Valid: 40.97%, Test: 41.25%
Epoch: 400, Loss: 1.3783, Train: 41.68%, Valid: 40.97%, Test: 41.09%
Epoch: 425, Loss: 1.3711, Train: 41.99%, Valid: 41.00%, Test: 41.27%
Epoch: 450, Loss: 1.3618, Train: 42.71%, Valid: 41.88%, Test: 42.17%
Epoch: 475, Loss: 1.4103, Train: 37.31%, Valid: 36.46%, Test: 36.82%
Epoch: 500, Loss: 1.3565, Train: 42.71%, Valid: 41.70%, Test: 42.09%
Epoch: 525, Loss: 1.3658, Train: 42.04%, Valid: 41.07%, Test: 41.48%
Epoch: 550, Loss: 1.3489, Train: 42.01%, Valid: 41.14%, Test: 41.51%
Epoch: 575, Loss: 1.3986, Train: 42.02%, Valid: 41.33%, Test: 41.66%
Epoch: 600, Loss: 1.3563, Train: 43.08%, Valid: 42.41%, Test: 42.74%
Epoch: 625, Loss: 1.3553, Train: 42.77%, Valid: 42.17%, Test: 42.49%
Epoch: 650, Loss: 1.3502, Train: 43.65%, Valid: 42.98%, Test: 43.22%
Epoch: 675, Loss: 1.3594, Train: 41.98%, Valid: 41.54%, Test: 41.38%
Epoch: 700, Loss: 1.3492, Train: 44.03%, Valid: 43.15%, Test: 43.38%
Epoch: 725, Loss: 1.3456, Train: 43.74%, Valid: 43.17%, Test: 43.10%
Epoch: 750, Loss: 1.3409, Train: 43.12%, Valid: 42.13%, Test: 42.68%
Epoch: 775, Loss: 1.3592, Train: 42.84%, Valid: 42.12%, Test: 42.27%
Epoch: 800, Loss: 1.3353, Train: 43.65%, Valid: 42.85%, Test: 43.00%
Epoch: 825, Loss: 1.3461, Train: 43.62%, Valid: 42.90%, Test: 43.03%
Epoch: 850, Loss: 1.3374, Train: 43.23%, Valid: 42.49%, Test: 42.96%
Epoch: 875, Loss: 1.3644, Train: 44.25%, Valid: 43.73%, Test: 43.73%
Epoch: 900, Loss: 1.3261, Train: 43.91%, Valid: 43.33%, Test: 43.15%
Epoch: 925, Loss: 1.3487, Train: 42.92%, Valid: 41.87%, Test: 42.39%
Epoch: 950, Loss: 1.3528, Train: 44.09%, Valid: 43.49%, Test: 43.65%
Epoch: 975, Loss: 1.3474, Train: 42.90%, Valid: 42.34%, Test: 42.65%
Run 01:
Highest Train: 44.39
Highest Valid: 43.83
  Final Train: 44.38
   Final Test: 43.86
All runs:
Highest Train: 44.39, nan
Highest Valid: 43.83, nan
  Final Train: 44.38, nan
   Final Test: 43.86, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6742, Train: 22.88%, Valid: 22.75%, Test: 22.89%
Epoch: 25, Loss: 1.4728, Train: 37.51%, Valid: 37.21%, Test: 37.59%
Epoch: 50, Loss: 1.4386, Train: 37.64%, Valid: 37.27%, Test: 37.61%
Epoch: 75, Loss: 1.4166, Train: 38.11%, Valid: 37.78%, Test: 38.10%
Epoch: 100, Loss: 1.4104, Train: 39.99%, Valid: 39.53%, Test: 39.85%
Epoch: 125, Loss: 1.3865, Train: 40.36%, Valid: 39.58%, Test: 39.88%
Epoch: 150, Loss: 1.4421, Train: 39.43%, Valid: 38.77%, Test: 39.02%
Epoch: 175, Loss: 1.4001, Train: 37.20%, Valid: 36.70%, Test: 36.94%
Epoch: 200, Loss: 1.3876, Train: 40.22%, Valid: 39.51%, Test: 39.73%
Epoch: 225, Loss: 1.3760, Train: 40.70%, Valid: 40.01%, Test: 40.38%
Epoch: 250, Loss: 1.3935, Train: 40.32%, Valid: 39.79%, Test: 39.96%
Epoch: 275, Loss: 1.3624, Train: 39.64%, Valid: 39.03%, Test: 39.39%
Epoch: 300, Loss: 1.3701, Train: 39.97%, Valid: 39.48%, Test: 39.65%
Epoch: 325, Loss: 1.3761, Train: 41.11%, Valid: 40.70%, Test: 40.93%
Epoch: 350, Loss: 1.3613, Train: 40.28%, Valid: 39.81%, Test: 40.10%
Epoch: 375, Loss: 1.3819, Train: 38.80%, Valid: 38.28%, Test: 38.58%
Epoch: 400, Loss: 1.3587, Train: 41.72%, Valid: 41.47%, Test: 41.64%
Epoch: 425, Loss: 1.3646, Train: 41.27%, Valid: 40.57%, Test: 40.89%
Epoch: 450, Loss: 1.3614, Train: 39.87%, Valid: 39.37%, Test: 39.50%
Epoch: 475, Loss: 1.3763, Train: 40.64%, Valid: 39.87%, Test: 40.18%
Epoch: 500, Loss: 1.3479, Train: 42.18%, Valid: 41.49%, Test: 41.93%
Epoch: 525, Loss: 1.3474, Train: 39.13%, Valid: 38.62%, Test: 39.32%
Epoch: 550, Loss: 1.3495, Train: 41.98%, Valid: 41.57%, Test: 41.83%
Epoch: 575, Loss: 1.3477, Train: 42.12%, Valid: 41.62%, Test: 41.93%
Epoch: 600, Loss: 1.3543, Train: 42.43%, Valid: 41.93%, Test: 42.01%
Epoch: 625, Loss: 1.3459, Train: 41.09%, Valid: 40.53%, Test: 41.10%
Epoch: 650, Loss: 1.3482, Train: 41.80%, Valid: 41.33%, Test: 41.85%
Epoch: 675, Loss: 1.3877, Train: 42.54%, Valid: 42.20%, Test: 42.55%
Epoch: 700, Loss: 1.3461, Train: 42.68%, Valid: 42.19%, Test: 42.44%
Epoch: 725, Loss: 1.3424, Train: 42.79%, Valid: 42.17%, Test: 42.41%
Epoch: 750, Loss: 1.3405, Train: 42.90%, Valid: 42.24%, Test: 42.50%
Epoch: 775, Loss: 1.3773, Train: 40.20%, Valid: 39.81%, Test: 39.82%
Epoch: 800, Loss: 1.3487, Train: 41.73%, Valid: 41.18%, Test: 41.58%
Epoch: 825, Loss: 1.3490, Train: 42.20%, Valid: 41.63%, Test: 41.75%
Epoch: 850, Loss: 1.3413, Train: 42.26%, Valid: 41.86%, Test: 42.21%
Epoch: 875, Loss: 1.3625, Train: 41.55%, Valid: 41.11%, Test: 41.24%
Epoch: 900, Loss: 1.3406, Train: 43.32%, Valid: 42.86%, Test: 43.22%
Epoch: 925, Loss: 1.3296, Train: 42.91%, Valid: 42.52%, Test: 42.68%
Epoch: 950, Loss: 1.3534, Train: 41.93%, Valid: 41.49%, Test: 41.70%
Epoch: 975, Loss: 1.3379, Train: 42.72%, Valid: 42.41%, Test: 42.61%
Run 01:
Highest Train: 43.41
Highest Valid: 42.91
  Final Train: 43.31
   Final Test: 43.19
All runs:
Highest Train: 43.41, nan
Highest Valid: 42.91, nan
  Final Train: 43.31, nan
   Final Test: 43.19, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6038, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4990, Train: 34.69%, Valid: 34.55%, Test: 34.63%
Epoch: 50, Loss: 1.4766, Train: 36.12%, Valid: 35.49%, Test: 35.86%
Epoch: 75, Loss: 1.4651, Train: 37.02%, Valid: 35.84%, Test: 36.32%
Epoch: 100, Loss: 1.4572, Train: 37.55%, Valid: 36.14%, Test: 36.66%
Epoch: 125, Loss: 1.4518, Train: 37.85%, Valid: 36.14%, Test: 36.64%
Epoch: 150, Loss: 1.4469, Train: 38.21%, Valid: 36.19%, Test: 36.55%
Epoch: 175, Loss: 1.4434, Train: 38.48%, Valid: 36.19%, Test: 36.58%
Epoch: 200, Loss: 1.4411, Train: 38.65%, Valid: 36.23%, Test: 36.67%
Epoch: 225, Loss: 1.4417, Train: 38.68%, Valid: 36.32%, Test: 36.68%
Epoch: 250, Loss: 1.4366, Train: 38.98%, Valid: 36.26%, Test: 36.68%
Epoch: 275, Loss: 1.4352, Train: 39.11%, Valid: 36.28%, Test: 36.55%
Epoch: 300, Loss: 1.4334, Train: 39.25%, Valid: 36.27%, Test: 36.56%
Epoch: 325, Loss: 1.4309, Train: 39.44%, Valid: 36.21%, Test: 36.52%
Epoch: 350, Loss: 1.4302, Train: 39.53%, Valid: 36.37%, Test: 36.57%
Epoch: 375, Loss: 1.4286, Train: 39.59%, Valid: 36.26%, Test: 36.68%
Epoch: 400, Loss: 1.4244, Train: 39.91%, Valid: 36.38%, Test: 36.62%
Epoch: 425, Loss: 1.4245, Train: 39.96%, Valid: 36.23%, Test: 36.53%
Epoch: 450, Loss: 1.4223, Train: 39.95%, Valid: 36.31%, Test: 36.55%
Epoch: 475, Loss: 1.4214, Train: 39.98%, Valid: 36.28%, Test: 36.45%
Epoch: 500, Loss: 1.4214, Train: 40.07%, Valid: 36.31%, Test: 36.54%
Epoch: 525, Loss: 1.4189, Train: 40.04%, Valid: 36.27%, Test: 36.47%
Epoch: 550, Loss: 1.4174, Train: 40.29%, Valid: 36.31%, Test: 36.49%
Epoch: 575, Loss: 1.4166, Train: 40.40%, Valid: 36.22%, Test: 36.49%
Epoch: 600, Loss: 1.4161, Train: 40.65%, Valid: 36.23%, Test: 36.54%
Epoch: 625, Loss: 1.4142, Train: 40.62%, Valid: 36.17%, Test: 36.49%
Epoch: 650, Loss: 1.4136, Train: 40.66%, Valid: 36.23%, Test: 36.48%
Epoch: 675, Loss: 1.4119, Train: 40.80%, Valid: 36.24%, Test: 36.50%
Epoch: 700, Loss: 1.4102, Train: 40.94%, Valid: 36.36%, Test: 36.56%
Epoch: 725, Loss: 1.4119, Train: 40.87%, Valid: 36.36%, Test: 36.47%
Epoch: 750, Loss: 1.4083, Train: 41.11%, Valid: 36.44%, Test: 36.48%
Epoch: 775, Loss: 1.4058, Train: 41.36%, Valid: 36.32%, Test: 36.65%
Epoch: 800, Loss: 1.4057, Train: 41.38%, Valid: 36.51%, Test: 36.69%
Epoch: 825, Loss: 1.4047, Train: 41.43%, Valid: 36.44%, Test: 36.66%
Epoch: 850, Loss: 1.4068, Train: 41.32%, Valid: 36.39%, Test: 36.63%
Epoch: 875, Loss: 1.4061, Train: 41.39%, Valid: 36.57%, Test: 36.86%
Epoch: 900, Loss: 1.4027, Train: 41.37%, Valid: 36.68%, Test: 36.82%
Epoch: 925, Loss: 1.4008, Train: 41.81%, Valid: 36.70%, Test: 36.76%
Epoch: 950, Loss: 1.3998, Train: 41.69%, Valid: 36.83%, Test: 37.04%
Epoch: 975, Loss: 1.3998, Train: 41.83%, Valid: 36.79%, Test: 36.91%
Run 01:
Highest Train: 42.04
Highest Valid: 36.97
  Final Train: 41.94
   Final Test: 37.10
All runs:
Highest Train: 42.04, nan
Highest Valid: 36.97, nan
  Final Train: 41.94, nan
   Final Test: 37.10, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6145, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4983, Train: 34.86%, Valid: 34.62%, Test: 34.85%
Epoch: 50, Loss: 1.4583, Train: 36.78%, Valid: 36.24%, Test: 36.28%
Epoch: 75, Loss: 1.4336, Train: 38.55%, Valid: 37.60%, Test: 37.81%
Epoch: 100, Loss: 1.4119, Train: 39.44%, Valid: 38.29%, Test: 38.59%
Epoch: 125, Loss: 1.4154, Train: 40.10%, Valid: 39.02%, Test: 39.08%
Epoch: 150, Loss: 1.4192, Train: 39.19%, Valid: 38.30%, Test: 38.40%
Epoch: 175, Loss: 1.4093, Train: 40.43%, Valid: 39.32%, Test: 39.57%
Epoch: 200, Loss: 1.3695, Train: 41.23%, Valid: 39.71%, Test: 40.18%
Epoch: 225, Loss: 1.3894, Train: 40.48%, Valid: 39.09%, Test: 39.53%
Epoch: 250, Loss: 1.3706, Train: 42.27%, Valid: 40.78%, Test: 40.99%
Epoch: 275, Loss: 1.3869, Train: 42.36%, Valid: 41.19%, Test: 41.58%
Epoch: 300, Loss: 1.3850, Train: 40.15%, Valid: 39.15%, Test: 39.44%
Epoch: 325, Loss: 1.3731, Train: 42.94%, Valid: 41.56%, Test: 41.89%
Epoch: 350, Loss: 1.3630, Train: 41.38%, Valid: 39.85%, Test: 40.33%
Epoch: 375, Loss: 1.3584, Train: 42.06%, Valid: 40.89%, Test: 41.12%
Epoch: 400, Loss: 1.3875, Train: 40.39%, Valid: 39.13%, Test: 39.48%
Epoch: 425, Loss: 1.3553, Train: 40.95%, Valid: 39.73%, Test: 39.93%
Epoch: 450, Loss: 1.3531, Train: 42.90%, Valid: 41.69%, Test: 41.94%
Epoch: 475, Loss: 1.3602, Train: 43.20%, Valid: 41.91%, Test: 41.96%
Epoch: 500, Loss: 1.3875, Train: 41.11%, Valid: 40.08%, Test: 40.18%
Epoch: 525, Loss: 1.3593, Train: 42.14%, Valid: 41.09%, Test: 41.32%
Epoch: 550, Loss: 1.3474, Train: 43.82%, Valid: 42.67%, Test: 42.79%
Epoch: 575, Loss: 1.3672, Train: 43.42%, Valid: 42.12%, Test: 42.59%
Epoch: 600, Loss: 1.3626, Train: 43.68%, Valid: 42.44%, Test: 42.79%
Epoch: 625, Loss: 1.4018, Train: 40.85%, Valid: 39.92%, Test: 40.19%
Epoch: 650, Loss: 1.3357, Train: 44.22%, Valid: 43.10%, Test: 43.33%
Epoch: 675, Loss: 1.3439, Train: 43.81%, Valid: 42.84%, Test: 43.11%
Epoch: 700, Loss: 1.3638, Train: 44.12%, Valid: 42.95%, Test: 43.26%
Epoch: 725, Loss: 1.3649, Train: 41.63%, Valid: 40.57%, Test: 40.89%
Epoch: 750, Loss: 1.3381, Train: 44.07%, Valid: 42.77%, Test: 43.20%
Epoch: 775, Loss: 1.3439, Train: 44.40%, Valid: 43.34%, Test: 43.46%
Epoch: 800, Loss: 1.3395, Train: 44.05%, Valid: 42.88%, Test: 43.33%
Epoch: 825, Loss: 1.3464, Train: 43.76%, Valid: 42.84%, Test: 43.03%
Epoch: 850, Loss: 1.3667, Train: 41.93%, Valid: 41.14%, Test: 41.24%
Epoch: 875, Loss: 1.3576, Train: 42.73%, Valid: 42.11%, Test: 42.49%
Epoch: 900, Loss: 1.3525, Train: 43.19%, Valid: 42.46%, Test: 42.68%
Epoch: 925, Loss: 1.3706, Train: 42.94%, Valid: 42.28%, Test: 42.74%
Epoch: 950, Loss: 1.3383, Train: 42.12%, Valid: 41.21%, Test: 41.66%
Epoch: 975, Loss: 1.3799, Train: 43.61%, Valid: 42.75%, Test: 43.15%
Run 01:
Highest Train: 44.57
Highest Valid: 43.45
  Final Train: 44.33
   Final Test: 43.68
All runs:
Highest Train: 44.57, nan
Highest Valid: 43.45, nan
  Final Train: 44.33, nan
   Final Test: 43.68, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.5418, Train: 24.90%, Valid: 25.20%, Test: 25.21%
Epoch: 25, Loss: 1.4593, Train: 33.22%, Valid: 33.07%, Test: 33.85%
Epoch: 50, Loss: 1.4069, Train: 32.97%, Valid: 32.76%, Test: 32.72%
Epoch: 75, Loss: 1.3977, Train: 39.05%, Valid: 38.45%, Test: 38.77%
Epoch: 100, Loss: 1.3884, Train: 37.30%, Valid: 36.82%, Test: 37.29%
Epoch: 125, Loss: 1.3989, Train: 39.95%, Valid: 39.45%, Test: 39.69%
Epoch: 150, Loss: 1.3674, Train: 39.55%, Valid: 39.20%, Test: 39.78%
Epoch: 175, Loss: 1.3750, Train: 36.25%, Valid: 36.19%, Test: 36.55%
Epoch: 200, Loss: 1.3652, Train: 38.40%, Valid: 37.84%, Test: 38.29%
Epoch: 225, Loss: 1.3741, Train: 40.32%, Valid: 39.80%, Test: 40.15%
Epoch: 250, Loss: 1.3999, Train: 40.83%, Valid: 40.64%, Test: 40.96%
Epoch: 275, Loss: 1.3665, Train: 41.27%, Valid: 41.09%, Test: 41.28%
Epoch: 300, Loss: 1.3449, Train: 42.43%, Valid: 41.81%, Test: 42.21%
Epoch: 325, Loss: 1.3554, Train: 41.78%, Valid: 41.57%, Test: 41.63%
Epoch: 350, Loss: 1.3503, Train: 42.80%, Valid: 42.32%, Test: 42.62%
Epoch: 375, Loss: 1.3491, Train: 41.36%, Valid: 40.89%, Test: 41.47%
Epoch: 400, Loss: 1.3403, Train: 42.26%, Valid: 41.90%, Test: 42.35%
Epoch: 425, Loss: 1.3453, Train: 42.97%, Valid: 42.69%, Test: 42.82%
Epoch: 450, Loss: 1.3315, Train: 42.89%, Valid: 42.66%, Test: 42.90%
Epoch: 475, Loss: 1.3297, Train: 42.47%, Valid: 42.14%, Test: 42.39%
Epoch: 500, Loss: 1.3404, Train: 42.51%, Valid: 42.23%, Test: 42.48%
Epoch: 525, Loss: 1.3450, Train: 42.85%, Valid: 42.61%, Test: 42.92%
Epoch: 550, Loss: 1.3607, Train: 42.42%, Valid: 42.01%, Test: 42.44%
Epoch: 575, Loss: 1.3426, Train: 43.21%, Valid: 42.86%, Test: 43.16%
Epoch: 600, Loss: 1.3316, Train: 42.66%, Valid: 42.43%, Test: 42.66%
Epoch: 625, Loss: 1.3323, Train: 42.43%, Valid: 42.02%, Test: 42.32%
Epoch: 650, Loss: 1.3312, Train: 41.57%, Valid: 41.35%, Test: 41.63%
Epoch: 675, Loss: 1.3394, Train: 43.18%, Valid: 42.70%, Test: 42.93%
Epoch: 700, Loss: 1.3312, Train: 42.71%, Valid: 42.46%, Test: 42.67%
Epoch: 725, Loss: 1.3332, Train: 43.32%, Valid: 42.85%, Test: 43.19%
Epoch: 750, Loss: 1.3208, Train: 42.25%, Valid: 41.98%, Test: 42.29%
Epoch: 775, Loss: 1.3246, Train: 41.91%, Valid: 41.52%, Test: 42.03%
Epoch: 800, Loss: 1.3289, Train: 41.50%, Valid: 41.24%, Test: 41.60%
Epoch: 825, Loss: 1.3946, Train: 38.31%, Valid: 38.52%, Test: 38.72%
Epoch: 850, Loss: 1.3918, Train: 42.05%, Valid: 41.72%, Test: 41.94%
Epoch: 875, Loss: 1.3572, Train: 41.80%, Valid: 41.40%, Test: 41.77%
Epoch: 900, Loss: 1.3468, Train: 42.57%, Valid: 42.39%, Test: 42.70%
Epoch: 925, Loss: 1.3430, Train: 42.76%, Valid: 42.45%, Test: 42.82%
Epoch: 950, Loss: 1.3480, Train: 42.17%, Valid: 42.08%, Test: 42.16%
Epoch: 975, Loss: 1.3346, Train: 42.61%, Valid: 42.59%, Test: 42.66%
Run 01:
Highest Train: 43.59
Highest Valid: 43.30
  Final Train: 43.59
   Final Test: 43.57
All runs:
Highest Train: 43.59, nan
Highest Valid: 43.30, nan
  Final Train: 43.59, nan
   Final Test: 43.57, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6095, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4897, Train: 34.77%, Valid: 34.44%, Test: 34.74%
Epoch: 50, Loss: 1.4647, Train: 36.29%, Valid: 35.49%, Test: 35.79%
Epoch: 75, Loss: 1.4456, Train: 37.16%, Valid: 36.13%, Test: 36.23%
Epoch: 100, Loss: 1.4341, Train: 37.83%, Valid: 36.22%, Test: 36.49%
Epoch: 125, Loss: 1.4205, Train: 38.64%, Valid: 36.04%, Test: 36.19%
Epoch: 150, Loss: 1.4106, Train: 39.37%, Valid: 36.00%, Test: 36.19%
Epoch: 175, Loss: 1.3921, Train: 40.17%, Valid: 35.67%, Test: 35.92%
Epoch: 200, Loss: 1.3750, Train: 40.81%, Valid: 35.12%, Test: 35.48%
Epoch: 225, Loss: 1.3609, Train: 41.86%, Valid: 35.20%, Test: 35.35%
Epoch: 250, Loss: 1.3492, Train: 42.37%, Valid: 35.29%, Test: 35.49%
Epoch: 275, Loss: 1.3312, Train: 43.30%, Valid: 34.67%, Test: 35.08%
Epoch: 300, Loss: 1.3320, Train: 43.27%, Valid: 34.71%, Test: 34.99%
Epoch: 325, Loss: 1.3131, Train: 44.15%, Valid: 34.25%, Test: 34.64%
Epoch: 350, Loss: 1.3121, Train: 44.34%, Valid: 34.53%, Test: 34.75%
Epoch: 375, Loss: 1.2975, Train: 44.85%, Valid: 33.60%, Test: 33.76%
Epoch: 400, Loss: 1.3027, Train: 44.91%, Valid: 32.59%, Test: 33.33%
Epoch: 425, Loss: 1.2908, Train: 45.20%, Valid: 32.96%, Test: 33.34%
Epoch: 450, Loss: 1.2780, Train: 46.82%, Valid: 33.01%, Test: 33.69%
Epoch: 475, Loss: 1.2662, Train: 47.15%, Valid: 33.74%, Test: 34.27%
Epoch: 500, Loss: 1.2350, Train: 48.29%, Valid: 33.51%, Test: 33.86%
Epoch: 525, Loss: 1.2318, Train: 48.58%, Valid: 32.96%, Test: 33.31%
Epoch: 550, Loss: 1.2383, Train: 48.37%, Valid: 33.22%, Test: 33.70%
Epoch: 575, Loss: 1.2180, Train: 49.00%, Valid: 32.77%, Test: 33.20%
Epoch: 600, Loss: 1.2108, Train: 49.23%, Valid: 32.15%, Test: 32.63%
Epoch: 625, Loss: 1.2153, Train: 49.40%, Valid: 32.89%, Test: 33.02%
Epoch: 650, Loss: 1.1920, Train: 49.99%, Valid: 32.35%, Test: 32.60%
Epoch: 675, Loss: 1.1751, Train: 50.84%, Valid: 32.08%, Test: 32.69%
Epoch: 700, Loss: 1.1773, Train: 51.23%, Valid: 32.40%, Test: 32.54%
Epoch: 725, Loss: 1.1729, Train: 50.75%, Valid: 32.28%, Test: 32.53%
Epoch: 750, Loss: 1.1634, Train: 51.64%, Valid: 32.31%, Test: 32.69%
Epoch: 775, Loss: 1.1646, Train: 50.55%, Valid: 31.15%, Test: 31.30%
Epoch: 800, Loss: 1.1691, Train: 51.93%, Valid: 32.63%, Test: 32.91%
Epoch: 825, Loss: 1.1320, Train: 53.16%, Valid: 32.18%, Test: 32.55%
Epoch: 850, Loss: 1.1618, Train: 52.23%, Valid: 32.52%, Test: 32.64%
Epoch: 875, Loss: 1.1252, Train: 53.52%, Valid: 31.64%, Test: 31.89%
Epoch: 900, Loss: 1.1250, Train: 53.48%, Valid: 31.45%, Test: 32.02%
Epoch: 925, Loss: 1.1142, Train: 54.10%, Valid: 31.84%, Test: 32.18%
Epoch: 950, Loss: 1.1379, Train: 53.22%, Valid: 31.99%, Test: 32.49%
Epoch: 975, Loss: 1.1136, Train: 53.56%, Valid: 31.48%, Test: 31.83%
Run 01:
Highest Train: 54.97
Highest Valid: 36.30
  Final Train: 37.98
   Final Test: 36.37
All runs:
Highest Train: 54.97, nan
Highest Valid: 36.30, nan
  Final Train: 37.98, nan
   Final Test: 36.37, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6123, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4919, Train: 34.68%, Valid: 34.50%, Test: 34.68%
Epoch: 50, Loss: 1.4515, Train: 36.41%, Valid: 35.84%, Test: 36.10%
Epoch: 75, Loss: 1.4147, Train: 38.44%, Valid: 37.44%, Test: 37.67%
Epoch: 100, Loss: 1.3891, Train: 39.74%, Valid: 38.74%, Test: 38.89%
Epoch: 125, Loss: 1.3625, Train: 40.66%, Valid: 39.44%, Test: 39.61%
Epoch: 150, Loss: 1.3402, Train: 42.84%, Valid: 41.57%, Test: 41.57%
Epoch: 175, Loss: 1.3009, Train: 43.86%, Valid: 42.59%, Test: 42.60%
Epoch: 200, Loss: 1.2835, Train: 44.69%, Valid: 42.78%, Test: 43.22%
Epoch: 225, Loss: 1.2738, Train: 45.28%, Valid: 43.38%, Test: 43.55%
Epoch: 250, Loss: 1.2702, Train: 44.19%, Valid: 42.75%, Test: 42.85%
Epoch: 275, Loss: 1.2345, Train: 46.89%, Valid: 44.61%, Test: 44.65%
Epoch: 300, Loss: 1.3155, Train: 43.35%, Valid: 42.30%, Test: 42.56%
Epoch: 325, Loss: 1.2551, Train: 45.56%, Valid: 44.09%, Test: 44.21%
Epoch: 350, Loss: 1.2394, Train: 46.46%, Valid: 44.58%, Test: 44.64%
Epoch: 375, Loss: 1.2200, Train: 46.58%, Valid: 44.46%, Test: 44.50%
Epoch: 400, Loss: 1.2094, Train: 47.09%, Valid: 44.77%, Test: 44.88%
Epoch: 425, Loss: 1.2020, Train: 47.73%, Valid: 44.98%, Test: 44.92%
Epoch: 450, Loss: 1.2034, Train: 47.92%, Valid: 45.36%, Test: 45.51%
Epoch: 475, Loss: 1.2004, Train: 47.94%, Valid: 45.22%, Test: 45.25%
Epoch: 500, Loss: 1.1877, Train: 48.85%, Valid: 45.84%, Test: 45.58%
Epoch: 525, Loss: 1.1963, Train: 48.91%, Valid: 45.53%, Test: 45.45%
Epoch: 550, Loss: 1.1703, Train: 49.67%, Valid: 46.00%, Test: 46.12%
Epoch: 575, Loss: 1.1695, Train: 49.43%, Valid: 46.02%, Test: 45.88%
Epoch: 600, Loss: 1.1512, Train: 50.27%, Valid: 46.31%, Test: 46.30%
Epoch: 625, Loss: 1.1746, Train: 48.48%, Valid: 44.85%, Test: 44.78%
Epoch: 650, Loss: 1.1433, Train: 50.80%, Valid: 46.44%, Test: 46.51%
Epoch: 675, Loss: 1.2045, Train: 48.22%, Valid: 44.15%, Test: 44.10%
Epoch: 700, Loss: 1.1682, Train: 49.87%, Valid: 46.60%, Test: 46.40%
Epoch: 725, Loss: 1.1835, Train: 49.60%, Valid: 45.49%, Test: 45.83%
Epoch: 750, Loss: 1.1403, Train: 50.67%, Valid: 46.51%, Test: 46.45%
Epoch: 775, Loss: 1.1553, Train: 50.24%, Valid: 45.87%, Test: 45.91%
Epoch: 800, Loss: 1.2652, Train: 47.15%, Valid: 43.68%, Test: 44.12%
Epoch: 825, Loss: 1.1499, Train: 50.29%, Valid: 46.35%, Test: 46.12%
Epoch: 850, Loss: 1.1286, Train: 51.57%, Valid: 46.85%, Test: 46.65%
Epoch: 875, Loss: 1.1375, Train: 51.11%, Valid: 46.54%, Test: 46.57%
Epoch: 900, Loss: 1.1159, Train: 51.99%, Valid: 46.76%, Test: 46.76%
Epoch: 925, Loss: 1.1518, Train: 50.46%, Valid: 46.25%, Test: 46.67%
Epoch: 950, Loss: 1.1157, Train: 51.93%, Valid: 46.70%, Test: 46.77%
Epoch: 975, Loss: 1.1251, Train: 51.89%, Valid: 46.34%, Test: 46.44%
Run 01:
Highest Train: 52.58
Highest Valid: 47.07
  Final Train: 52.36
   Final Test: 47.06
All runs:
Highest Train: 52.58, nan
Highest Valid: 47.07, nan
  Final Train: 52.36, nan
   Final Test: 47.06, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.5990, Train: 28.69%, Valid: 28.51%, Test: 28.79%
Epoch: 25, Loss: 1.4377, Train: 36.53%, Valid: 36.10%, Test: 36.68%
Epoch: 50, Loss: 1.4316, Train: 37.77%, Valid: 37.42%, Test: 37.74%
Epoch: 75, Loss: 1.4134, Train: 36.35%, Valid: 35.99%, Test: 36.21%
Epoch: 100, Loss: 1.3724, Train: 39.44%, Valid: 38.90%, Test: 39.42%
Epoch: 125, Loss: 1.3654, Train: 39.76%, Valid: 39.07%, Test: 39.28%
Epoch: 150, Loss: 1.3415, Train: 40.98%, Valid: 40.49%, Test: 40.43%
Epoch: 175, Loss: 1.3622, Train: 39.65%, Valid: 39.29%, Test: 39.41%
Epoch: 200, Loss: 1.3718, Train: 39.86%, Valid: 39.60%, Test: 39.68%
Epoch: 225, Loss: 1.3555, Train: 40.43%, Valid: 39.62%, Test: 39.88%
Epoch: 250, Loss: 1.3400, Train: 41.52%, Valid: 40.86%, Test: 41.13%
Epoch: 275, Loss: 1.3233, Train: 41.72%, Valid: 41.08%, Test: 41.27%
Epoch: 300, Loss: 1.3457, Train: 40.87%, Valid: 40.38%, Test: 40.36%
Epoch: 325, Loss: 1.3263, Train: 41.64%, Valid: 40.91%, Test: 41.01%
Epoch: 350, Loss: 1.3210, Train: 41.93%, Valid: 41.08%, Test: 41.71%
Epoch: 375, Loss: 1.3241, Train: 42.29%, Valid: 41.67%, Test: 41.78%
Epoch: 400, Loss: 1.3585, Train: 40.31%, Valid: 39.67%, Test: 39.90%
Epoch: 425, Loss: 1.3188, Train: 42.39%, Valid: 41.44%, Test: 41.91%
Epoch: 450, Loss: 1.2891, Train: 43.37%, Valid: 42.43%, Test: 42.79%
Epoch: 475, Loss: 1.2861, Train: 42.88%, Valid: 41.99%, Test: 42.34%
Epoch: 500, Loss: 1.3413, Train: 41.01%, Valid: 40.38%, Test: 40.54%
Epoch: 525, Loss: 1.3102, Train: 42.27%, Valid: 41.70%, Test: 41.73%
Epoch: 550, Loss: 1.3239, Train: 41.75%, Valid: 41.11%, Test: 41.15%
Epoch: 575, Loss: 1.3604, Train: 42.32%, Valid: 41.61%, Test: 41.89%
Epoch: 600, Loss: 1.3328, Train: 41.27%, Valid: 40.55%, Test: 40.80%
Epoch: 625, Loss: 1.3695, Train: 41.96%, Valid: 41.47%, Test: 41.57%
Epoch: 650, Loss: 1.3132, Train: 42.39%, Valid: 41.77%, Test: 41.83%
Epoch: 675, Loss: 1.3008, Train: 42.34%, Valid: 41.76%, Test: 41.79%
Epoch: 700, Loss: 1.3239, Train: 41.76%, Valid: 41.06%, Test: 41.20%
Epoch: 725, Loss: 1.3011, Train: 42.94%, Valid: 41.97%, Test: 42.31%
Epoch: 750, Loss: 1.2877, Train: 42.90%, Valid: 41.93%, Test: 42.11%
Epoch: 775, Loss: 1.2742, Train: 43.70%, Valid: 42.64%, Test: 42.92%
Epoch: 800, Loss: 1.3299, Train: 41.74%, Valid: 40.83%, Test: 40.98%
Epoch: 825, Loss: 1.2887, Train: 43.20%, Valid: 42.30%, Test: 42.35%
Epoch: 850, Loss: 1.2794, Train: 43.39%, Valid: 42.46%, Test: 42.53%
Epoch: 875, Loss: 1.2976, Train: 43.40%, Valid: 41.97%, Test: 42.36%
Epoch: 900, Loss: 1.2736, Train: 43.71%, Valid: 42.60%, Test: 42.73%
Epoch: 925, Loss: 1.2801, Train: 43.91%, Valid: 42.65%, Test: 42.93%
Epoch: 950, Loss: 1.3004, Train: 44.49%, Valid: 43.27%, Test: 43.35%
Epoch: 975, Loss: 1.2658, Train: 44.10%, Valid: 43.00%, Test: 43.11%
Run 01:
Highest Train: 44.86
Highest Valid: 43.80
  Final Train: 44.85
   Final Test: 43.99
All runs:
Highest Train: 44.86, nan
Highest Valid: 43.80, nan
  Final Train: 44.85, nan
   Final Test: 43.99, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6246, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4906, Train: 34.74%, Valid: 34.38%, Test: 34.69%
Epoch: 50, Loss: 1.4662, Train: 36.26%, Valid: 35.65%, Test: 35.82%
Epoch: 75, Loss: 1.4452, Train: 37.33%, Valid: 36.11%, Test: 36.37%
Epoch: 100, Loss: 1.4244, Train: 38.17%, Valid: 36.28%, Test: 36.71%
Epoch: 125, Loss: 1.4139, Train: 38.70%, Valid: 36.12%, Test: 36.51%
Epoch: 150, Loss: 1.3935, Train: 39.56%, Valid: 35.89%, Test: 36.39%
Epoch: 175, Loss: 1.3810, Train: 40.18%, Valid: 35.72%, Test: 36.13%
Epoch: 200, Loss: 1.3587, Train: 41.47%, Valid: 35.15%, Test: 35.64%
Epoch: 225, Loss: 1.3485, Train: 41.92%, Valid: 35.55%, Test: 35.98%
Epoch: 250, Loss: 1.3301, Train: 42.59%, Valid: 35.26%, Test: 35.61%
Epoch: 275, Loss: 1.3150, Train: 43.50%, Valid: 34.84%, Test: 35.39%
Epoch: 300, Loss: 1.3019, Train: 43.89%, Valid: 34.36%, Test: 34.72%
Epoch: 325, Loss: 1.2934, Train: 44.68%, Valid: 33.99%, Test: 34.42%
Epoch: 350, Loss: 1.2826, Train: 45.26%, Valid: 33.55%, Test: 33.79%
Epoch: 375, Loss: 1.2705, Train: 46.84%, Valid: 33.78%, Test: 34.23%
Epoch: 400, Loss: 1.2410, Train: 46.54%, Valid: 34.12%, Test: 34.51%
Epoch: 425, Loss: 1.2351, Train: 48.20%, Valid: 33.08%, Test: 33.23%
Epoch: 450, Loss: 1.2242, Train: 47.89%, Valid: 32.62%, Test: 32.65%
Epoch: 475, Loss: 1.2197, Train: 48.57%, Valid: 33.66%, Test: 34.07%
Epoch: 500, Loss: 1.2052, Train: 49.42%, Valid: 32.04%, Test: 32.21%
Epoch: 525, Loss: 1.2345, Train: 49.10%, Valid: 31.50%, Test: 31.91%
Epoch: 550, Loss: 1.1880, Train: 50.47%, Valid: 32.48%, Test: 32.78%
Epoch: 575, Loss: 1.1663, Train: 51.21%, Valid: 32.75%, Test: 33.09%
Epoch: 600, Loss: 1.2170, Train: 48.49%, Valid: 32.04%, Test: 32.38%
Epoch: 625, Loss: 1.1434, Train: 52.50%, Valid: 33.11%, Test: 33.40%
Epoch: 650, Loss: 1.1623, Train: 50.66%, Valid: 33.12%, Test: 33.39%
Epoch: 675, Loss: 1.1540, Train: 51.75%, Valid: 33.56%, Test: 33.60%
Epoch: 700, Loss: 1.1593, Train: 50.56%, Valid: 33.31%, Test: 33.63%
Epoch: 725, Loss: 1.1012, Train: 54.42%, Valid: 32.57%, Test: 32.69%
Epoch: 750, Loss: 1.0881, Train: 54.27%, Valid: 32.50%, Test: 32.44%
Epoch: 775, Loss: 1.0967, Train: 54.44%, Valid: 33.19%, Test: 33.53%
Epoch: 800, Loss: 1.0740, Train: 55.74%, Valid: 32.68%, Test: 32.94%
Epoch: 825, Loss: 1.0754, Train: 55.02%, Valid: 33.15%, Test: 33.46%
Epoch: 850, Loss: 1.0530, Train: 56.65%, Valid: 32.47%, Test: 32.74%
Epoch: 875, Loss: 1.1004, Train: 51.94%, Valid: 32.46%, Test: 32.88%
Epoch: 900, Loss: 1.0520, Train: 57.03%, Valid: 33.24%, Test: 33.39%
Epoch: 925, Loss: 1.0438, Train: 57.54%, Valid: 32.58%, Test: 32.64%
Epoch: 950, Loss: 1.0347, Train: 57.64%, Valid: 32.47%, Test: 32.50%
Epoch: 975, Loss: 1.0693, Train: 55.69%, Valid: 32.95%, Test: 33.27%
Run 01:
Highest Train: 58.65
Highest Valid: 36.34
  Final Train: 38.07
   Final Test: 36.66
All runs:
Highest Train: 58.65, nan
Highest Valid: 36.34, nan
  Final Train: 38.07, nan
   Final Test: 36.66, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6039, Train: 28.72%, Valid: 28.53%, Test: 28.82%
Epoch: 25, Loss: 1.4852, Train: 34.95%, Valid: 34.55%, Test: 34.94%
Epoch: 50, Loss: 1.4318, Train: 37.54%, Valid: 36.66%, Test: 37.07%
Epoch: 75, Loss: 1.3912, Train: 39.45%, Valid: 38.22%, Test: 38.46%
Epoch: 100, Loss: 1.3490, Train: 41.19%, Valid: 39.33%, Test: 39.64%
Epoch: 125, Loss: 1.3257, Train: 42.41%, Valid: 40.32%, Test: 40.54%
Epoch: 150, Loss: 1.2874, Train: 44.41%, Valid: 41.89%, Test: 42.04%
Epoch: 175, Loss: 1.2579, Train: 45.66%, Valid: 42.24%, Test: 42.80%
Epoch: 200, Loss: 1.2410, Train: 46.59%, Valid: 43.03%, Test: 43.41%
Epoch: 225, Loss: 1.2828, Train: 46.07%, Valid: 42.94%, Test: 43.24%
Epoch: 250, Loss: 1.2019, Train: 48.20%, Valid: 43.58%, Test: 43.99%
Epoch: 275, Loss: 1.1944, Train: 48.75%, Valid: 43.93%, Test: 44.16%
Epoch: 300, Loss: 1.1946, Train: 48.70%, Valid: 44.05%, Test: 44.08%
Epoch: 325, Loss: 1.1854, Train: 49.53%, Valid: 44.14%, Test: 44.02%
Epoch: 350, Loss: 1.2265, Train: 44.61%, Valid: 40.18%, Test: 40.22%
Epoch: 375, Loss: 1.1651, Train: 49.89%, Valid: 44.48%, Test: 44.81%
Epoch: 400, Loss: 1.1324, Train: 51.47%, Valid: 44.66%, Test: 44.90%
Epoch: 425, Loss: 1.1348, Train: 51.46%, Valid: 44.68%, Test: 44.58%
Epoch: 450, Loss: 1.1354, Train: 51.23%, Valid: 44.84%, Test: 44.74%
Epoch: 475, Loss: 1.0826, Train: 53.68%, Valid: 44.91%, Test: 44.89%
Epoch: 500, Loss: 1.1760, Train: 49.85%, Valid: 44.11%, Test: 44.27%
Epoch: 525, Loss: 1.0994, Train: 52.71%, Valid: 44.85%, Test: 44.80%
Epoch: 550, Loss: 1.0695, Train: 54.01%, Valid: 45.12%, Test: 45.11%
Epoch: 575, Loss: 1.0916, Train: 53.59%, Valid: 45.19%, Test: 45.27%
Epoch: 600, Loss: 1.0640, Train: 54.69%, Valid: 44.81%, Test: 44.82%
Epoch: 625, Loss: 1.0349, Train: 55.00%, Valid: 45.06%, Test: 44.96%
Epoch: 650, Loss: 1.0399, Train: 55.95%, Valid: 45.23%, Test: 44.92%
Epoch: 675, Loss: 1.0333, Train: 56.15%, Valid: 44.93%, Test: 44.89%
Epoch: 700, Loss: 0.9959, Train: 57.40%, Valid: 44.85%, Test: 44.95%
Epoch: 725, Loss: 1.0098, Train: 56.66%, Valid: 44.79%, Test: 44.89%
Epoch: 750, Loss: 0.9914, Train: 57.46%, Valid: 44.31%, Test: 44.24%
Epoch: 775, Loss: 1.0242, Train: 58.15%, Valid: 43.73%, Test: 43.90%
Epoch: 800, Loss: 1.0065, Train: 57.57%, Valid: 44.60%, Test: 44.65%
Epoch: 825, Loss: 0.9607, Train: 59.36%, Valid: 44.61%, Test: 44.70%
Epoch: 850, Loss: 0.9389, Train: 60.21%, Valid: 44.72%, Test: 44.87%
Epoch: 875, Loss: 0.9252, Train: 60.56%, Valid: 43.94%, Test: 44.15%
Epoch: 900, Loss: 0.9326, Train: 60.64%, Valid: 43.73%, Test: 43.94%
Epoch: 925, Loss: 0.9541, Train: 59.31%, Valid: 43.50%, Test: 43.61%
Epoch: 950, Loss: 0.9074, Train: 61.55%, Valid: 44.45%, Test: 44.45%
Epoch: 975, Loss: 0.8976, Train: 60.89%, Valid: 42.90%, Test: 42.91%
Run 01:
Highest Train: 62.56
Highest Valid: 45.40
  Final Train: 52.99
   Final Test: 45.35
All runs:
Highest Train: 62.56, nan
Highest Valid: 45.40, nan
  Final Train: 52.99, nan
   Final Test: 45.35, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.5724, Train: 18.45%, Valid: 18.62%, Test: 18.20%
Epoch: 25, Loss: 1.4779, Train: 35.37%, Valid: 34.97%, Test: 35.28%
Epoch: 50, Loss: 1.4478, Train: 36.67%, Valid: 36.13%, Test: 36.59%
Epoch: 75, Loss: 1.4163, Train: 38.50%, Valid: 38.08%, Test: 38.65%
Epoch: 100, Loss: 1.3961, Train: 39.25%, Valid: 38.71%, Test: 39.09%
Epoch: 125, Loss: 1.3880, Train: 38.40%, Valid: 37.94%, Test: 38.04%
Epoch: 150, Loss: 1.3775, Train: 41.32%, Valid: 40.80%, Test: 41.16%
Epoch: 175, Loss: 1.3503, Train: 42.18%, Valid: 42.02%, Test: 42.22%
Epoch: 200, Loss: 1.3985, Train: 40.96%, Valid: 40.79%, Test: 40.78%
Epoch: 225, Loss: 1.3545, Train: 42.09%, Valid: 41.73%, Test: 42.08%
Epoch: 250, Loss: 1.3304, Train: 42.52%, Valid: 42.25%, Test: 42.71%
Epoch: 275, Loss: 1.3281, Train: 42.94%, Valid: 42.69%, Test: 43.20%
Epoch: 300, Loss: 1.3942, Train: 41.46%, Valid: 41.08%, Test: 41.44%
Epoch: 325, Loss: 1.3446, Train: 42.08%, Valid: 41.66%, Test: 42.28%
Epoch: 350, Loss: 1.3101, Train: 43.04%, Valid: 42.59%, Test: 43.20%
Epoch: 375, Loss: 1.3161, Train: 43.14%, Valid: 42.74%, Test: 43.35%
Epoch: 400, Loss: 1.2998, Train: 43.55%, Valid: 43.16%, Test: 43.47%
Epoch: 425, Loss: 1.2906, Train: 43.59%, Valid: 43.28%, Test: 43.63%
Epoch: 450, Loss: 1.3544, Train: 41.94%, Valid: 41.61%, Test: 42.02%
Epoch: 475, Loss: 1.3062, Train: 42.71%, Valid: 42.39%, Test: 42.78%
Epoch: 500, Loss: 1.2950, Train: 43.71%, Valid: 43.18%, Test: 43.55%
Epoch: 525, Loss: 1.2835, Train: 43.89%, Valid: 43.39%, Test: 43.76%
Epoch: 550, Loss: 1.2836, Train: 44.22%, Valid: 43.47%, Test: 44.03%
Epoch: 575, Loss: 1.2984, Train: 43.02%, Valid: 42.48%, Test: 42.85%
Epoch: 600, Loss: 1.2897, Train: 43.25%, Valid: 42.75%, Test: 43.19%
Epoch: 625, Loss: 1.2725, Train: 44.08%, Valid: 43.60%, Test: 43.65%
Epoch: 650, Loss: 1.3331, Train: 42.81%, Valid: 42.13%, Test: 42.58%
Epoch: 675, Loss: 1.2982, Train: 42.68%, Valid: 42.43%, Test: 42.66%
Epoch: 700, Loss: 1.2786, Train: 44.15%, Valid: 43.68%, Test: 44.07%
Epoch: 725, Loss: 1.3195, Train: 44.24%, Valid: 43.84%, Test: 43.95%
Epoch: 750, Loss: 1.2688, Train: 44.68%, Valid: 44.13%, Test: 44.51%
Epoch: 775, Loss: 1.2676, Train: 44.66%, Valid: 44.32%, Test: 44.44%
Epoch: 800, Loss: 1.2575, Train: 45.38%, Valid: 44.67%, Test: 44.99%
Epoch: 825, Loss: 1.2656, Train: 44.99%, Valid: 44.44%, Test: 44.46%
Epoch: 850, Loss: 1.3009, Train: 45.36%, Valid: 44.63%, Test: 44.71%
Epoch: 875, Loss: 1.2526, Train: 45.80%, Valid: 45.05%, Test: 45.22%
Epoch: 900, Loss: 1.2440, Train: 45.85%, Valid: 45.10%, Test: 44.99%
Epoch: 925, Loss: 1.2508, Train: 46.08%, Valid: 45.09%, Test: 45.15%
Epoch: 950, Loss: 1.2622, Train: 44.99%, Valid: 44.15%, Test: 44.06%
Epoch: 975, Loss: 1.2361, Train: 46.71%, Valid: 45.87%, Test: 45.67%
Run 01:
Highest Train: 46.71
Highest Valid: 45.87
  Final Train: 46.71
   Final Test: 45.67
All runs:
Highest Train: 46.71, nan
Highest Valid: 45.87, nan
  Final Train: 46.71, nan
   Final Test: 45.67, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6042, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.5051, Train: 34.60%, Valid: 34.38%, Test: 34.58%
Epoch: 50, Loss: 1.4826, Train: 35.76%, Valid: 35.13%, Test: 35.37%
Epoch: 75, Loss: 1.4712, Train: 36.57%, Valid: 35.54%, Test: 35.94%
Epoch: 100, Loss: 1.4660, Train: 36.99%, Valid: 35.74%, Test: 36.07%
Epoch: 125, Loss: 1.4637, Train: 37.31%, Valid: 35.93%, Test: 36.12%
Epoch: 150, Loss: 1.4586, Train: 37.53%, Valid: 35.97%, Test: 36.21%
Epoch: 175, Loss: 1.4567, Train: 37.84%, Valid: 36.00%, Test: 36.37%
Epoch: 200, Loss: 1.4520, Train: 37.94%, Valid: 35.97%, Test: 36.31%
Epoch: 225, Loss: 1.4509, Train: 38.28%, Valid: 35.99%, Test: 36.36%
Epoch: 250, Loss: 1.4470, Train: 38.35%, Valid: 36.07%, Test: 36.38%
Epoch: 275, Loss: 1.4458, Train: 38.40%, Valid: 36.10%, Test: 36.59%
Epoch: 300, Loss: 1.4441, Train: 38.69%, Valid: 36.12%, Test: 36.54%
Epoch: 325, Loss: 1.4406, Train: 38.86%, Valid: 36.22%, Test: 36.59%
Epoch: 350, Loss: 1.4369, Train: 38.77%, Valid: 36.15%, Test: 36.70%
Epoch: 375, Loss: 1.4333, Train: 38.93%, Valid: 36.35%, Test: 36.74%
Epoch: 400, Loss: 1.4273, Train: 39.39%, Valid: 36.55%, Test: 36.92%
Epoch: 425, Loss: 1.4295, Train: 39.23%, Valid: 36.66%, Test: 37.12%
Epoch: 450, Loss: 1.4204, Train: 39.80%, Valid: 37.05%, Test: 37.44%
Epoch: 475, Loss: 1.4155, Train: 40.04%, Valid: 37.35%, Test: 37.73%
Epoch: 500, Loss: 1.4145, Train: 39.85%, Valid: 37.28%, Test: 37.68%
Epoch: 525, Loss: 1.4096, Train: 40.33%, Valid: 37.77%, Test: 37.92%
Epoch: 550, Loss: 1.4123, Train: 41.11%, Valid: 38.59%, Test: 39.05%
Epoch: 575, Loss: 1.4024, Train: 41.91%, Valid: 38.94%, Test: 39.31%
Epoch: 600, Loss: 1.3988, Train: 41.38%, Valid: 38.91%, Test: 39.14%
Epoch: 625, Loss: 1.3957, Train: 42.05%, Valid: 39.66%, Test: 39.81%
Epoch: 650, Loss: 1.3905, Train: 42.26%, Valid: 39.87%, Test: 40.20%
Epoch: 675, Loss: 1.3849, Train: 42.73%, Valid: 39.96%, Test: 40.42%
Epoch: 700, Loss: 1.3969, Train: 42.69%, Valid: 40.32%, Test: 40.65%
Epoch: 725, Loss: 1.4139, Train: 42.67%, Valid: 40.37%, Test: 40.63%
Epoch: 750, Loss: 1.3889, Train: 42.76%, Valid: 40.34%, Test: 40.63%
Epoch: 775, Loss: 1.3881, Train: 42.93%, Valid: 40.50%, Test: 40.84%
Epoch: 800, Loss: 1.3856, Train: 42.41%, Valid: 39.83%, Test: 40.15%
Epoch: 825, Loss: 1.3825, Train: 43.16%, Valid: 40.30%, Test: 40.77%
Epoch: 850, Loss: 1.3780, Train: 43.52%, Valid: 40.69%, Test: 41.13%
Epoch: 875, Loss: 1.3720, Train: 44.00%, Valid: 41.13%, Test: 41.24%
Epoch: 900, Loss: 1.3748, Train: 43.59%, Valid: 40.74%, Test: 41.19%
Epoch: 925, Loss: 1.3758, Train: 43.24%, Valid: 40.49%, Test: 40.78%
Epoch: 950, Loss: 1.3755, Train: 43.96%, Valid: 40.90%, Test: 41.42%
Epoch: 975, Loss: 1.3836, Train: 44.18%, Valid: 41.36%, Test: 41.65%
Run 01:
Highest Train: 44.24
Highest Valid: 41.56
  Final Train: 43.96
   Final Test: 41.70
All runs:
Highest Train: 44.24, nan
Highest Valid: 41.56, nan
  Final Train: 43.96, nan
   Final Test: 41.70, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6143, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4979, Train: 34.89%, Valid: 34.67%, Test: 34.80%
Epoch: 50, Loss: 1.4698, Train: 36.40%, Valid: 35.84%, Test: 36.10%
Epoch: 75, Loss: 1.4408, Train: 37.91%, Valid: 36.97%, Test: 37.10%
Epoch: 100, Loss: 1.4126, Train: 39.27%, Valid: 37.93%, Test: 38.04%
Epoch: 125, Loss: 1.4045, Train: 39.77%, Valid: 38.54%, Test: 38.82%
Epoch: 150, Loss: 1.3922, Train: 40.07%, Valid: 38.71%, Test: 39.29%
Epoch: 175, Loss: 1.3767, Train: 41.96%, Valid: 40.65%, Test: 40.84%
Epoch: 200, Loss: 1.3634, Train: 42.12%, Valid: 41.14%, Test: 40.80%
Epoch: 225, Loss: 1.3396, Train: 42.68%, Valid: 41.76%, Test: 41.67%
Epoch: 250, Loss: 1.3507, Train: 41.08%, Valid: 40.05%, Test: 40.22%
Epoch: 275, Loss: 1.3396, Train: 41.25%, Valid: 40.36%, Test: 40.65%
Epoch: 300, Loss: 1.3341, Train: 43.96%, Valid: 42.72%, Test: 42.98%
Epoch: 325, Loss: 1.3430, Train: 42.45%, Valid: 41.42%, Test: 41.58%
Epoch: 350, Loss: 1.3632, Train: 40.05%, Valid: 39.46%, Test: 39.64%
Epoch: 375, Loss: 1.3563, Train: 40.51%, Valid: 39.84%, Test: 39.99%
Epoch: 400, Loss: 1.3338, Train: 42.46%, Valid: 41.42%, Test: 41.66%
Epoch: 425, Loss: 1.3283, Train: 43.42%, Valid: 42.59%, Test: 42.52%
Epoch: 450, Loss: 1.3167, Train: 43.15%, Valid: 42.34%, Test: 42.31%
Epoch: 475, Loss: 1.3297, Train: 44.34%, Valid: 43.17%, Test: 43.28%
Epoch: 500, Loss: 1.3248, Train: 43.44%, Valid: 42.41%, Test: 42.63%
Epoch: 525, Loss: 1.3185, Train: 43.60%, Valid: 42.83%, Test: 42.76%
Epoch: 550, Loss: 1.3096, Train: 42.29%, Valid: 41.68%, Test: 41.72%
Epoch: 575, Loss: 1.3328, Train: 43.07%, Valid: 42.32%, Test: 42.26%
Epoch: 600, Loss: 1.3159, Train: 43.94%, Valid: 43.25%, Test: 43.29%
Epoch: 625, Loss: 1.3322, Train: 42.72%, Valid: 42.02%, Test: 41.90%
Epoch: 650, Loss: 1.3292, Train: 43.80%, Valid: 42.92%, Test: 43.23%
Epoch: 675, Loss: 1.3682, Train: 43.35%, Valid: 42.70%, Test: 42.74%
Epoch: 700, Loss: 1.3190, Train: 42.39%, Valid: 41.80%, Test: 41.82%
Epoch: 725, Loss: 1.3102, Train: 43.01%, Valid: 42.39%, Test: 42.30%
Epoch: 750, Loss: 1.3333, Train: 42.09%, Valid: 41.35%, Test: 41.45%
Epoch: 775, Loss: 1.3547, Train: 42.85%, Valid: 41.76%, Test: 41.85%
Epoch: 800, Loss: 1.3094, Train: 43.88%, Valid: 42.98%, Test: 42.96%
Epoch: 825, Loss: 1.3180, Train: 44.17%, Valid: 43.44%, Test: 43.37%
Epoch: 850, Loss: 1.3066, Train: 43.83%, Valid: 43.22%, Test: 43.38%
Epoch: 875, Loss: 1.3258, Train: 42.10%, Valid: 41.30%, Test: 41.76%
Epoch: 900, Loss: 1.3367, Train: 42.48%, Valid: 41.65%, Test: 42.01%
Epoch: 925, Loss: 1.3233, Train: 43.19%, Valid: 42.61%, Test: 42.79%
Epoch: 950, Loss: 1.3143, Train: 42.42%, Valid: 41.86%, Test: 41.83%
Epoch: 975, Loss: 1.3012, Train: 42.69%, Valid: 42.11%, Test: 42.30%
Run 01:
Highest Train: 44.68
Highest Valid: 43.90
  Final Train: 44.66
   Final Test: 44.08
All runs:
Highest Train: 44.68, nan
Highest Valid: 43.90, nan
  Final Train: 44.66, nan
   Final Test: 44.08, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6675, Train: 15.22%, Valid: 15.21%, Test: 15.07%
Epoch: 25, Loss: 1.4832, Train: 34.94%, Valid: 34.71%, Test: 34.95%
Epoch: 50, Loss: 1.4694, Train: 33.69%, Valid: 33.48%, Test: 33.59%
Epoch: 75, Loss: 1.4568, Train: 36.03%, Valid: 35.71%, Test: 35.77%
Epoch: 100, Loss: 1.4229, Train: 29.25%, Valid: 28.93%, Test: 28.98%
Epoch: 125, Loss: 1.3821, Train: 38.62%, Valid: 38.06%, Test: 38.43%
Epoch: 150, Loss: 1.3782, Train: 39.55%, Valid: 38.96%, Test: 39.41%
Epoch: 175, Loss: 1.3658, Train: 39.78%, Valid: 39.30%, Test: 39.56%
Epoch: 200, Loss: 1.3676, Train: 39.84%, Valid: 39.35%, Test: 39.65%
Epoch: 225, Loss: 1.3547, Train: 40.48%, Valid: 39.90%, Test: 40.15%
Epoch: 250, Loss: 1.3531, Train: 41.46%, Valid: 40.89%, Test: 41.03%
Epoch: 275, Loss: 1.3427, Train: 41.25%, Valid: 40.54%, Test: 40.80%
Epoch: 300, Loss: 1.3386, Train: 41.57%, Valid: 40.97%, Test: 41.08%
Epoch: 325, Loss: 1.3520, Train: 41.76%, Valid: 41.22%, Test: 41.22%
Epoch: 350, Loss: 1.3413, Train: 41.56%, Valid: 41.07%, Test: 41.25%
Epoch: 375, Loss: 1.3614, Train: 40.94%, Valid: 40.60%, Test: 40.64%
Epoch: 400, Loss: 1.3627, Train: 41.48%, Valid: 40.76%, Test: 40.98%
Epoch: 425, Loss: 1.3696, Train: 41.09%, Valid: 40.59%, Test: 40.75%
Epoch: 450, Loss: 1.3425, Train: 41.70%, Valid: 41.22%, Test: 41.25%
Epoch: 475, Loss: 1.3485, Train: 41.78%, Valid: 41.07%, Test: 41.25%
Epoch: 500, Loss: 1.3553, Train: 41.66%, Valid: 41.13%, Test: 41.25%
Epoch: 525, Loss: 1.3364, Train: 41.88%, Valid: 41.35%, Test: 41.50%
Epoch: 550, Loss: 1.3515, Train: 41.82%, Valid: 41.35%, Test: 41.34%
Epoch: 575, Loss: 1.3491, Train: 41.76%, Valid: 41.13%, Test: 41.43%
Epoch: 600, Loss: 1.3390, Train: 42.63%, Valid: 42.04%, Test: 41.98%
Epoch: 625, Loss: 1.3284, Train: 42.89%, Valid: 42.38%, Test: 42.51%
Epoch: 650, Loss: 1.3476, Train: 42.05%, Valid: 41.65%, Test: 41.76%
Epoch: 675, Loss: 1.3407, Train: 42.47%, Valid: 41.96%, Test: 42.15%
Epoch: 700, Loss: 1.3344, Train: 42.92%, Valid: 42.30%, Test: 42.62%
Epoch: 725, Loss: 1.3324, Train: 42.95%, Valid: 42.41%, Test: 42.71%
Epoch: 750, Loss: 1.3279, Train: 42.15%, Valid: 41.58%, Test: 41.67%
Epoch: 775, Loss: 1.3346, Train: 42.61%, Valid: 42.07%, Test: 42.28%
Epoch: 800, Loss: 1.3269, Train: 42.01%, Valid: 41.75%, Test: 41.78%
Epoch: 825, Loss: 1.3382, Train: 43.22%, Valid: 42.71%, Test: 43.07%
Epoch: 850, Loss: 1.3325, Train: 42.53%, Valid: 41.94%, Test: 42.21%
Epoch: 875, Loss: 1.3991, Train: 42.09%, Valid: 41.54%, Test: 42.02%
Epoch: 900, Loss: 1.3602, Train: 41.09%, Valid: 40.60%, Test: 40.82%
Epoch: 925, Loss: 1.3515, Train: 41.50%, Valid: 41.00%, Test: 41.31%
Epoch: 950, Loss: 1.3354, Train: 41.71%, Valid: 41.41%, Test: 41.55%
Epoch: 975, Loss: 1.3380, Train: 41.70%, Valid: 41.28%, Test: 41.38%
Run 01:
Highest Train: 43.58
Highest Valid: 43.03
  Final Train: 43.58
   Final Test: 43.24
All runs:
Highest Train: 43.58, nan
Highest Valid: 43.03, nan
  Final Train: 43.58, nan
   Final Test: 43.24, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6138, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4977, Train: 34.94%, Valid: 34.66%, Test: 34.84%
Epoch: 50, Loss: 1.4761, Train: 36.49%, Valid: 35.55%, Test: 35.94%
Epoch: 75, Loss: 1.4650, Train: 37.20%, Valid: 35.95%, Test: 36.25%
Epoch: 100, Loss: 1.4567, Train: 37.70%, Valid: 36.10%, Test: 36.39%
Epoch: 125, Loss: 1.4520, Train: 38.11%, Valid: 36.11%, Test: 36.57%
Epoch: 150, Loss: 1.4474, Train: 38.38%, Valid: 36.08%, Test: 36.55%
Epoch: 175, Loss: 1.4437, Train: 38.61%, Valid: 36.07%, Test: 36.56%
Epoch: 200, Loss: 1.4410, Train: 38.73%, Valid: 36.03%, Test: 36.52%
Epoch: 225, Loss: 1.4370, Train: 38.94%, Valid: 36.04%, Test: 36.53%
Epoch: 250, Loss: 1.4336, Train: 39.19%, Valid: 36.18%, Test: 36.67%
Epoch: 275, Loss: 1.4290, Train: 39.40%, Valid: 36.32%, Test: 36.57%
Epoch: 300, Loss: 1.4278, Train: 39.47%, Valid: 36.36%, Test: 36.63%
Epoch: 325, Loss: 1.4230, Train: 39.25%, Valid: 36.36%, Test: 36.60%
Epoch: 350, Loss: 1.4193, Train: 39.32%, Valid: 36.30%, Test: 36.40%
Epoch: 375, Loss: 1.4149, Train: 39.97%, Valid: 36.77%, Test: 36.98%
Epoch: 400, Loss: 1.4112, Train: 39.98%, Valid: 36.78%, Test: 37.03%
Epoch: 425, Loss: 1.4112, Train: 40.49%, Valid: 37.09%, Test: 37.24%
Epoch: 450, Loss: 1.4010, Train: 40.29%, Valid: 37.00%, Test: 37.35%
Epoch: 475, Loss: 1.3951, Train: 41.41%, Valid: 37.82%, Test: 38.00%
Epoch: 500, Loss: 1.3968, Train: 41.34%, Valid: 37.98%, Test: 38.26%
Epoch: 525, Loss: 1.3938, Train: 41.91%, Valid: 38.51%, Test: 38.74%
Epoch: 550, Loss: 1.3960, Train: 42.13%, Valid: 38.73%, Test: 39.11%
Epoch: 575, Loss: 1.3820, Train: 42.45%, Valid: 38.99%, Test: 39.37%
Epoch: 600, Loss: 1.3942, Train: 41.89%, Valid: 38.77%, Test: 39.04%
Epoch: 625, Loss: 1.3822, Train: 41.67%, Valid: 38.44%, Test: 38.92%
Epoch: 650, Loss: 1.3809, Train: 42.32%, Valid: 39.12%, Test: 39.44%
Epoch: 675, Loss: 1.3845, Train: 41.80%, Valid: 38.82%, Test: 39.38%
Epoch: 700, Loss: 1.3784, Train: 43.80%, Valid: 40.51%, Test: 40.97%
Epoch: 725, Loss: 1.3625, Train: 43.57%, Valid: 40.14%, Test: 40.58%
Epoch: 750, Loss: 1.3650, Train: 44.45%, Valid: 41.35%, Test: 41.69%
Epoch: 775, Loss: 1.3706, Train: 43.79%, Valid: 40.62%, Test: 40.85%
Epoch: 800, Loss: 1.3485, Train: 44.47%, Valid: 41.18%, Test: 41.40%
Epoch: 825, Loss: 1.3405, Train: 45.20%, Valid: 41.88%, Test: 42.11%
Epoch: 850, Loss: 1.3328, Train: 45.80%, Valid: 42.31%, Test: 42.58%
Epoch: 875, Loss: 1.3329, Train: 45.83%, Valid: 42.81%, Test: 43.02%
Epoch: 900, Loss: 1.3482, Train: 45.51%, Valid: 42.75%, Test: 43.09%
Epoch: 925, Loss: 1.3308, Train: 45.58%, Valid: 42.99%, Test: 43.08%
Epoch: 950, Loss: 1.3301, Train: 45.80%, Valid: 42.80%, Test: 43.17%
Epoch: 975, Loss: 1.3350, Train: 44.68%, Valid: 42.50%, Test: 42.77%
Run 01:
Highest Train: 46.20
Highest Valid: 43.18
  Final Train: 45.98
   Final Test: 43.44
All runs:
Highest Train: 46.20, nan
Highest Valid: 43.18, nan
  Final Train: 45.98, nan
   Final Test: 43.44, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6120, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.5046, Train: 34.56%, Valid: 34.24%, Test: 34.50%
Epoch: 50, Loss: 1.4770, Train: 36.24%, Valid: 35.47%, Test: 35.81%
Epoch: 75, Loss: 1.4381, Train: 38.07%, Valid: 36.89%, Test: 37.35%
Epoch: 100, Loss: 1.4242, Train: 38.92%, Valid: 37.85%, Test: 38.21%
Epoch: 125, Loss: 1.4102, Train: 38.26%, Valid: 37.16%, Test: 37.53%
Epoch: 150, Loss: 1.4009, Train: 37.23%, Valid: 36.24%, Test: 36.57%
Epoch: 175, Loss: 1.3900, Train: 39.12%, Valid: 38.08%, Test: 38.46%
Epoch: 200, Loss: 1.3981, Train: 39.05%, Valid: 38.00%, Test: 38.20%
Epoch: 225, Loss: 1.4180, Train: 40.23%, Valid: 39.35%, Test: 39.80%
Epoch: 250, Loss: 1.3700, Train: 40.14%, Valid: 39.34%, Test: 39.91%
Epoch: 275, Loss: 1.3825, Train: 40.37%, Valid: 39.52%, Test: 39.79%
Epoch: 300, Loss: 1.3660, Train: 41.26%, Valid: 40.42%, Test: 40.91%
Epoch: 325, Loss: 1.3580, Train: 41.35%, Valid: 40.73%, Test: 41.12%
Epoch: 350, Loss: 1.3450, Train: 41.06%, Valid: 40.52%, Test: 40.97%
Epoch: 375, Loss: 1.3524, Train: 31.66%, Valid: 31.09%, Test: 31.14%
Epoch: 400, Loss: 1.3538, Train: 40.19%, Valid: 39.48%, Test: 40.05%
Epoch: 425, Loss: 1.3227, Train: 40.10%, Valid: 39.67%, Test: 39.87%
Epoch: 450, Loss: 1.3319, Train: 42.19%, Valid: 41.58%, Test: 41.98%
Epoch: 475, Loss: 1.3217, Train: 42.09%, Valid: 41.60%, Test: 41.77%
Epoch: 500, Loss: 1.3362, Train: 43.31%, Valid: 42.67%, Test: 42.86%
Epoch: 525, Loss: 1.3384, Train: 42.22%, Valid: 41.75%, Test: 41.72%
Epoch: 550, Loss: 1.3137, Train: 43.17%, Valid: 42.70%, Test: 42.60%
Epoch: 575, Loss: 1.3182, Train: 43.59%, Valid: 43.06%, Test: 43.09%
Epoch: 600, Loss: 1.3229, Train: 41.49%, Valid: 40.89%, Test: 40.95%
Epoch: 625, Loss: 1.3231, Train: 44.14%, Valid: 43.89%, Test: 43.69%
Epoch: 650, Loss: 1.3300, Train: 43.02%, Valid: 42.54%, Test: 42.76%
Epoch: 675, Loss: 1.3451, Train: 42.80%, Valid: 42.75%, Test: 42.51%
Epoch: 700, Loss: 1.3387, Train: 42.70%, Valid: 42.19%, Test: 42.20%
Epoch: 725, Loss: 1.3435, Train: 43.07%, Valid: 42.78%, Test: 42.78%
Epoch: 750, Loss: 1.3269, Train: 42.34%, Valid: 41.84%, Test: 41.84%
Epoch: 775, Loss: 1.3468, Train: 42.66%, Valid: 42.29%, Test: 42.14%
Epoch: 800, Loss: 1.3241, Train: 43.45%, Valid: 43.06%, Test: 42.80%
Epoch: 825, Loss: 1.3268, Train: 42.95%, Valid: 42.59%, Test: 42.46%
Epoch: 850, Loss: 1.3180, Train: 44.03%, Valid: 43.54%, Test: 43.57%
Epoch: 875, Loss: 1.3107, Train: 43.43%, Valid: 42.89%, Test: 42.95%
Epoch: 900, Loss: 1.3227, Train: 44.09%, Valid: 43.91%, Test: 43.64%
Epoch: 925, Loss: 1.3644, Train: 42.31%, Valid: 41.94%, Test: 42.01%
Epoch: 950, Loss: 1.3287, Train: 43.86%, Valid: 43.25%, Test: 43.38%
Epoch: 975, Loss: 1.3255, Train: 43.97%, Valid: 43.53%, Test: 43.49%
Run 01:
Highest Train: 44.90
Highest Valid: 44.33
  Final Train: 44.88
   Final Test: 44.38
All runs:
Highest Train: 44.90, nan
Highest Valid: 44.33, nan
  Final Train: 44.88, nan
   Final Test: 44.38, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.5917, Train: 28.62%, Valid: 28.43%, Test: 28.73%
Epoch: 25, Loss: 1.4654, Train: 36.09%, Valid: 35.85%, Test: 36.22%
Epoch: 50, Loss: 1.4574, Train: 35.57%, Valid: 35.20%, Test: 35.66%
Epoch: 75, Loss: 1.4618, Train: 37.75%, Valid: 37.48%, Test: 37.83%
Epoch: 100, Loss: 1.4262, Train: 38.26%, Valid: 37.66%, Test: 37.94%
Epoch: 125, Loss: 1.4131, Train: 38.35%, Valid: 37.78%, Test: 38.13%
Epoch: 150, Loss: 1.4048, Train: 35.90%, Valid: 35.64%, Test: 35.70%
Epoch: 175, Loss: 1.3728, Train: 36.43%, Valid: 36.13%, Test: 36.25%
Epoch: 200, Loss: 1.3633, Train: 36.90%, Valid: 36.66%, Test: 36.80%
Epoch: 225, Loss: 1.3604, Train: 39.99%, Valid: 39.52%, Test: 39.72%
Epoch: 250, Loss: 1.3508, Train: 40.53%, Valid: 39.92%, Test: 40.15%
Epoch: 275, Loss: 1.3718, Train: 41.03%, Valid: 40.57%, Test: 40.97%
Epoch: 300, Loss: 1.3691, Train: 41.81%, Valid: 41.31%, Test: 41.43%
Epoch: 325, Loss: 1.3483, Train: 41.90%, Valid: 41.35%, Test: 41.35%
Epoch: 350, Loss: 1.3416, Train: 42.17%, Valid: 41.62%, Test: 41.93%
Epoch: 375, Loss: 1.3454, Train: 41.93%, Valid: 41.25%, Test: 41.58%
Epoch: 400, Loss: 1.4137, Train: 39.78%, Valid: 39.62%, Test: 39.57%
Epoch: 425, Loss: 1.3815, Train: 40.65%, Valid: 40.36%, Test: 40.66%
Epoch: 450, Loss: 1.3586, Train: 41.05%, Valid: 40.78%, Test: 40.81%
Epoch: 475, Loss: 1.3472, Train: 41.55%, Valid: 41.02%, Test: 41.16%
Epoch: 500, Loss: 1.3639, Train: 41.93%, Valid: 41.44%, Test: 41.55%
Epoch: 525, Loss: 1.3558, Train: 41.68%, Valid: 41.02%, Test: 41.23%
Epoch: 550, Loss: 1.3464, Train: 42.02%, Valid: 41.51%, Test: 41.68%
Epoch: 575, Loss: 1.3455, Train: 41.56%, Valid: 41.14%, Test: 41.17%
Epoch: 600, Loss: 1.3418, Train: 41.97%, Valid: 41.57%, Test: 41.84%
Epoch: 625, Loss: 1.3526, Train: 42.54%, Valid: 42.17%, Test: 42.29%
Epoch: 650, Loss: 1.3441, Train: 41.02%, Valid: 40.45%, Test: 40.62%
Epoch: 675, Loss: 1.3376, Train: 42.50%, Valid: 42.02%, Test: 42.11%
Epoch: 700, Loss: 1.3548, Train: 41.91%, Valid: 41.62%, Test: 41.73%
Epoch: 725, Loss: 1.3400, Train: 41.88%, Valid: 41.53%, Test: 41.85%
Epoch: 750, Loss: 1.3434, Train: 42.67%, Valid: 42.10%, Test: 42.24%
Epoch: 775, Loss: 1.3417, Train: 41.70%, Valid: 41.25%, Test: 41.56%
Epoch: 800, Loss: 1.3569, Train: 41.90%, Valid: 41.41%, Test: 41.49%
Epoch: 825, Loss: 1.3569, Train: 41.39%, Valid: 40.96%, Test: 41.12%
Epoch: 850, Loss: 1.3477, Train: 42.82%, Valid: 42.56%, Test: 42.79%
Epoch: 875, Loss: 1.3342, Train: 42.38%, Valid: 41.94%, Test: 42.14%
Epoch: 900, Loss: 1.3487, Train: 42.25%, Valid: 41.81%, Test: 42.02%
Epoch: 925, Loss: 1.3376, Train: 42.26%, Valid: 41.69%, Test: 41.92%
Epoch: 950, Loss: 1.3315, Train: 42.33%, Valid: 42.02%, Test: 42.25%
Epoch: 975, Loss: 1.3328, Train: 42.82%, Valid: 42.52%, Test: 42.83%
Run 01:
Highest Train: 43.49
Highest Valid: 43.03
  Final Train: 43.49
   Final Test: 43.42
All runs:
Highest Train: 43.49, nan
Highest Valid: 43.03, nan
  Final Train: 43.49, nan
   Final Test: 43.42, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6153, Train: 28.73%, Valid: 28.53%, Test: 28.82%
Epoch: 25, Loss: 1.4994, Train: 32.15%, Valid: 31.81%, Test: 32.04%
Epoch: 50, Loss: 1.4286, Train: 38.82%, Valid: 38.24%, Test: 38.68%
Epoch: 75, Loss: 1.3898, Train: 39.97%, Valid: 39.12%, Test: 39.41%
Epoch: 100, Loss: 1.3602, Train: 41.92%, Valid: 40.89%, Test: 41.22%
Epoch: 125, Loss: 1.3326, Train: 43.08%, Valid: 41.79%, Test: 41.88%
Epoch: 150, Loss: 1.3142, Train: 44.05%, Valid: 42.29%, Test: 42.48%
Epoch: 175, Loss: 1.3128, Train: 44.49%, Valid: 42.44%, Test: 42.58%
Epoch: 200, Loss: 1.2729, Train: 45.92%, Valid: 43.52%, Test: 43.53%
Epoch: 225, Loss: 1.2977, Train: 44.80%, Valid: 41.97%, Test: 41.94%
Epoch: 250, Loss: 1.2333, Train: 48.21%, Valid: 44.55%, Test: 44.72%
Epoch: 275, Loss: 1.2284, Train: 48.51%, Valid: 44.41%, Test: 44.66%
Epoch: 300, Loss: 1.2054, Train: 49.43%, Valid: 44.16%, Test: 44.70%
Epoch: 325, Loss: 1.1811, Train: 50.56%, Valid: 45.28%, Test: 45.51%
Epoch: 350, Loss: 1.1617, Train: 50.76%, Valid: 45.01%, Test: 45.22%
Epoch: 375, Loss: 1.1081, Train: 53.75%, Valid: 45.88%, Test: 46.24%
Epoch: 400, Loss: 1.0897, Train: 53.11%, Valid: 45.29%, Test: 45.54%
Epoch: 425, Loss: 1.0768, Train: 55.35%, Valid: 46.00%, Test: 46.36%
Epoch: 450, Loss: 1.0708, Train: 55.95%, Valid: 46.29%, Test: 46.49%
Epoch: 475, Loss: 1.0288, Train: 57.45%, Valid: 46.24%, Test: 46.64%
Epoch: 500, Loss: 1.0071, Train: 58.60%, Valid: 45.86%, Test: 46.26%
Epoch: 525, Loss: 1.0258, Train: 57.43%, Valid: 46.51%, Test: 46.74%
Epoch: 550, Loss: 0.9755, Train: 60.09%, Valid: 46.13%, Test: 46.54%
Epoch: 575, Loss: 1.0105, Train: 58.77%, Valid: 45.90%, Test: 46.13%
Epoch: 600, Loss: 0.9646, Train: 60.47%, Valid: 46.28%, Test: 46.49%
Epoch: 625, Loss: 0.9243, Train: 62.50%, Valid: 46.71%, Test: 47.14%
Epoch: 650, Loss: 0.9282, Train: 62.87%, Valid: 46.58%, Test: 47.14%
Epoch: 675, Loss: 0.9086, Train: 63.17%, Valid: 45.77%, Test: 46.45%
Epoch: 700, Loss: 0.8780, Train: 63.67%, Valid: 45.66%, Test: 46.19%
Epoch: 725, Loss: 0.8859, Train: 64.07%, Valid: 46.22%, Test: 47.14%
Epoch: 750, Loss: 0.8647, Train: 64.98%, Valid: 46.47%, Test: 47.39%
Epoch: 775, Loss: 0.8417, Train: 66.40%, Valid: 46.17%, Test: 46.89%
Epoch: 800, Loss: 0.8485, Train: 65.64%, Valid: 44.92%, Test: 45.57%
Epoch: 825, Loss: 0.8917, Train: 65.28%, Valid: 45.32%, Test: 46.34%
Epoch: 850, Loss: 0.8112, Train: 67.38%, Valid: 46.06%, Test: 47.10%
Epoch: 875, Loss: 0.7888, Train: 68.50%, Valid: 46.04%, Test: 46.78%
Epoch: 900, Loss: 0.7905, Train: 68.29%, Valid: 45.86%, Test: 46.94%
Epoch: 925, Loss: 0.7870, Train: 68.23%, Valid: 45.73%, Test: 46.66%
Epoch: 950, Loss: 0.7628, Train: 69.43%, Valid: 45.21%, Test: 46.03%
Epoch: 975, Loss: 0.7517, Train: 69.95%, Valid: 45.21%, Test: 46.21%
Run 01:
Highest Train: 70.09
Highest Valid: 46.93
  Final Train: 61.82
   Final Test: 47.23
All runs:
Highest Train: 70.09, nan
Highest Valid: 46.93, nan
  Final Train: 61.82, nan
   Final Test: 47.23, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6589, Train: 24.64%, Valid: 24.36%, Test: 24.63%
Epoch: 25, Loss: 1.4542, Train: 33.42%, Valid: 33.35%, Test: 33.38%
Epoch: 50, Loss: 1.3983, Train: 38.78%, Valid: 38.61%, Test: 38.77%
Epoch: 75, Loss: 1.3788, Train: 40.72%, Valid: 40.56%, Test: 40.56%
Epoch: 100, Loss: 1.3566, Train: 41.50%, Valid: 41.50%, Test: 41.41%
Epoch: 125, Loss: 1.3369, Train: 42.98%, Valid: 42.86%, Test: 42.74%
Epoch: 150, Loss: 1.3736, Train: 41.05%, Valid: 40.97%, Test: 40.84%
Epoch: 175, Loss: 1.3437, Train: 41.90%, Valid: 41.84%, Test: 41.93%
Epoch: 200, Loss: 1.3249, Train: 42.72%, Valid: 42.67%, Test: 42.79%
Epoch: 225, Loss: 1.3239, Train: 43.14%, Valid: 43.03%, Test: 42.96%
Epoch: 250, Loss: 1.3028, Train: 44.02%, Valid: 43.91%, Test: 43.75%
Epoch: 275, Loss: 1.3007, Train: 44.01%, Valid: 43.80%, Test: 43.91%
Epoch: 300, Loss: 1.2851, Train: 44.42%, Valid: 44.12%, Test: 44.30%
Epoch: 325, Loss: 1.2741, Train: 44.79%, Valid: 44.48%, Test: 44.59%
Epoch: 350, Loss: 1.2742, Train: 44.99%, Valid: 44.63%, Test: 44.68%
Epoch: 375, Loss: 1.2594, Train: 45.73%, Valid: 45.28%, Test: 45.36%
Epoch: 400, Loss: 1.2612, Train: 45.85%, Valid: 45.63%, Test: 45.78%
Epoch: 425, Loss: 1.2780, Train: 44.88%, Valid: 44.65%, Test: 44.71%
Epoch: 450, Loss: 1.3589, Train: 44.44%, Valid: 44.13%, Test: 44.12%
Epoch: 475, Loss: 1.3523, Train: 42.25%, Valid: 41.93%, Test: 42.01%
Epoch: 500, Loss: 1.2923, Train: 44.41%, Valid: 44.14%, Test: 44.20%
Epoch: 525, Loss: 1.3033, Train: 44.56%, Valid: 44.37%, Test: 44.39%
Epoch: 550, Loss: 1.2616, Train: 45.45%, Valid: 45.35%, Test: 45.14%
Epoch: 575, Loss: 1.2449, Train: 46.45%, Valid: 46.03%, Test: 45.87%
Epoch: 600, Loss: 1.2522, Train: 46.09%, Valid: 45.73%, Test: 45.66%
Epoch: 625, Loss: 1.2337, Train: 46.77%, Valid: 46.39%, Test: 46.30%
Epoch: 650, Loss: 1.2925, Train: 45.54%, Valid: 45.01%, Test: 45.09%
Epoch: 675, Loss: 1.2358, Train: 46.63%, Valid: 46.24%, Test: 46.29%
Epoch: 700, Loss: 1.2217, Train: 47.41%, Valid: 46.94%, Test: 46.94%
Epoch: 725, Loss: 1.2290, Train: 47.09%, Valid: 46.68%, Test: 46.68%
Epoch: 750, Loss: 1.2131, Train: 47.70%, Valid: 47.16%, Test: 47.06%
Epoch: 775, Loss: 1.2095, Train: 47.58%, Valid: 47.07%, Test: 47.11%
Epoch: 800, Loss: 1.2100, Train: 47.70%, Valid: 47.24%, Test: 47.24%
Epoch: 825, Loss: 1.2038, Train: 48.27%, Valid: 47.80%, Test: 47.74%
Epoch: 850, Loss: 1.1998, Train: 48.39%, Valid: 47.79%, Test: 47.70%
Epoch: 875, Loss: 1.2237, Train: 47.93%, Valid: 47.42%, Test: 47.45%
Epoch: 900, Loss: 1.1965, Train: 48.60%, Valid: 48.07%, Test: 48.01%
Epoch: 925, Loss: 1.1896, Train: 48.99%, Valid: 48.44%, Test: 48.21%
Epoch: 950, Loss: 1.1963, Train: 48.66%, Valid: 48.18%, Test: 48.05%
Epoch: 975, Loss: 1.1874, Train: 49.15%, Valid: 48.31%, Test: 48.39%
Run 01:
Highest Train: 49.31
Highest Valid: 48.59
  Final Train: 49.31
   Final Test: 48.54
All runs:
Highest Train: 49.31, nan
Highest Valid: 48.59, nan
  Final Train: 49.31, nan
   Final Test: 48.54, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 232.4224, Train: 31.71%, Valid: 31.59%, Test: 32.11%
Epoch: 25, Loss: 2.8087, Train: 27.79%, Valid: 27.69%, Test: 28.02%
Epoch: 50, Loss: 1.9643, Train: 28.05%, Valid: 27.87%, Test: 28.30%
Epoch: 75, Loss: 1.7735, Train: 28.34%, Valid: 28.23%, Test: 28.50%
Epoch: 100, Loss: 1.6450, Train: 28.47%, Valid: 28.27%, Test: 28.58%
Epoch: 125, Loss: 1.5584, Train: 28.83%, Valid: 28.63%, Test: 29.04%
Epoch: 150, Loss: 1.5034, Train: 29.23%, Valid: 29.15%, Test: 29.43%
Epoch: 175, Loss: 1.4736, Train: 29.47%, Valid: 29.36%, Test: 29.80%
Epoch: 200, Loss: 1.4431, Train: 30.40%, Valid: 30.26%, Test: 30.57%
Epoch: 225, Loss: 1.4843, Train: 26.81%, Valid: 26.81%, Test: 26.87%
Epoch: 250, Loss: 1.4424, Train: 30.31%, Valid: 30.29%, Test: 30.41%
Epoch: 275, Loss: 1.4248, Train: 31.41%, Valid: 31.16%, Test: 31.35%
Epoch: 300, Loss: 1.4135, Train: 32.78%, Valid: 32.72%, Test: 32.82%
Epoch: 325, Loss: 1.4050, Train: 34.22%, Valid: 34.29%, Test: 34.35%
Epoch: 350, Loss: 1.3979, Train: 35.56%, Valid: 35.68%, Test: 35.87%
Epoch: 375, Loss: 1.3920, Train: 36.80%, Valid: 36.95%, Test: 36.98%
Epoch: 400, Loss: 1.3872, Train: 37.78%, Valid: 37.86%, Test: 37.96%
Epoch: 425, Loss: 1.3931, Train: 34.02%, Valid: 34.19%, Test: 34.19%
Epoch: 450, Loss: 1.4359, Train: 35.84%, Valid: 35.62%, Test: 35.70%
Epoch: 475, Loss: 1.4027, Train: 37.50%, Valid: 37.38%, Test: 37.54%
Epoch: 500, Loss: 1.3904, Train: 38.74%, Valid: 38.67%, Test: 38.89%
Epoch: 525, Loss: 1.3837, Train: 39.45%, Valid: 39.49%, Test: 39.70%
Epoch: 550, Loss: 1.3796, Train: 39.97%, Valid: 39.88%, Test: 40.17%
Epoch: 575, Loss: 1.3771, Train: 40.06%, Valid: 39.83%, Test: 40.31%
Epoch: 600, Loss: 1.4151, Train: 37.23%, Valid: 36.94%, Test: 37.81%
Epoch: 625, Loss: 1.4003, Train: 37.37%, Valid: 37.22%, Test: 37.80%
Epoch: 650, Loss: 1.3841, Train: 39.24%, Valid: 39.19%, Test: 39.66%
Epoch: 675, Loss: 1.3773, Train: 40.18%, Valid: 39.96%, Test: 40.28%
Epoch: 700, Loss: 1.3731, Train: 40.64%, Valid: 40.37%, Test: 40.65%
Epoch: 725, Loss: 1.3701, Train: 40.75%, Valid: 40.58%, Test: 40.91%
Epoch: 750, Loss: 1.3677, Train: 40.91%, Valid: 40.72%, Test: 41.11%
Epoch: 775, Loss: 1.4459, Train: 39.34%, Valid: 38.91%, Test: 39.15%
Epoch: 800, Loss: 1.3894, Train: 39.93%, Valid: 39.63%, Test: 40.10%
Epoch: 825, Loss: 1.3759, Train: 40.57%, Valid: 40.31%, Test: 40.67%
Epoch: 850, Loss: 1.3682, Train: 40.93%, Valid: 40.68%, Test: 41.09%
Epoch: 875, Loss: 1.3647, Train: 41.00%, Valid: 40.85%, Test: 41.25%
Epoch: 900, Loss: 1.3623, Train: 41.06%, Valid: 40.94%, Test: 41.25%
Epoch: 925, Loss: 1.3602, Train: 41.10%, Valid: 41.00%, Test: 41.36%
Epoch: 950, Loss: 1.3582, Train: 41.16%, Valid: 40.97%, Test: 41.44%
Epoch: 975, Loss: 1.3858, Train: 37.65%, Valid: 37.59%, Test: 37.89%
Run 01:
Highest Train: 41.23
Highest Valid: 41.04
  Final Train: 41.19
   Final Test: 41.46
All runs:
Highest Train: 41.23, nan
Highest Valid: 41.04, nan
  Final Train: 41.19, nan
   Final Test: 41.46, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6177, Train: 28.88%, Valid: 28.61%, Test: 28.96%
Epoch: 25, Loss: 1.4833, Train: 34.96%, Valid: 34.65%, Test: 34.98%
Epoch: 50, Loss: 1.4373, Train: 37.69%, Valid: 37.25%, Test: 37.28%
Epoch: 75, Loss: 1.3886, Train: 39.85%, Valid: 39.09%, Test: 39.30%
Epoch: 100, Loss: 1.3637, Train: 40.99%, Valid: 39.99%, Test: 40.21%
Epoch: 125, Loss: 1.3987, Train: 41.04%, Valid: 40.38%, Test: 40.30%
Epoch: 150, Loss: 1.3270, Train: 43.09%, Valid: 41.93%, Test: 42.07%
Epoch: 175, Loss: 1.3042, Train: 44.53%, Valid: 42.89%, Test: 42.75%
Epoch: 200, Loss: 1.2702, Train: 45.60%, Valid: 43.70%, Test: 43.34%
Epoch: 225, Loss: 1.2662, Train: 46.27%, Valid: 43.64%, Test: 43.52%
Epoch: 250, Loss: 1.2322, Train: 47.82%, Valid: 44.75%, Test: 44.49%
Epoch: 275, Loss: 1.2108, Train: 48.78%, Valid: 45.34%, Test: 45.08%
Epoch: 300, Loss: 1.1901, Train: 49.57%, Valid: 45.22%, Test: 45.14%
Epoch: 325, Loss: 1.1594, Train: 51.11%, Valid: 45.99%, Test: 45.99%
Epoch: 350, Loss: 1.2019, Train: 49.07%, Valid: 45.14%, Test: 44.93%
Epoch: 375, Loss: 1.1586, Train: 51.28%, Valid: 45.56%, Test: 45.78%
Epoch: 400, Loss: 1.1102, Train: 53.40%, Valid: 46.48%, Test: 46.47%
Epoch: 425, Loss: 1.1194, Train: 53.01%, Valid: 45.85%, Test: 45.90%
Epoch: 450, Loss: 1.0729, Train: 55.31%, Valid: 46.49%, Test: 46.39%
Epoch: 475, Loss: 1.0557, Train: 55.98%, Valid: 46.75%, Test: 46.99%
Epoch: 500, Loss: 1.0289, Train: 56.86%, Valid: 46.73%, Test: 46.99%
Epoch: 525, Loss: 1.0321, Train: 57.64%, Valid: 46.36%, Test: 46.62%
Epoch: 550, Loss: 1.0321, Train: 55.75%, Valid: 45.74%, Test: 46.23%
Epoch: 575, Loss: 0.9784, Train: 59.81%, Valid: 46.90%, Test: 47.09%
Epoch: 600, Loss: 0.9611, Train: 60.43%, Valid: 47.02%, Test: 47.16%
Epoch: 625, Loss: 0.9570, Train: 60.70%, Valid: 46.67%, Test: 46.89%
Epoch: 650, Loss: 0.9545, Train: 59.93%, Valid: 46.54%, Test: 46.57%
Epoch: 675, Loss: 0.9268, Train: 61.85%, Valid: 46.72%, Test: 46.95%
Epoch: 700, Loss: 0.9271, Train: 62.47%, Valid: 46.97%, Test: 47.24%
Epoch: 725, Loss: 0.9023, Train: 62.99%, Valid: 46.97%, Test: 47.11%
Epoch: 750, Loss: 0.9287, Train: 61.15%, Valid: 46.42%, Test: 46.92%
Epoch: 775, Loss: 0.8861, Train: 63.61%, Valid: 46.23%, Test: 46.55%
Epoch: 800, Loss: 0.8855, Train: 63.61%, Valid: 46.47%, Test: 46.77%
Epoch: 825, Loss: 0.8430, Train: 65.43%, Valid: 47.04%, Test: 47.39%
Epoch: 850, Loss: 0.8498, Train: 65.24%, Valid: 46.72%, Test: 46.75%
Epoch: 875, Loss: 0.8489, Train: 65.86%, Valid: 46.33%, Test: 46.58%
Epoch: 900, Loss: 0.8533, Train: 65.62%, Valid: 46.94%, Test: 47.52%
Epoch: 925, Loss: 0.8404, Train: 65.54%, Valid: 45.64%, Test: 45.79%
Epoch: 950, Loss: 0.8272, Train: 67.05%, Valid: 46.86%, Test: 46.93%
Epoch: 975, Loss: 0.8109, Train: 67.27%, Valid: 46.10%, Test: 46.56%
Run 01:
Highest Train: 67.80
Highest Valid: 47.30
  Final Train: 60.96
   Final Test: 47.56
All runs:
Highest Train: 67.80, nan
Highest Valid: 47.30, nan
  Final Train: 60.96, nan
   Final Test: 47.56, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.0718, Train: 20.40%, Valid: 20.34%, Test: 20.49%
Epoch: 25, Loss: 1.4595, Train: 31.52%, Valid: 31.38%, Test: 31.99%
Epoch: 50, Loss: 1.4040, Train: 39.56%, Valid: 39.25%, Test: 39.23%
Epoch: 75, Loss: 1.3753, Train: 40.79%, Valid: 40.67%, Test: 40.59%
Epoch: 100, Loss: 1.3815, Train: 41.19%, Valid: 41.11%, Test: 41.07%
Epoch: 125, Loss: 1.3539, Train: 41.92%, Valid: 41.92%, Test: 41.93%
Epoch: 150, Loss: 1.3514, Train: 42.16%, Valid: 42.04%, Test: 41.92%
Epoch: 175, Loss: 1.3390, Train: 42.53%, Valid: 42.32%, Test: 42.20%
Epoch: 200, Loss: 1.3181, Train: 43.49%, Valid: 43.32%, Test: 43.40%
Epoch: 225, Loss: 1.3904, Train: 41.28%, Valid: 41.16%, Test: 40.99%
Epoch: 250, Loss: 1.3276, Train: 42.85%, Valid: 42.83%, Test: 42.84%
Epoch: 275, Loss: 1.3061, Train: 44.07%, Valid: 44.20%, Test: 44.04%
Epoch: 300, Loss: 1.3206, Train: 44.29%, Valid: 44.17%, Test: 44.06%
Epoch: 325, Loss: 1.2854, Train: 44.99%, Valid: 44.90%, Test: 44.58%
Epoch: 350, Loss: 1.3077, Train: 44.68%, Valid: 44.48%, Test: 44.20%
Epoch: 375, Loss: 1.2698, Train: 45.66%, Valid: 45.70%, Test: 45.39%
Epoch: 400, Loss: 1.2905, Train: 44.89%, Valid: 44.68%, Test: 44.63%
Epoch: 425, Loss: 1.2588, Train: 46.01%, Valid: 45.76%, Test: 45.51%
Epoch: 450, Loss: 1.2483, Train: 46.59%, Valid: 46.30%, Test: 46.20%
Epoch: 475, Loss: 1.2485, Train: 46.93%, Valid: 46.74%, Test: 46.50%
Epoch: 500, Loss: 1.2877, Train: 45.24%, Valid: 45.01%, Test: 44.70%
Epoch: 525, Loss: 1.2469, Train: 46.55%, Valid: 46.41%, Test: 46.01%
Epoch: 550, Loss: 1.2332, Train: 47.23%, Valid: 47.01%, Test: 46.80%
Epoch: 575, Loss: 1.2485, Train: 46.69%, Valid: 46.57%, Test: 46.15%
Epoch: 600, Loss: 1.2256, Train: 47.50%, Valid: 47.26%, Test: 47.04%
Epoch: 625, Loss: 1.2623, Train: 46.63%, Valid: 46.47%, Test: 46.19%
Epoch: 650, Loss: 1.2244, Train: 47.64%, Valid: 47.27%, Test: 47.10%
Epoch: 675, Loss: 1.2406, Train: 47.04%, Valid: 46.74%, Test: 46.36%
Epoch: 700, Loss: 1.2181, Train: 47.76%, Valid: 47.47%, Test: 47.28%
Epoch: 725, Loss: 1.2102, Train: 48.19%, Valid: 47.74%, Test: 47.44%
Epoch: 750, Loss: 1.2939, Train: 45.86%, Valid: 45.87%, Test: 45.60%
Epoch: 775, Loss: 1.2281, Train: 47.30%, Valid: 47.20%, Test: 46.84%
Epoch: 800, Loss: 1.2110, Train: 47.84%, Valid: 47.73%, Test: 47.41%
Epoch: 825, Loss: 1.2178, Train: 47.88%, Valid: 47.53%, Test: 47.33%
Epoch: 850, Loss: 1.2081, Train: 47.63%, Valid: 47.19%, Test: 46.92%
Epoch: 875, Loss: 1.1993, Train: 48.57%, Valid: 48.14%, Test: 47.94%
Epoch: 900, Loss: 1.1956, Train: 48.65%, Valid: 48.17%, Test: 48.07%
Epoch: 925, Loss: 1.2157, Train: 47.72%, Valid: 47.18%, Test: 47.02%
Epoch: 950, Loss: 1.2084, Train: 48.14%, Valid: 47.51%, Test: 47.39%
Epoch: 975, Loss: 1.1987, Train: 48.55%, Valid: 48.01%, Test: 47.83%
Run 01:
Highest Train: 49.49
Highest Valid: 48.93
  Final Train: 49.49
   Final Test: 48.61
All runs:
Highest Train: 49.49, nan
Highest Valid: 48.93, nan
  Final Train: 49.49, nan
   Final Test: 48.61, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 123.0367, Train: 28.11%, Valid: 27.78%, Test: 28.23%
Epoch: 25, Loss: 2.9434, Train: 28.59%, Valid: 28.32%, Test: 28.63%
Epoch: 50, Loss: 2.3538, Train: 28.28%, Valid: 28.09%, Test: 28.43%
Epoch: 75, Loss: 2.0818, Train: 28.41%, Valid: 28.06%, Test: 28.46%
Epoch: 100, Loss: 1.8668, Train: 28.59%, Valid: 28.29%, Test: 28.72%
Epoch: 125, Loss: 1.7203, Train: 28.68%, Valid: 28.43%, Test: 28.79%
Epoch: 150, Loss: 1.6235, Train: 28.92%, Valid: 28.61%, Test: 29.02%
Epoch: 175, Loss: 1.5579, Train: 29.15%, Valid: 28.83%, Test: 29.21%
Epoch: 200, Loss: 1.5108, Train: 29.30%, Valid: 29.09%, Test: 29.47%
Epoch: 225, Loss: 1.4765, Train: 29.59%, Valid: 29.30%, Test: 29.70%
Epoch: 250, Loss: 1.4515, Train: 30.04%, Valid: 30.03%, Test: 30.37%
Epoch: 275, Loss: 1.4332, Train: 30.87%, Valid: 30.70%, Test: 31.28%
Epoch: 300, Loss: 1.4194, Train: 31.85%, Valid: 31.76%, Test: 32.21%
Epoch: 325, Loss: 1.4102, Train: 32.90%, Valid: 32.76%, Test: 33.29%
Epoch: 350, Loss: 1.3986, Train: 33.66%, Valid: 33.64%, Test: 34.17%
Epoch: 375, Loss: 1.3903, Train: 34.34%, Valid: 34.28%, Test: 34.94%
Epoch: 400, Loss: 1.3830, Train: 36.93%, Valid: 36.75%, Test: 37.28%
Epoch: 425, Loss: 1.3768, Train: 40.23%, Valid: 40.37%, Test: 40.25%
Epoch: 450, Loss: 1.3714, Train: 41.47%, Valid: 41.53%, Test: 41.27%
Epoch: 475, Loss: 1.3771, Train: 41.17%, Valid: 40.95%, Test: 40.99%
Epoch: 500, Loss: 1.3805, Train: 40.73%, Valid: 40.56%, Test: 40.67%
Epoch: 525, Loss: 1.3708, Train: 41.35%, Valid: 41.35%, Test: 41.25%
Epoch: 550, Loss: 1.3638, Train: 41.48%, Valid: 41.38%, Test: 41.36%
Epoch: 575, Loss: 1.3600, Train: 41.47%, Valid: 41.31%, Test: 41.37%
Epoch: 600, Loss: 1.3574, Train: 41.46%, Valid: 41.37%, Test: 41.39%
Epoch: 625, Loss: 1.3554, Train: 41.49%, Valid: 41.39%, Test: 41.47%
Epoch: 650, Loss: 1.3536, Train: 41.56%, Valid: 41.45%, Test: 41.64%
Epoch: 675, Loss: 1.3521, Train: 41.63%, Valid: 41.50%, Test: 41.65%
Epoch: 700, Loss: 1.3506, Train: 41.69%, Valid: 41.57%, Test: 41.69%
Epoch: 725, Loss: 1.3603, Train: 40.60%, Valid: 40.43%, Test: 40.56%
Epoch: 750, Loss: 1.3688, Train: 40.67%, Valid: 40.42%, Test: 40.67%
Epoch: 775, Loss: 1.3557, Train: 41.38%, Valid: 41.17%, Test: 41.33%
Epoch: 800, Loss: 1.3499, Train: 41.81%, Valid: 41.62%, Test: 41.69%
Epoch: 825, Loss: 1.3470, Train: 41.96%, Valid: 41.75%, Test: 41.83%
Epoch: 850, Loss: 1.3450, Train: 42.06%, Valid: 41.81%, Test: 41.94%
Epoch: 875, Loss: 1.3433, Train: 42.16%, Valid: 41.94%, Test: 42.11%
Epoch: 900, Loss: 1.3431, Train: 42.26%, Valid: 42.05%, Test: 42.27%
Epoch: 925, Loss: 1.3409, Train: 42.20%, Valid: 41.91%, Test: 42.10%
Epoch: 950, Loss: 1.3393, Train: 42.37%, Valid: 42.17%, Test: 42.36%
Epoch: 975, Loss: 1.3378, Train: 42.46%, Valid: 42.22%, Test: 42.44%
Run 01:
Highest Train: 42.61
Highest Valid: 42.44
  Final Train: 42.60
   Final Test: 42.59
All runs:
Highest Train: 42.61, nan
Highest Valid: 42.44, nan
  Final Train: 42.60, nan
   Final Test: 42.59, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6087, Train: 28.63%, Valid: 28.44%, Test: 28.75%
Epoch: 25, Loss: 1.4985, Train: 34.67%, Valid: 34.35%, Test: 34.64%
Epoch: 50, Loss: 1.4733, Train: 36.35%, Valid: 35.77%, Test: 35.99%
Epoch: 75, Loss: 1.4398, Train: 38.36%, Valid: 37.31%, Test: 37.54%
Epoch: 100, Loss: 1.4115, Train: 40.59%, Valid: 39.37%, Test: 39.71%
Epoch: 125, Loss: 1.3908, Train: 42.76%, Valid: 41.37%, Test: 41.72%
Epoch: 150, Loss: 1.3668, Train: 43.75%, Valid: 42.09%, Test: 42.38%
Epoch: 175, Loss: 1.3582, Train: 43.97%, Valid: 42.31%, Test: 42.31%
Epoch: 200, Loss: 1.3353, Train: 45.14%, Valid: 42.94%, Test: 43.22%
Epoch: 225, Loss: 1.3283, Train: 45.09%, Valid: 42.78%, Test: 43.20%
Epoch: 250, Loss: 1.3142, Train: 44.95%, Valid: 42.46%, Test: 42.90%
Epoch: 275, Loss: 1.3052, Train: 46.90%, Valid: 44.35%, Test: 44.53%
Epoch: 300, Loss: 1.2864, Train: 47.03%, Valid: 44.29%, Test: 44.59%
Epoch: 325, Loss: 1.2825, Train: 46.09%, Valid: 43.00%, Test: 43.77%
Epoch: 350, Loss: 1.2796, Train: 47.16%, Valid: 44.02%, Test: 44.31%
Epoch: 375, Loss: 1.2693, Train: 46.69%, Valid: 43.58%, Test: 43.75%
Epoch: 400, Loss: 1.2609, Train: 47.55%, Valid: 44.13%, Test: 44.38%
Epoch: 425, Loss: 1.2491, Train: 47.98%, Valid: 44.55%, Test: 44.97%
Epoch: 450, Loss: 1.2483, Train: 49.13%, Valid: 45.27%, Test: 45.95%
Epoch: 475, Loss: 1.2370, Train: 48.46%, Valid: 44.64%, Test: 45.24%
Epoch: 500, Loss: 1.2367, Train: 47.61%, Valid: 44.16%, Test: 44.61%
Epoch: 525, Loss: 1.2344, Train: 49.63%, Valid: 45.87%, Test: 46.22%
Epoch: 550, Loss: 1.2226, Train: 49.45%, Valid: 45.61%, Test: 46.21%
Epoch: 575, Loss: 1.2196, Train: 48.80%, Valid: 44.73%, Test: 45.23%
Epoch: 600, Loss: 1.2220, Train: 47.48%, Valid: 43.90%, Test: 44.25%
Epoch: 625, Loss: 1.2124, Train: 49.60%, Valid: 45.74%, Test: 46.28%
Epoch: 650, Loss: 1.2198, Train: 50.28%, Valid: 45.93%, Test: 46.64%
Epoch: 675, Loss: 1.2055, Train: 49.65%, Valid: 45.36%, Test: 45.74%
Epoch: 700, Loss: 1.2067, Train: 50.26%, Valid: 45.88%, Test: 46.39%
Epoch: 725, Loss: 1.2017, Train: 49.29%, Valid: 44.93%, Test: 45.44%
Epoch: 750, Loss: 1.1993, Train: 50.58%, Valid: 46.45%, Test: 47.08%
Epoch: 775, Loss: 1.1929, Train: 50.18%, Valid: 45.93%, Test: 46.22%
Epoch: 800, Loss: 1.1849, Train: 50.14%, Valid: 45.86%, Test: 46.21%
Epoch: 825, Loss: 1.1898, Train: 50.78%, Valid: 46.35%, Test: 46.60%
Epoch: 850, Loss: 1.1824, Train: 51.31%, Valid: 46.25%, Test: 46.69%
Epoch: 875, Loss: 1.1996, Train: 50.33%, Valid: 46.06%, Test: 46.30%
Epoch: 900, Loss: 1.1884, Train: 48.34%, Valid: 44.75%, Test: 44.67%
Epoch: 925, Loss: 1.1911, Train: 50.86%, Valid: 46.07%, Test: 46.30%
Epoch: 950, Loss: 1.1866, Train: 50.09%, Valid: 45.56%, Test: 45.94%
Epoch: 975, Loss: 1.1920, Train: 50.22%, Valid: 45.82%, Test: 45.85%
Run 01:
Highest Train: 52.60
Highest Valid: 47.44
  Final Train: 52.60
   Final Test: 47.82
All runs:
Highest Train: 52.60, nan
Highest Valid: 47.44, nan
  Final Train: 52.60, nan
   Final Test: 47.82, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.5435, Train: 27.07%, Valid: 26.83%, Test: 27.31%
Epoch: 25, Loss: 1.5075, Train: 29.40%, Valid: 28.99%, Test: 29.55%
Epoch: 50, Loss: 1.4519, Train: 28.90%, Valid: 29.02%, Test: 29.48%
Epoch: 75, Loss: 1.4419, Train: 28.14%, Valid: 28.05%, Test: 28.77%
Epoch: 100, Loss: 1.4208, Train: 30.10%, Valid: 29.78%, Test: 30.87%
Epoch: 125, Loss: 1.4079, Train: 26.82%, Valid: 26.85%, Test: 26.95%
Epoch: 150, Loss: 1.3978, Train: 36.48%, Valid: 35.87%, Test: 35.85%
Epoch: 175, Loss: 1.4854, Train: 34.27%, Valid: 33.78%, Test: 33.84%
Epoch: 200, Loss: 1.3930, Train: 33.41%, Valid: 33.34%, Test: 32.79%
Epoch: 225, Loss: 1.3866, Train: 34.00%, Valid: 33.83%, Test: 33.73%
Epoch: 250, Loss: 1.3865, Train: 39.31%, Valid: 38.97%, Test: 39.07%
Epoch: 275, Loss: 1.3638, Train: 38.90%, Valid: 38.58%, Test: 38.81%
Epoch: 300, Loss: 1.3645, Train: 38.54%, Valid: 38.51%, Test: 38.33%
Epoch: 325, Loss: 1.3742, Train: 37.58%, Valid: 37.48%, Test: 37.44%
Epoch: 350, Loss: 1.3650, Train: 35.40%, Valid: 35.05%, Test: 35.07%
Epoch: 375, Loss: 1.3779, Train: 36.33%, Valid: 35.98%, Test: 35.99%
Epoch: 400, Loss: 1.3576, Train: 34.54%, Valid: 34.40%, Test: 34.27%
Epoch: 425, Loss: 1.3747, Train: 34.70%, Valid: 34.52%, Test: 34.32%
Epoch: 450, Loss: 1.3507, Train: 34.45%, Valid: 34.37%, Test: 34.14%
Epoch: 475, Loss: 1.3297, Train: 34.46%, Valid: 34.39%, Test: 34.21%
Epoch: 500, Loss: 1.3462, Train: 33.70%, Valid: 33.67%, Test: 33.48%
Epoch: 525, Loss: 1.3227, Train: 35.24%, Valid: 35.09%, Test: 34.99%
Epoch: 550, Loss: 1.3324, Train: 34.10%, Valid: 34.04%, Test: 33.99%
Epoch: 575, Loss: 1.3226, Train: 36.86%, Valid: 36.66%, Test: 36.73%
Epoch: 600, Loss: 1.3239, Train: 36.06%, Valid: 35.82%, Test: 35.73%
Epoch: 625, Loss: 1.3648, Train: 33.64%, Valid: 33.38%, Test: 32.98%
Epoch: 650, Loss: 1.3289, Train: 39.00%, Valid: 38.91%, Test: 39.00%
Epoch: 675, Loss: 1.3131, Train: 35.65%, Valid: 35.48%, Test: 35.22%
Epoch: 700, Loss: 1.3615, Train: 35.80%, Valid: 35.75%, Test: 35.55%
Epoch: 725, Loss: 1.3424, Train: 35.97%, Valid: 35.91%, Test: 35.68%
Epoch: 750, Loss: 1.3162, Train: 39.56%, Valid: 39.49%, Test: 39.45%
Epoch: 775, Loss: 1.3321, Train: 36.78%, Valid: 36.82%, Test: 36.50%
Epoch: 800, Loss: 1.3381, Train: 35.14%, Valid: 35.22%, Test: 34.94%
Epoch: 825, Loss: 1.3187, Train: 36.12%, Valid: 35.91%, Test: 35.79%
Epoch: 850, Loss: 1.3471, Train: 35.88%, Valid: 35.40%, Test: 35.36%
Epoch: 875, Loss: 1.3322, Train: 35.44%, Valid: 35.61%, Test: 34.96%
Epoch: 900, Loss: 1.3100, Train: 34.86%, Valid: 34.84%, Test: 34.46%
Epoch: 925, Loss: 1.3095, Train: 39.21%, Valid: 38.89%, Test: 39.10%
Epoch: 950, Loss: 1.3075, Train: 38.52%, Valid: 38.27%, Test: 38.27%
Epoch: 975, Loss: 1.3103, Train: 37.09%, Valid: 37.10%, Test: 36.82%
Run 01:
Highest Train: 43.92
Highest Valid: 43.42
  Final Train: 43.53
   Final Test: 43.47
All runs:
Highest Train: 43.92, nan
Highest Valid: 43.42, nan
  Final Train: 43.53, nan
   Final Test: 43.47, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 41.6167, Train: 18.64%, Valid: 18.77%, Test: 18.80%
Epoch: 25, Loss: 2.9093, Train: 20.52%, Valid: 20.56%, Test: 20.45%
Epoch: 50, Loss: 1.9403, Train: 20.61%, Valid: 20.74%, Test: 20.62%
Epoch: 75, Loss: 1.7022, Train: 21.01%, Valid: 21.07%, Test: 21.16%
Epoch: 100, Loss: 1.6144, Train: 21.76%, Valid: 21.78%, Test: 21.89%
Epoch: 125, Loss: 1.5982, Train: 22.37%, Valid: 22.31%, Test: 22.54%
Epoch: 150, Loss: 1.5695, Train: 23.94%, Valid: 23.92%, Test: 24.16%
Epoch: 175, Loss: 1.5465, Train: 24.79%, Valid: 24.62%, Test: 25.09%
Epoch: 200, Loss: 1.5469, Train: 25.87%, Valid: 25.88%, Test: 26.25%
Epoch: 225, Loss: 1.5186, Train: 27.26%, Valid: 27.24%, Test: 27.72%
Epoch: 250, Loss: 1.5180, Train: 30.15%, Valid: 30.08%, Test: 30.50%
Epoch: 275, Loss: 1.5054, Train: 31.91%, Valid: 31.73%, Test: 32.26%
Epoch: 300, Loss: 1.4839, Train: 37.04%, Valid: 36.75%, Test: 36.90%
Epoch: 325, Loss: 1.4597, Train: 37.79%, Valid: 37.59%, Test: 37.60%
Epoch: 350, Loss: 1.4400, Train: 38.86%, Valid: 38.83%, Test: 38.81%
Epoch: 375, Loss: 1.4725, Train: 35.49%, Valid: 35.54%, Test: 35.32%
Epoch: 400, Loss: 1.4842, Train: 38.38%, Valid: 38.34%, Test: 38.35%
Epoch: 425, Loss: 1.4310, Train: 37.75%, Valid: 37.67%, Test: 37.78%
Epoch: 450, Loss: 1.4310, Train: 37.47%, Valid: 37.39%, Test: 37.37%
Epoch: 475, Loss: 1.4529, Train: 37.60%, Valid: 37.60%, Test: 37.59%
Epoch: 500, Loss: 1.4301, Train: 38.32%, Valid: 38.42%, Test: 38.24%
Epoch: 525, Loss: 1.4215, Train: 32.81%, Valid: 32.93%, Test: 32.85%
Epoch: 550, Loss: 1.4070, Train: 31.32%, Valid: 31.53%, Test: 31.40%
Epoch: 575, Loss: 1.4229, Train: 31.27%, Valid: 31.43%, Test: 31.42%
Epoch: 600, Loss: 1.4063, Train: 33.32%, Valid: 33.57%, Test: 33.09%
Epoch: 625, Loss: 1.4095, Train: 31.13%, Valid: 31.42%, Test: 31.23%
Epoch: 650, Loss: 1.4153, Train: 30.11%, Valid: 30.36%, Test: 29.84%
Epoch: 675, Loss: 1.4043, Train: 31.66%, Valid: 31.89%, Test: 31.48%
Epoch: 700, Loss: 1.4075, Train: 31.20%, Valid: 31.37%, Test: 30.87%
Epoch: 725, Loss: 1.4096, Train: 32.37%, Valid: 32.63%, Test: 32.02%
Epoch: 750, Loss: 1.3960, Train: 30.29%, Valid: 30.49%, Test: 30.17%
Epoch: 775, Loss: 1.3827, Train: 30.50%, Valid: 30.54%, Test: 30.28%
Epoch: 800, Loss: 1.4405, Train: 20.47%, Valid: 20.48%, Test: 20.45%
Epoch: 825, Loss: 1.4066, Train: 32.43%, Valid: 32.55%, Test: 32.43%
Epoch: 850, Loss: 1.4026, Train: 33.81%, Valid: 33.88%, Test: 33.92%
Epoch: 875, Loss: 1.4161, Train: 29.53%, Valid: 29.78%, Test: 29.32%
Epoch: 900, Loss: 1.3904, Train: 30.49%, Valid: 30.71%, Test: 30.27%
Epoch: 925, Loss: 1.3840, Train: 27.63%, Valid: 27.84%, Test: 27.41%
Epoch: 950, Loss: 1.3844, Train: 26.08%, Valid: 26.23%, Test: 25.90%
Epoch: 975, Loss: 1.3746, Train: 26.18%, Valid: 26.33%, Test: 25.98%
Run 01:
Highest Train: 38.89
Highest Valid: 38.91
  Final Train: 38.86
   Final Test: 38.77
All runs:
Highest Train: 38.89, nan
Highest Valid: 38.91, nan
  Final Train: 38.86, nan
   Final Test: 38.77, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6221, Train: 28.00%, Valid: 27.91%, Test: 28.54%
Epoch: 25, Loss: 1.4979, Train: 34.67%, Valid: 34.41%, Test: 34.57%
Epoch: 50, Loss: 1.4775, Train: 36.00%, Valid: 35.38%, Test: 35.85%
Epoch: 75, Loss: 1.4565, Train: 37.35%, Valid: 36.37%, Test: 36.73%
Epoch: 100, Loss: 1.4339, Train: 38.88%, Valid: 37.66%, Test: 37.77%
Epoch: 125, Loss: 1.4138, Train: 40.55%, Valid: 39.16%, Test: 39.32%
Epoch: 150, Loss: 1.3847, Train: 41.92%, Valid: 40.39%, Test: 40.72%
Epoch: 175, Loss: 1.3584, Train: 42.98%, Valid: 41.27%, Test: 41.64%
Epoch: 200, Loss: 1.3449, Train: 44.08%, Valid: 42.23%, Test: 42.87%
Epoch: 225, Loss: 1.3224, Train: 43.74%, Valid: 41.92%, Test: 42.28%
Epoch: 250, Loss: 1.3319, Train: 44.42%, Valid: 42.55%, Test: 42.94%
Epoch: 275, Loss: 1.3010, Train: 45.10%, Valid: 43.03%, Test: 43.80%
Epoch: 300, Loss: 1.3007, Train: 45.46%, Valid: 43.40%, Test: 43.85%
Epoch: 325, Loss: 1.2976, Train: 46.44%, Valid: 44.03%, Test: 44.51%
Epoch: 350, Loss: 1.2979, Train: 46.32%, Valid: 44.11%, Test: 44.49%
Epoch: 375, Loss: 1.2852, Train: 46.39%, Valid: 44.19%, Test: 44.51%
Epoch: 400, Loss: 1.2712, Train: 46.84%, Valid: 44.39%, Test: 44.97%
Epoch: 425, Loss: 1.2569, Train: 47.92%, Valid: 45.35%, Test: 45.42%
Epoch: 450, Loss: 1.2630, Train: 47.86%, Valid: 45.38%, Test: 45.45%
Epoch: 475, Loss: 1.2655, Train: 48.24%, Valid: 45.65%, Test: 45.70%
Epoch: 500, Loss: 1.2598, Train: 47.62%, Valid: 44.89%, Test: 45.02%
Epoch: 525, Loss: 1.2412, Train: 48.30%, Valid: 45.53%, Test: 45.80%
Epoch: 550, Loss: 1.2436, Train: 45.75%, Valid: 43.22%, Test: 43.42%
Epoch: 575, Loss: 1.2459, Train: 47.56%, Valid: 44.81%, Test: 44.92%
Epoch: 600, Loss: 1.2349, Train: 48.23%, Valid: 45.57%, Test: 45.55%
Epoch: 625, Loss: 1.2347, Train: 47.47%, Valid: 44.67%, Test: 44.88%
Epoch: 650, Loss: 1.2224, Train: 48.51%, Valid: 45.23%, Test: 45.66%
Epoch: 675, Loss: 1.2280, Train: 49.24%, Valid: 46.17%, Test: 46.51%
Epoch: 700, Loss: 1.2171, Train: 49.67%, Valid: 46.21%, Test: 46.48%
Epoch: 725, Loss: 1.2327, Train: 48.38%, Valid: 45.36%, Test: 45.37%
Epoch: 750, Loss: 1.2198, Train: 50.03%, Valid: 46.79%, Test: 46.80%
Epoch: 775, Loss: 1.2122, Train: 50.15%, Valid: 46.80%, Test: 46.82%
Epoch: 800, Loss: 1.2119, Train: 49.19%, Valid: 45.66%, Test: 45.90%
Epoch: 825, Loss: 1.2083, Train: 47.97%, Valid: 44.70%, Test: 44.88%
Epoch: 850, Loss: 1.2077, Train: 49.35%, Valid: 45.95%, Test: 45.73%
Epoch: 875, Loss: 1.2050, Train: 50.14%, Valid: 46.55%, Test: 46.48%
Epoch: 900, Loss: 1.2023, Train: 48.39%, Valid: 45.17%, Test: 45.00%
Epoch: 925, Loss: 1.1998, Train: 48.61%, Valid: 45.29%, Test: 45.21%
Epoch: 950, Loss: 1.2098, Train: 50.31%, Valid: 46.64%, Test: 46.69%
Epoch: 975, Loss: 1.1990, Train: 49.31%, Valid: 45.83%, Test: 45.68%
Run 01:
Highest Train: 50.97
Highest Valid: 47.25
  Final Train: 50.74
   Final Test: 47.36
All runs:
Highest Train: 50.97, nan
Highest Valid: 47.25, nan
  Final Train: 50.74, nan
   Final Test: 47.36, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.4758, Train: 21.42%, Valid: 21.34%, Test: 21.23%
Epoch: 25, Loss: 1.5137, Train: 29.15%, Valid: 28.87%, Test: 29.23%
Epoch: 50, Loss: 1.4631, Train: 36.15%, Valid: 35.93%, Test: 36.20%
Epoch: 75, Loss: 1.4275, Train: 31.91%, Valid: 32.37%, Test: 31.96%
Epoch: 100, Loss: 1.4154, Train: 30.24%, Valid: 30.42%, Test: 30.02%
Epoch: 125, Loss: 1.4138, Train: 30.21%, Valid: 30.36%, Test: 30.01%
Epoch: 150, Loss: 1.4000, Train: 30.35%, Valid: 30.49%, Test: 29.97%
Epoch: 175, Loss: 1.3914, Train: 31.39%, Valid: 31.68%, Test: 31.26%
Epoch: 200, Loss: 1.3910, Train: 34.23%, Valid: 34.19%, Test: 33.84%
Epoch: 225, Loss: 1.3756, Train: 31.98%, Valid: 32.10%, Test: 31.92%
Epoch: 250, Loss: 1.3783, Train: 32.33%, Valid: 32.57%, Test: 32.07%
Epoch: 275, Loss: 1.3616, Train: 32.27%, Valid: 32.60%, Test: 32.26%
Epoch: 300, Loss: 1.3520, Train: 32.27%, Valid: 32.29%, Test: 32.09%
Epoch: 325, Loss: 1.3679, Train: 33.55%, Valid: 33.70%, Test: 33.31%
Epoch: 350, Loss: 1.3632, Train: 35.78%, Valid: 35.84%, Test: 35.53%
Epoch: 375, Loss: 1.3390, Train: 35.34%, Valid: 35.15%, Test: 34.88%
Epoch: 400, Loss: 1.3422, Train: 34.14%, Valid: 34.18%, Test: 34.03%
Epoch: 425, Loss: 1.3458, Train: 34.77%, Valid: 34.59%, Test: 34.40%
Epoch: 450, Loss: 1.3736, Train: 36.39%, Valid: 36.68%, Test: 36.33%
Epoch: 475, Loss: 1.3775, Train: 35.69%, Valid: 35.53%, Test: 35.31%
Epoch: 500, Loss: 1.3378, Train: 34.24%, Valid: 34.35%, Test: 34.10%
Epoch: 525, Loss: 1.3399, Train: 34.41%, Valid: 34.37%, Test: 34.12%
Epoch: 550, Loss: 1.3186, Train: 36.01%, Valid: 35.81%, Test: 35.71%
Epoch: 575, Loss: 1.3201, Train: 36.47%, Valid: 36.52%, Test: 36.17%
Epoch: 600, Loss: 1.3275, Train: 36.45%, Valid: 36.32%, Test: 36.09%
Epoch: 625, Loss: 1.3303, Train: 40.25%, Valid: 39.90%, Test: 40.18%
Epoch: 650, Loss: 1.3133, Train: 40.19%, Valid: 39.90%, Test: 40.21%
Epoch: 675, Loss: 1.3159, Train: 40.30%, Valid: 39.97%, Test: 40.04%
Epoch: 700, Loss: 1.3143, Train: 38.14%, Valid: 37.97%, Test: 37.95%
Epoch: 725, Loss: 1.3193, Train: 37.21%, Valid: 36.97%, Test: 36.78%
Epoch: 750, Loss: 1.3168, Train: 39.51%, Valid: 39.27%, Test: 39.10%
Epoch: 775, Loss: 1.3196, Train: 39.58%, Valid: 39.38%, Test: 39.28%
Epoch: 800, Loss: 1.3117, Train: 41.34%, Valid: 40.86%, Test: 41.00%
Epoch: 825, Loss: 1.2964, Train: 40.52%, Valid: 40.02%, Test: 40.14%
Epoch: 850, Loss: 1.3226, Train: 40.94%, Valid: 40.56%, Test: 40.57%
Epoch: 875, Loss: 1.2911, Train: 40.09%, Valid: 39.65%, Test: 39.73%
Epoch: 900, Loss: 1.3057, Train: 38.34%, Valid: 38.17%, Test: 38.04%
Epoch: 925, Loss: 1.3060, Train: 41.23%, Valid: 40.84%, Test: 41.01%
Epoch: 950, Loss: 1.3068, Train: 38.94%, Valid: 38.63%, Test: 38.46%
Epoch: 975, Loss: 1.3081, Train: 40.77%, Valid: 40.32%, Test: 40.60%
Run 01:
Highest Train: 44.78
Highest Valid: 44.58
  Final Train: 44.78
   Final Test: 44.43
All runs:
Highest Train: 44.78, nan
Highest Valid: 44.58, nan
  Final Train: 44.78, nan
   Final Test: 44.43, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 108.5830, Train: 27.48%, Valid: 27.12%, Test: 27.37%
Epoch: 25, Loss: 2.7236, Train: 24.98%, Valid: 24.73%, Test: 25.07%
Epoch: 50, Loss: 1.8354, Train: 21.17%, Valid: 21.55%, Test: 21.74%
Epoch: 75, Loss: 1.6305, Train: 23.45%, Valid: 23.59%, Test: 24.26%
Epoch: 100, Loss: 1.5910, Train: 23.99%, Valid: 24.24%, Test: 24.84%
Epoch: 125, Loss: 1.6294, Train: 24.11%, Valid: 24.27%, Test: 24.84%
Epoch: 150, Loss: 1.5147, Train: 22.84%, Valid: 23.29%, Test: 23.62%
Epoch: 175, Loss: 1.5614, Train: 23.30%, Valid: 23.39%, Test: 24.01%
Epoch: 200, Loss: 1.5062, Train: 23.77%, Valid: 23.98%, Test: 24.38%
Epoch: 225, Loss: 1.4940, Train: 24.39%, Valid: 24.51%, Test: 25.00%
Epoch: 250, Loss: 1.4984, Train: 31.14%, Valid: 31.21%, Test: 31.76%
Epoch: 275, Loss: 1.5053, Train: 27.14%, Valid: 27.27%, Test: 27.25%
Epoch: 300, Loss: 1.4719, Train: 27.63%, Valid: 27.79%, Test: 27.85%
Epoch: 325, Loss: 1.4749, Train: 28.25%, Valid: 28.26%, Test: 28.92%
Epoch: 350, Loss: 1.5104, Train: 31.15%, Valid: 31.25%, Test: 31.87%
Epoch: 375, Loss: 1.4694, Train: 28.50%, Valid: 28.42%, Test: 28.51%
Epoch: 400, Loss: 1.5106, Train: 35.50%, Valid: 35.52%, Test: 35.84%
Epoch: 425, Loss: 1.4651, Train: 33.81%, Valid: 33.65%, Test: 34.15%
Epoch: 450, Loss: 1.4710, Train: 36.19%, Valid: 36.12%, Test: 36.50%
Epoch: 475, Loss: 1.4560, Train: 38.15%, Valid: 38.01%, Test: 38.30%
Epoch: 500, Loss: 1.4651, Train: 36.09%, Valid: 36.05%, Test: 36.51%
Epoch: 525, Loss: 1.4512, Train: 38.42%, Valid: 38.30%, Test: 38.42%
Epoch: 550, Loss: 1.4490, Train: 36.12%, Valid: 36.07%, Test: 36.37%
Epoch: 575, Loss: 1.4653, Train: 35.34%, Valid: 35.39%, Test: 35.75%
Epoch: 600, Loss: 1.4531, Train: 33.76%, Valid: 33.37%, Test: 33.44%
Epoch: 625, Loss: 1.4433, Train: 36.43%, Valid: 36.16%, Test: 36.72%
Epoch: 650, Loss: 1.4369, Train: 24.87%, Valid: 24.59%, Test: 24.73%
Epoch: 675, Loss: 1.4261, Train: 36.42%, Valid: 36.09%, Test: 36.76%
Epoch: 700, Loss: 1.4414, Train: 25.78%, Valid: 25.51%, Test: 25.60%
Epoch: 725, Loss: 1.4330, Train: 25.72%, Valid: 25.41%, Test: 25.83%
Epoch: 750, Loss: 1.4300, Train: 23.97%, Valid: 23.76%, Test: 23.75%
Epoch: 775, Loss: 1.4524, Train: 23.17%, Valid: 23.07%, Test: 23.00%
Epoch: 800, Loss: 1.4198, Train: 22.53%, Valid: 22.53%, Test: 22.53%
Epoch: 825, Loss: 1.4344, Train: 22.78%, Valid: 22.56%, Test: 22.47%
Epoch: 850, Loss: 1.4121, Train: 22.27%, Valid: 22.12%, Test: 22.11%
Epoch: 875, Loss: 1.4153, Train: 22.59%, Valid: 22.34%, Test: 22.24%
Epoch: 900, Loss: 1.4104, Train: 22.16%, Valid: 21.89%, Test: 22.06%
Epoch: 925, Loss: 1.4031, Train: 22.10%, Valid: 21.76%, Test: 21.99%
Epoch: 950, Loss: 1.4022, Train: 22.10%, Valid: 21.88%, Test: 22.03%
Epoch: 975, Loss: 1.4209, Train: 23.07%, Valid: 22.88%, Test: 22.76%
Run 01:
Highest Train: 39.00
Highest Valid: 38.92
  Final Train: 39.00
   Final Test: 38.90
All runs:
Highest Train: 39.00, nan
Highest Valid: 38.92, nan
  Final Train: 39.00, nan
   Final Test: 38.90, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6002, Train: 25.98%, Valid: 25.66%, Test: 26.11%
Epoch: 25, Loss: 1.4743, Train: 35.52%, Valid: 35.16%, Test: 35.33%
Epoch: 50, Loss: 1.4479, Train: 37.10%, Valid: 36.77%, Test: 36.78%
Epoch: 75, Loss: 1.4085, Train: 38.85%, Valid: 38.40%, Test: 38.52%
Epoch: 100, Loss: 1.3796, Train: 40.27%, Valid: 39.76%, Test: 39.82%
Epoch: 125, Loss: 1.3425, Train: 42.73%, Valid: 42.01%, Test: 41.99%
Epoch: 150, Loss: 1.3019, Train: 44.26%, Valid: 43.06%, Test: 42.99%
Epoch: 175, Loss: 1.3143, Train: 43.26%, Valid: 42.24%, Test: 42.33%
Epoch: 200, Loss: 1.2465, Train: 46.38%, Valid: 44.74%, Test: 44.74%
Epoch: 225, Loss: 1.2393, Train: 47.38%, Valid: 45.39%, Test: 45.45%
Epoch: 250, Loss: 1.1906, Train: 49.30%, Valid: 46.37%, Test: 46.54%
Epoch: 275, Loss: 1.1539, Train: 50.84%, Valid: 47.13%, Test: 47.11%
Epoch: 300, Loss: 1.1312, Train: 51.93%, Valid: 47.29%, Test: 47.15%
Epoch: 325, Loss: 1.1017, Train: 53.04%, Valid: 47.24%, Test: 47.64%
Epoch: 350, Loss: 1.0610, Train: 55.01%, Valid: 47.90%, Test: 48.26%
Epoch: 375, Loss: 1.1115, Train: 53.47%, Valid: 47.49%, Test: 47.43%
Epoch: 400, Loss: 1.0162, Train: 56.82%, Valid: 48.40%, Test: 48.52%
Epoch: 425, Loss: 0.9814, Train: 58.24%, Valid: 48.20%, Test: 48.44%
Epoch: 450, Loss: 0.9555, Train: 59.34%, Valid: 48.66%, Test: 49.10%
Epoch: 475, Loss: 0.9446, Train: 60.04%, Valid: 48.36%, Test: 48.61%
Epoch: 500, Loss: 0.9167, Train: 61.30%, Valid: 48.46%, Test: 48.78%
Epoch: 525, Loss: 1.0452, Train: 56.38%, Valid: 47.34%, Test: 47.61%
Epoch: 550, Loss: 0.9078, Train: 61.92%, Valid: 48.60%, Test: 48.85%
Epoch: 575, Loss: 0.8684, Train: 63.44%, Valid: 48.43%, Test: 48.77%
Epoch: 600, Loss: 0.8669, Train: 62.32%, Valid: 47.52%, Test: 47.69%
Epoch: 625, Loss: 0.8460, Train: 64.36%, Valid: 47.91%, Test: 48.22%
Epoch: 650, Loss: 0.8997, Train: 62.52%, Valid: 47.45%, Test: 47.77%
Epoch: 675, Loss: 0.8004, Train: 66.19%, Valid: 48.15%, Test: 48.18%
Epoch: 700, Loss: 0.8070, Train: 66.48%, Valid: 48.12%, Test: 48.44%
Epoch: 725, Loss: 0.7837, Train: 66.48%, Valid: 47.98%, Test: 48.31%
Epoch: 750, Loss: 0.8008, Train: 66.49%, Valid: 47.72%, Test: 47.86%
Epoch: 775, Loss: 0.8468, Train: 64.40%, Valid: 47.53%, Test: 47.63%
Epoch: 800, Loss: 0.7803, Train: 67.52%, Valid: 48.26%, Test: 48.42%
Epoch: 825, Loss: 0.7240, Train: 69.25%, Valid: 47.98%, Test: 48.29%
Epoch: 850, Loss: 0.7193, Train: 70.10%, Valid: 47.44%, Test: 47.59%
Epoch: 875, Loss: 0.7451, Train: 69.00%, Valid: 47.95%, Test: 48.11%
Epoch: 900, Loss: 0.7095, Train: 70.90%, Valid: 47.90%, Test: 47.94%
Epoch: 925, Loss: 0.7006, Train: 70.84%, Valid: 47.44%, Test: 47.66%
Epoch: 950, Loss: 0.7466, Train: 67.99%, Valid: 47.33%, Test: 47.36%
Epoch: 975, Loss: 0.6733, Train: 72.14%, Valid: 47.41%, Test: 47.60%
Run 01:
Highest Train: 72.27
Highest Valid: 48.79
  Final Train: 62.76
   Final Test: 48.93
All runs:
Highest Train: 72.27, nan
Highest Valid: 48.79, nan
  Final Train: 62.76, nan
   Final Test: 48.93, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.8334, Train: 22.98%, Valid: 23.16%, Test: 23.26%
Epoch: 25, Loss: 1.4646, Train: 32.23%, Valid: 31.69%, Test: 32.52%
Epoch: 50, Loss: 1.4446, Train: 34.30%, Valid: 33.83%, Test: 34.69%
Epoch: 75, Loss: 1.3870, Train: 40.78%, Valid: 40.65%, Test: 40.89%
Epoch: 100, Loss: 1.3634, Train: 42.09%, Valid: 41.82%, Test: 41.95%
Epoch: 125, Loss: 1.3573, Train: 42.06%, Valid: 41.90%, Test: 42.11%
Epoch: 150, Loss: 1.3626, Train: 42.34%, Valid: 42.09%, Test: 42.24%
Epoch: 175, Loss: 1.3358, Train: 43.23%, Valid: 42.78%, Test: 43.07%
Epoch: 200, Loss: 1.3201, Train: 43.72%, Valid: 43.34%, Test: 43.61%
Epoch: 225, Loss: 1.3089, Train: 44.41%, Valid: 43.99%, Test: 44.26%
Epoch: 250, Loss: 1.3019, Train: 44.61%, Valid: 44.38%, Test: 44.53%
Epoch: 275, Loss: 1.3125, Train: 44.27%, Valid: 43.99%, Test: 44.12%
Epoch: 300, Loss: 1.2949, Train: 44.74%, Valid: 44.45%, Test: 44.53%
Epoch: 325, Loss: 1.3091, Train: 43.46%, Valid: 43.39%, Test: 43.43%
Epoch: 350, Loss: 1.2850, Train: 45.22%, Valid: 44.86%, Test: 45.04%
Epoch: 375, Loss: 1.2915, Train: 44.82%, Valid: 44.76%, Test: 44.85%
Epoch: 400, Loss: 1.3244, Train: 44.45%, Valid: 44.14%, Test: 44.43%
Epoch: 425, Loss: 1.2978, Train: 44.50%, Valid: 44.45%, Test: 44.66%
Epoch: 450, Loss: 1.2699, Train: 45.74%, Valid: 45.50%, Test: 45.58%
Epoch: 475, Loss: 1.2701, Train: 45.77%, Valid: 45.48%, Test: 45.57%
Epoch: 500, Loss: 1.2685, Train: 46.04%, Valid: 45.69%, Test: 45.81%
Epoch: 525, Loss: 1.2674, Train: 46.27%, Valid: 45.94%, Test: 45.88%
Epoch: 550, Loss: 1.2580, Train: 46.14%, Valid: 45.84%, Test: 45.87%
Epoch: 575, Loss: 1.2840, Train: 46.52%, Valid: 45.99%, Test: 45.86%
Epoch: 600, Loss: 1.2588, Train: 46.58%, Valid: 46.09%, Test: 46.20%
Epoch: 625, Loss: 1.2570, Train: 46.56%, Valid: 46.11%, Test: 46.05%
Epoch: 650, Loss: 1.2908, Train: 45.57%, Valid: 45.15%, Test: 45.08%
Epoch: 675, Loss: 1.2580, Train: 46.69%, Valid: 46.19%, Test: 46.03%
Epoch: 700, Loss: 1.2526, Train: 46.47%, Valid: 46.07%, Test: 46.17%
Epoch: 725, Loss: 1.2351, Train: 47.56%, Valid: 46.74%, Test: 47.03%
Epoch: 750, Loss: 1.2522, Train: 46.85%, Valid: 46.28%, Test: 46.50%
Epoch: 775, Loss: 1.2378, Train: 47.27%, Valid: 46.58%, Test: 46.91%
Epoch: 800, Loss: 1.2339, Train: 46.82%, Valid: 46.16%, Test: 46.39%
Epoch: 825, Loss: 1.2237, Train: 47.70%, Valid: 46.89%, Test: 47.08%
Epoch: 850, Loss: 1.2718, Train: 47.37%, Valid: 46.55%, Test: 46.91%
Epoch: 875, Loss: 1.2255, Train: 47.91%, Valid: 47.03%, Test: 47.43%
Epoch: 900, Loss: 1.2661, Train: 47.19%, Valid: 46.22%, Test: 46.25%
Epoch: 925, Loss: 1.2456, Train: 47.04%, Valid: 46.21%, Test: 46.59%
Epoch: 950, Loss: 1.2188, Train: 47.55%, Valid: 46.85%, Test: 46.93%
Epoch: 975, Loss: 1.2190, Train: 48.06%, Valid: 47.32%, Test: 47.44%
Run 01:
Highest Train: 48.31
Highest Valid: 47.56
  Final Train: 48.31
   Final Test: 47.64
All runs:
Highest Train: 48.31, nan
Highest Valid: 47.56, nan
  Final Train: 48.31, nan
   Final Test: 47.64, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 6.8163, Train: 28.05%, Valid: 27.68%, Test: 27.85%
Epoch: 25, Loss: 1.5774, Train: 28.96%, Valid: 28.62%, Test: 28.88%
Epoch: 50, Loss: 1.4694, Train: 29.00%, Valid: 28.73%, Test: 29.15%
Epoch: 75, Loss: 1.4585, Train: 29.30%, Valid: 28.88%, Test: 29.31%
Epoch: 100, Loss: 1.4725, Train: 28.72%, Valid: 28.43%, Test: 28.71%
Epoch: 125, Loss: 1.4261, Train: 29.50%, Valid: 29.22%, Test: 29.35%
Epoch: 150, Loss: 1.4033, Train: 35.76%, Valid: 35.62%, Test: 35.90%
Epoch: 175, Loss: 1.4023, Train: 39.41%, Valid: 39.22%, Test: 39.21%
Epoch: 200, Loss: 1.3795, Train: 39.79%, Valid: 39.67%, Test: 39.52%
Epoch: 225, Loss: 1.4107, Train: 39.12%, Valid: 39.02%, Test: 38.92%
Epoch: 250, Loss: 1.3824, Train: 39.76%, Valid: 39.47%, Test: 39.47%
Epoch: 275, Loss: 1.3791, Train: 38.23%, Valid: 37.94%, Test: 38.08%
Epoch: 300, Loss: 1.4679, Train: 31.41%, Valid: 31.38%, Test: 31.90%
Epoch: 325, Loss: 1.4216, Train: 36.60%, Valid: 36.24%, Test: 36.42%
Epoch: 350, Loss: 1.4057, Train: 37.32%, Valid: 36.93%, Test: 37.11%
Epoch: 375, Loss: 1.3949, Train: 37.66%, Valid: 37.35%, Test: 37.38%
Epoch: 400, Loss: 1.3861, Train: 38.03%, Valid: 37.71%, Test: 37.88%
Epoch: 425, Loss: 1.3782, Train: 38.50%, Valid: 38.15%, Test: 38.28%
Epoch: 450, Loss: 1.3710, Train: 38.93%, Valid: 38.67%, Test: 38.86%
Epoch: 475, Loss: 1.3647, Train: 39.51%, Valid: 39.16%, Test: 39.34%
Epoch: 500, Loss: 1.3631, Train: 39.47%, Valid: 39.16%, Test: 39.45%
Epoch: 525, Loss: 1.3549, Train: 39.84%, Valid: 39.54%, Test: 39.85%
Epoch: 550, Loss: 1.3692, Train: 39.56%, Valid: 39.11%, Test: 39.50%
Epoch: 575, Loss: 1.3572, Train: 40.27%, Valid: 39.91%, Test: 40.29%
Epoch: 600, Loss: 1.3469, Train: 40.63%, Valid: 40.30%, Test: 40.69%
Epoch: 625, Loss: 1.3635, Train: 40.79%, Valid: 40.45%, Test: 40.95%
Epoch: 650, Loss: 1.3449, Train: 40.52%, Valid: 40.17%, Test: 40.52%
Epoch: 675, Loss: 1.3371, Train: 40.81%, Valid: 40.52%, Test: 40.86%
Epoch: 700, Loss: 1.3749, Train: 39.46%, Valid: 39.02%, Test: 39.49%
Epoch: 725, Loss: 1.3611, Train: 40.48%, Valid: 40.01%, Test: 40.44%
Epoch: 750, Loss: 1.3530, Train: 40.69%, Valid: 40.32%, Test: 40.69%
Epoch: 775, Loss: 1.3467, Train: 41.03%, Valid: 40.61%, Test: 40.99%
Epoch: 800, Loss: 1.3448, Train: 40.86%, Valid: 40.50%, Test: 40.99%
Epoch: 825, Loss: 1.3569, Train: 40.72%, Valid: 40.52%, Test: 40.75%
Epoch: 850, Loss: 1.3430, Train: 41.13%, Valid: 40.78%, Test: 41.13%
Epoch: 875, Loss: 1.3895, Train: 40.57%, Valid: 40.19%, Test: 40.66%
Epoch: 900, Loss: 1.3548, Train: 39.54%, Valid: 39.19%, Test: 39.44%
Epoch: 925, Loss: 1.3405, Train: 41.19%, Valid: 40.79%, Test: 41.30%
Epoch: 950, Loss: 1.3321, Train: 41.10%, Valid: 40.88%, Test: 41.34%
Epoch: 975, Loss: 1.3623, Train: 39.58%, Valid: 39.19%, Test: 39.50%
Run 01:
Highest Train: 41.65
Highest Valid: 41.30
  Final Train: 41.65
   Final Test: 41.68
All runs:
Highest Train: 41.65, nan
Highest Valid: 41.30, nan
  Final Train: 41.65, nan
   Final Test: 41.68, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.5970, Train: 24.73%, Valid: 24.63%, Test: 24.73%
Epoch: 25, Loss: 1.4780, Train: 35.51%, Valid: 34.93%, Test: 35.17%
Epoch: 50, Loss: 1.4482, Train: 37.73%, Valid: 37.32%, Test: 37.56%
Epoch: 75, Loss: 1.4131, Train: 39.20%, Valid: 38.57%, Test: 38.78%
Epoch: 100, Loss: 1.3875, Train: 40.73%, Valid: 40.18%, Test: 40.35%
Epoch: 125, Loss: 1.3633, Train: 41.86%, Valid: 41.03%, Test: 41.23%
Epoch: 150, Loss: 1.3306, Train: 41.65%, Valid: 40.79%, Test: 40.64%
Epoch: 175, Loss: 1.3150, Train: 43.93%, Valid: 42.66%, Test: 42.62%
Epoch: 200, Loss: 1.3144, Train: 44.04%, Valid: 42.64%, Test: 42.55%
Epoch: 225, Loss: 1.2851, Train: 45.45%, Valid: 42.97%, Test: 43.24%
Epoch: 250, Loss: 1.2487, Train: 47.05%, Valid: 44.22%, Test: 44.55%
Epoch: 275, Loss: 1.2827, Train: 45.57%, Valid: 43.40%, Test: 43.29%
Epoch: 300, Loss: 1.2376, Train: 47.64%, Valid: 44.31%, Test: 44.73%
Epoch: 325, Loss: 1.3890, Train: 42.00%, Valid: 39.81%, Test: 39.85%
Epoch: 350, Loss: 1.2225, Train: 48.43%, Valid: 44.81%, Test: 45.29%
Epoch: 375, Loss: 1.1724, Train: 50.43%, Valid: 45.50%, Test: 45.57%
Epoch: 400, Loss: 1.1502, Train: 51.40%, Valid: 45.95%, Test: 46.21%
Epoch: 425, Loss: 1.1741, Train: 50.24%, Valid: 44.84%, Test: 44.85%
Epoch: 450, Loss: 1.0846, Train: 53.96%, Valid: 46.70%, Test: 46.96%
Epoch: 475, Loss: 1.0763, Train: 54.45%, Valid: 46.78%, Test: 47.18%
Epoch: 500, Loss: 1.1112, Train: 53.50%, Valid: 46.22%, Test: 46.70%
Epoch: 525, Loss: 1.0184, Train: 56.84%, Valid: 47.13%, Test: 47.67%
Epoch: 550, Loss: 1.0161, Train: 57.27%, Valid: 47.21%, Test: 47.67%
Epoch: 575, Loss: 1.0851, Train: 54.23%, Valid: 46.35%, Test: 46.29%
Epoch: 600, Loss: 0.9691, Train: 59.00%, Valid: 47.34%, Test: 47.91%
Epoch: 625, Loss: 1.0680, Train: 54.99%, Valid: 45.82%, Test: 46.37%
Epoch: 650, Loss: 0.9491, Train: 59.87%, Valid: 47.54%, Test: 47.98%
Epoch: 675, Loss: 0.9752, Train: 58.34%, Valid: 46.12%, Test: 46.38%
Epoch: 700, Loss: 0.8990, Train: 61.97%, Valid: 47.43%, Test: 48.00%
Epoch: 725, Loss: 0.9295, Train: 61.16%, Valid: 46.90%, Test: 47.82%
Epoch: 750, Loss: 0.8788, Train: 63.01%, Valid: 47.16%, Test: 47.81%
Epoch: 775, Loss: 0.9201, Train: 61.62%, Valid: 47.04%, Test: 47.66%
Epoch: 800, Loss: 0.8523, Train: 64.01%, Valid: 47.11%, Test: 47.97%
Epoch: 825, Loss: 1.0483, Train: 57.35%, Valid: 45.78%, Test: 46.39%
Epoch: 850, Loss: 0.8706, Train: 63.41%, Valid: 47.42%, Test: 48.22%
Epoch: 875, Loss: 0.8208, Train: 65.55%, Valid: 47.22%, Test: 47.78%
Epoch: 900, Loss: 0.8394, Train: 64.92%, Valid: 47.38%, Test: 47.85%
Epoch: 925, Loss: 0.8068, Train: 64.55%, Valid: 45.71%, Test: 46.60%
Epoch: 950, Loss: 0.8109, Train: 65.89%, Valid: 46.89%, Test: 47.28%
Epoch: 975, Loss: 0.9772, Train: 62.66%, Valid: 45.79%, Test: 46.11%
Run 01:
Highest Train: 66.92
Highest Valid: 47.75
  Final Train: 62.58
   Final Test: 48.07
All runs:
Highest Train: 66.92, nan
Highest Valid: 47.75, nan
  Final Train: 62.58, nan
   Final Test: 48.07, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.7175, Train: 13.59%, Valid: 13.59%, Test: 13.40%
Epoch: 25, Loss: 1.4746, Train: 28.36%, Valid: 28.11%, Test: 28.31%
Epoch: 50, Loss: 1.4750, Train: 33.78%, Valid: 33.70%, Test: 34.44%
Epoch: 75, Loss: 1.4326, Train: 30.97%, Valid: 30.68%, Test: 31.65%
Epoch: 100, Loss: 1.4167, Train: 34.25%, Valid: 34.01%, Test: 34.83%
Epoch: 125, Loss: 1.3786, Train: 39.85%, Valid: 39.61%, Test: 39.69%
Epoch: 150, Loss: 1.3424, Train: 42.48%, Valid: 42.34%, Test: 42.41%
Epoch: 175, Loss: 1.3378, Train: 42.92%, Valid: 42.71%, Test: 42.90%
Epoch: 200, Loss: 1.3224, Train: 43.41%, Valid: 43.01%, Test: 43.45%
Epoch: 225, Loss: 1.3164, Train: 43.31%, Valid: 43.04%, Test: 43.33%
Epoch: 250, Loss: 1.3012, Train: 44.83%, Valid: 44.32%, Test: 44.47%
Epoch: 275, Loss: 1.3097, Train: 43.42%, Valid: 43.18%, Test: 43.44%
Epoch: 300, Loss: 1.2856, Train: 45.39%, Valid: 44.83%, Test: 44.95%
Epoch: 325, Loss: 1.2839, Train: 45.48%, Valid: 44.90%, Test: 44.98%
Epoch: 350, Loss: 1.2772, Train: 45.30%, Valid: 44.85%, Test: 44.95%
Epoch: 375, Loss: 1.2763, Train: 45.67%, Valid: 45.06%, Test: 45.26%
Epoch: 400, Loss: 1.2676, Train: 46.12%, Valid: 45.54%, Test: 45.70%
Epoch: 425, Loss: 1.2896, Train: 44.97%, Valid: 44.52%, Test: 44.77%
Epoch: 450, Loss: 1.2673, Train: 46.18%, Valid: 45.42%, Test: 45.62%
Epoch: 475, Loss: 1.2588, Train: 46.54%, Valid: 45.84%, Test: 46.35%
Epoch: 500, Loss: 1.2801, Train: 46.31%, Valid: 45.61%, Test: 45.98%
Epoch: 525, Loss: 1.2632, Train: 46.50%, Valid: 45.72%, Test: 46.11%
Epoch: 550, Loss: 1.2626, Train: 46.01%, Valid: 45.44%, Test: 45.83%
Epoch: 575, Loss: 1.2454, Train: 47.01%, Valid: 46.17%, Test: 46.46%
Epoch: 600, Loss: 1.2533, Train: 46.41%, Valid: 45.86%, Test: 46.23%
Epoch: 625, Loss: 1.2768, Train: 46.63%, Valid: 46.07%, Test: 46.34%
Epoch: 650, Loss: 1.2424, Train: 47.38%, Valid: 46.40%, Test: 46.83%
Epoch: 675, Loss: 1.2642, Train: 43.81%, Valid: 43.34%, Test: 43.65%
Epoch: 700, Loss: 1.2415, Train: 47.29%, Valid: 46.53%, Test: 46.89%
Epoch: 725, Loss: 1.2684, Train: 46.59%, Valid: 46.00%, Test: 46.29%
Epoch: 750, Loss: 1.2298, Train: 47.51%, Valid: 46.69%, Test: 47.36%
Epoch: 775, Loss: 1.2372, Train: 47.44%, Valid: 46.69%, Test: 47.31%
Epoch: 800, Loss: 1.2723, Train: 46.48%, Valid: 45.98%, Test: 46.31%
Epoch: 825, Loss: 1.2297, Train: 47.78%, Valid: 46.90%, Test: 47.40%
Epoch: 850, Loss: 1.2370, Train: 47.38%, Valid: 46.68%, Test: 46.96%
Epoch: 875, Loss: 1.2157, Train: 48.15%, Valid: 47.43%, Test: 47.71%
Epoch: 900, Loss: 1.2579, Train: 46.46%, Valid: 45.79%, Test: 46.23%
Epoch: 925, Loss: 1.2249, Train: 47.88%, Valid: 46.90%, Test: 47.44%
Epoch: 950, Loss: 1.2301, Train: 47.38%, Valid: 46.58%, Test: 46.86%
Epoch: 975, Loss: 1.2127, Train: 48.17%, Valid: 47.29%, Test: 47.84%
Run 01:
Highest Train: 48.52
Highest Valid: 47.55
  Final Train: 48.34
   Final Test: 47.82
All runs:
Highest Train: 48.52, nan
Highest Valid: 47.55, nan
  Final Train: 48.34, nan
   Final Test: 47.82, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 7.1096, Train: 13.43%, Valid: 13.47%, Test: 13.50%
Epoch: 25, Loss: 1.5299, Train: 31.33%, Valid: 30.66%, Test: 31.56%
Epoch: 50, Loss: 1.4527, Train: 29.81%, Valid: 29.43%, Test: 29.85%
Epoch: 75, Loss: 1.4407, Train: 29.30%, Valid: 29.04%, Test: 29.35%
Epoch: 100, Loss: 1.5049, Train: 30.54%, Valid: 30.19%, Test: 30.70%
Epoch: 125, Loss: 1.4478, Train: 29.43%, Valid: 29.01%, Test: 29.32%
Epoch: 150, Loss: 1.4386, Train: 29.44%, Valid: 28.94%, Test: 29.34%
Epoch: 175, Loss: 1.4278, Train: 29.58%, Valid: 29.16%, Test: 29.33%
Epoch: 200, Loss: 1.4326, Train: 29.86%, Valid: 29.45%, Test: 29.66%
Epoch: 225, Loss: 1.4317, Train: 29.90%, Valid: 29.51%, Test: 29.70%
Epoch: 250, Loss: 1.4151, Train: 29.65%, Valid: 29.43%, Test: 29.85%
Epoch: 275, Loss: 1.4578, Train: 33.45%, Valid: 33.04%, Test: 33.42%
Epoch: 300, Loss: 1.4104, Train: 34.96%, Valid: 34.68%, Test: 35.08%
Epoch: 325, Loss: 1.3997, Train: 36.56%, Valid: 36.33%, Test: 36.53%
Epoch: 350, Loss: 1.3987, Train: 37.62%, Valid: 37.43%, Test: 37.39%
Epoch: 375, Loss: 1.4227, Train: 35.97%, Valid: 35.63%, Test: 36.04%
Epoch: 400, Loss: 1.3999, Train: 37.85%, Valid: 37.42%, Test: 37.63%
Epoch: 425, Loss: 1.3906, Train: 39.58%, Valid: 39.44%, Test: 39.48%
Epoch: 450, Loss: 1.3833, Train: 40.17%, Valid: 39.95%, Test: 39.96%
Epoch: 475, Loss: 1.6949, Train: 37.64%, Valid: 37.55%, Test: 37.82%
Epoch: 500, Loss: 1.4226, Train: 38.55%, Valid: 38.35%, Test: 38.32%
Epoch: 525, Loss: 1.3940, Train: 39.31%, Valid: 39.12%, Test: 39.02%
Epoch: 550, Loss: 1.3737, Train: 40.07%, Valid: 40.04%, Test: 39.97%
Epoch: 575, Loss: 1.3632, Train: 40.53%, Valid: 40.50%, Test: 40.47%
Epoch: 600, Loss: 1.3644, Train: 41.71%, Valid: 41.49%, Test: 41.45%
Epoch: 625, Loss: 1.4518, Train: 29.58%, Valid: 29.78%, Test: 29.27%
Epoch: 650, Loss: 2.0198, Train: 32.71%, Valid: 32.53%, Test: 32.55%
Epoch: 675, Loss: 1.7923, Train: 32.68%, Valid: 32.70%, Test: 33.35%
Epoch: 700, Loss: 1.4765, Train: 34.97%, Valid: 34.83%, Test: 34.98%
Epoch: 725, Loss: 1.4527, Train: 35.18%, Valid: 34.98%, Test: 35.19%
Epoch: 750, Loss: 1.4183, Train: 36.21%, Valid: 35.90%, Test: 36.24%
Epoch: 775, Loss: 1.4291, Train: 37.39%, Valid: 37.13%, Test: 37.68%
Epoch: 800, Loss: 1.3941, Train: 37.72%, Valid: 37.45%, Test: 37.84%
Epoch: 825, Loss: 1.3838, Train: 39.02%, Valid: 38.86%, Test: 39.21%
Epoch: 850, Loss: 1.3952, Train: 38.72%, Valid: 38.46%, Test: 38.86%
Epoch: 875, Loss: 1.3752, Train: 39.56%, Valid: 39.43%, Test: 39.78%
Epoch: 900, Loss: 1.3677, Train: 39.86%, Valid: 39.69%, Test: 40.17%
Epoch: 925, Loss: 1.3645, Train: 39.95%, Valid: 39.60%, Test: 40.23%
Epoch: 950, Loss: 1.3674, Train: 39.94%, Valid: 39.62%, Test: 40.30%
Epoch: 975, Loss: 1.3619, Train: 40.61%, Valid: 40.25%, Test: 40.82%
Run 01:
Highest Train: 41.89
Highest Valid: 41.80
  Final Train: 41.89
   Final Test: 41.64
All runs:
Highest Train: 41.89, nan
Highest Valid: 41.80, nan
  Final Train: 41.89, nan
   Final Test: 41.64, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6029, Train: 28.51%, Valid: 28.28%, Test: 28.60%
Epoch: 25, Loss: 1.4994, Train: 35.35%, Valid: 34.99%, Test: 35.23%
Epoch: 50, Loss: 1.4617, Train: 36.42%, Valid: 35.96%, Test: 36.33%
Epoch: 75, Loss: 1.4334, Train: 38.66%, Valid: 38.08%, Test: 38.30%
Epoch: 100, Loss: 1.4080, Train: 40.72%, Valid: 39.96%, Test: 39.96%
Epoch: 125, Loss: 1.3901, Train: 41.19%, Valid: 40.06%, Test: 40.21%
Epoch: 150, Loss: 1.3751, Train: 42.84%, Valid: 41.93%, Test: 42.12%
Epoch: 175, Loss: 1.3682, Train: 43.12%, Valid: 42.26%, Test: 42.29%
Epoch: 200, Loss: 1.3595, Train: 43.72%, Valid: 42.65%, Test: 42.74%
Epoch: 225, Loss: 1.3438, Train: 43.94%, Valid: 42.83%, Test: 43.02%
Epoch: 250, Loss: 1.3317, Train: 44.48%, Valid: 43.39%, Test: 43.44%
Epoch: 275, Loss: 1.3226, Train: 44.57%, Valid: 43.15%, Test: 43.34%
Epoch: 300, Loss: 1.3122, Train: 45.01%, Valid: 43.43%, Test: 43.78%
Epoch: 325, Loss: 1.3071, Train: 45.39%, Valid: 44.25%, Test: 44.46%
Epoch: 350, Loss: 1.2988, Train: 45.14%, Valid: 44.03%, Test: 43.86%
Epoch: 375, Loss: 1.2979, Train: 45.06%, Valid: 43.74%, Test: 43.79%
Epoch: 400, Loss: 1.2875, Train: 45.77%, Valid: 44.65%, Test: 44.60%
Epoch: 425, Loss: 1.2810, Train: 45.77%, Valid: 44.37%, Test: 44.49%
Epoch: 450, Loss: 1.2730, Train: 46.05%, Valid: 44.67%, Test: 44.66%
Epoch: 475, Loss: 1.2835, Train: 46.10%, Valid: 44.90%, Test: 44.74%
Epoch: 500, Loss: 1.2774, Train: 45.58%, Valid: 44.26%, Test: 44.35%
Epoch: 525, Loss: 1.2681, Train: 46.32%, Valid: 44.71%, Test: 44.76%
Epoch: 550, Loss: 1.2601, Train: 47.33%, Valid: 45.61%, Test: 45.61%
Epoch: 575, Loss: 1.2644, Train: 46.47%, Valid: 44.74%, Test: 44.79%
Epoch: 600, Loss: 1.2605, Train: 47.39%, Valid: 45.91%, Test: 45.98%
Epoch: 625, Loss: 1.2648, Train: 45.33%, Valid: 43.95%, Test: 43.82%
Epoch: 650, Loss: 1.2614, Train: 47.46%, Valid: 45.68%, Test: 45.70%
Epoch: 675, Loss: 1.2502, Train: 47.56%, Valid: 45.52%, Test: 45.61%
Epoch: 700, Loss: 1.2566, Train: 47.59%, Valid: 45.57%, Test: 45.73%
Epoch: 725, Loss: 1.2586, Train: 47.64%, Valid: 45.78%, Test: 45.88%
Epoch: 750, Loss: 1.2585, Train: 47.19%, Valid: 45.40%, Test: 45.47%
Epoch: 775, Loss: 1.2523, Train: 47.27%, Valid: 45.30%, Test: 45.38%
Epoch: 800, Loss: 1.2456, Train: 47.88%, Valid: 45.72%, Test: 45.74%
Epoch: 825, Loss: 1.2588, Train: 47.29%, Valid: 45.34%, Test: 45.41%
Epoch: 850, Loss: 1.2356, Train: 46.91%, Valid: 44.79%, Test: 44.85%
Epoch: 875, Loss: 1.2331, Train: 45.85%, Valid: 44.09%, Test: 44.25%
Epoch: 900, Loss: 1.2397, Train: 47.81%, Valid: 45.47%, Test: 45.56%
Epoch: 925, Loss: 1.2316, Train: 47.94%, Valid: 45.56%, Test: 45.79%
Epoch: 950, Loss: 1.2410, Train: 47.42%, Valid: 45.36%, Test: 45.72%
Epoch: 975, Loss: 1.2406, Train: 47.00%, Valid: 44.79%, Test: 44.71%
Run 01:
Highest Train: 48.67
Highest Valid: 46.42
  Final Train: 48.67
   Final Test: 46.44
All runs:
Highest Train: 48.67, nan
Highest Valid: 46.42, nan
  Final Train: 48.67, nan
   Final Test: 46.44, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.9699, Train: 28.89%, Valid: 28.79%, Test: 28.58%
Epoch: 25, Loss: 1.5509, Train: 29.27%, Valid: 29.10%, Test: 29.50%
Epoch: 50, Loss: 1.4619, Train: 30.84%, Valid: 30.58%, Test: 31.27%
Epoch: 75, Loss: 1.4480, Train: 28.98%, Valid: 28.55%, Test: 29.51%
Epoch: 100, Loss: 1.4600, Train: 29.68%, Valid: 29.45%, Test: 29.87%
Epoch: 125, Loss: 1.4526, Train: 29.15%, Valid: 28.81%, Test: 29.68%
Epoch: 150, Loss: 1.4411, Train: 31.72%, Valid: 31.39%, Test: 31.99%
Epoch: 175, Loss: 1.4300, Train: 34.92%, Valid: 34.30%, Test: 34.95%
Epoch: 200, Loss: 1.4246, Train: 34.72%, Valid: 34.13%, Test: 34.87%
Epoch: 225, Loss: 1.4252, Train: 30.80%, Valid: 30.38%, Test: 31.39%
Epoch: 250, Loss: 1.4175, Train: 35.18%, Valid: 34.75%, Test: 35.17%
Epoch: 275, Loss: 1.4146, Train: 38.30%, Valid: 37.88%, Test: 38.07%
Epoch: 300, Loss: 1.4183, Train: 38.90%, Valid: 38.52%, Test: 38.76%
Epoch: 325, Loss: 1.4121, Train: 39.35%, Valid: 38.74%, Test: 39.23%
Epoch: 350, Loss: 1.3976, Train: 38.08%, Valid: 37.89%, Test: 38.17%
Epoch: 375, Loss: 1.4063, Train: 38.68%, Valid: 38.42%, Test: 38.57%
Epoch: 400, Loss: 1.3856, Train: 38.97%, Valid: 38.70%, Test: 39.10%
Epoch: 425, Loss: 1.3846, Train: 39.13%, Valid: 38.78%, Test: 39.02%
Epoch: 450, Loss: 1.3939, Train: 38.71%, Valid: 38.62%, Test: 38.79%
Epoch: 475, Loss: 1.3837, Train: 38.99%, Valid: 38.81%, Test: 39.27%
Epoch: 500, Loss: 1.3726, Train: 39.07%, Valid: 38.84%, Test: 39.08%
Epoch: 525, Loss: 1.3822, Train: 39.00%, Valid: 38.71%, Test: 39.10%
Epoch: 550, Loss: 1.3746, Train: 37.21%, Valid: 36.77%, Test: 37.36%
Epoch: 575, Loss: 1.3778, Train: 39.39%, Valid: 39.10%, Test: 39.40%
Epoch: 600, Loss: 1.3811, Train: 40.73%, Valid: 40.41%, Test: 40.90%
Epoch: 625, Loss: 1.3696, Train: 40.54%, Valid: 40.37%, Test: 40.59%
Epoch: 650, Loss: 1.3867, Train: 39.52%, Valid: 39.28%, Test: 39.63%
Epoch: 675, Loss: 1.3779, Train: 40.40%, Valid: 40.20%, Test: 40.51%
Epoch: 700, Loss: 1.3803, Train: 40.50%, Valid: 39.95%, Test: 40.28%
Epoch: 725, Loss: 1.3639, Train: 41.14%, Valid: 40.89%, Test: 41.00%
Epoch: 750, Loss: 1.3783, Train: 40.61%, Valid: 40.09%, Test: 40.55%
Epoch: 775, Loss: 1.3583, Train: 41.55%, Valid: 41.07%, Test: 41.47%
Epoch: 800, Loss: 1.3619, Train: 40.11%, Valid: 39.52%, Test: 39.88%
Epoch: 825, Loss: 1.3781, Train: 41.34%, Valid: 41.02%, Test: 41.55%
Epoch: 850, Loss: 1.3607, Train: 40.72%, Valid: 40.25%, Test: 40.96%
Epoch: 875, Loss: 1.3578, Train: 40.74%, Valid: 40.53%, Test: 41.09%
Epoch: 900, Loss: 1.3617, Train: 41.13%, Valid: 40.92%, Test: 41.32%
Epoch: 925, Loss: 1.3705, Train: 40.55%, Valid: 40.23%, Test: 40.17%
Epoch: 950, Loss: 1.3892, Train: 40.72%, Valid: 40.35%, Test: 40.62%
Epoch: 975, Loss: 1.3730, Train: 39.53%, Valid: 39.22%, Test: 39.87%
Run 01:
Highest Train: 41.96
Highest Valid: 41.48
  Final Train: 41.96
   Final Test: 41.75
All runs:
Highest Train: 41.96, nan
Highest Valid: 41.48, nan
  Final Train: 41.96, nan
   Final Test: 41.75, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 6.6020, Train: 27.95%, Valid: 27.75%, Test: 28.07%
Epoch: 25, Loss: 1.6186, Train: 12.13%, Valid: 12.26%, Test: 12.28%
Epoch: 50, Loss: 1.5949, Train: 24.17%, Valid: 24.12%, Test: 24.60%
Epoch: 75, Loss: 1.4982, Train: 28.37%, Valid: 28.21%, Test: 28.58%
Epoch: 100, Loss: 1.4901, Train: 28.60%, Valid: 28.45%, Test: 28.88%
Epoch: 125, Loss: 1.4631, Train: 29.49%, Valid: 29.30%, Test: 29.66%
Epoch: 150, Loss: 1.4427, Train: 29.11%, Valid: 28.90%, Test: 29.34%
Epoch: 175, Loss: 1.4282, Train: 28.82%, Valid: 28.56%, Test: 29.12%
Epoch: 200, Loss: 1.4253, Train: 29.20%, Valid: 29.08%, Test: 29.51%
Epoch: 225, Loss: 1.4205, Train: 28.84%, Valid: 28.73%, Test: 29.08%
Epoch: 250, Loss: 1.4145, Train: 28.58%, Valid: 28.44%, Test: 28.76%
Epoch: 275, Loss: 1.4162, Train: 30.32%, Valid: 30.15%, Test: 30.46%
Epoch: 300, Loss: 1.4191, Train: 32.99%, Valid: 32.74%, Test: 32.78%
Epoch: 325, Loss: 1.4078, Train: 38.24%, Valid: 38.03%, Test: 38.05%
Epoch: 350, Loss: 1.4068, Train: 37.69%, Valid: 37.57%, Test: 37.59%
Epoch: 375, Loss: 1.4012, Train: 39.79%, Valid: 39.62%, Test: 39.67%
Epoch: 400, Loss: 1.3965, Train: 39.67%, Valid: 39.58%, Test: 39.57%
Epoch: 425, Loss: 1.4042, Train: 39.51%, Valid: 39.32%, Test: 39.30%
Epoch: 450, Loss: 1.3872, Train: 39.81%, Valid: 39.69%, Test: 39.56%
Epoch: 475, Loss: 1.3904, Train: 40.04%, Valid: 39.99%, Test: 39.86%
Epoch: 500, Loss: 1.3968, Train: 39.58%, Valid: 39.54%, Test: 39.48%
Epoch: 525, Loss: 1.3880, Train: 39.91%, Valid: 39.77%, Test: 39.74%
Epoch: 550, Loss: 1.3864, Train: 40.02%, Valid: 40.00%, Test: 39.89%
Epoch: 575, Loss: 1.3760, Train: 40.15%, Valid: 40.12%, Test: 39.97%
Epoch: 600, Loss: 1.3723, Train: 40.50%, Valid: 40.62%, Test: 40.33%
Epoch: 625, Loss: 1.3729, Train: 40.03%, Valid: 39.89%, Test: 40.04%
Epoch: 650, Loss: 1.3750, Train: 38.21%, Valid: 38.15%, Test: 38.04%
Epoch: 675, Loss: 1.3782, Train: 40.90%, Valid: 40.77%, Test: 40.54%
Epoch: 700, Loss: 1.3695, Train: 39.13%, Valid: 39.06%, Test: 39.07%
Epoch: 725, Loss: 1.3663, Train: 40.89%, Valid: 40.65%, Test: 40.62%
Epoch: 750, Loss: 1.3607, Train: 39.14%, Valid: 39.02%, Test: 39.09%
Epoch: 775, Loss: 1.3677, Train: 40.77%, Valid: 40.61%, Test: 40.56%
Epoch: 800, Loss: 1.3602, Train: 41.28%, Valid: 41.20%, Test: 40.99%
Epoch: 825, Loss: 1.3748, Train: 38.80%, Valid: 38.82%, Test: 38.72%
Epoch: 850, Loss: 1.3605, Train: 39.82%, Valid: 39.71%, Test: 39.63%
Epoch: 875, Loss: 1.3625, Train: 39.88%, Valid: 39.87%, Test: 39.85%
Epoch: 900, Loss: 1.3577, Train: 39.94%, Valid: 39.94%, Test: 39.93%
Epoch: 925, Loss: 1.3544, Train: 40.41%, Valid: 40.32%, Test: 40.25%
Epoch: 950, Loss: 1.3673, Train: 39.11%, Valid: 39.06%, Test: 39.13%
Epoch: 975, Loss: 1.3582, Train: 41.09%, Valid: 40.94%, Test: 41.07%
Run 01:
Highest Train: 42.08
Highest Valid: 41.88
  Final Train: 42.08
   Final Test: 41.74
All runs:
Highest Train: 42.08, nan
Highest Valid: 41.88, nan
  Final Train: 42.08, nan
   Final Test: 41.74, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6235, Train: 28.61%, Valid: 28.45%, Test: 28.73%
Epoch: 25, Loss: 1.5051, Train: 34.22%, Valid: 34.02%, Test: 34.20%
Epoch: 50, Loss: 1.4720, Train: 35.58%, Valid: 35.23%, Test: 35.45%
Epoch: 75, Loss: 1.4464, Train: 37.56%, Valid: 37.04%, Test: 37.34%
Epoch: 100, Loss: 1.4242, Train: 39.52%, Valid: 38.66%, Test: 38.94%
Epoch: 125, Loss: 1.3933, Train: 41.69%, Valid: 40.53%, Test: 41.01%
Epoch: 150, Loss: 1.3843, Train: 42.72%, Valid: 41.76%, Test: 41.73%
Epoch: 175, Loss: 1.3613, Train: 43.98%, Valid: 42.64%, Test: 43.08%
Epoch: 200, Loss: 1.3462, Train: 43.89%, Valid: 42.52%, Test: 43.08%
Epoch: 225, Loss: 1.3501, Train: 43.78%, Valid: 42.62%, Test: 42.82%
Epoch: 250, Loss: 1.3294, Train: 44.92%, Valid: 43.71%, Test: 43.99%
Epoch: 275, Loss: 1.3185, Train: 45.21%, Valid: 44.02%, Test: 44.32%
Epoch: 300, Loss: 1.3108, Train: 45.04%, Valid: 43.74%, Test: 43.92%
Epoch: 325, Loss: 1.3043, Train: 44.20%, Valid: 43.17%, Test: 43.27%
Epoch: 350, Loss: 1.3034, Train: 45.35%, Valid: 43.93%, Test: 44.22%
Epoch: 375, Loss: 1.3016, Train: 45.92%, Valid: 44.46%, Test: 44.59%
Epoch: 400, Loss: 1.2909, Train: 45.81%, Valid: 44.37%, Test: 44.59%
Epoch: 425, Loss: 1.2959, Train: 45.92%, Valid: 44.52%, Test: 44.49%
Epoch: 450, Loss: 1.2862, Train: 46.75%, Valid: 45.22%, Test: 45.50%
Epoch: 475, Loss: 1.2788, Train: 47.11%, Valid: 45.51%, Test: 45.85%
Epoch: 500, Loss: 1.2679, Train: 46.77%, Valid: 45.43%, Test: 45.48%
Epoch: 525, Loss: 1.2692, Train: 45.37%, Valid: 43.91%, Test: 44.00%
Epoch: 550, Loss: 1.2653, Train: 46.80%, Valid: 45.35%, Test: 45.25%
Epoch: 575, Loss: 1.2990, Train: 47.35%, Valid: 45.79%, Test: 45.79%
Epoch: 600, Loss: 1.2720, Train: 46.44%, Valid: 44.87%, Test: 45.01%
Epoch: 625, Loss: 1.2581, Train: 47.23%, Valid: 45.43%, Test: 45.52%
Epoch: 650, Loss: 1.2652, Train: 46.40%, Valid: 44.89%, Test: 44.91%
Epoch: 675, Loss: 1.2597, Train: 47.11%, Valid: 45.74%, Test: 45.75%
Epoch: 700, Loss: 1.2537, Train: 47.48%, Valid: 45.95%, Test: 45.61%
Epoch: 725, Loss: 1.2567, Train: 46.96%, Valid: 45.25%, Test: 45.27%
Epoch: 750, Loss: 1.2534, Train: 47.10%, Valid: 45.46%, Test: 45.57%
Epoch: 775, Loss: 1.2548, Train: 46.94%, Valid: 45.39%, Test: 45.46%
Epoch: 800, Loss: 1.2449, Train: 47.48%, Valid: 45.62%, Test: 45.78%
Epoch: 825, Loss: 1.2464, Train: 47.15%, Valid: 45.50%, Test: 45.36%
Epoch: 850, Loss: 1.2370, Train: 47.47%, Valid: 45.47%, Test: 45.72%
Epoch: 875, Loss: 1.2457, Train: 46.50%, Valid: 44.88%, Test: 44.77%
Epoch: 900, Loss: 1.2396, Train: 47.97%, Valid: 45.81%, Test: 45.91%
Epoch: 925, Loss: 1.2321, Train: 47.63%, Valid: 45.59%, Test: 45.73%
Epoch: 950, Loss: 1.2321, Train: 47.69%, Valid: 45.49%, Test: 45.54%
Epoch: 975, Loss: 1.2370, Train: 46.73%, Valid: 45.08%, Test: 44.95%
Run 01:
Highest Train: 48.44
Highest Valid: 46.43
  Final Train: 48.44
   Final Test: 46.43
All runs:
Highest Train: 48.44, nan
Highest Valid: 46.43, nan
  Final Train: 48.44, nan
   Final Test: 46.43, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.7772, Train: 19.42%, Valid: 19.63%, Test: 19.51%
Epoch: 25, Loss: 1.5349, Train: 24.69%, Valid: 24.57%, Test: 25.39%
Epoch: 50, Loss: 1.4911, Train: 29.45%, Valid: 29.14%, Test: 29.72%
Epoch: 75, Loss: 1.5272, Train: 32.34%, Valid: 32.11%, Test: 32.82%
Epoch: 100, Loss: 1.4676, Train: 24.13%, Valid: 24.29%, Test: 24.89%
Epoch: 125, Loss: 1.4719, Train: 32.29%, Valid: 32.07%, Test: 32.80%
Epoch: 150, Loss: 1.4401, Train: 35.48%, Valid: 35.13%, Test: 35.78%
Epoch: 175, Loss: 1.4261, Train: 37.17%, Valid: 36.86%, Test: 37.19%
Epoch: 200, Loss: 1.4308, Train: 37.33%, Valid: 37.02%, Test: 37.24%
Epoch: 225, Loss: 1.4223, Train: 37.14%, Valid: 36.72%, Test: 37.21%
Epoch: 250, Loss: 1.4122, Train: 37.91%, Valid: 37.50%, Test: 37.98%
Epoch: 275, Loss: 1.4189, Train: 37.70%, Valid: 37.40%, Test: 37.79%
Epoch: 300, Loss: 1.4097, Train: 38.34%, Valid: 38.02%, Test: 38.44%
Epoch: 325, Loss: 1.3891, Train: 38.15%, Valid: 37.69%, Test: 38.12%
Epoch: 350, Loss: 1.4043, Train: 38.07%, Valid: 37.98%, Test: 38.16%
Epoch: 375, Loss: 1.3914, Train: 38.60%, Valid: 38.39%, Test: 38.53%
Epoch: 400, Loss: 1.3780, Train: 39.97%, Valid: 40.00%, Test: 40.18%
Epoch: 425, Loss: 1.3748, Train: 39.22%, Valid: 38.79%, Test: 39.21%
Epoch: 450, Loss: 1.3966, Train: 38.49%, Valid: 38.10%, Test: 38.49%
Epoch: 475, Loss: 1.3687, Train: 39.06%, Valid: 38.80%, Test: 39.07%
Epoch: 500, Loss: 1.3715, Train: 39.91%, Valid: 39.63%, Test: 39.88%
Epoch: 525, Loss: 1.4099, Train: 38.67%, Valid: 38.46%, Test: 38.91%
Epoch: 550, Loss: 1.3736, Train: 38.73%, Valid: 38.34%, Test: 38.68%
Epoch: 575, Loss: 1.3846, Train: 39.49%, Valid: 39.23%, Test: 39.54%
Epoch: 600, Loss: 1.3758, Train: 40.46%, Valid: 40.25%, Test: 40.46%
Epoch: 625, Loss: 1.3599, Train: 40.62%, Valid: 40.50%, Test: 40.69%
Epoch: 650, Loss: 1.3608, Train: 39.52%, Valid: 39.11%, Test: 39.50%
Epoch: 675, Loss: 1.3698, Train: 40.68%, Valid: 40.52%, Test: 40.60%
Epoch: 700, Loss: 1.3770, Train: 39.61%, Valid: 39.29%, Test: 39.44%
Epoch: 725, Loss: 1.3575, Train: 39.61%, Valid: 39.23%, Test: 39.71%
Epoch: 750, Loss: 1.3539, Train: 40.33%, Valid: 40.07%, Test: 40.35%
Epoch: 775, Loss: 1.3528, Train: 39.36%, Valid: 38.85%, Test: 39.29%
Epoch: 800, Loss: 1.3454, Train: 40.32%, Valid: 39.86%, Test: 40.14%
Epoch: 825, Loss: 1.3495, Train: 40.79%, Valid: 40.39%, Test: 40.72%
Epoch: 850, Loss: 1.3430, Train: 42.18%, Valid: 41.99%, Test: 42.38%
Epoch: 875, Loss: 1.3454, Train: 40.66%, Valid: 40.13%, Test: 40.54%
Epoch: 900, Loss: 1.3404, Train: 41.13%, Valid: 40.67%, Test: 40.92%
Epoch: 925, Loss: 1.3444, Train: 39.71%, Valid: 39.16%, Test: 39.62%
Epoch: 950, Loss: 1.3532, Train: 40.68%, Valid: 40.09%, Test: 40.65%
Epoch: 975, Loss: 1.3506, Train: 41.98%, Valid: 41.50%, Test: 41.94%
Run 01:
Highest Train: 42.43
Highest Valid: 42.17
  Final Train: 42.43
   Final Test: 42.49
All runs:
Highest Train: 42.43, nan
Highest Valid: 42.17, nan
  Final Train: 42.43, nan
   Final Test: 42.49, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 4.9898, Train: 19.00%, Valid: 19.14%, Test: 19.23%
Epoch: 25, Loss: 1.9454, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 50, Loss: 1.5676, Train: 28.60%, Valid: 28.39%, Test: 28.66%
Epoch: 75, Loss: 1.4996, Train: 30.27%, Valid: 30.18%, Test: 31.07%
Epoch: 100, Loss: 1.4726, Train: 24.33%, Valid: 24.40%, Test: 24.32%
Epoch: 125, Loss: 1.4890, Train: 33.16%, Valid: 33.10%, Test: 33.65%
Epoch: 150, Loss: 1.4584, Train: 23.16%, Valid: 23.30%, Test: 23.60%
Epoch: 175, Loss: 1.4596, Train: 25.35%, Valid: 25.45%, Test: 25.61%
Epoch: 200, Loss: 1.4580, Train: 24.82%, Valid: 24.79%, Test: 24.97%
Epoch: 225, Loss: 1.4729, Train: 32.90%, Valid: 32.84%, Test: 32.67%
Epoch: 250, Loss: 1.4715, Train: 24.72%, Valid: 24.63%, Test: 24.62%
Epoch: 275, Loss: 1.4471, Train: 24.19%, Valid: 24.19%, Test: 24.39%
Epoch: 300, Loss: 1.4541, Train: 25.15%, Valid: 25.24%, Test: 25.09%
Epoch: 325, Loss: 1.4600, Train: 25.39%, Valid: 25.35%, Test: 25.29%
Epoch: 350, Loss: 1.4285, Train: 26.40%, Valid: 26.32%, Test: 26.28%
Epoch: 375, Loss: 1.4363, Train: 26.45%, Valid: 26.45%, Test: 26.38%
Epoch: 400, Loss: 1.4361, Train: 25.89%, Valid: 26.02%, Test: 25.83%
Epoch: 425, Loss: 1.4473, Train: 36.82%, Valid: 36.46%, Test: 37.05%
Epoch: 450, Loss: 1.4440, Train: 24.45%, Valid: 24.28%, Test: 24.57%
Epoch: 475, Loss: 1.4622, Train: 29.43%, Valid: 29.36%, Test: 29.39%
Epoch: 500, Loss: 1.4158, Train: 26.81%, Valid: 26.54%, Test: 26.78%
Epoch: 525, Loss: 1.4214, Train: 32.12%, Valid: 31.90%, Test: 32.35%
Epoch: 550, Loss: 1.4299, Train: 35.76%, Valid: 35.48%, Test: 35.79%
Epoch: 575, Loss: 1.4226, Train: 32.64%, Valid: 32.44%, Test: 33.01%
Epoch: 600, Loss: 1.4313, Train: 36.89%, Valid: 36.72%, Test: 36.76%
Epoch: 625, Loss: 1.4235, Train: 38.00%, Valid: 37.79%, Test: 38.33%
Epoch: 650, Loss: 1.4110, Train: 27.29%, Valid: 27.17%, Test: 27.50%
Epoch: 675, Loss: 1.4108, Train: 27.99%, Valid: 27.87%, Test: 28.07%
Epoch: 700, Loss: 1.4136, Train: 38.48%, Valid: 38.00%, Test: 38.59%
Epoch: 725, Loss: 1.3986, Train: 28.48%, Valid: 28.31%, Test: 28.60%
Epoch: 750, Loss: 1.4003, Train: 38.76%, Valid: 38.19%, Test: 38.77%
Epoch: 775, Loss: 1.3975, Train: 29.13%, Valid: 28.86%, Test: 29.18%
Epoch: 800, Loss: 1.4027, Train: 39.00%, Valid: 38.60%, Test: 39.19%
Epoch: 825, Loss: 1.4140, Train: 38.20%, Valid: 37.80%, Test: 38.28%
Epoch: 850, Loss: 1.4082, Train: 38.05%, Valid: 37.69%, Test: 38.16%
Epoch: 875, Loss: 1.3968, Train: 39.14%, Valid: 38.76%, Test: 39.19%
Epoch: 900, Loss: 1.3944, Train: 39.01%, Valid: 38.72%, Test: 39.05%
Epoch: 925, Loss: 1.3932, Train: 38.80%, Valid: 38.53%, Test: 38.88%
Epoch: 950, Loss: 1.3929, Train: 38.54%, Valid: 38.27%, Test: 38.55%
Epoch: 975, Loss: 1.4189, Train: 38.39%, Valid: 38.09%, Test: 38.27%
Run 01:
Highest Train: 39.29
Highest Valid: 38.88
  Final Train: 39.18
   Final Test: 39.14
All runs:
Highest Train: 39.29, nan
Highest Valid: 38.88, nan
  Final Train: 39.18, nan
   Final Test: 39.14, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6048, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4741, Train: 35.25%, Valid: 35.02%, Test: 35.18%
Epoch: 50, Loss: 1.4381, Train: 36.86%, Valid: 36.06%, Test: 36.65%
Epoch: 75, Loss: 1.3999, Train: 38.72%, Valid: 37.72%, Test: 38.15%
Epoch: 100, Loss: 1.3697, Train: 40.16%, Valid: 39.04%, Test: 39.31%
Epoch: 125, Loss: 1.3392, Train: 41.55%, Valid: 40.34%, Test: 40.67%
Epoch: 150, Loss: 1.3217, Train: 42.91%, Valid: 41.01%, Test: 41.24%
Epoch: 175, Loss: 1.3019, Train: 43.23%, Valid: 41.43%, Test: 41.75%
Epoch: 200, Loss: 1.2739, Train: 45.07%, Valid: 42.02%, Test: 42.25%
Epoch: 225, Loss: 1.2482, Train: 45.89%, Valid: 42.36%, Test: 42.77%
Epoch: 250, Loss: 1.2442, Train: 46.19%, Valid: 41.93%, Test: 42.71%
Epoch: 275, Loss: 1.2223, Train: 47.41%, Valid: 42.72%, Test: 43.24%
Epoch: 300, Loss: 1.2240, Train: 48.61%, Valid: 43.44%, Test: 43.74%
Epoch: 325, Loss: 1.1885, Train: 48.99%, Valid: 43.22%, Test: 43.74%
Epoch: 350, Loss: 1.1726, Train: 49.89%, Valid: 44.44%, Test: 44.84%
Epoch: 375, Loss: 1.1562, Train: 51.02%, Valid: 44.49%, Test: 44.99%
Epoch: 400, Loss: 1.1475, Train: 51.44%, Valid: 44.85%, Test: 45.27%
Epoch: 425, Loss: 1.1390, Train: 51.89%, Valid: 44.91%, Test: 45.31%
Epoch: 450, Loss: 1.1443, Train: 52.14%, Valid: 45.29%, Test: 45.67%
Epoch: 475, Loss: 1.1167, Train: 52.74%, Valid: 45.27%, Test: 45.72%
Epoch: 500, Loss: 1.1104, Train: 52.96%, Valid: 45.48%, Test: 45.81%
Epoch: 525, Loss: 1.0937, Train: 53.42%, Valid: 44.86%, Test: 45.46%
Epoch: 550, Loss: 1.0971, Train: 53.43%, Valid: 44.77%, Test: 45.57%
Epoch: 575, Loss: 1.1075, Train: 53.90%, Valid: 45.55%, Test: 46.02%
Epoch: 600, Loss: 1.0671, Train: 55.21%, Valid: 46.03%, Test: 46.24%
Epoch: 625, Loss: 1.0935, Train: 54.27%, Valid: 45.47%, Test: 45.67%
Epoch: 650, Loss: 1.1493, Train: 53.56%, Valid: 45.35%, Test: 45.61%
Epoch: 675, Loss: 1.0533, Train: 55.75%, Valid: 46.12%, Test: 46.29%
Epoch: 700, Loss: 1.0542, Train: 54.83%, Valid: 45.14%, Test: 45.41%
Epoch: 725, Loss: 1.0366, Train: 56.77%, Valid: 46.39%, Test: 46.47%
Epoch: 750, Loss: 1.0371, Train: 56.75%, Valid: 46.27%, Test: 46.61%
Epoch: 775, Loss: 1.0289, Train: 56.77%, Valid: 46.08%, Test: 45.98%
Epoch: 800, Loss: 1.0185, Train: 57.73%, Valid: 46.12%, Test: 46.32%
Epoch: 825, Loss: 1.0191, Train: 57.48%, Valid: 46.68%, Test: 46.31%
Epoch: 850, Loss: 1.0064, Train: 58.34%, Valid: 46.95%, Test: 46.93%
Epoch: 875, Loss: 1.0187, Train: 57.53%, Valid: 46.46%, Test: 46.46%
Epoch: 900, Loss: 1.0045, Train: 58.68%, Valid: 46.38%, Test: 46.53%
Epoch: 925, Loss: 1.0275, Train: 58.13%, Valid: 45.91%, Test: 45.91%
Epoch: 950, Loss: 0.9818, Train: 59.28%, Valid: 46.33%, Test: 46.25%
Epoch: 975, Loss: 0.9827, Train: 59.19%, Valid: 46.69%, Test: 46.55%
Run 01:
Highest Train: 59.80
Highest Valid: 47.29
  Final Train: 58.77
   Final Test: 46.96
All runs:
Highest Train: 59.80, nan
Highest Valid: 47.29, nan
  Final Train: 58.77, nan
   Final Test: 46.96, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.5906, Train: 17.64%, Valid: 17.78%, Test: 17.32%
Epoch: 25, Loss: 1.4402, Train: 31.31%, Valid: 31.08%, Test: 31.50%
Epoch: 50, Loss: 1.4081, Train: 37.93%, Valid: 37.69%, Test: 38.02%
Epoch: 75, Loss: 1.3694, Train: 38.81%, Valid: 38.20%, Test: 38.51%
Epoch: 100, Loss: 1.3435, Train: 40.66%, Valid: 40.26%, Test: 40.69%
Epoch: 125, Loss: 1.3404, Train: 41.50%, Valid: 40.99%, Test: 41.37%
Epoch: 150, Loss: 1.3067, Train: 42.42%, Valid: 41.82%, Test: 42.25%
Epoch: 175, Loss: 1.3052, Train: 42.83%, Valid: 42.12%, Test: 42.80%
Epoch: 200, Loss: 1.2794, Train: 43.50%, Valid: 42.98%, Test: 43.09%
Epoch: 225, Loss: 1.2803, Train: 44.16%, Valid: 43.53%, Test: 43.85%
Epoch: 250, Loss: 1.2560, Train: 45.16%, Valid: 44.46%, Test: 44.42%
Epoch: 275, Loss: 1.2487, Train: 45.60%, Valid: 44.77%, Test: 44.83%
Epoch: 300, Loss: 1.2529, Train: 45.36%, Valid: 44.66%, Test: 44.59%
Epoch: 325, Loss: 1.2848, Train: 44.99%, Valid: 44.43%, Test: 44.16%
Epoch: 350, Loss: 1.2317, Train: 46.41%, Valid: 45.58%, Test: 45.40%
Epoch: 375, Loss: 1.2172, Train: 47.11%, Valid: 46.26%, Test: 46.13%
Epoch: 400, Loss: 1.2270, Train: 46.89%, Valid: 46.21%, Test: 45.98%
Epoch: 425, Loss: 1.2045, Train: 47.66%, Valid: 46.67%, Test: 46.55%
Epoch: 450, Loss: 1.2183, Train: 47.31%, Valid: 46.33%, Test: 46.13%
Epoch: 475, Loss: 1.1964, Train: 48.25%, Valid: 47.21%, Test: 46.93%
Epoch: 500, Loss: 1.1893, Train: 48.60%, Valid: 47.22%, Test: 47.11%
Epoch: 525, Loss: 1.1972, Train: 47.86%, Valid: 46.74%, Test: 46.48%
Epoch: 550, Loss: 1.1809, Train: 48.98%, Valid: 47.67%, Test: 47.44%
Epoch: 575, Loss: 1.1845, Train: 48.91%, Valid: 47.29%, Test: 47.25%
Epoch: 600, Loss: 1.1725, Train: 49.42%, Valid: 47.54%, Test: 47.51%
Epoch: 625, Loss: 1.1713, Train: 49.68%, Valid: 47.76%, Test: 48.02%
Epoch: 650, Loss: 1.1666, Train: 49.90%, Valid: 47.93%, Test: 47.88%
Epoch: 675, Loss: 1.1787, Train: 49.51%, Valid: 47.76%, Test: 47.82%
Epoch: 700, Loss: 1.1589, Train: 49.75%, Valid: 47.82%, Test: 47.86%
Epoch: 725, Loss: 1.1567, Train: 50.19%, Valid: 48.13%, Test: 48.28%
Epoch: 750, Loss: 1.1833, Train: 49.63%, Valid: 47.83%, Test: 47.93%
Epoch: 775, Loss: 1.1693, Train: 49.97%, Valid: 47.91%, Test: 48.10%
Epoch: 800, Loss: 1.1564, Train: 50.12%, Valid: 48.10%, Test: 48.14%
Epoch: 825, Loss: 1.1458, Train: 50.57%, Valid: 48.20%, Test: 48.54%
Epoch: 850, Loss: 1.1622, Train: 50.03%, Valid: 47.92%, Test: 47.95%
Epoch: 875, Loss: 1.1448, Train: 50.77%, Valid: 48.38%, Test: 48.60%
Epoch: 900, Loss: 1.1396, Train: 50.68%, Valid: 48.28%, Test: 48.61%
Epoch: 925, Loss: 1.1462, Train: 50.71%, Valid: 48.21%, Test: 48.47%
Epoch: 950, Loss: 1.2671, Train: 45.08%, Valid: 43.75%, Test: 43.90%
Epoch: 975, Loss: 1.1784, Train: 49.14%, Valid: 47.51%, Test: 47.64%
Run 01:
Highest Train: 50.99
Highest Valid: 48.53
  Final Train: 50.88
   Final Test: 48.75
All runs:
Highest Train: 50.99, nan
Highest Valid: 48.53, nan
  Final Train: 50.88, nan
   Final Test: 48.75, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 47.3216, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 4.9326, Train: 25.54%, Valid: 25.41%, Test: 25.68%
Epoch: 50, Loss: 1.4873, Train: 29.24%, Valid: 29.09%, Test: 29.25%
Epoch: 75, Loss: 1.4378, Train: 38.05%, Valid: 37.61%, Test: 37.93%
Epoch: 100, Loss: 1.4420, Train: 37.49%, Valid: 37.11%, Test: 37.46%
Epoch: 125, Loss: 1.4045, Train: 37.89%, Valid: 37.49%, Test: 37.81%
Epoch: 150, Loss: 1.3974, Train: 38.24%, Valid: 37.76%, Test: 38.08%
Epoch: 175, Loss: 1.4036, Train: 37.64%, Valid: 37.17%, Test: 37.52%
Epoch: 200, Loss: 1.3944, Train: 38.69%, Valid: 38.40%, Test: 38.57%
Epoch: 225, Loss: 1.3905, Train: 38.75%, Valid: 38.44%, Test: 38.61%
Epoch: 250, Loss: 1.3880, Train: 39.13%, Valid: 38.89%, Test: 39.00%
Epoch: 275, Loss: 1.3873, Train: 39.09%, Valid: 38.84%, Test: 39.01%
Epoch: 300, Loss: 1.3855, Train: 39.07%, Valid: 38.88%, Test: 39.06%
Epoch: 325, Loss: 1.3859, Train: 39.11%, Valid: 39.00%, Test: 39.13%
Epoch: 350, Loss: 1.3820, Train: 39.23%, Valid: 39.09%, Test: 39.18%
Epoch: 375, Loss: 1.3803, Train: 39.23%, Valid: 39.04%, Test: 39.27%
Epoch: 400, Loss: 1.3751, Train: 39.72%, Valid: 39.42%, Test: 39.53%
Epoch: 425, Loss: 1.3764, Train: 39.24%, Valid: 39.06%, Test: 39.31%
Epoch: 450, Loss: 1.3662, Train: 40.29%, Valid: 39.88%, Test: 40.05%
Epoch: 475, Loss: 1.4134, Train: 39.24%, Valid: 39.04%, Test: 39.11%
Epoch: 500, Loss: 1.3754, Train: 39.17%, Valid: 38.83%, Test: 38.99%
Epoch: 525, Loss: 1.3627, Train: 40.45%, Valid: 40.01%, Test: 40.03%
Epoch: 550, Loss: 1.3576, Train: 40.77%, Valid: 40.47%, Test: 40.69%
Epoch: 575, Loss: 1.3533, Train: 40.95%, Valid: 40.70%, Test: 40.91%
Epoch: 600, Loss: 1.3869, Train: 37.67%, Valid: 37.47%, Test: 37.66%
Epoch: 625, Loss: 1.3615, Train: 39.16%, Valid: 38.81%, Test: 39.35%
Epoch: 650, Loss: 1.3527, Train: 40.75%, Valid: 40.48%, Test: 40.59%
Epoch: 675, Loss: 1.3439, Train: 41.21%, Valid: 40.97%, Test: 41.29%
Epoch: 700, Loss: 1.3387, Train: 41.51%, Valid: 41.22%, Test: 41.51%
Epoch: 725, Loss: 1.3345, Train: 41.68%, Valid: 41.50%, Test: 41.74%
Epoch: 750, Loss: 1.3310, Train: 41.83%, Valid: 41.68%, Test: 41.97%
Epoch: 775, Loss: 1.3334, Train: 41.88%, Valid: 41.62%, Test: 41.96%
Epoch: 800, Loss: 1.4201, Train: 39.56%, Valid: 39.25%, Test: 39.56%
Epoch: 825, Loss: 1.3538, Train: 40.51%, Valid: 40.24%, Test: 40.58%
Epoch: 850, Loss: 1.3383, Train: 41.32%, Valid: 40.96%, Test: 41.29%
Epoch: 875, Loss: 1.3304, Train: 41.70%, Valid: 41.43%, Test: 41.74%
Epoch: 900, Loss: 1.3264, Train: 41.86%, Valid: 41.63%, Test: 41.94%
Epoch: 925, Loss: 1.3235, Train: 42.08%, Valid: 41.85%, Test: 42.10%
Epoch: 950, Loss: 1.3209, Train: 42.20%, Valid: 41.98%, Test: 42.29%
Epoch: 975, Loss: 1.3186, Train: 42.33%, Valid: 42.09%, Test: 42.40%
Run 01:
Highest Train: 42.40
Highest Valid: 42.19
  Final Train: 42.40
   Final Test: 42.50
All runs:
Highest Train: 42.40, nan
Highest Valid: 42.19, nan
  Final Train: 42.40, nan
   Final Test: 42.50, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6094, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4767, Train: 35.32%, Valid: 35.12%, Test: 35.28%
Epoch: 50, Loss: 1.4377, Train: 37.14%, Valid: 36.28%, Test: 36.67%
Epoch: 75, Loss: 1.4000, Train: 39.42%, Valid: 38.41%, Test: 38.50%
Epoch: 100, Loss: 1.3656, Train: 41.12%, Valid: 39.79%, Test: 39.92%
Epoch: 125, Loss: 1.3458, Train: 41.96%, Valid: 40.60%, Test: 40.55%
Epoch: 150, Loss: 1.3132, Train: 43.28%, Valid: 41.53%, Test: 41.59%
Epoch: 175, Loss: 1.2972, Train: 44.15%, Valid: 41.51%, Test: 41.48%
Epoch: 200, Loss: 1.2765, Train: 45.54%, Valid: 42.84%, Test: 42.76%
Epoch: 225, Loss: 1.2520, Train: 45.67%, Valid: 42.28%, Test: 42.24%
Epoch: 250, Loss: 1.2366, Train: 47.17%, Valid: 43.42%, Test: 43.39%
Epoch: 275, Loss: 1.2110, Train: 48.25%, Valid: 43.70%, Test: 43.77%
Epoch: 300, Loss: 1.1965, Train: 48.54%, Valid: 44.00%, Test: 43.99%
Epoch: 325, Loss: 1.1882, Train: 49.29%, Valid: 44.02%, Test: 44.06%
Epoch: 350, Loss: 1.1666, Train: 50.24%, Valid: 44.48%, Test: 44.75%
Epoch: 375, Loss: 1.1568, Train: 50.84%, Valid: 44.74%, Test: 45.07%
Epoch: 400, Loss: 1.1496, Train: 51.20%, Valid: 45.16%, Test: 45.20%
Epoch: 425, Loss: 1.1395, Train: 51.97%, Valid: 44.84%, Test: 45.31%
Epoch: 450, Loss: 1.1282, Train: 51.85%, Valid: 45.04%, Test: 45.18%
Epoch: 475, Loss: 1.1152, Train: 52.38%, Valid: 45.15%, Test: 45.25%
Epoch: 500, Loss: 1.1089, Train: 53.26%, Valid: 45.14%, Test: 45.35%
Epoch: 525, Loss: 1.1089, Train: 52.71%, Valid: 45.04%, Test: 45.36%
Epoch: 550, Loss: 1.0998, Train: 53.15%, Valid: 44.97%, Test: 45.20%
Epoch: 575, Loss: 1.0962, Train: 54.30%, Valid: 45.29%, Test: 45.70%
Epoch: 600, Loss: 1.0904, Train: 54.19%, Valid: 44.74%, Test: 45.23%
Epoch: 625, Loss: 1.0750, Train: 53.63%, Valid: 44.54%, Test: 44.89%
Epoch: 650, Loss: 1.0695, Train: 54.89%, Valid: 45.75%, Test: 45.99%
Epoch: 675, Loss: 1.0561, Train: 55.65%, Valid: 45.83%, Test: 46.39%
Epoch: 700, Loss: 1.0744, Train: 54.89%, Valid: 45.74%, Test: 46.04%
Epoch: 725, Loss: 1.0525, Train: 55.39%, Valid: 46.26%, Test: 46.38%
Epoch: 750, Loss: 1.0409, Train: 56.38%, Valid: 45.77%, Test: 46.37%
Epoch: 775, Loss: 1.0570, Train: 56.12%, Valid: 45.38%, Test: 45.80%
Epoch: 800, Loss: 1.0400, Train: 56.53%, Valid: 45.18%, Test: 45.70%
Epoch: 825, Loss: 1.0304, Train: 56.56%, Valid: 45.97%, Test: 46.25%
Epoch: 850, Loss: 1.0220, Train: 57.43%, Valid: 45.82%, Test: 46.35%
Epoch: 875, Loss: 1.0283, Train: 57.20%, Valid: 46.32%, Test: 46.66%
Epoch: 900, Loss: 1.0497, Train: 56.07%, Valid: 45.21%, Test: 45.38%
Epoch: 925, Loss: 1.0088, Train: 58.09%, Valid: 46.04%, Test: 46.52%
Epoch: 950, Loss: 1.0245, Train: 57.78%, Valid: 46.21%, Test: 46.44%
Epoch: 975, Loss: 1.0026, Train: 58.23%, Valid: 46.00%, Test: 46.29%
Run 01:
Highest Train: 58.49
Highest Valid: 46.49
  Final Train: 57.80
   Final Test: 46.67
All runs:
Highest Train: 58.49, nan
Highest Valid: 46.49, nan
  Final Train: 57.80, nan
   Final Test: 46.67, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.5509, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4649, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 50, Loss: 1.4380, Train: 32.30%, Valid: 32.01%, Test: 32.75%
Epoch: 75, Loss: 1.4148, Train: 35.43%, Valid: 35.07%, Test: 35.94%
Epoch: 100, Loss: 1.4011, Train: 38.39%, Valid: 37.90%, Test: 38.29%
Epoch: 125, Loss: 1.3849, Train: 38.83%, Valid: 38.44%, Test: 38.46%
Epoch: 150, Loss: 1.3799, Train: 39.53%, Valid: 39.32%, Test: 39.30%
Epoch: 175, Loss: 1.3694, Train: 39.94%, Valid: 39.86%, Test: 39.59%
Epoch: 200, Loss: 1.3641, Train: 40.05%, Valid: 39.65%, Test: 39.58%
Epoch: 225, Loss: 1.3595, Train: 40.73%, Valid: 40.57%, Test: 40.32%
Epoch: 250, Loss: 1.3501, Train: 41.25%, Valid: 41.21%, Test: 40.90%
Epoch: 275, Loss: 1.3573, Train: 42.00%, Valid: 41.68%, Test: 41.60%
Epoch: 300, Loss: 1.3435, Train: 41.94%, Valid: 41.56%, Test: 41.51%
Epoch: 325, Loss: 1.3338, Train: 42.25%, Valid: 41.87%, Test: 41.72%
Epoch: 350, Loss: 1.3967, Train: 41.61%, Valid: 41.56%, Test: 41.40%
Epoch: 375, Loss: 1.3440, Train: 42.15%, Valid: 41.86%, Test: 41.85%
Epoch: 400, Loss: 1.3318, Train: 42.65%, Valid: 42.49%, Test: 42.35%
Epoch: 425, Loss: 1.3275, Train: 42.60%, Valid: 42.34%, Test: 42.17%
Epoch: 450, Loss: 1.3201, Train: 42.82%, Valid: 42.69%, Test: 42.40%
Epoch: 475, Loss: 1.3457, Train: 40.67%, Valid: 40.39%, Test: 40.39%
Epoch: 500, Loss: 1.3180, Train: 43.51%, Valid: 43.54%, Test: 43.35%
Epoch: 525, Loss: 1.3091, Train: 43.51%, Valid: 43.44%, Test: 43.11%
Epoch: 550, Loss: 1.3202, Train: 43.68%, Valid: 43.51%, Test: 43.47%
Epoch: 575, Loss: 1.3089, Train: 43.95%, Valid: 43.62%, Test: 43.58%
Epoch: 600, Loss: 1.3021, Train: 43.91%, Valid: 43.64%, Test: 43.53%
Epoch: 625, Loss: 1.3255, Train: 42.98%, Valid: 42.58%, Test: 42.60%
Epoch: 650, Loss: 1.3002, Train: 44.32%, Valid: 43.98%, Test: 43.92%
Epoch: 675, Loss: 1.3051, Train: 43.94%, Valid: 43.71%, Test: 43.63%
Epoch: 700, Loss: 1.2911, Train: 44.43%, Valid: 43.98%, Test: 44.02%
Epoch: 725, Loss: 1.2993, Train: 42.52%, Valid: 42.17%, Test: 42.08%
Epoch: 750, Loss: 1.3083, Train: 44.13%, Valid: 43.82%, Test: 43.48%
Epoch: 775, Loss: 1.2916, Train: 45.25%, Valid: 44.66%, Test: 44.54%
Epoch: 800, Loss: 1.2854, Train: 45.16%, Valid: 44.61%, Test: 44.60%
Epoch: 825, Loss: 1.2834, Train: 45.04%, Valid: 44.49%, Test: 44.38%
Epoch: 850, Loss: 1.2830, Train: 44.91%, Valid: 44.24%, Test: 44.21%
Epoch: 875, Loss: 1.3354, Train: 41.80%, Valid: 41.60%, Test: 41.57%
Epoch: 900, Loss: 1.2983, Train: 44.24%, Valid: 43.75%, Test: 43.66%
Epoch: 925, Loss: 1.2875, Train: 44.94%, Valid: 44.41%, Test: 44.41%
Epoch: 950, Loss: 1.2822, Train: 45.21%, Valid: 44.67%, Test: 44.75%
Epoch: 975, Loss: 1.2882, Train: 45.31%, Valid: 44.62%, Test: 44.71%
Run 01:
Highest Train: 45.68
Highest Valid: 45.05
  Final Train: 45.68
   Final Test: 44.96
All runs:
Highest Train: 45.68, nan
Highest Valid: 45.05, nan
  Final Train: 45.68, nan
   Final Test: 44.96, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 32.7864, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 5.5079, Train: 28.37%, Valid: 28.15%, Test: 28.48%
Epoch: 50, Loss: 1.7448, Train: 28.70%, Valid: 28.71%, Test: 28.57%
Epoch: 75, Loss: 1.4711, Train: 32.30%, Valid: 32.22%, Test: 33.01%
Epoch: 100, Loss: 1.4313, Train: 36.59%, Valid: 36.21%, Test: 36.63%
Epoch: 125, Loss: 1.4078, Train: 37.39%, Valid: 37.02%, Test: 37.42%
Epoch: 150, Loss: 1.3893, Train: 38.27%, Valid: 37.97%, Test: 38.26%
Epoch: 175, Loss: 1.3769, Train: 39.01%, Valid: 38.72%, Test: 39.03%
Epoch: 200, Loss: 1.3675, Train: 39.64%, Valid: 39.27%, Test: 39.68%
Epoch: 225, Loss: 1.3589, Train: 40.14%, Valid: 39.75%, Test: 40.10%
Epoch: 250, Loss: 1.3523, Train: 39.71%, Valid: 39.44%, Test: 39.86%
Epoch: 275, Loss: 1.3758, Train: 38.51%, Valid: 38.31%, Test: 38.57%
Epoch: 300, Loss: 1.3557, Train: 40.41%, Valid: 40.12%, Test: 40.33%
Epoch: 325, Loss: 1.3466, Train: 40.87%, Valid: 40.57%, Test: 40.88%
Epoch: 350, Loss: 1.3416, Train: 41.26%, Valid: 40.95%, Test: 41.37%
Epoch: 375, Loss: 1.3379, Train: 41.48%, Valid: 41.12%, Test: 41.45%
Epoch: 400, Loss: 1.3346, Train: 41.70%, Valid: 41.28%, Test: 41.60%
Epoch: 425, Loss: 1.3316, Train: 41.77%, Valid: 41.45%, Test: 41.74%
Epoch: 450, Loss: 1.3295, Train: 41.95%, Valid: 41.58%, Test: 41.87%
Epoch: 475, Loss: 1.3284, Train: 41.90%, Valid: 41.55%, Test: 41.81%
Epoch: 500, Loss: 1.3356, Train: 41.60%, Valid: 41.17%, Test: 41.55%
Epoch: 525, Loss: 1.3355, Train: 41.81%, Valid: 41.45%, Test: 41.69%
Epoch: 550, Loss: 1.4601, Train: 29.35%, Valid: 29.34%, Test: 30.04%
Epoch: 575, Loss: 1.4313, Train: 34.04%, Valid: 34.26%, Test: 34.68%
Epoch: 600, Loss: 1.3935, Train: 37.35%, Valid: 37.29%, Test: 37.55%
Epoch: 625, Loss: 1.3746, Train: 38.82%, Valid: 38.71%, Test: 38.98%
Epoch: 650, Loss: 1.3636, Train: 39.51%, Valid: 39.32%, Test: 39.54%
Epoch: 675, Loss: 1.3548, Train: 40.16%, Valid: 39.91%, Test: 40.21%
Epoch: 700, Loss: 1.4309, Train: 37.15%, Valid: 36.86%, Test: 36.99%
Epoch: 725, Loss: 1.3711, Train: 39.34%, Valid: 39.02%, Test: 39.33%
Epoch: 750, Loss: 1.3504, Train: 40.58%, Valid: 40.35%, Test: 40.50%
Epoch: 775, Loss: 1.3428, Train: 41.23%, Valid: 40.93%, Test: 41.13%
Epoch: 800, Loss: 1.3382, Train: 41.44%, Valid: 41.25%, Test: 41.42%
Epoch: 825, Loss: 1.3348, Train: 41.59%, Valid: 41.38%, Test: 41.58%
Epoch: 850, Loss: 1.3320, Train: 41.74%, Valid: 41.52%, Test: 41.76%
Epoch: 875, Loss: 1.3293, Train: 41.89%, Valid: 41.66%, Test: 41.90%
Epoch: 900, Loss: 1.3268, Train: 42.06%, Valid: 41.82%, Test: 42.08%
Epoch: 925, Loss: 1.3244, Train: 42.18%, Valid: 41.94%, Test: 42.23%
Epoch: 950, Loss: 1.3482, Train: 41.43%, Valid: 41.16%, Test: 41.46%
Epoch: 975, Loss: 1.7065, Train: 37.85%, Valid: 37.53%, Test: 37.64%
Run 01:
Highest Train: 42.53
Highest Valid: 42.10
  Final Train: 42.41
   Final Test: 42.29
All runs:
Highest Train: 42.53, nan
Highest Valid: 42.10, nan
  Final Train: 42.41, nan
   Final Test: 42.29, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6149, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4839, Train: 35.17%, Valid: 34.97%, Test: 35.15%
Epoch: 50, Loss: 1.4508, Train: 37.24%, Valid: 36.54%, Test: 36.75%
Epoch: 75, Loss: 1.4213, Train: 39.25%, Valid: 38.12%, Test: 38.31%
Epoch: 100, Loss: 1.3984, Train: 39.88%, Valid: 38.73%, Test: 39.00%
Epoch: 125, Loss: 1.3779, Train: 41.33%, Valid: 40.34%, Test: 40.39%
Epoch: 150, Loss: 1.3603, Train: 41.46%, Valid: 40.27%, Test: 40.46%
Epoch: 175, Loss: 1.3465, Train: 43.28%, Valid: 41.95%, Test: 42.04%
Epoch: 200, Loss: 1.3384, Train: 43.77%, Valid: 42.21%, Test: 42.40%
Epoch: 225, Loss: 1.3285, Train: 43.27%, Valid: 41.77%, Test: 41.98%
Epoch: 250, Loss: 1.3180, Train: 44.59%, Valid: 42.77%, Test: 43.00%
Epoch: 275, Loss: 1.3220, Train: 44.53%, Valid: 43.08%, Test: 43.09%
Epoch: 300, Loss: 1.3113, Train: 44.74%, Valid: 43.39%, Test: 43.21%
Epoch: 325, Loss: 1.3057, Train: 45.71%, Valid: 43.62%, Test: 43.92%
Epoch: 350, Loss: 1.2995, Train: 44.23%, Valid: 42.84%, Test: 42.97%
Epoch: 375, Loss: 1.2938, Train: 45.60%, Valid: 44.03%, Test: 43.99%
Epoch: 400, Loss: 1.2862, Train: 45.40%, Valid: 43.85%, Test: 43.82%
Epoch: 425, Loss: 1.2940, Train: 46.28%, Valid: 44.34%, Test: 44.42%
Epoch: 450, Loss: 1.2785, Train: 46.29%, Valid: 44.46%, Test: 44.42%
Epoch: 475, Loss: 1.2795, Train: 46.70%, Valid: 44.81%, Test: 44.68%
Epoch: 500, Loss: 1.2775, Train: 45.78%, Valid: 44.25%, Test: 43.98%
Epoch: 525, Loss: 1.2700, Train: 46.41%, Valid: 44.63%, Test: 44.44%
Epoch: 550, Loss: 1.2731, Train: 47.46%, Valid: 45.14%, Test: 45.21%
Epoch: 575, Loss: 1.2843, Train: 47.53%, Valid: 45.29%, Test: 45.26%
Epoch: 600, Loss: 1.2592, Train: 47.62%, Valid: 45.22%, Test: 45.18%
Epoch: 625, Loss: 1.2690, Train: 47.17%, Valid: 45.23%, Test: 44.93%
Epoch: 650, Loss: 1.2670, Train: 48.09%, Valid: 45.74%, Test: 45.75%
Epoch: 675, Loss: 1.2554, Train: 48.54%, Valid: 45.95%, Test: 45.85%
Epoch: 700, Loss: 1.2547, Train: 47.83%, Valid: 45.68%, Test: 45.28%
Epoch: 725, Loss: 1.2514, Train: 48.40%, Valid: 45.76%, Test: 45.59%
Epoch: 750, Loss: 1.2479, Train: 48.11%, Valid: 45.83%, Test: 45.73%
Epoch: 775, Loss: 1.2550, Train: 47.54%, Valid: 45.31%, Test: 45.09%
Epoch: 800, Loss: 1.2577, Train: 47.88%, Valid: 45.36%, Test: 45.40%
Epoch: 825, Loss: 1.2463, Train: 47.33%, Valid: 45.18%, Test: 45.12%
Epoch: 850, Loss: 1.2589, Train: 48.08%, Valid: 45.71%, Test: 45.49%
Epoch: 875, Loss: 1.2478, Train: 47.65%, Valid: 45.29%, Test: 45.42%
Epoch: 900, Loss: 1.2488, Train: 49.21%, Valid: 46.58%, Test: 46.43%
Epoch: 925, Loss: 1.2461, Train: 47.96%, Valid: 45.64%, Test: 45.63%
Epoch: 950, Loss: 1.2565, Train: 48.61%, Valid: 45.71%, Test: 45.69%
Epoch: 975, Loss: 1.2470, Train: 49.00%, Valid: 46.07%, Test: 46.00%
Run 01:
Highest Train: 49.78
Highest Valid: 46.91
  Final Train: 49.78
   Final Test: 46.85
All runs:
Highest Train: 49.78, nan
Highest Valid: 46.91, nan
  Final Train: 49.78, nan
   Final Test: 46.85, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.0069, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4626, Train: 33.78%, Valid: 33.51%, Test: 34.39%
Epoch: 50, Loss: 1.4264, Train: 39.23%, Valid: 38.74%, Test: 39.21%
Epoch: 75, Loss: 1.4089, Train: 39.97%, Valid: 39.50%, Test: 39.75%
Epoch: 100, Loss: 1.3914, Train: 40.73%, Valid: 40.05%, Test: 40.48%
Epoch: 125, Loss: 1.3740, Train: 41.43%, Valid: 40.73%, Test: 41.13%
Epoch: 150, Loss: 1.3780, Train: 42.03%, Valid: 41.45%, Test: 41.57%
Epoch: 175, Loss: 1.3654, Train: 39.49%, Valid: 39.02%, Test: 39.47%
Epoch: 200, Loss: 1.3787, Train: 42.57%, Valid: 41.99%, Test: 42.31%
Epoch: 225, Loss: 1.3693, Train: 42.37%, Valid: 41.78%, Test: 42.02%
Epoch: 250, Loss: 1.3621, Train: 42.97%, Valid: 42.59%, Test: 42.77%
Epoch: 275, Loss: 1.3583, Train: 41.27%, Valid: 41.05%, Test: 41.37%
Epoch: 300, Loss: 1.3655, Train: 42.87%, Valid: 42.38%, Test: 42.61%
Epoch: 325, Loss: 1.3477, Train: 42.84%, Valid: 42.44%, Test: 42.70%
Epoch: 350, Loss: 1.3565, Train: 40.54%, Valid: 40.18%, Test: 40.52%
Epoch: 375, Loss: 1.3506, Train: 42.78%, Valid: 42.29%, Test: 42.71%
Epoch: 400, Loss: 1.3650, Train: 36.89%, Valid: 36.93%, Test: 36.80%
Epoch: 425, Loss: 1.3692, Train: 43.53%, Valid: 42.87%, Test: 43.26%
Epoch: 450, Loss: 1.3425, Train: 41.32%, Valid: 41.05%, Test: 41.13%
Epoch: 475, Loss: 1.3277, Train: 43.72%, Valid: 43.24%, Test: 43.29%
Epoch: 500, Loss: 1.3769, Train: 43.76%, Valid: 43.21%, Test: 43.44%
Epoch: 525, Loss: 1.3377, Train: 43.29%, Valid: 43.07%, Test: 43.17%
Epoch: 550, Loss: 1.3347, Train: 43.30%, Valid: 43.09%, Test: 43.06%
Epoch: 575, Loss: 1.3256, Train: 41.99%, Valid: 41.80%, Test: 41.88%
Epoch: 600, Loss: 1.3182, Train: 43.79%, Valid: 43.62%, Test: 43.40%
Epoch: 625, Loss: 1.3202, Train: 41.93%, Valid: 41.74%, Test: 41.79%
Epoch: 650, Loss: 1.3325, Train: 43.18%, Valid: 43.13%, Test: 43.17%
Epoch: 675, Loss: 1.3232, Train: 39.70%, Valid: 39.70%, Test: 39.67%
Epoch: 700, Loss: 1.3176, Train: 43.79%, Valid: 43.58%, Test: 43.69%
Epoch: 725, Loss: 1.3199, Train: 43.42%, Valid: 43.17%, Test: 43.39%
Epoch: 750, Loss: 1.3243, Train: 43.10%, Valid: 42.99%, Test: 42.90%
Epoch: 775, Loss: 1.3139, Train: 43.52%, Valid: 43.18%, Test: 43.27%
Epoch: 800, Loss: 1.3089, Train: 43.14%, Valid: 42.89%, Test: 42.97%
Epoch: 825, Loss: 1.3012, Train: 44.75%, Valid: 44.43%, Test: 44.49%
Epoch: 850, Loss: 1.3153, Train: 44.69%, Valid: 44.33%, Test: 44.41%
Epoch: 875, Loss: 1.3026, Train: 43.68%, Valid: 43.38%, Test: 43.62%
Epoch: 900, Loss: 1.3042, Train: 44.45%, Valid: 44.29%, Test: 44.26%
Epoch: 925, Loss: 1.2901, Train: 41.80%, Valid: 41.58%, Test: 41.55%
Epoch: 950, Loss: 1.3179, Train: 43.45%, Valid: 43.41%, Test: 43.24%
Epoch: 975, Loss: 1.3148, Train: 42.14%, Valid: 41.81%, Test: 41.83%
Run 01:
Highest Train: 44.95
Highest Valid: 44.72
  Final Train: 44.88
   Final Test: 44.72
All runs:
Highest Train: 44.95, nan
Highest Valid: 44.72, nan
  Final Train: 44.88, nan
   Final Test: 44.72, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 8.1620, Train: 15.64%, Valid: 15.78%, Test: 15.62%
Epoch: 25, Loss: 10.5337, Train: 33.31%, Valid: 33.21%, Test: 33.10%
Epoch: 50, Loss: 3.3242, Train: 28.11%, Valid: 27.91%, Test: 28.28%
Epoch: 75, Loss: 2.1759, Train: 36.03%, Valid: 35.87%, Test: 35.92%
Epoch: 100, Loss: 1.7796, Train: 34.29%, Valid: 34.13%, Test: 34.28%
Epoch: 125, Loss: 1.6164, Train: 34.31%, Valid: 34.19%, Test: 34.44%
Epoch: 150, Loss: 1.5346, Train: 34.10%, Valid: 33.90%, Test: 34.15%
Epoch: 175, Loss: 1.5211, Train: 34.59%, Valid: 34.34%, Test: 34.83%
Epoch: 200, Loss: 1.5009, Train: 36.26%, Valid: 36.08%, Test: 36.82%
Epoch: 225, Loss: 1.4777, Train: 35.89%, Valid: 35.81%, Test: 36.17%
Epoch: 250, Loss: 1.4592, Train: 36.99%, Valid: 36.88%, Test: 37.49%
Epoch: 275, Loss: 1.4775, Train: 38.11%, Valid: 37.90%, Test: 38.30%
Epoch: 300, Loss: 1.4572, Train: 38.17%, Valid: 37.93%, Test: 38.28%
Epoch: 325, Loss: 1.4502, Train: 38.13%, Valid: 38.00%, Test: 38.38%
Epoch: 350, Loss: 1.4324, Train: 38.27%, Valid: 38.12%, Test: 38.44%
Epoch: 375, Loss: 1.4805, Train: 37.95%, Valid: 37.79%, Test: 38.07%
Epoch: 400, Loss: 1.4433, Train: 38.10%, Valid: 38.01%, Test: 38.39%
Epoch: 425, Loss: 1.4222, Train: 38.47%, Valid: 38.36%, Test: 38.64%
Epoch: 450, Loss: 1.4146, Train: 38.93%, Valid: 38.75%, Test: 38.96%
Epoch: 475, Loss: 1.4126, Train: 37.67%, Valid: 37.59%, Test: 38.00%
Epoch: 500, Loss: 1.4122, Train: 38.92%, Valid: 38.70%, Test: 39.08%
Epoch: 525, Loss: 1.4195, Train: 39.20%, Valid: 39.01%, Test: 39.37%
Epoch: 550, Loss: 1.4040, Train: 39.24%, Valid: 39.02%, Test: 39.38%
Epoch: 575, Loss: 1.4206, Train: 38.89%, Valid: 38.60%, Test: 38.76%
Epoch: 600, Loss: 1.4027, Train: 37.61%, Valid: 37.43%, Test: 37.66%
Epoch: 625, Loss: 1.4074, Train: 39.33%, Valid: 39.13%, Test: 39.41%
Epoch: 650, Loss: 1.4117, Train: 39.29%, Valid: 38.94%, Test: 39.28%
Epoch: 675, Loss: 1.4005, Train: 40.03%, Valid: 39.65%, Test: 39.95%
Epoch: 700, Loss: 1.4037, Train: 39.57%, Valid: 39.24%, Test: 39.55%
Epoch: 725, Loss: 1.3885, Train: 40.27%, Valid: 39.94%, Test: 40.17%
Epoch: 750, Loss: 1.4081, Train: 40.16%, Valid: 39.82%, Test: 40.01%
Epoch: 775, Loss: 1.3929, Train: 40.34%, Valid: 39.94%, Test: 40.22%
Epoch: 800, Loss: 1.3891, Train: 40.25%, Valid: 39.82%, Test: 39.97%
Epoch: 825, Loss: 1.3805, Train: 40.64%, Valid: 40.25%, Test: 40.42%
Epoch: 850, Loss: 1.3834, Train: 40.69%, Valid: 40.35%, Test: 40.62%
Epoch: 875, Loss: 1.3912, Train: 40.40%, Valid: 40.02%, Test: 40.12%
Epoch: 900, Loss: 1.4105, Train: 41.26%, Valid: 40.88%, Test: 40.97%
Epoch: 925, Loss: 1.3838, Train: 40.27%, Valid: 39.80%, Test: 40.02%
Epoch: 950, Loss: 1.3821, Train: 40.70%, Valid: 40.34%, Test: 40.51%
Epoch: 975, Loss: 1.3915, Train: 40.96%, Valid: 40.68%, Test: 40.85%
Run 01:
Highest Train: 41.68
Highest Valid: 41.39
  Final Train: 41.55
   Final Test: 41.42
All runs:
Highest Train: 41.68, nan
Highest Valid: 41.39, nan
  Final Train: 41.55, nan
   Final Test: 41.42, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6035, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4861, Train: 34.97%, Valid: 34.82%, Test: 35.11%
Epoch: 50, Loss: 1.4529, Train: 36.58%, Valid: 36.13%, Test: 36.39%
Epoch: 75, Loss: 1.4261, Train: 38.05%, Valid: 37.12%, Test: 37.56%
Epoch: 100, Loss: 1.3988, Train: 39.16%, Valid: 38.14%, Test: 38.52%
Epoch: 125, Loss: 1.3782, Train: 40.67%, Valid: 39.59%, Test: 39.95%
Epoch: 150, Loss: 1.3699, Train: 42.17%, Valid: 40.99%, Test: 41.32%
Epoch: 175, Loss: 1.3513, Train: 42.92%, Valid: 41.39%, Test: 41.80%
Epoch: 200, Loss: 1.3370, Train: 42.89%, Valid: 41.59%, Test: 41.85%
Epoch: 225, Loss: 1.3339, Train: 42.76%, Valid: 41.33%, Test: 41.61%
Epoch: 250, Loss: 1.3240, Train: 44.08%, Valid: 42.62%, Test: 42.76%
Epoch: 275, Loss: 1.3145, Train: 44.25%, Valid: 42.83%, Test: 42.86%
Epoch: 300, Loss: 1.3028, Train: 45.06%, Valid: 43.30%, Test: 43.42%
Epoch: 325, Loss: 1.3023, Train: 45.32%, Valid: 43.46%, Test: 43.49%
Epoch: 350, Loss: 1.2976, Train: 45.69%, Valid: 43.96%, Test: 43.87%
Epoch: 375, Loss: 1.2932, Train: 45.28%, Valid: 43.78%, Test: 43.66%
Epoch: 400, Loss: 1.2882, Train: 45.61%, Valid: 44.05%, Test: 43.91%
Epoch: 425, Loss: 1.2797, Train: 45.95%, Valid: 44.13%, Test: 44.16%
Epoch: 450, Loss: 1.2883, Train: 46.21%, Valid: 44.33%, Test: 44.24%
Epoch: 475, Loss: 1.2856, Train: 46.56%, Valid: 44.57%, Test: 44.52%
Epoch: 500, Loss: 1.2650, Train: 46.08%, Valid: 44.15%, Test: 44.22%
Epoch: 525, Loss: 1.2735, Train: 46.46%, Valid: 44.49%, Test: 44.50%
Epoch: 550, Loss: 1.2659, Train: 46.85%, Valid: 44.77%, Test: 44.85%
Epoch: 575, Loss: 1.2631, Train: 47.18%, Valid: 44.84%, Test: 44.82%
Epoch: 600, Loss: 1.2635, Train: 46.73%, Valid: 44.40%, Test: 44.28%
Epoch: 625, Loss: 1.2605, Train: 47.40%, Valid: 44.82%, Test: 44.82%
Epoch: 650, Loss: 1.2552, Train: 46.70%, Valid: 44.69%, Test: 44.66%
Epoch: 675, Loss: 1.2579, Train: 47.36%, Valid: 44.99%, Test: 45.05%
Epoch: 700, Loss: 1.2568, Train: 47.08%, Valid: 44.94%, Test: 44.84%
Epoch: 725, Loss: 1.2531, Train: 47.98%, Valid: 45.55%, Test: 45.50%
Epoch: 750, Loss: 1.2481, Train: 48.39%, Valid: 45.72%, Test: 45.84%
Epoch: 775, Loss: 1.2637, Train: 47.69%, Valid: 45.17%, Test: 45.24%
Epoch: 800, Loss: 1.2541, Train: 48.19%, Valid: 45.45%, Test: 45.56%
Epoch: 825, Loss: 1.2493, Train: 47.69%, Valid: 45.17%, Test: 45.26%
Epoch: 850, Loss: 1.2451, Train: 48.07%, Valid: 45.78%, Test: 45.48%
Epoch: 875, Loss: 1.2449, Train: 48.94%, Valid: 45.95%, Test: 45.82%
Epoch: 900, Loss: 1.2526, Train: 48.79%, Valid: 45.88%, Test: 45.86%
Epoch: 925, Loss: 1.2370, Train: 48.33%, Valid: 45.13%, Test: 45.28%
Epoch: 950, Loss: 1.2338, Train: 49.14%, Valid: 46.25%, Test: 46.21%
Epoch: 975, Loss: 1.2385, Train: 48.55%, Valid: 45.72%, Test: 45.76%
Run 01:
Highest Train: 49.24
Highest Valid: 46.27
  Final Train: 49.22
   Final Test: 46.33
All runs:
Highest Train: 49.24, nan
Highest Valid: 46.27, nan
  Final Train: 49.22, nan
   Final Test: 46.33, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.8076, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4358, Train: 36.71%, Valid: 36.32%, Test: 36.54%
Epoch: 50, Loss: 1.4021, Train: 38.34%, Valid: 37.79%, Test: 38.22%
Epoch: 75, Loss: 1.3785, Train: 39.22%, Valid: 38.64%, Test: 39.15%
Epoch: 100, Loss: 1.3633, Train: 40.16%, Valid: 39.78%, Test: 40.02%
Epoch: 125, Loss: 1.3468, Train: 41.36%, Valid: 41.02%, Test: 41.01%
Epoch: 150, Loss: 1.3504, Train: 42.06%, Valid: 41.76%, Test: 41.80%
Epoch: 175, Loss: 1.3457, Train: 42.36%, Valid: 42.01%, Test: 41.97%
Epoch: 200, Loss: 1.3197, Train: 42.81%, Valid: 42.61%, Test: 42.72%
Epoch: 225, Loss: 1.3309, Train: 43.32%, Valid: 43.10%, Test: 42.95%
Epoch: 250, Loss: 1.3209, Train: 42.78%, Valid: 42.41%, Test: 42.37%
Epoch: 275, Loss: 1.3063, Train: 43.37%, Valid: 42.95%, Test: 43.14%
Epoch: 300, Loss: 1.3089, Train: 43.91%, Valid: 43.51%, Test: 43.59%
Epoch: 325, Loss: 1.3594, Train: 42.04%, Valid: 41.66%, Test: 41.76%
Epoch: 350, Loss: 1.3207, Train: 43.22%, Valid: 42.88%, Test: 43.03%
Epoch: 375, Loss: 1.3020, Train: 43.89%, Valid: 43.38%, Test: 43.53%
Epoch: 400, Loss: 1.3297, Train: 43.61%, Valid: 43.28%, Test: 43.04%
Epoch: 425, Loss: 1.2984, Train: 44.42%, Valid: 44.07%, Test: 44.19%
Epoch: 450, Loss: 1.2910, Train: 44.21%, Valid: 43.87%, Test: 44.02%
Epoch: 475, Loss: 1.3168, Train: 44.08%, Valid: 43.81%, Test: 43.77%
Epoch: 500, Loss: 1.2857, Train: 44.39%, Valid: 43.97%, Test: 44.33%
Epoch: 525, Loss: 1.2858, Train: 44.33%, Valid: 43.83%, Test: 44.13%
Epoch: 550, Loss: 1.2993, Train: 44.25%, Valid: 43.69%, Test: 43.94%
Epoch: 575, Loss: 1.2876, Train: 44.83%, Valid: 44.37%, Test: 44.52%
Epoch: 600, Loss: 1.2898, Train: 44.94%, Valid: 44.47%, Test: 44.57%
Epoch: 625, Loss: 1.2811, Train: 44.72%, Valid: 44.30%, Test: 44.61%
Epoch: 650, Loss: 1.2946, Train: 45.01%, Valid: 44.63%, Test: 44.65%
Epoch: 675, Loss: 1.2787, Train: 43.79%, Valid: 43.22%, Test: 43.59%
Epoch: 700, Loss: 1.2814, Train: 45.64%, Valid: 45.05%, Test: 45.18%
Epoch: 725, Loss: 1.2714, Train: 44.75%, Valid: 44.35%, Test: 44.56%
Epoch: 750, Loss: 1.2834, Train: 45.41%, Valid: 44.87%, Test: 44.94%
Epoch: 775, Loss: 1.2745, Train: 45.62%, Valid: 45.06%, Test: 45.34%
Epoch: 800, Loss: 1.2958, Train: 43.54%, Valid: 42.98%, Test: 43.33%
Epoch: 825, Loss: 1.2685, Train: 45.53%, Valid: 44.81%, Test: 45.05%
Epoch: 850, Loss: 1.2757, Train: 45.32%, Valid: 44.63%, Test: 44.79%
Epoch: 875, Loss: 1.2620, Train: 45.93%, Valid: 45.26%, Test: 45.61%
Epoch: 900, Loss: 1.2659, Train: 43.32%, Valid: 43.04%, Test: 42.95%
Epoch: 925, Loss: 1.2687, Train: 45.55%, Valid: 45.05%, Test: 44.89%
Epoch: 950, Loss: 1.2690, Train: 45.72%, Valid: 44.96%, Test: 45.31%
Epoch: 975, Loss: 1.2838, Train: 43.88%, Valid: 43.30%, Test: 43.43%
Run 01:
Highest Train: 46.24
Highest Valid: 45.67
  Final Train: 46.19
   Final Test: 45.73
All runs:
Highest Train: 46.24, nan
Highest Valid: 45.67, nan
  Final Train: 46.19, nan
   Final Test: 45.73, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 13.5642, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 6.2341, Train: 24.67%, Valid: 24.65%, Test: 24.48%
Epoch: 50, Loss: 2.7645, Train: 26.65%, Valid: 26.39%, Test: 26.46%
Epoch: 75, Loss: 1.7770, Train: 28.79%, Valid: 28.40%, Test: 29.09%
Epoch: 100, Loss: 1.6171, Train: 30.54%, Valid: 30.77%, Test: 30.73%
Epoch: 125, Loss: 1.5968, Train: 24.72%, Valid: 24.72%, Test: 25.14%
Epoch: 150, Loss: 1.6565, Train: 36.64%, Valid: 36.63%, Test: 36.60%
Epoch: 175, Loss: 1.5261, Train: 31.84%, Valid: 31.69%, Test: 31.93%
Epoch: 200, Loss: 1.5178, Train: 37.18%, Valid: 37.04%, Test: 36.95%
Epoch: 225, Loss: 1.4686, Train: 37.62%, Valid: 37.13%, Test: 37.40%
Epoch: 250, Loss: 1.5139, Train: 38.83%, Valid: 38.46%, Test: 38.55%
Epoch: 275, Loss: 1.4324, Train: 38.52%, Valid: 38.10%, Test: 38.14%
Epoch: 300, Loss: 1.4347, Train: 37.89%, Valid: 37.64%, Test: 37.63%
Epoch: 325, Loss: 1.4258, Train: 34.15%, Valid: 34.03%, Test: 34.10%
Epoch: 350, Loss: 1.4101, Train: 38.87%, Valid: 38.57%, Test: 38.49%
Epoch: 375, Loss: 1.4132, Train: 39.14%, Valid: 38.69%, Test: 38.84%
Epoch: 400, Loss: 1.4260, Train: 37.17%, Valid: 36.62%, Test: 36.81%
Epoch: 425, Loss: 1.4249, Train: 39.68%, Valid: 39.18%, Test: 39.26%
Epoch: 450, Loss: 1.4009, Train: 39.42%, Valid: 38.98%, Test: 39.12%
Epoch: 475, Loss: 1.4085, Train: 39.51%, Valid: 39.05%, Test: 39.10%
Epoch: 500, Loss: 1.4068, Train: 39.44%, Valid: 38.93%, Test: 39.18%
Epoch: 525, Loss: 1.3983, Train: 39.92%, Valid: 39.46%, Test: 39.53%
Epoch: 550, Loss: 1.4125, Train: 39.77%, Valid: 39.47%, Test: 39.48%
Epoch: 575, Loss: 1.3950, Train: 39.65%, Valid: 39.19%, Test: 39.32%
Epoch: 600, Loss: 1.4007, Train: 39.54%, Valid: 39.11%, Test: 39.25%
Epoch: 625, Loss: 1.4180, Train: 39.11%, Valid: 38.57%, Test: 38.98%
Epoch: 650, Loss: 1.4187, Train: 39.62%, Valid: 39.13%, Test: 39.39%
Epoch: 675, Loss: 1.3896, Train: 39.64%, Valid: 39.31%, Test: 39.48%
Epoch: 700, Loss: 1.3858, Train: 39.78%, Valid: 39.34%, Test: 39.57%
Epoch: 725, Loss: 1.4028, Train: 39.90%, Valid: 39.51%, Test: 39.55%
Epoch: 750, Loss: 1.3898, Train: 39.61%, Valid: 39.27%, Test: 39.40%
Epoch: 775, Loss: 1.3893, Train: 38.18%, Valid: 37.75%, Test: 38.18%
Epoch: 800, Loss: 1.3879, Train: 39.04%, Valid: 38.67%, Test: 38.94%
Epoch: 825, Loss: 1.3984, Train: 38.58%, Valid: 38.18%, Test: 38.41%
Epoch: 850, Loss: 1.3827, Train: 39.79%, Valid: 39.35%, Test: 39.59%
Epoch: 875, Loss: 1.3828, Train: 39.91%, Valid: 39.49%, Test: 39.65%
Epoch: 900, Loss: 1.3808, Train: 39.97%, Valid: 39.57%, Test: 39.74%
Epoch: 925, Loss: 1.3776, Train: 40.08%, Valid: 39.61%, Test: 39.83%
Epoch: 950, Loss: 1.3808, Train: 39.81%, Valid: 39.45%, Test: 39.61%
Epoch: 975, Loss: 1.3762, Train: 40.21%, Valid: 39.70%, Test: 39.98%
Run 01:
Highest Train: 40.31
Highest Valid: 39.86
  Final Train: 40.30
   Final Test: 40.09
All runs:
Highest Train: 40.31, nan
Highest Valid: 39.86, nan
  Final Train: 40.30, nan
   Final Test: 40.09, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6178, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4779, Train: 35.11%, Valid: 34.82%, Test: 35.04%
Epoch: 50, Loss: 1.4375, Train: 37.20%, Valid: 36.89%, Test: 36.95%
Epoch: 75, Loss: 1.4016, Train: 39.10%, Valid: 38.46%, Test: 38.64%
Epoch: 100, Loss: 1.3718, Train: 40.80%, Valid: 40.04%, Test: 40.28%
Epoch: 125, Loss: 1.3489, Train: 42.09%, Valid: 41.17%, Test: 41.23%
Epoch: 150, Loss: 1.3132, Train: 43.09%, Valid: 41.93%, Test: 42.03%
Epoch: 175, Loss: 1.2846, Train: 44.32%, Valid: 42.61%, Test: 42.95%
Epoch: 200, Loss: 1.2702, Train: 45.52%, Valid: 43.69%, Test: 44.21%
Epoch: 225, Loss: 1.2580, Train: 46.47%, Valid: 44.27%, Test: 44.68%
Epoch: 250, Loss: 1.2380, Train: 46.99%, Valid: 44.22%, Test: 44.75%
Epoch: 275, Loss: 1.2194, Train: 47.99%, Valid: 44.99%, Test: 45.09%
Epoch: 300, Loss: 1.2116, Train: 48.29%, Valid: 45.08%, Test: 45.05%
Epoch: 325, Loss: 1.1949, Train: 48.76%, Valid: 45.17%, Test: 45.22%
Epoch: 350, Loss: 1.2192, Train: 48.31%, Valid: 45.06%, Test: 45.41%
Epoch: 375, Loss: 1.1730, Train: 50.09%, Valid: 45.82%, Test: 45.90%
Epoch: 400, Loss: 1.1648, Train: 50.14%, Valid: 45.57%, Test: 45.85%
Epoch: 425, Loss: 1.1812, Train: 49.78%, Valid: 45.30%, Test: 45.97%
Epoch: 450, Loss: 1.1452, Train: 51.43%, Valid: 46.14%, Test: 46.53%
Epoch: 475, Loss: 1.1374, Train: 51.77%, Valid: 46.32%, Test: 46.43%
Epoch: 500, Loss: 1.1312, Train: 52.23%, Valid: 46.55%, Test: 46.99%
Epoch: 525, Loss: 1.1543, Train: 50.75%, Valid: 45.12%, Test: 45.44%
Epoch: 550, Loss: 1.1214, Train: 52.68%, Valid: 46.81%, Test: 47.38%
Epoch: 575, Loss: 1.1225, Train: 52.25%, Valid: 46.41%, Test: 46.88%
Epoch: 600, Loss: 1.0984, Train: 53.70%, Valid: 46.97%, Test: 47.45%
Epoch: 625, Loss: 1.1301, Train: 52.37%, Valid: 46.35%, Test: 46.65%
Epoch: 650, Loss: 1.0885, Train: 53.93%, Valid: 46.87%, Test: 47.58%
Epoch: 675, Loss: 1.0978, Train: 53.66%, Valid: 46.60%, Test: 47.14%
Epoch: 700, Loss: 1.0785, Train: 54.40%, Valid: 47.28%, Test: 47.75%
Epoch: 725, Loss: 1.0784, Train: 54.17%, Valid: 47.41%, Test: 47.88%
Epoch: 750, Loss: 1.0752, Train: 54.18%, Valid: 47.07%, Test: 47.47%
Epoch: 775, Loss: 1.0808, Train: 54.84%, Valid: 47.39%, Test: 47.88%
Epoch: 800, Loss: 1.0615, Train: 55.13%, Valid: 47.40%, Test: 47.95%
Epoch: 825, Loss: 1.0678, Train: 54.14%, Valid: 46.16%, Test: 46.82%
Epoch: 850, Loss: 1.0508, Train: 55.81%, Valid: 47.84%, Test: 48.44%
Epoch: 875, Loss: 1.0463, Train: 55.36%, Valid: 47.71%, Test: 48.23%
Epoch: 900, Loss: 1.0561, Train: 55.65%, Valid: 47.48%, Test: 48.10%
Epoch: 925, Loss: 1.0372, Train: 56.38%, Valid: 47.64%, Test: 48.56%
Epoch: 950, Loss: 1.0349, Train: 56.67%, Valid: 48.05%, Test: 48.43%
Epoch: 975, Loss: 1.0362, Train: 55.96%, Valid: 47.36%, Test: 48.07%
Run 01:
Highest Train: 57.20
Highest Valid: 48.05
  Final Train: 56.67
   Final Test: 48.43
All runs:
Highest Train: 57.20, nan
Highest Valid: 48.05, nan
  Final Train: 56.67, nan
   Final Test: 48.43, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.5558, Train: 19.57%, Valid: 19.84%, Test: 19.41%
Epoch: 25, Loss: 1.5629, Train: 31.07%, Valid: 30.86%, Test: 30.80%
Epoch: 50, Loss: 1.4026, Train: 40.87%, Valid: 40.37%, Test: 40.77%
Epoch: 75, Loss: 1.4029, Train: 41.00%, Valid: 40.57%, Test: 41.16%
Epoch: 100, Loss: 1.3664, Train: 41.27%, Valid: 40.91%, Test: 41.24%
Epoch: 125, Loss: 1.3777, Train: 40.67%, Valid: 40.33%, Test: 40.57%
Epoch: 150, Loss: 1.3695, Train: 41.32%, Valid: 40.92%, Test: 41.19%
Epoch: 175, Loss: 1.3539, Train: 41.36%, Valid: 41.11%, Test: 41.51%
Epoch: 200, Loss: 1.3456, Train: 42.27%, Valid: 41.83%, Test: 41.96%
Epoch: 225, Loss: 1.3421, Train: 41.86%, Valid: 41.58%, Test: 41.84%
Epoch: 250, Loss: 1.3372, Train: 40.67%, Valid: 40.29%, Test: 40.78%
Epoch: 275, Loss: 1.3645, Train: 40.66%, Valid: 40.45%, Test: 40.68%
Epoch: 300, Loss: 1.3415, Train: 41.95%, Valid: 41.67%, Test: 41.97%
Epoch: 325, Loss: 1.3510, Train: 41.98%, Valid: 41.67%, Test: 41.94%
Epoch: 350, Loss: 1.3404, Train: 41.80%, Valid: 41.40%, Test: 41.85%
Epoch: 375, Loss: 1.3337, Train: 42.62%, Valid: 42.15%, Test: 42.50%
Epoch: 400, Loss: 1.3222, Train: 42.82%, Valid: 42.41%, Test: 42.70%
Epoch: 425, Loss: 1.3328, Train: 42.35%, Valid: 41.84%, Test: 42.30%
Epoch: 450, Loss: 1.3286, Train: 42.43%, Valid: 42.04%, Test: 42.47%
Epoch: 475, Loss: 1.3179, Train: 43.40%, Valid: 42.89%, Test: 43.21%
Epoch: 500, Loss: 1.3516, Train: 41.55%, Valid: 41.31%, Test: 41.51%
Epoch: 525, Loss: 1.3275, Train: 42.75%, Valid: 42.36%, Test: 42.71%
Epoch: 550, Loss: 1.3308, Train: 43.04%, Valid: 42.70%, Test: 42.82%
Epoch: 575, Loss: 1.3211, Train: 43.13%, Valid: 42.67%, Test: 43.07%
Epoch: 600, Loss: 1.3154, Train: 43.20%, Valid: 42.71%, Test: 43.02%
Epoch: 625, Loss: 1.3197, Train: 42.88%, Valid: 42.33%, Test: 42.47%
Epoch: 650, Loss: 1.4267, Train: 39.00%, Valid: 38.39%, Test: 38.72%
Epoch: 675, Loss: 1.3612, Train: 41.30%, Valid: 40.86%, Test: 41.13%
Epoch: 700, Loss: 1.3464, Train: 41.79%, Valid: 41.40%, Test: 41.77%
Epoch: 725, Loss: 1.3311, Train: 42.09%, Valid: 41.81%, Test: 42.20%
Epoch: 750, Loss: 1.3379, Train: 41.61%, Valid: 41.18%, Test: 41.52%
Epoch: 775, Loss: 1.3214, Train: 42.96%, Valid: 42.50%, Test: 42.82%
Epoch: 800, Loss: 1.3208, Train: 42.84%, Valid: 42.51%, Test: 42.76%
Epoch: 825, Loss: 1.3131, Train: 43.43%, Valid: 43.00%, Test: 43.20%
Epoch: 850, Loss: 1.3197, Train: 43.12%, Valid: 42.64%, Test: 42.93%
Epoch: 875, Loss: 1.3096, Train: 43.59%, Valid: 43.16%, Test: 43.34%
Epoch: 900, Loss: 1.3091, Train: 43.71%, Valid: 43.31%, Test: 43.47%
Epoch: 925, Loss: 1.3630, Train: 40.64%, Valid: 40.21%, Test: 40.12%
Epoch: 950, Loss: 1.3363, Train: 42.25%, Valid: 41.82%, Test: 42.18%
Epoch: 975, Loss: 1.3183, Train: 42.95%, Valid: 42.68%, Test: 42.92%
Run 01:
Highest Train: 43.76
Highest Valid: 43.35
  Final Train: 43.66
   Final Test: 43.44
All runs:
Highest Train: 43.76, nan
Highest Valid: 43.35, nan
  Final Train: 43.66, nan
   Final Test: 43.44, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 15.3032, Train: 19.81%, Valid: 19.87%, Test: 19.74%
Epoch: 25, Loss: 5.5721, Train: 21.60%, Valid: 21.51%, Test: 21.46%
Epoch: 50, Loss: 1.9712, Train: 24.80%, Valid: 24.97%, Test: 24.84%
Epoch: 75, Loss: 1.6631, Train: 27.79%, Valid: 27.84%, Test: 27.74%
Epoch: 100, Loss: 1.5922, Train: 28.35%, Valid: 28.83%, Test: 28.18%
Epoch: 125, Loss: 1.5521, Train: 28.85%, Valid: 29.21%, Test: 28.82%
Epoch: 150, Loss: 1.5205, Train: 28.78%, Valid: 28.75%, Test: 28.77%
Epoch: 175, Loss: 1.6192, Train: 28.94%, Valid: 29.10%, Test: 29.06%
Epoch: 200, Loss: 1.5122, Train: 30.17%, Valid: 30.23%, Test: 30.32%
Epoch: 225, Loss: 1.4885, Train: 30.17%, Valid: 30.23%, Test: 30.27%
Epoch: 250, Loss: 1.4998, Train: 30.55%, Valid: 30.59%, Test: 30.77%
Epoch: 275, Loss: 1.4674, Train: 31.06%, Valid: 31.13%, Test: 31.37%
Epoch: 300, Loss: 1.4773, Train: 30.66%, Valid: 30.86%, Test: 30.80%
Epoch: 325, Loss: 1.4769, Train: 30.10%, Valid: 30.05%, Test: 30.39%
Epoch: 350, Loss: 1.5479, Train: 31.87%, Valid: 31.82%, Test: 32.35%
Epoch: 375, Loss: 1.4678, Train: 34.81%, Valid: 34.97%, Test: 34.72%
Epoch: 400, Loss: 1.4994, Train: 31.79%, Valid: 31.79%, Test: 32.26%
Epoch: 425, Loss: 1.4580, Train: 31.58%, Valid: 31.63%, Test: 32.20%
Epoch: 450, Loss: 1.4345, Train: 34.03%, Valid: 33.90%, Test: 34.29%
Epoch: 475, Loss: 1.4426, Train: 33.59%, Valid: 33.53%, Test: 34.05%
Epoch: 500, Loss: 1.5454, Train: 31.76%, Valid: 31.73%, Test: 32.43%
Epoch: 525, Loss: 1.4464, Train: 35.01%, Valid: 34.94%, Test: 35.47%
Epoch: 550, Loss: 1.4308, Train: 36.82%, Valid: 36.63%, Test: 37.27%
Epoch: 575, Loss: 1.4148, Train: 37.76%, Valid: 37.55%, Test: 38.12%
Epoch: 600, Loss: 1.4001, Train: 38.37%, Valid: 38.01%, Test: 38.57%
Epoch: 625, Loss: 1.3923, Train: 38.73%, Valid: 38.42%, Test: 38.98%
Epoch: 650, Loss: 1.3871, Train: 39.00%, Valid: 38.72%, Test: 39.21%
Epoch: 675, Loss: 1.4286, Train: 39.52%, Valid: 39.08%, Test: 39.59%
Epoch: 700, Loss: 1.3885, Train: 38.85%, Valid: 38.48%, Test: 39.00%
Epoch: 725, Loss: 1.3789, Train: 39.43%, Valid: 39.02%, Test: 39.42%
Epoch: 750, Loss: 1.3793, Train: 39.72%, Valid: 39.28%, Test: 39.76%
Epoch: 775, Loss: 1.5086, Train: 30.81%, Valid: 30.80%, Test: 31.25%
Epoch: 800, Loss: 1.4262, Train: 36.45%, Valid: 36.15%, Test: 36.88%
Epoch: 825, Loss: 1.4144, Train: 37.63%, Valid: 37.18%, Test: 37.94%
Epoch: 850, Loss: 1.4056, Train: 37.84%, Valid: 37.43%, Test: 38.02%
Epoch: 875, Loss: 1.3971, Train: 38.30%, Valid: 37.85%, Test: 38.49%
Epoch: 900, Loss: 1.3891, Train: 38.79%, Valid: 38.35%, Test: 38.85%
Epoch: 925, Loss: 1.3827, Train: 39.30%, Valid: 38.88%, Test: 39.21%
Epoch: 950, Loss: 1.3986, Train: 38.92%, Valid: 38.59%, Test: 38.89%
Epoch: 975, Loss: 1.3781, Train: 39.03%, Valid: 38.72%, Test: 39.01%
Run 01:
Highest Train: 39.84
Highest Valid: 39.46
  Final Train: 39.73
   Final Test: 39.84
All runs:
Highest Train: 39.84, nan
Highest Valid: 39.46, nan
  Final Train: 39.73, nan
   Final Test: 39.84, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6195, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4729, Train: 35.33%, Valid: 35.00%, Test: 35.22%
Epoch: 50, Loss: 1.4312, Train: 37.83%, Valid: 36.88%, Test: 37.47%
Epoch: 75, Loss: 1.3813, Train: 40.28%, Valid: 39.38%, Test: 39.61%
Epoch: 100, Loss: 1.3432, Train: 42.10%, Valid: 40.82%, Test: 41.08%
Epoch: 125, Loss: 1.3021, Train: 43.44%, Valid: 42.17%, Test: 42.39%
Epoch: 150, Loss: 1.2746, Train: 45.02%, Valid: 43.48%, Test: 43.72%
Epoch: 175, Loss: 1.2575, Train: 45.25%, Valid: 43.15%, Test: 43.63%
Epoch: 200, Loss: 1.2399, Train: 45.75%, Valid: 43.36%, Test: 43.62%
Epoch: 225, Loss: 1.2811, Train: 43.05%, Valid: 41.65%, Test: 41.56%
Epoch: 250, Loss: 1.2021, Train: 48.32%, Valid: 45.28%, Test: 46.03%
Epoch: 275, Loss: 1.1796, Train: 49.20%, Valid: 45.61%, Test: 46.03%
Epoch: 300, Loss: 1.1643, Train: 50.21%, Valid: 46.17%, Test: 46.69%
Epoch: 325, Loss: 1.1484, Train: 50.49%, Valid: 46.39%, Test: 46.91%
Epoch: 350, Loss: 1.1997, Train: 49.55%, Valid: 45.50%, Test: 45.90%
Epoch: 375, Loss: 1.1153, Train: 52.35%, Valid: 46.95%, Test: 47.62%
Epoch: 400, Loss: 1.1174, Train: 52.61%, Valid: 46.65%, Test: 47.50%
Epoch: 425, Loss: 1.1060, Train: 52.71%, Valid: 46.85%, Test: 47.35%
Epoch: 450, Loss: 1.1175, Train: 53.21%, Valid: 46.75%, Test: 47.46%
Epoch: 475, Loss: 1.0657, Train: 54.54%, Valid: 47.33%, Test: 48.39%
Epoch: 500, Loss: 1.0838, Train: 54.08%, Valid: 47.37%, Test: 47.95%
Epoch: 525, Loss: 1.0474, Train: 55.40%, Valid: 47.66%, Test: 48.62%
Epoch: 550, Loss: 1.0533, Train: 55.05%, Valid: 47.55%, Test: 48.38%
Epoch: 575, Loss: 1.0592, Train: 55.04%, Valid: 46.73%, Test: 47.60%
Epoch: 600, Loss: 1.0308, Train: 56.31%, Valid: 48.33%, Test: 48.78%
Epoch: 625, Loss: 1.0153, Train: 56.51%, Valid: 47.97%, Test: 48.65%
Epoch: 650, Loss: 1.0267, Train: 56.24%, Valid: 48.13%, Test: 48.54%
Epoch: 675, Loss: 1.0062, Train: 57.16%, Valid: 48.09%, Test: 48.73%
Epoch: 700, Loss: 1.0042, Train: 57.33%, Valid: 48.18%, Test: 48.77%
Epoch: 725, Loss: 0.9977, Train: 57.43%, Valid: 48.23%, Test: 49.04%
Epoch: 750, Loss: 0.9869, Train: 58.16%, Valid: 48.41%, Test: 49.14%
Epoch: 775, Loss: 0.9822, Train: 58.51%, Valid: 48.47%, Test: 49.25%
Epoch: 800, Loss: 1.0104, Train: 56.88%, Valid: 47.98%, Test: 48.61%
Epoch: 825, Loss: 0.9675, Train: 58.92%, Valid: 48.67%, Test: 49.52%
Epoch: 850, Loss: 0.9797, Train: 58.58%, Valid: 48.44%, Test: 49.32%
Epoch: 875, Loss: 1.0109, Train: 57.64%, Valid: 48.31%, Test: 48.68%
Epoch: 900, Loss: 0.9600, Train: 59.48%, Valid: 48.82%, Test: 49.42%
Epoch: 925, Loss: 0.9950, Train: 56.97%, Valid: 46.38%, Test: 47.31%
Epoch: 950, Loss: 1.0331, Train: 56.41%, Valid: 48.29%, Test: 48.93%
Epoch: 975, Loss: 0.9620, Train: 59.43%, Valid: 48.90%, Test: 49.77%
Run 01:
Highest Train: 60.15
Highest Valid: 49.06
  Final Train: 59.76
   Final Test: 49.73
All runs:
Highest Train: 60.15, nan
Highest Valid: 49.06, nan
  Final Train: 59.76, nan
   Final Test: 49.73, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.2438, Train: 21.34%, Valid: 21.46%, Test: 22.04%
Epoch: 25, Loss: 1.4184, Train: 38.69%, Valid: 38.51%, Test: 38.89%
Epoch: 50, Loss: 1.4024, Train: 39.60%, Valid: 39.20%, Test: 39.74%
Epoch: 75, Loss: 1.3696, Train: 40.82%, Valid: 40.55%, Test: 40.74%
Epoch: 100, Loss: 1.3968, Train: 40.46%, Valid: 40.13%, Test: 40.25%
Epoch: 125, Loss: 1.3586, Train: 40.70%, Valid: 40.32%, Test: 40.74%
Epoch: 150, Loss: 1.3692, Train: 40.05%, Valid: 39.95%, Test: 40.16%
Epoch: 175, Loss: 1.3568, Train: 41.04%, Valid: 40.86%, Test: 41.16%
Epoch: 200, Loss: 1.3422, Train: 42.17%, Valid: 41.78%, Test: 42.21%
Epoch: 225, Loss: 1.4289, Train: 35.99%, Valid: 35.66%, Test: 36.27%
Epoch: 250, Loss: 1.3855, Train: 41.32%, Valid: 40.88%, Test: 41.11%
Epoch: 275, Loss: 1.3625, Train: 41.48%, Valid: 41.15%, Test: 41.56%
Epoch: 300, Loss: 1.3434, Train: 41.70%, Valid: 41.43%, Test: 41.81%
Epoch: 325, Loss: 1.3386, Train: 42.19%, Valid: 41.77%, Test: 42.30%
Epoch: 350, Loss: 1.3459, Train: 42.84%, Valid: 42.52%, Test: 42.77%
Epoch: 375, Loss: 1.3310, Train: 42.15%, Valid: 41.90%, Test: 42.40%
Epoch: 400, Loss: 1.3259, Train: 43.20%, Valid: 42.79%, Test: 43.21%
Epoch: 425, Loss: 1.3474, Train: 43.22%, Valid: 42.94%, Test: 43.19%
Epoch: 450, Loss: 1.3178, Train: 43.33%, Valid: 42.90%, Test: 43.31%
Epoch: 475, Loss: 1.3169, Train: 43.27%, Valid: 42.88%, Test: 43.22%
Epoch: 500, Loss: 1.3612, Train: 41.39%, Valid: 41.03%, Test: 41.51%
Epoch: 525, Loss: 1.3489, Train: 41.99%, Valid: 41.78%, Test: 42.01%
Epoch: 550, Loss: 1.3258, Train: 43.30%, Valid: 42.91%, Test: 43.24%
Epoch: 575, Loss: 1.3393, Train: 41.23%, Valid: 40.91%, Test: 41.29%
Epoch: 600, Loss: 1.3166, Train: 43.18%, Valid: 42.83%, Test: 43.23%
Epoch: 625, Loss: 1.3095, Train: 42.63%, Valid: 42.42%, Test: 42.70%
Epoch: 650, Loss: 1.3158, Train: 43.45%, Valid: 43.12%, Test: 43.41%
Epoch: 675, Loss: 1.3133, Train: 43.12%, Valid: 42.63%, Test: 43.05%
Epoch: 700, Loss: 1.3074, Train: 43.05%, Valid: 42.47%, Test: 42.89%
Epoch: 725, Loss: 1.3778, Train: 38.02%, Valid: 37.88%, Test: 37.81%
Epoch: 750, Loss: 1.3323, Train: 41.79%, Valid: 41.51%, Test: 41.93%
Epoch: 775, Loss: 1.3156, Train: 43.11%, Valid: 42.71%, Test: 43.12%
Epoch: 800, Loss: 1.3458, Train: 42.85%, Valid: 42.59%, Test: 42.85%
Epoch: 825, Loss: 1.3123, Train: 43.37%, Valid: 42.97%, Test: 43.43%
Epoch: 850, Loss: 1.3023, Train: 43.95%, Valid: 43.41%, Test: 43.76%
Epoch: 875, Loss: 1.3344, Train: 43.73%, Valid: 43.16%, Test: 43.63%
Epoch: 900, Loss: 1.3026, Train: 44.01%, Valid: 43.51%, Test: 43.88%
Epoch: 925, Loss: 1.2987, Train: 44.08%, Valid: 43.65%, Test: 43.86%
Epoch: 950, Loss: 1.3696, Train: 42.44%, Valid: 42.11%, Test: 42.32%
Epoch: 975, Loss: 1.3167, Train: 43.05%, Valid: 42.55%, Test: 43.03%
Run 01:
Highest Train: 44.55
Highest Valid: 43.90
  Final Train: 44.43
   Final Test: 44.10
All runs:
Highest Train: 44.55, nan
Highest Valid: 43.90, nan
  Final Train: 44.43, nan
   Final Test: 44.10, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 7.3328, Train: 24.14%, Valid: 24.06%, Test: 23.53%
Epoch: 25, Loss: 2.1153, Train: 28.80%, Valid: 28.49%, Test: 28.93%
Epoch: 50, Loss: 1.9205, Train: 28.36%, Valid: 28.14%, Test: 28.32%
Epoch: 75, Loss: 1.5290, Train: 28.91%, Valid: 28.77%, Test: 28.96%
Epoch: 100, Loss: 1.4767, Train: 31.23%, Valid: 31.23%, Test: 31.45%
Epoch: 125, Loss: 1.4462, Train: 31.68%, Valid: 31.50%, Test: 31.98%
Epoch: 150, Loss: 1.4279, Train: 33.02%, Valid: 32.85%, Test: 33.22%
Epoch: 175, Loss: 1.4117, Train: 34.83%, Valid: 34.61%, Test: 35.07%
Epoch: 200, Loss: 1.4020, Train: 35.60%, Valid: 35.42%, Test: 35.71%
Epoch: 225, Loss: 1.3917, Train: 37.87%, Valid: 37.62%, Test: 37.91%
Epoch: 250, Loss: 1.3858, Train: 38.96%, Valid: 38.73%, Test: 38.82%
Epoch: 275, Loss: 1.3776, Train: 39.77%, Valid: 39.30%, Test: 39.71%
Epoch: 300, Loss: 1.3743, Train: 40.22%, Valid: 39.91%, Test: 40.12%
Epoch: 325, Loss: 1.3655, Train: 40.91%, Valid: 40.78%, Test: 40.70%
Epoch: 350, Loss: 1.3638, Train: 41.04%, Valid: 40.91%, Test: 40.77%
Epoch: 375, Loss: 1.3640, Train: 41.16%, Valid: 40.94%, Test: 40.86%
Epoch: 400, Loss: 1.3550, Train: 41.64%, Valid: 41.46%, Test: 41.47%
Epoch: 425, Loss: 1.3588, Train: 41.17%, Valid: 41.01%, Test: 41.06%
Epoch: 450, Loss: 1.3510, Train: 39.27%, Valid: 39.08%, Test: 39.26%
Epoch: 475, Loss: 1.3755, Train: 40.51%, Valid: 40.13%, Test: 40.50%
Epoch: 500, Loss: 1.3556, Train: 41.15%, Valid: 40.85%, Test: 40.91%
Epoch: 525, Loss: 1.3469, Train: 41.55%, Valid: 41.39%, Test: 41.50%
Epoch: 550, Loss: 1.5605, Train: 32.76%, Valid: 32.86%, Test: 32.59%
Epoch: 575, Loss: 1.3815, Train: 40.13%, Valid: 39.85%, Test: 40.23%
Epoch: 600, Loss: 1.3544, Train: 40.94%, Valid: 40.64%, Test: 40.91%
Epoch: 625, Loss: 1.3450, Train: 41.60%, Valid: 41.23%, Test: 41.38%
Epoch: 650, Loss: 1.3466, Train: 40.74%, Valid: 40.51%, Test: 40.91%
Epoch: 675, Loss: 1.3363, Train: 42.38%, Valid: 41.98%, Test: 42.32%
Epoch: 700, Loss: 1.3715, Train: 40.95%, Valid: 40.56%, Test: 40.90%
Epoch: 725, Loss: 1.3424, Train: 41.59%, Valid: 41.16%, Test: 41.52%
Epoch: 750, Loss: 1.3313, Train: 42.19%, Valid: 41.79%, Test: 42.20%
Epoch: 775, Loss: 1.3510, Train: 42.14%, Valid: 41.83%, Test: 42.18%
Epoch: 800, Loss: 1.3307, Train: 42.45%, Valid: 42.17%, Test: 42.53%
Epoch: 825, Loss: 1.3351, Train: 42.27%, Valid: 41.71%, Test: 42.36%
Epoch: 850, Loss: 1.3303, Train: 42.41%, Valid: 41.87%, Test: 42.24%
Epoch: 875, Loss: 1.3354, Train: 42.62%, Valid: 42.15%, Test: 42.53%
Epoch: 900, Loss: 1.3545, Train: 41.77%, Valid: 41.43%, Test: 41.93%
Epoch: 925, Loss: 1.3263, Train: 43.11%, Valid: 42.53%, Test: 42.89%
Epoch: 950, Loss: 1.3232, Train: 39.76%, Valid: 39.27%, Test: 40.07%
Epoch: 975, Loss: 1.3252, Train: 43.00%, Valid: 42.50%, Test: 42.90%
Run 01:
Highest Train: 43.40
Highest Valid: 43.01
  Final Train: 43.40
   Final Test: 43.43
All runs:
Highest Train: 43.40, nan
Highest Valid: 43.01, nan
  Final Train: 43.40, nan
   Final Test: 43.43, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6043, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4869, Train: 34.93%, Valid: 34.74%, Test: 34.93%
Epoch: 50, Loss: 1.4427, Train: 37.54%, Valid: 36.82%, Test: 37.30%
Epoch: 75, Loss: 1.4078, Train: 39.07%, Valid: 38.28%, Test: 38.68%
Epoch: 100, Loss: 1.3792, Train: 40.68%, Valid: 39.86%, Test: 40.07%
Epoch: 125, Loss: 1.3709, Train: 41.58%, Valid: 40.87%, Test: 40.87%
Epoch: 150, Loss: 1.3524, Train: 41.75%, Valid: 40.95%, Test: 41.32%
Epoch: 175, Loss: 1.3466, Train: 41.07%, Valid: 40.41%, Test: 40.74%
Epoch: 200, Loss: 1.3491, Train: 42.39%, Valid: 41.62%, Test: 41.92%
Epoch: 225, Loss: 1.3365, Train: 42.11%, Valid: 41.43%, Test: 41.68%
Epoch: 250, Loss: 1.3359, Train: 41.61%, Valid: 40.86%, Test: 41.09%
Epoch: 275, Loss: 1.3369, Train: 41.30%, Valid: 40.75%, Test: 40.90%
Epoch: 300, Loss: 1.3203, Train: 43.37%, Valid: 42.47%, Test: 42.81%
Epoch: 325, Loss: 1.3101, Train: 42.75%, Valid: 41.86%, Test: 42.06%
Epoch: 350, Loss: 1.3097, Train: 42.99%, Valid: 41.95%, Test: 42.20%
Epoch: 375, Loss: 1.3090, Train: 44.15%, Valid: 43.21%, Test: 43.34%
Epoch: 400, Loss: 1.3053, Train: 44.33%, Valid: 43.37%, Test: 43.66%
Epoch: 425, Loss: 1.2986, Train: 44.28%, Valid: 43.47%, Test: 43.52%
Epoch: 450, Loss: 1.3060, Train: 44.61%, Valid: 43.59%, Test: 43.85%
Epoch: 475, Loss: 1.2972, Train: 44.73%, Valid: 43.77%, Test: 44.03%
Epoch: 500, Loss: 1.3001, Train: 44.82%, Valid: 43.81%, Test: 44.25%
Epoch: 525, Loss: 1.2960, Train: 43.47%, Valid: 42.46%, Test: 42.79%
Epoch: 550, Loss: 1.2960, Train: 44.34%, Valid: 43.60%, Test: 43.51%
Epoch: 575, Loss: 1.2862, Train: 45.53%, Valid: 44.64%, Test: 44.95%
Epoch: 600, Loss: 1.2900, Train: 44.33%, Valid: 43.43%, Test: 43.72%
Epoch: 625, Loss: 1.2958, Train: 42.71%, Valid: 41.93%, Test: 42.05%
Epoch: 650, Loss: 1.2933, Train: 44.59%, Valid: 43.54%, Test: 43.68%
Epoch: 675, Loss: 1.2853, Train: 46.23%, Valid: 45.04%, Test: 45.59%
Epoch: 700, Loss: 1.3229, Train: 43.77%, Valid: 42.99%, Test: 43.08%
Epoch: 725, Loss: 1.2956, Train: 44.53%, Valid: 43.48%, Test: 43.68%
Epoch: 750, Loss: 1.2838, Train: 43.95%, Valid: 42.79%, Test: 43.44%
Epoch: 775, Loss: 1.2766, Train: 46.07%, Valid: 44.70%, Test: 45.24%
Epoch: 800, Loss: 1.2988, Train: 44.98%, Valid: 44.05%, Test: 44.20%
Epoch: 825, Loss: 1.2800, Train: 45.50%, Valid: 44.49%, Test: 44.78%
Epoch: 850, Loss: 1.2812, Train: 46.04%, Valid: 44.53%, Test: 45.00%
Epoch: 875, Loss: 1.2777, Train: 45.87%, Valid: 44.59%, Test: 45.01%
Epoch: 900, Loss: 1.2853, Train: 45.46%, Valid: 44.29%, Test: 44.65%
Epoch: 925, Loss: 1.2857, Train: 44.77%, Valid: 43.70%, Test: 44.00%
Epoch: 950, Loss: 1.2760, Train: 45.88%, Valid: 44.79%, Test: 44.98%
Epoch: 975, Loss: 1.2834, Train: 44.84%, Valid: 43.95%, Test: 44.32%
Run 01:
Highest Train: 46.82
Highest Valid: 45.34
  Final Train: 46.82
   Final Test: 45.84
All runs:
Highest Train: 46.82, nan
Highest Valid: 45.34, nan
  Final Train: 46.82, nan
   Final Test: 45.84, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.2450, Train: 17.67%, Valid: 17.81%, Test: 17.35%
Epoch: 25, Loss: 1.4875, Train: 28.83%, Valid: 28.65%, Test: 28.91%
Epoch: 50, Loss: 1.4520, Train: 29.03%, Valid: 28.80%, Test: 29.07%
Epoch: 75, Loss: 1.4744, Train: 32.70%, Valid: 32.29%, Test: 32.66%
Epoch: 100, Loss: 1.4369, Train: 30.72%, Valid: 30.40%, Test: 30.65%
Epoch: 125, Loss: 1.4380, Train: 29.14%, Valid: 28.89%, Test: 29.16%
Epoch: 150, Loss: 1.4441, Train: 34.57%, Valid: 33.91%, Test: 35.05%
Epoch: 175, Loss: 1.4241, Train: 37.82%, Valid: 37.50%, Test: 37.81%
Epoch: 200, Loss: 1.4112, Train: 38.51%, Valid: 37.95%, Test: 38.37%
Epoch: 225, Loss: 1.4224, Train: 39.21%, Valid: 38.88%, Test: 38.65%
Epoch: 250, Loss: 1.4173, Train: 39.60%, Valid: 39.26%, Test: 39.26%
Epoch: 275, Loss: 1.4065, Train: 39.10%, Valid: 38.77%, Test: 38.85%
Epoch: 300, Loss: 1.4002, Train: 39.67%, Valid: 39.34%, Test: 39.28%
Epoch: 325, Loss: 1.4056, Train: 39.26%, Valid: 38.87%, Test: 38.91%
Epoch: 350, Loss: 1.3980, Train: 39.77%, Valid: 39.52%, Test: 39.52%
Epoch: 375, Loss: 1.3919, Train: 39.18%, Valid: 38.78%, Test: 38.80%
Epoch: 400, Loss: 1.3913, Train: 39.68%, Valid: 39.22%, Test: 39.49%
Epoch: 425, Loss: 1.3969, Train: 40.61%, Valid: 40.19%, Test: 40.26%
Epoch: 450, Loss: 1.3906, Train: 40.25%, Valid: 39.76%, Test: 40.03%
Epoch: 475, Loss: 1.3705, Train: 40.10%, Valid: 39.62%, Test: 39.91%
Epoch: 500, Loss: 1.3780, Train: 40.35%, Valid: 39.75%, Test: 40.13%
Epoch: 525, Loss: 1.3809, Train: 39.83%, Valid: 39.36%, Test: 39.61%
Epoch: 550, Loss: 1.3734, Train: 40.17%, Valid: 39.64%, Test: 40.01%
Epoch: 575, Loss: 1.3632, Train: 39.98%, Valid: 39.38%, Test: 39.67%
Epoch: 600, Loss: 1.3758, Train: 39.90%, Valid: 39.40%, Test: 39.68%
Epoch: 625, Loss: 1.3790, Train: 40.42%, Valid: 40.10%, Test: 40.27%
Epoch: 650, Loss: 1.3805, Train: 39.31%, Valid: 38.88%, Test: 39.11%
Epoch: 675, Loss: 1.3777, Train: 40.46%, Valid: 39.93%, Test: 40.18%
Epoch: 700, Loss: 1.3883, Train: 39.99%, Valid: 39.43%, Test: 39.74%
Epoch: 725, Loss: 1.4158, Train: 40.57%, Valid: 40.20%, Test: 40.24%
Epoch: 750, Loss: 1.3846, Train: 40.40%, Valid: 40.06%, Test: 40.18%
Epoch: 775, Loss: 1.3844, Train: 39.99%, Valid: 39.55%, Test: 39.79%
Epoch: 800, Loss: 1.3735, Train: 39.98%, Valid: 39.53%, Test: 39.76%
Epoch: 825, Loss: 1.3692, Train: 40.13%, Valid: 39.54%, Test: 39.81%
Epoch: 850, Loss: 1.3626, Train: 40.19%, Valid: 39.77%, Test: 40.02%
Epoch: 875, Loss: 1.3628, Train: 40.54%, Valid: 40.04%, Test: 40.29%
Epoch: 900, Loss: 1.3682, Train: 40.47%, Valid: 39.88%, Test: 40.17%
Epoch: 925, Loss: 1.3633, Train: 41.00%, Valid: 40.63%, Test: 40.77%
Epoch: 950, Loss: 1.3824, Train: 40.49%, Valid: 40.14%, Test: 40.07%
Epoch: 975, Loss: 1.3788, Train: 40.27%, Valid: 39.76%, Test: 39.94%
Run 01:
Highest Train: 41.51
Highest Valid: 40.88
  Final Train: 41.40
   Final Test: 41.08
All runs:
Highest Train: 41.51, nan
Highest Valid: 40.88, nan
  Final Train: 41.40, nan
   Final Test: 41.08, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 33.3389, Train: 17.14%, Valid: 17.21%, Test: 17.67%
Epoch: 25, Loss: 6.7255, Train: 26.43%, Valid: 26.33%, Test: 26.41%
Epoch: 50, Loss: 4.1677, Train: 22.07%, Valid: 22.05%, Test: 22.68%
Epoch: 75, Loss: 2.4555, Train: 27.99%, Valid: 27.69%, Test: 27.98%
Epoch: 100, Loss: 1.9863, Train: 30.56%, Valid: 30.04%, Test: 30.21%
Epoch: 125, Loss: 1.7692, Train: 32.20%, Valid: 31.98%, Test: 32.49%
Epoch: 150, Loss: 1.6644, Train: 31.14%, Valid: 30.73%, Test: 31.37%
Epoch: 175, Loss: 1.5843, Train: 34.43%, Valid: 33.74%, Test: 34.61%
Epoch: 200, Loss: 1.5564, Train: 35.10%, Valid: 34.58%, Test: 35.54%
Epoch: 225, Loss: 1.5204, Train: 37.68%, Valid: 37.19%, Test: 38.07%
Epoch: 250, Loss: 1.5468, Train: 36.64%, Valid: 36.37%, Test: 36.98%
Epoch: 275, Loss: 1.5090, Train: 37.68%, Valid: 37.09%, Test: 38.03%
Epoch: 300, Loss: 1.4987, Train: 36.06%, Valid: 35.87%, Test: 36.70%
Epoch: 325, Loss: 1.4955, Train: 36.49%, Valid: 36.25%, Test: 36.62%
Epoch: 350, Loss: 1.5069, Train: 24.05%, Valid: 23.83%, Test: 24.48%
Epoch: 375, Loss: 1.4802, Train: 37.78%, Valid: 37.39%, Test: 37.88%
Epoch: 400, Loss: 1.4800, Train: 37.93%, Valid: 37.63%, Test: 38.20%
Epoch: 425, Loss: 1.4624, Train: 37.32%, Valid: 36.87%, Test: 37.44%
Epoch: 450, Loss: 1.4645, Train: 35.10%, Valid: 34.93%, Test: 35.17%
Epoch: 475, Loss: 1.4960, Train: 36.46%, Valid: 36.18%, Test: 36.83%
Epoch: 500, Loss: 1.4557, Train: 38.77%, Valid: 38.36%, Test: 38.76%
Epoch: 525, Loss: 1.4748, Train: 36.82%, Valid: 36.45%, Test: 36.80%
Epoch: 550, Loss: 1.4472, Train: 37.96%, Valid: 37.65%, Test: 37.88%
Epoch: 575, Loss: 1.4642, Train: 37.00%, Valid: 36.77%, Test: 36.99%
Epoch: 600, Loss: 1.4521, Train: 37.86%, Valid: 37.51%, Test: 37.81%
Epoch: 625, Loss: 1.4615, Train: 37.84%, Valid: 37.43%, Test: 37.85%
Epoch: 650, Loss: 1.4430, Train: 36.90%, Valid: 36.42%, Test: 36.86%
Epoch: 675, Loss: 1.4418, Train: 38.03%, Valid: 37.80%, Test: 38.07%
Epoch: 700, Loss: 1.4496, Train: 39.46%, Valid: 39.00%, Test: 39.36%
Epoch: 725, Loss: 1.4780, Train: 37.96%, Valid: 37.80%, Test: 37.92%
Epoch: 750, Loss: 1.4476, Train: 37.05%, Valid: 36.69%, Test: 36.97%
Epoch: 775, Loss: 1.4606, Train: 38.33%, Valid: 38.03%, Test: 38.27%
Epoch: 800, Loss: 1.4550, Train: 37.97%, Valid: 37.69%, Test: 37.76%
Epoch: 825, Loss: 1.4610, Train: 38.77%, Valid: 38.40%, Test: 38.86%
Epoch: 850, Loss: 1.4355, Train: 29.38%, Valid: 29.26%, Test: 29.18%
Epoch: 875, Loss: 1.4361, Train: 37.76%, Valid: 37.22%, Test: 37.54%
Epoch: 900, Loss: 1.4365, Train: 37.89%, Valid: 37.56%, Test: 37.74%
Epoch: 925, Loss: 1.4743, Train: 37.06%, Valid: 36.70%, Test: 37.03%
Epoch: 950, Loss: 1.5332, Train: 38.34%, Valid: 38.02%, Test: 38.17%
Epoch: 975, Loss: 1.4304, Train: 37.56%, Valid: 37.20%, Test: 37.48%
Run 01:
Highest Train: 39.77
Highest Valid: 39.40
  Final Train: 39.77
   Final Test: 39.62
All runs:
Highest Train: 39.77, nan
Highest Valid: 39.40, nan
  Final Train: 39.77, nan
   Final Test: 39.62, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6006, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.4738, Train: 35.51%, Valid: 35.23%, Test: 35.43%
Epoch: 50, Loss: 1.4311, Train: 37.94%, Valid: 37.05%, Test: 37.64%
Epoch: 75, Loss: 1.3947, Train: 39.38%, Valid: 38.43%, Test: 38.90%
Epoch: 100, Loss: 1.3632, Train: 40.29%, Valid: 39.57%, Test: 39.72%
Epoch: 125, Loss: 1.3538, Train: 43.11%, Valid: 42.22%, Test: 42.50%
Epoch: 150, Loss: 1.3392, Train: 43.33%, Valid: 42.44%, Test: 42.53%
Epoch: 175, Loss: 1.3275, Train: 43.46%, Valid: 42.35%, Test: 42.70%
Epoch: 200, Loss: 1.3323, Train: 43.01%, Valid: 42.21%, Test: 42.53%
Epoch: 225, Loss: 1.3059, Train: 44.19%, Valid: 43.15%, Test: 43.39%
Epoch: 250, Loss: 1.3143, Train: 44.51%, Valid: 43.62%, Test: 43.96%
Epoch: 275, Loss: 1.2950, Train: 45.03%, Valid: 43.85%, Test: 44.25%
Epoch: 300, Loss: 1.3197, Train: 43.52%, Valid: 42.34%, Test: 42.70%
Epoch: 325, Loss: 1.3040, Train: 44.32%, Valid: 43.24%, Test: 43.61%
Epoch: 350, Loss: 1.2992, Train: 45.01%, Valid: 43.91%, Test: 44.38%
Epoch: 375, Loss: 1.2897, Train: 43.58%, Valid: 42.66%, Test: 42.99%
Epoch: 400, Loss: 1.2978, Train: 44.20%, Valid: 43.15%, Test: 43.65%
Epoch: 425, Loss: 1.3028, Train: 43.52%, Valid: 42.51%, Test: 42.82%
Epoch: 450, Loss: 1.3143, Train: 45.14%, Valid: 44.08%, Test: 44.41%
Epoch: 475, Loss: 1.2838, Train: 43.71%, Valid: 42.44%, Test: 42.93%
Epoch: 500, Loss: 1.2825, Train: 45.53%, Valid: 44.33%, Test: 44.59%
Epoch: 525, Loss: 1.2756, Train: 44.65%, Valid: 43.83%, Test: 43.94%
Epoch: 550, Loss: 1.2798, Train: 45.43%, Valid: 44.12%, Test: 44.58%
Epoch: 575, Loss: 1.2941, Train: 44.99%, Valid: 43.62%, Test: 43.85%
Epoch: 600, Loss: 1.2905, Train: 44.74%, Valid: 43.57%, Test: 43.90%
Epoch: 625, Loss: 1.2891, Train: 45.71%, Valid: 44.50%, Test: 44.90%
Epoch: 650, Loss: 1.2787, Train: 44.37%, Valid: 43.11%, Test: 43.36%
Epoch: 675, Loss: 1.2771, Train: 45.00%, Valid: 43.68%, Test: 44.16%
Epoch: 700, Loss: 1.2681, Train: 46.16%, Valid: 44.89%, Test: 45.37%
Epoch: 725, Loss: 1.2786, Train: 45.00%, Valid: 43.63%, Test: 43.92%
Epoch: 750, Loss: 1.2734, Train: 45.61%, Valid: 44.35%, Test: 44.63%
Epoch: 775, Loss: 1.2700, Train: 45.38%, Valid: 44.00%, Test: 44.43%
Epoch: 800, Loss: 1.2693, Train: 45.46%, Valid: 44.12%, Test: 44.62%
Epoch: 825, Loss: 1.2678, Train: 45.36%, Valid: 43.99%, Test: 44.59%
Epoch: 850, Loss: 1.2815, Train: 45.65%, Valid: 44.27%, Test: 44.70%
Epoch: 875, Loss: 1.2602, Train: 45.93%, Valid: 44.49%, Test: 44.91%
Epoch: 900, Loss: 1.2632, Train: 46.00%, Valid: 44.77%, Test: 44.87%
Epoch: 925, Loss: 1.2693, Train: 46.37%, Valid: 44.79%, Test: 45.40%
Epoch: 950, Loss: 1.2586, Train: 45.41%, Valid: 44.26%, Test: 44.51%
Epoch: 975, Loss: 1.2691, Train: 45.03%, Valid: 43.63%, Test: 43.71%
Run 01:
Highest Train: 47.07
Highest Valid: 45.57
  Final Train: 47.07
   Final Test: 46.01
All runs:
Highest Train: 47.07, nan
Highest Valid: 45.57, nan
  Final Train: 47.07, nan
   Final Test: 46.01, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.9410, Train: 27.73%, Valid: 27.51%, Test: 27.76%
Epoch: 25, Loss: 1.5339, Train: 28.74%, Valid: 28.44%, Test: 28.79%
Epoch: 50, Loss: 1.4574, Train: 37.54%, Valid: 37.29%, Test: 37.60%
Epoch: 75, Loss: 1.4353, Train: 39.31%, Valid: 38.94%, Test: 39.05%
Epoch: 100, Loss: 1.4143, Train: 39.87%, Valid: 39.42%, Test: 39.52%
Epoch: 125, Loss: 1.4055, Train: 39.93%, Valid: 39.56%, Test: 39.66%
Epoch: 150, Loss: 1.4179, Train: 40.14%, Valid: 39.55%, Test: 39.84%
Epoch: 175, Loss: 1.3958, Train: 39.82%, Valid: 39.50%, Test: 39.72%
Epoch: 200, Loss: 1.4056, Train: 39.39%, Valid: 38.86%, Test: 39.03%
Epoch: 225, Loss: 1.4001, Train: 39.97%, Valid: 39.68%, Test: 39.75%
Epoch: 250, Loss: 1.4329, Train: 39.86%, Valid: 39.57%, Test: 39.60%
Epoch: 275, Loss: 1.3903, Train: 40.13%, Valid: 39.74%, Test: 39.81%
Epoch: 300, Loss: 1.3837, Train: 40.38%, Valid: 40.06%, Test: 40.25%
Epoch: 325, Loss: 1.3811, Train: 40.28%, Valid: 40.00%, Test: 40.08%
Epoch: 350, Loss: 1.3824, Train: 39.87%, Valid: 39.45%, Test: 39.76%
Epoch: 375, Loss: 1.3735, Train: 40.08%, Valid: 39.81%, Test: 39.89%
Epoch: 400, Loss: 1.3941, Train: 40.54%, Valid: 40.38%, Test: 40.27%
Epoch: 425, Loss: 1.3965, Train: 40.19%, Valid: 39.84%, Test: 39.78%
Epoch: 450, Loss: 1.4625, Train: 37.61%, Valid: 37.04%, Test: 37.40%
Epoch: 475, Loss: 1.4227, Train: 39.54%, Valid: 39.31%, Test: 39.21%
Epoch: 500, Loss: 1.4038, Train: 39.75%, Valid: 39.36%, Test: 39.50%
Epoch: 525, Loss: 1.3947, Train: 39.93%, Valid: 39.53%, Test: 39.71%
Epoch: 550, Loss: 1.3880, Train: 39.95%, Valid: 39.73%, Test: 39.88%
Epoch: 575, Loss: 1.3849, Train: 39.96%, Valid: 39.47%, Test: 39.74%
Epoch: 600, Loss: 1.3903, Train: 40.12%, Valid: 39.68%, Test: 39.89%
Epoch: 625, Loss: 1.3789, Train: 40.22%, Valid: 39.82%, Test: 40.01%
Epoch: 650, Loss: 1.3784, Train: 39.83%, Valid: 39.52%, Test: 39.48%
Epoch: 675, Loss: 1.3802, Train: 40.16%, Valid: 39.77%, Test: 39.99%
Epoch: 700, Loss: 1.3771, Train: 40.04%, Valid: 39.84%, Test: 39.73%
Epoch: 725, Loss: 1.3701, Train: 40.91%, Valid: 40.46%, Test: 40.59%
Epoch: 750, Loss: 1.3713, Train: 40.43%, Valid: 40.15%, Test: 40.18%
Epoch: 775, Loss: 1.3722, Train: 40.34%, Valid: 40.00%, Test: 40.15%
Epoch: 800, Loss: 1.4375, Train: 39.36%, Valid: 39.00%, Test: 39.05%
Epoch: 825, Loss: 1.4152, Train: 39.58%, Valid: 39.37%, Test: 39.30%
Epoch: 850, Loss: 1.4038, Train: 39.85%, Valid: 39.54%, Test: 39.54%
Epoch: 875, Loss: 1.3925, Train: 40.11%, Valid: 39.75%, Test: 39.90%
Epoch: 900, Loss: 1.3921, Train: 39.56%, Valid: 39.38%, Test: 39.37%
Epoch: 925, Loss: 1.3909, Train: 40.04%, Valid: 39.56%, Test: 39.69%
Epoch: 950, Loss: 1.3834, Train: 40.06%, Valid: 39.80%, Test: 39.89%
Epoch: 975, Loss: 1.3817, Train: 39.25%, Valid: 38.81%, Test: 39.04%
Run 01:
Highest Train: 41.58
Highest Valid: 41.20
  Final Train: 41.58
   Final Test: 41.45
All runs:
Highest Train: 41.58, nan
Highest Valid: 41.20, nan
  Final Train: 41.58, nan
   Final Test: 41.45, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=1000.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 13.2825, Train: 16.32%, Valid: 16.37%, Test: 16.12%
Epoch: 25, Loss: 4.3647, Train: 27.94%, Valid: 27.66%, Test: 27.95%
Epoch: 50, Loss: 3.0886, Train: 25.59%, Valid: 25.42%, Test: 25.49%
Epoch: 75, Loss: 2.0578, Train: 26.80%, Valid: 26.91%, Test: 27.09%
Epoch: 100, Loss: 1.8366, Train: 24.68%, Valid: 24.75%, Test: 24.99%
Epoch: 125, Loss: 1.7765, Train: 24.18%, Valid: 24.01%, Test: 24.62%
Epoch: 150, Loss: 1.6299, Train: 31.40%, Valid: 31.31%, Test: 31.72%
Epoch: 175, Loss: 1.5604, Train: 31.42%, Valid: 31.33%, Test: 31.58%
Epoch: 200, Loss: 1.5354, Train: 23.61%, Valid: 23.64%, Test: 23.49%
Epoch: 225, Loss: 1.5155, Train: 27.91%, Valid: 27.79%, Test: 28.20%
Epoch: 250, Loss: 1.5045, Train: 31.93%, Valid: 31.71%, Test: 32.26%
Epoch: 275, Loss: 1.4961, Train: 31.96%, Valid: 31.71%, Test: 32.26%
Epoch: 300, Loss: 1.5217, Train: 35.72%, Valid: 35.69%, Test: 35.68%
Epoch: 325, Loss: 1.4845, Train: 30.60%, Valid: 30.52%, Test: 30.89%
Epoch: 350, Loss: 1.4832, Train: 30.19%, Valid: 30.01%, Test: 30.53%
Epoch: 375, Loss: 1.4835, Train: 34.36%, Valid: 34.16%, Test: 34.40%
Epoch: 400, Loss: 1.4769, Train: 34.67%, Valid: 34.35%, Test: 34.95%
Epoch: 425, Loss: 1.5014, Train: 34.18%, Valid: 34.01%, Test: 34.25%
Epoch: 450, Loss: 1.4854, Train: 33.19%, Valid: 33.21%, Test: 33.31%
Epoch: 475, Loss: 1.4689, Train: 34.76%, Valid: 34.69%, Test: 34.74%
Epoch: 500, Loss: 1.4578, Train: 36.90%, Valid: 36.70%, Test: 36.64%
Epoch: 525, Loss: 1.4667, Train: 34.98%, Valid: 34.81%, Test: 34.93%
Epoch: 550, Loss: 1.4476, Train: 33.25%, Valid: 33.06%, Test: 33.34%
Epoch: 575, Loss: 1.4639, Train: 36.96%, Valid: 36.87%, Test: 36.91%
Epoch: 600, Loss: 1.4571, Train: 34.41%, Valid: 34.22%, Test: 34.64%
Epoch: 625, Loss: 1.4587, Train: 37.36%, Valid: 37.45%, Test: 37.15%
Epoch: 650, Loss: 1.4701, Train: 36.60%, Valid: 36.58%, Test: 36.65%
Epoch: 675, Loss: 1.4560, Train: 37.66%, Valid: 37.55%, Test: 37.46%
Epoch: 700, Loss: 1.4423, Train: 37.75%, Valid: 37.62%, Test: 37.73%
Epoch: 725, Loss: 1.4621, Train: 38.54%, Valid: 38.55%, Test: 38.46%
Epoch: 750, Loss: 1.4563, Train: 35.74%, Valid: 35.59%, Test: 35.75%
Epoch: 775, Loss: 1.4558, Train: 37.96%, Valid: 37.65%, Test: 37.87%
Epoch: 800, Loss: 1.4481, Train: 38.20%, Valid: 37.94%, Test: 37.80%
Epoch: 825, Loss: 1.4322, Train: 38.66%, Valid: 38.37%, Test: 38.34%
Epoch: 850, Loss: 1.4162, Train: 38.30%, Valid: 38.27%, Test: 38.16%
Epoch: 875, Loss: 1.4340, Train: 35.92%, Valid: 35.71%, Test: 35.82%
Epoch: 900, Loss: 1.4199, Train: 38.11%, Valid: 37.93%, Test: 37.94%
Epoch: 925, Loss: 1.4198, Train: 36.84%, Valid: 36.45%, Test: 36.59%
Epoch: 950, Loss: 1.4117, Train: 38.72%, Valid: 38.46%, Test: 38.72%
Epoch: 975, Loss: 1.4462, Train: 37.77%, Valid: 37.33%, Test: 37.78%
Run 01:
Highest Train: 38.99
Highest Valid: 38.86
  Final Train: 38.99
   Final Test: 38.83
All runs:
Highest Train: 38.99, nan
Highest Valid: 38.86, nan
  Final Train: 38.99, nan
   Final Test: 38.83, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6113, Train: 18.46%, Valid: 18.32%, Test: 18.09%
Epoch: 25, Loss: 1.5935, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 50, Loss: 1.5645, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 75, Loss: 1.5305, Train: 32.65%, Valid: 32.38%, Test: 32.76%
Epoch: 100, Loss: 1.5029, Train: 34.17%, Valid: 33.99%, Test: 34.17%
Epoch: 125, Loss: 1.4951, Train: 34.53%, Valid: 34.22%, Test: 34.46%
Epoch: 150, Loss: 1.4898, Train: 34.75%, Valid: 34.44%, Test: 34.58%
Epoch: 175, Loss: 1.4845, Train: 34.95%, Valid: 34.64%, Test: 34.82%
Epoch: 200, Loss: 1.4780, Train: 35.40%, Valid: 34.95%, Test: 35.30%
Epoch: 225, Loss: 1.4707, Train: 35.85%, Valid: 35.18%, Test: 35.76%
Epoch: 250, Loss: 1.4640, Train: 36.25%, Valid: 35.55%, Test: 35.94%
Epoch: 275, Loss: 1.4580, Train: 36.60%, Valid: 35.77%, Test: 36.16%
Epoch: 300, Loss: 1.4529, Train: 36.81%, Valid: 35.96%, Test: 36.35%
Epoch: 325, Loss: 1.4485, Train: 37.02%, Valid: 36.02%, Test: 36.49%
Epoch: 350, Loss: 1.4445, Train: 37.29%, Valid: 36.16%, Test: 36.56%
Epoch: 375, Loss: 1.4410, Train: 37.44%, Valid: 36.27%, Test: 36.58%
Epoch: 400, Loss: 1.4379, Train: 37.63%, Valid: 36.25%, Test: 36.58%
Epoch: 425, Loss: 1.4353, Train: 37.68%, Valid: 36.21%, Test: 36.58%
Epoch: 450, Loss: 1.4328, Train: 37.88%, Valid: 36.24%, Test: 36.54%
Epoch: 475, Loss: 1.4305, Train: 37.97%, Valid: 36.18%, Test: 36.57%
Epoch: 500, Loss: 1.4282, Train: 38.06%, Valid: 36.09%, Test: 36.56%
Epoch: 525, Loss: 1.4261, Train: 38.18%, Valid: 36.15%, Test: 36.56%
Epoch: 550, Loss: 1.4238, Train: 38.25%, Valid: 36.18%, Test: 36.56%
Epoch: 575, Loss: 1.4216, Train: 38.39%, Valid: 36.15%, Test: 36.58%
Epoch: 600, Loss: 1.4194, Train: 38.48%, Valid: 36.23%, Test: 36.65%
Epoch: 625, Loss: 1.4175, Train: 38.57%, Valid: 36.14%, Test: 36.63%
Epoch: 650, Loss: 1.4157, Train: 38.67%, Valid: 36.13%, Test: 36.57%
Epoch: 675, Loss: 1.4140, Train: 38.79%, Valid: 36.10%, Test: 36.51%
Epoch: 700, Loss: 1.4121, Train: 38.91%, Valid: 36.11%, Test: 36.56%
Epoch: 725, Loss: 1.4103, Train: 38.99%, Valid: 36.08%, Test: 36.49%
Epoch: 750, Loss: 1.4089, Train: 39.06%, Valid: 36.05%, Test: 36.38%
Epoch: 775, Loss: 1.4071, Train: 39.23%, Valid: 36.04%, Test: 36.37%
Epoch: 800, Loss: 1.4061, Train: 39.24%, Valid: 36.04%, Test: 36.22%
Epoch: 825, Loss: 1.4038, Train: 39.31%, Valid: 36.06%, Test: 36.30%
Epoch: 850, Loss: 1.4030, Train: 39.35%, Valid: 35.99%, Test: 36.15%
Epoch: 875, Loss: 1.4010, Train: 39.44%, Valid: 35.97%, Test: 36.25%
Epoch: 900, Loss: 1.3996, Train: 39.51%, Valid: 35.95%, Test: 36.24%
Epoch: 925, Loss: 1.3982, Train: 39.56%, Valid: 35.92%, Test: 36.21%
Epoch: 950, Loss: 1.3969, Train: 39.64%, Valid: 35.86%, Test: 36.07%
Epoch: 975, Loss: 1.3959, Train: 39.68%, Valid: 35.79%, Test: 35.97%
Run 01:
Highest Train: 39.81
Highest Valid: 36.31
  Final Train: 37.80
   Final Test: 36.51
All runs:
Highest Train: 39.81, nan
Highest Valid: 36.31, nan
  Final Train: 37.80, nan
   Final Test: 36.51, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6082, Train: 33.38%, Valid: 33.17%, Test: 33.18%
Epoch: 25, Loss: 1.5909, Train: 34.78%, Valid: 34.58%, Test: 34.74%
Epoch: 50, Loss: 1.5586, Train: 32.22%, Valid: 32.04%, Test: 32.87%
Epoch: 75, Loss: 1.5267, Train: 32.79%, Valid: 32.56%, Test: 33.42%
Epoch: 100, Loss: 1.5141, Train: 28.71%, Valid: 28.53%, Test: 28.82%
Epoch: 125, Loss: 1.5038, Train: 29.16%, Valid: 28.80%, Test: 29.25%
Epoch: 150, Loss: 1.4936, Train: 33.57%, Valid: 33.28%, Test: 34.08%
Epoch: 175, Loss: 1.4840, Train: 33.65%, Valid: 33.30%, Test: 34.16%
Epoch: 200, Loss: 1.4756, Train: 33.80%, Valid: 33.44%, Test: 34.34%
Epoch: 225, Loss: 1.4680, Train: 34.11%, Valid: 33.73%, Test: 34.56%
Epoch: 250, Loss: 1.4606, Train: 34.73%, Valid: 34.34%, Test: 35.14%
Epoch: 275, Loss: 1.4539, Train: 38.24%, Valid: 37.78%, Test: 38.23%
Epoch: 300, Loss: 1.4481, Train: 39.95%, Valid: 39.66%, Test: 39.54%
Epoch: 325, Loss: 1.4470, Train: 39.51%, Valid: 39.12%, Test: 39.14%
Epoch: 350, Loss: 1.4399, Train: 39.93%, Valid: 39.63%, Test: 39.61%
Epoch: 375, Loss: 1.4357, Train: 39.71%, Valid: 39.51%, Test: 39.47%
Epoch: 400, Loss: 1.4321, Train: 39.53%, Valid: 39.26%, Test: 39.25%
Epoch: 425, Loss: 1.4287, Train: 39.40%, Valid: 39.19%, Test: 39.12%
Epoch: 450, Loss: 1.4255, Train: 39.35%, Valid: 39.10%, Test: 39.08%
Epoch: 475, Loss: 1.4225, Train: 39.30%, Valid: 39.06%, Test: 39.04%
Epoch: 500, Loss: 1.4195, Train: 39.27%, Valid: 39.03%, Test: 38.97%
Epoch: 525, Loss: 1.4193, Train: 38.48%, Valid: 38.33%, Test: 38.35%
Epoch: 550, Loss: 1.4261, Train: 38.92%, Valid: 38.79%, Test: 38.69%
Epoch: 575, Loss: 1.4171, Train: 39.48%, Valid: 39.27%, Test: 39.17%
Epoch: 600, Loss: 1.4138, Train: 39.51%, Valid: 39.32%, Test: 39.20%
Epoch: 625, Loss: 1.4115, Train: 39.52%, Valid: 39.33%, Test: 39.22%
Epoch: 650, Loss: 1.4093, Train: 39.51%, Valid: 39.34%, Test: 39.23%
Epoch: 675, Loss: 1.4074, Train: 39.54%, Valid: 39.36%, Test: 39.29%
Epoch: 700, Loss: 1.4055, Train: 39.55%, Valid: 39.36%, Test: 39.32%
Epoch: 725, Loss: 1.4037, Train: 39.58%, Valid: 39.38%, Test: 39.32%
Epoch: 750, Loss: 1.4019, Train: 39.59%, Valid: 39.38%, Test: 39.32%
Epoch: 775, Loss: 1.4001, Train: 39.66%, Valid: 39.43%, Test: 39.36%
Epoch: 800, Loss: 1.3982, Train: 39.72%, Valid: 39.47%, Test: 39.41%
Epoch: 825, Loss: 1.3964, Train: 39.76%, Valid: 39.54%, Test: 39.47%
Epoch: 850, Loss: 1.3945, Train: 39.83%, Valid: 39.58%, Test: 39.58%
Epoch: 875, Loss: 1.3926, Train: 39.91%, Valid: 39.65%, Test: 39.67%
Epoch: 900, Loss: 1.3908, Train: 40.00%, Valid: 39.78%, Test: 39.72%
Epoch: 925, Loss: 1.3888, Train: 40.32%, Valid: 40.17%, Test: 40.04%
Epoch: 950, Loss: 1.3867, Train: 40.68%, Valid: 40.46%, Test: 40.35%
Epoch: 975, Loss: 1.4680, Train: 35.43%, Valid: 35.20%, Test: 35.23%
Run 01:
Highest Train: 40.89
Highest Valid: 40.64
  Final Train: 40.87
   Final Test: 40.66
All runs:
Highest Train: 40.89, nan
Highest Valid: 40.64, nan
  Final Train: 40.87, nan
   Final Test: 40.66, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6593, Train: 28.22%, Valid: 28.06%, Test: 28.23%
Epoch: 25, Loss: 1.4682, Train: 30.57%, Valid: 30.49%, Test: 30.75%
Epoch: 50, Loss: 1.4478, Train: 30.98%, Valid: 30.87%, Test: 30.94%
Epoch: 75, Loss: 1.4257, Train: 31.17%, Valid: 31.06%, Test: 31.15%
Epoch: 100, Loss: 1.4151, Train: 32.89%, Valid: 32.92%, Test: 32.83%
Epoch: 125, Loss: 1.4103, Train: 33.35%, Valid: 33.19%, Test: 33.25%
Epoch: 150, Loss: 1.4038, Train: 33.14%, Valid: 33.05%, Test: 33.10%
Epoch: 175, Loss: 1.4047, Train: 34.24%, Valid: 34.16%, Test: 34.19%
Epoch: 200, Loss: 1.3964, Train: 34.49%, Valid: 34.42%, Test: 34.71%
Epoch: 225, Loss: 1.3936, Train: 34.72%, Valid: 34.60%, Test: 34.82%
Epoch: 250, Loss: 1.3973, Train: 34.61%, Valid: 34.54%, Test: 34.67%
Epoch: 275, Loss: 1.3932, Train: 34.71%, Valid: 34.65%, Test: 34.79%
Epoch: 300, Loss: 1.3883, Train: 34.72%, Valid: 34.71%, Test: 35.01%
Epoch: 325, Loss: 1.3858, Train: 34.91%, Valid: 34.88%, Test: 35.26%
Epoch: 350, Loss: 1.3853, Train: 34.73%, Valid: 34.82%, Test: 35.18%
Epoch: 375, Loss: 1.3825, Train: 35.06%, Valid: 35.03%, Test: 35.42%
Epoch: 400, Loss: 1.3817, Train: 35.15%, Valid: 35.07%, Test: 35.47%
Epoch: 425, Loss: 1.3807, Train: 35.31%, Valid: 35.24%, Test: 35.62%
Epoch: 450, Loss: 1.3794, Train: 34.76%, Valid: 34.77%, Test: 35.33%
Epoch: 475, Loss: 1.3773, Train: 34.64%, Valid: 34.69%, Test: 35.23%
Epoch: 500, Loss: 1.4460, Train: 29.09%, Valid: 28.91%, Test: 29.32%
Epoch: 525, Loss: 1.4095, Train: 31.37%, Valid: 31.42%, Test: 31.50%
Epoch: 550, Loss: 1.3955, Train: 33.37%, Valid: 33.18%, Test: 33.34%
Epoch: 575, Loss: 1.3884, Train: 33.92%, Valid: 33.96%, Test: 34.08%
Epoch: 600, Loss: 1.3847, Train: 34.19%, Valid: 34.22%, Test: 34.40%
Epoch: 625, Loss: 1.3823, Train: 34.46%, Valid: 34.41%, Test: 34.72%
Epoch: 650, Loss: 1.3806, Train: 34.56%, Valid: 34.54%, Test: 34.84%
Epoch: 675, Loss: 1.3808, Train: 34.66%, Valid: 34.59%, Test: 34.88%
Epoch: 700, Loss: 1.3776, Train: 34.58%, Valid: 34.68%, Test: 34.94%
Epoch: 725, Loss: 1.3768, Train: 34.52%, Valid: 34.60%, Test: 34.86%
Epoch: 750, Loss: 1.3752, Train: 35.00%, Valid: 34.98%, Test: 35.19%
Epoch: 775, Loss: 1.3752, Train: 35.02%, Valid: 35.01%, Test: 35.24%
Epoch: 800, Loss: 1.3740, Train: 35.06%, Valid: 34.97%, Test: 35.24%
Epoch: 825, Loss: 1.3715, Train: 35.21%, Valid: 35.14%, Test: 35.44%
Epoch: 850, Loss: 1.3726, Train: 35.01%, Valid: 35.07%, Test: 35.37%
Epoch: 875, Loss: 1.3694, Train: 35.20%, Valid: 35.21%, Test: 35.47%
Epoch: 900, Loss: 1.3728, Train: 35.40%, Valid: 35.42%, Test: 35.68%
Epoch: 925, Loss: 1.3677, Train: 35.22%, Valid: 35.24%, Test: 35.57%
Epoch: 950, Loss: 1.3665, Train: 35.09%, Valid: 35.19%, Test: 35.54%
Epoch: 975, Loss: 1.3659, Train: 35.63%, Valid: 35.63%, Test: 35.94%
Run 01:
Highest Train: 35.86
Highest Valid: 35.72
  Final Train: 35.56
   Final Test: 36.01
All runs:
Highest Train: 35.86, nan
Highest Valid: 35.72, nan
  Final Train: 35.56, nan
   Final Test: 36.01, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6118, Train: 17.64%, Valid: 17.78%, Test: 17.32%
Epoch: 25, Loss: 1.5852, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 50, Loss: 1.5631, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 75, Loss: 1.5319, Train: 32.41%, Valid: 32.21%, Test: 32.61%
Epoch: 100, Loss: 1.5035, Train: 34.14%, Valid: 33.98%, Test: 34.18%
Epoch: 125, Loss: 1.4954, Train: 34.46%, Valid: 34.22%, Test: 34.41%
Epoch: 150, Loss: 1.4899, Train: 34.69%, Valid: 34.38%, Test: 34.61%
Epoch: 175, Loss: 1.4842, Train: 35.00%, Valid: 34.72%, Test: 34.80%
Epoch: 200, Loss: 1.4778, Train: 35.46%, Valid: 34.96%, Test: 35.33%
Epoch: 225, Loss: 1.4708, Train: 35.93%, Valid: 35.31%, Test: 35.69%
Epoch: 250, Loss: 1.4642, Train: 36.25%, Valid: 35.49%, Test: 35.81%
Epoch: 275, Loss: 1.4582, Train: 36.51%, Valid: 35.74%, Test: 36.01%
Epoch: 300, Loss: 1.4528, Train: 36.79%, Valid: 35.97%, Test: 36.20%
Epoch: 325, Loss: 1.4480, Train: 37.08%, Valid: 36.10%, Test: 36.31%
Epoch: 350, Loss: 1.4437, Train: 37.34%, Valid: 36.17%, Test: 36.28%
Epoch: 375, Loss: 1.4398, Train: 37.54%, Valid: 36.19%, Test: 36.28%
Epoch: 400, Loss: 1.4362, Train: 37.70%, Valid: 36.22%, Test: 36.37%
Epoch: 425, Loss: 1.4327, Train: 37.87%, Valid: 36.26%, Test: 36.35%
Epoch: 450, Loss: 1.4295, Train: 37.99%, Valid: 36.24%, Test: 36.35%
Epoch: 475, Loss: 1.4265, Train: 38.15%, Valid: 36.19%, Test: 36.32%
Epoch: 500, Loss: 1.4240, Train: 38.28%, Valid: 36.23%, Test: 36.38%
Epoch: 525, Loss: 1.4213, Train: 38.48%, Valid: 36.18%, Test: 36.41%
Epoch: 550, Loss: 1.4192, Train: 38.50%, Valid: 36.22%, Test: 36.36%
Epoch: 575, Loss: 1.4167, Train: 38.69%, Valid: 36.22%, Test: 36.49%
Epoch: 600, Loss: 1.4150, Train: 38.69%, Valid: 36.14%, Test: 36.42%
Epoch: 625, Loss: 1.4123, Train: 38.87%, Valid: 36.08%, Test: 36.41%
Epoch: 650, Loss: 1.4103, Train: 38.98%, Valid: 36.09%, Test: 36.40%
Epoch: 675, Loss: 1.4084, Train: 39.15%, Valid: 36.12%, Test: 36.40%
Epoch: 700, Loss: 1.4067, Train: 39.27%, Valid: 36.01%, Test: 36.43%
Epoch: 725, Loss: 1.4047, Train: 39.39%, Valid: 35.88%, Test: 36.40%
Epoch: 750, Loss: 1.4030, Train: 39.40%, Valid: 35.94%, Test: 36.48%
Epoch: 775, Loss: 1.4014, Train: 39.58%, Valid: 35.78%, Test: 36.43%
Epoch: 800, Loss: 1.4000, Train: 39.68%, Valid: 35.74%, Test: 36.38%
Epoch: 825, Loss: 1.3984, Train: 39.78%, Valid: 35.75%, Test: 36.46%
Epoch: 850, Loss: 1.3971, Train: 39.82%, Valid: 35.69%, Test: 36.39%
Epoch: 875, Loss: 1.3957, Train: 39.89%, Valid: 35.81%, Test: 36.48%
Epoch: 900, Loss: 1.3943, Train: 39.92%, Valid: 35.68%, Test: 36.37%
Epoch: 925, Loss: 1.3928, Train: 40.01%, Valid: 35.80%, Test: 36.42%
Epoch: 950, Loss: 1.3918, Train: 40.04%, Valid: 35.76%, Test: 36.31%
Epoch: 975, Loss: 1.3900, Train: 40.11%, Valid: 35.77%, Test: 36.41%
Run 01:
Highest Train: 40.29
Highest Valid: 36.30
  Final Train: 37.79
   Final Test: 36.38
All runs:
Highest Train: 40.29, nan
Highest Valid: 36.30, nan
  Final Train: 37.79, nan
   Final Test: 36.38, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6070, Train: 29.87%, Valid: 29.85%, Test: 30.54%
Epoch: 25, Loss: 1.5851, Train: 28.75%, Valid: 28.46%, Test: 28.88%
Epoch: 50, Loss: 1.5519, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 75, Loss: 1.5317, Train: 28.72%, Valid: 28.53%, Test: 28.82%
Epoch: 100, Loss: 1.5166, Train: 28.71%, Valid: 28.49%, Test: 28.81%
Epoch: 125, Loss: 1.5062, Train: 30.21%, Valid: 29.95%, Test: 30.18%
Epoch: 150, Loss: 1.4986, Train: 32.95%, Valid: 32.70%, Test: 33.29%
Epoch: 175, Loss: 1.4919, Train: 33.13%, Valid: 32.87%, Test: 33.59%
Epoch: 200, Loss: 1.4852, Train: 34.00%, Valid: 33.60%, Test: 34.30%
Epoch: 225, Loss: 1.4777, Train: 35.70%, Valid: 35.47%, Test: 36.22%
Epoch: 250, Loss: 1.4697, Train: 37.97%, Valid: 37.80%, Test: 38.02%
Epoch: 275, Loss: 1.4618, Train: 39.25%, Valid: 39.07%, Test: 39.05%
Epoch: 300, Loss: 1.4544, Train: 39.73%, Valid: 39.40%, Test: 39.50%
Epoch: 325, Loss: 1.4475, Train: 39.84%, Valid: 39.51%, Test: 39.67%
Epoch: 350, Loss: 1.4603, Train: 38.93%, Valid: 38.77%, Test: 38.98%
Epoch: 375, Loss: 1.4390, Train: 39.80%, Valid: 39.51%, Test: 39.58%
Epoch: 400, Loss: 1.4331, Train: 39.91%, Valid: 39.63%, Test: 39.80%
Epoch: 425, Loss: 1.4285, Train: 39.97%, Valid: 39.63%, Test: 39.91%
Epoch: 450, Loss: 1.4242, Train: 40.09%, Valid: 39.79%, Test: 39.97%
Epoch: 475, Loss: 1.4203, Train: 40.21%, Valid: 39.91%, Test: 40.06%
Epoch: 500, Loss: 1.4167, Train: 40.29%, Valid: 40.11%, Test: 40.19%
Epoch: 525, Loss: 1.4131, Train: 40.39%, Valid: 40.26%, Test: 40.35%
Epoch: 550, Loss: 1.4097, Train: 40.55%, Valid: 40.39%, Test: 40.53%
Epoch: 575, Loss: 1.4064, Train: 40.76%, Valid: 40.51%, Test: 40.69%
Epoch: 600, Loss: 1.4031, Train: 40.86%, Valid: 40.68%, Test: 40.87%
Epoch: 625, Loss: 1.4486, Train: 38.05%, Valid: 37.80%, Test: 38.12%
Epoch: 650, Loss: 1.4082, Train: 40.60%, Valid: 40.35%, Test: 40.47%
Epoch: 675, Loss: 1.4007, Train: 40.48%, Valid: 40.27%, Test: 40.53%
Epoch: 700, Loss: 1.3976, Train: 40.48%, Valid: 40.35%, Test: 40.65%
Epoch: 725, Loss: 1.3950, Train: 40.48%, Valid: 40.31%, Test: 40.68%
Epoch: 750, Loss: 1.3928, Train: 40.54%, Valid: 40.38%, Test: 40.80%
Epoch: 775, Loss: 1.3903, Train: 40.67%, Valid: 40.48%, Test: 40.92%
Epoch: 800, Loss: 1.3876, Train: 40.73%, Valid: 40.55%, Test: 41.00%
Epoch: 825, Loss: 1.3854, Train: 40.78%, Valid: 40.62%, Test: 41.05%
Epoch: 850, Loss: 1.3833, Train: 40.89%, Valid: 40.67%, Test: 41.18%
Epoch: 875, Loss: 1.3815, Train: 40.85%, Valid: 40.62%, Test: 41.09%
Epoch: 900, Loss: 1.4850, Train: 37.51%, Valid: 37.17%, Test: 37.67%
Epoch: 925, Loss: 1.3989, Train: 40.38%, Valid: 40.17%, Test: 40.52%
Epoch: 950, Loss: 1.3828, Train: 40.61%, Valid: 40.26%, Test: 40.83%
Epoch: 975, Loss: 1.3785, Train: 40.88%, Valid: 40.42%, Test: 40.95%
Run 01:
Highest Train: 41.07
Highest Valid: 40.96
  Final Train: 41.05
   Final Test: 41.25
All runs:
Highest Train: 41.07, nan
Highest Valid: 40.96, nan
  Final Train: 41.05, nan
   Final Test: 41.25, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.5724, Train: 29.03%, Valid: 28.76%, Test: 28.98%
Epoch: 25, Loss: 1.4638, Train: 28.26%, Valid: 28.01%, Test: 28.32%
Epoch: 50, Loss: 1.4412, Train: 28.42%, Valid: 28.18%, Test: 28.38%
Epoch: 75, Loss: 1.4263, Train: 30.60%, Valid: 30.43%, Test: 30.45%
Epoch: 100, Loss: 1.4210, Train: 38.01%, Valid: 38.14%, Test: 37.98%
Epoch: 125, Loss: 1.4085, Train: 41.11%, Valid: 40.97%, Test: 40.83%
Epoch: 150, Loss: 1.4020, Train: 42.04%, Valid: 41.90%, Test: 41.98%
Epoch: 175, Loss: 1.3989, Train: 42.25%, Valid: 42.05%, Test: 42.07%
Epoch: 200, Loss: 1.3942, Train: 42.54%, Valid: 42.53%, Test: 42.64%
Epoch: 225, Loss: 1.3897, Train: 42.90%, Valid: 42.88%, Test: 42.90%
Epoch: 250, Loss: 1.3964, Train: 42.00%, Valid: 42.01%, Test: 41.74%
Epoch: 275, Loss: 1.3864, Train: 42.94%, Valid: 42.99%, Test: 42.96%
Epoch: 300, Loss: 1.3825, Train: 43.14%, Valid: 43.23%, Test: 43.19%
Epoch: 325, Loss: 1.3801, Train: 43.48%, Valid: 43.43%, Test: 43.47%
Epoch: 350, Loss: 1.3834, Train: 43.40%, Valid: 43.18%, Test: 43.41%
Epoch: 375, Loss: 1.3761, Train: 43.35%, Valid: 43.41%, Test: 43.45%
Epoch: 400, Loss: 1.3731, Train: 43.78%, Valid: 43.88%, Test: 43.81%
Epoch: 425, Loss: 1.3778, Train: 43.96%, Valid: 43.99%, Test: 43.98%
Epoch: 450, Loss: 1.3691, Train: 43.80%, Valid: 43.91%, Test: 43.95%
Epoch: 475, Loss: 1.3677, Train: 43.11%, Valid: 43.19%, Test: 43.21%
Epoch: 500, Loss: 1.3684, Train: 43.49%, Valid: 43.43%, Test: 43.64%
Epoch: 525, Loss: 1.3621, Train: 43.64%, Valid: 43.71%, Test: 43.81%
Epoch: 550, Loss: 1.3818, Train: 43.30%, Valid: 43.13%, Test: 43.08%
Epoch: 575, Loss: 1.3709, Train: 43.13%, Valid: 42.97%, Test: 43.04%
Epoch: 600, Loss: 1.3619, Train: 43.20%, Valid: 43.16%, Test: 43.21%
Epoch: 625, Loss: 1.3580, Train: 43.40%, Valid: 43.47%, Test: 43.51%
Epoch: 650, Loss: 1.3545, Train: 43.39%, Valid: 43.36%, Test: 43.47%
Epoch: 675, Loss: 1.3512, Train: 43.35%, Valid: 43.29%, Test: 43.35%
Epoch: 700, Loss: 1.3480, Train: 43.29%, Valid: 43.12%, Test: 43.22%
Epoch: 725, Loss: 1.3475, Train: 42.82%, Valid: 42.69%, Test: 43.01%
Epoch: 750, Loss: 1.3423, Train: 43.08%, Valid: 42.87%, Test: 43.07%
Epoch: 775, Loss: 1.3435, Train: 42.89%, Valid: 42.69%, Test: 42.94%
Epoch: 800, Loss: 1.3398, Train: 42.91%, Valid: 42.64%, Test: 42.88%
Epoch: 825, Loss: 1.3342, Train: 42.89%, Valid: 42.57%, Test: 42.79%
Epoch: 850, Loss: 1.3333, Train: 41.65%, Valid: 41.57%, Test: 41.73%
Epoch: 875, Loss: 1.3342, Train: 42.43%, Valid: 42.10%, Test: 42.27%
Epoch: 900, Loss: 1.3286, Train: 42.69%, Valid: 42.42%, Test: 42.79%
Epoch: 925, Loss: 1.3264, Train: 42.54%, Valid: 42.28%, Test: 42.56%
Epoch: 950, Loss: 1.3267, Train: 42.80%, Valid: 42.47%, Test: 42.71%
Epoch: 975, Loss: 1.3231, Train: 42.26%, Valid: 42.17%, Test: 42.45%
Run 01:
Highest Train: 44.05
Highest Valid: 44.11
  Final Train: 43.88
   Final Test: 44.04
All runs:
Highest Train: 44.05, nan
Highest Valid: 44.11, nan
  Final Train: 43.88, nan
   Final Test: 44.04, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6065, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.5822, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 50, Loss: 1.5668, Train: 28.74%, Valid: 28.54%, Test: 28.83%
Epoch: 75, Loss: 1.5347, Train: 33.21%, Valid: 33.03%, Test: 33.31%
Epoch: 100, Loss: 1.5126, Train: 34.25%, Valid: 34.07%, Test: 34.19%
Epoch: 125, Loss: 1.5035, Train: 34.58%, Valid: 34.29%, Test: 34.50%
Epoch: 150, Loss: 1.4971, Train: 34.85%, Valid: 34.64%, Test: 34.72%
Epoch: 175, Loss: 1.4919, Train: 35.11%, Valid: 34.80%, Test: 35.10%
Epoch: 200, Loss: 1.4859, Train: 35.45%, Valid: 35.08%, Test: 35.30%
Epoch: 225, Loss: 1.4828, Train: 35.70%, Valid: 35.20%, Test: 35.54%
Epoch: 250, Loss: 1.4789, Train: 36.02%, Valid: 35.42%, Test: 35.76%
Epoch: 275, Loss: 1.4766, Train: 36.25%, Valid: 35.58%, Test: 36.00%
Epoch: 300, Loss: 1.4751, Train: 36.57%, Valid: 35.74%, Test: 36.10%
Epoch: 325, Loss: 1.4713, Train: 36.77%, Valid: 35.83%, Test: 36.21%
Epoch: 350, Loss: 1.4692, Train: 36.91%, Valid: 35.96%, Test: 36.35%
Epoch: 375, Loss: 1.4657, Train: 37.07%, Valid: 36.08%, Test: 36.39%
Epoch: 400, Loss: 1.4652, Train: 37.22%, Valid: 36.19%, Test: 36.54%
Epoch: 425, Loss: 1.4636, Train: 37.34%, Valid: 36.27%, Test: 36.57%
Epoch: 450, Loss: 1.4615, Train: 37.45%, Valid: 36.26%, Test: 36.62%
Epoch: 475, Loss: 1.4610, Train: 37.51%, Valid: 36.32%, Test: 36.59%
Epoch: 500, Loss: 1.4580, Train: 37.58%, Valid: 36.36%, Test: 36.68%
Epoch: 525, Loss: 1.4579, Train: 37.65%, Valid: 36.39%, Test: 36.63%
Epoch: 550, Loss: 1.4562, Train: 37.77%, Valid: 36.41%, Test: 36.71%
Epoch: 575, Loss: 1.4550, Train: 37.87%, Valid: 36.41%, Test: 36.59%
Epoch: 600, Loss: 1.4540, Train: 37.99%, Valid: 36.43%, Test: 36.62%
Epoch: 625, Loss: 1.4518, Train: 38.00%, Valid: 36.45%, Test: 36.68%
Epoch: 650, Loss: 1.4523, Train: 38.12%, Valid: 36.48%, Test: 36.77%
Epoch: 675, Loss: 1.4511, Train: 38.17%, Valid: 36.49%, Test: 36.78%
Epoch: 700, Loss: 1.4495, Train: 38.25%, Valid: 36.51%, Test: 36.76%
Epoch: 725, Loss: 1.4479, Train: 38.38%, Valid: 36.49%, Test: 36.83%
Epoch: 750, Loss: 1.4485, Train: 38.40%, Valid: 36.49%, Test: 36.88%
Epoch: 775, Loss: 1.4468, Train: 38.44%, Valid: 36.55%, Test: 36.85%
Epoch: 800, Loss: 1.4474, Train: 38.51%, Valid: 36.56%, Test: 36.92%
Epoch: 825, Loss: 1.4460, Train: 38.57%, Valid: 36.55%, Test: 36.90%
Epoch: 850, Loss: 1.4452, Train: 38.55%, Valid: 36.45%, Test: 36.93%
Epoch: 875, Loss: 1.4448, Train: 38.59%, Valid: 36.48%, Test: 36.96%
Epoch: 900, Loss: 1.4435, Train: 38.63%, Valid: 36.47%, Test: 36.93%
Epoch: 925, Loss: 1.4436, Train: 38.73%, Valid: 36.48%, Test: 37.04%
Epoch: 950, Loss: 1.4419, Train: 38.75%, Valid: 36.41%, Test: 36.99%
Epoch: 975, Loss: 1.4417, Train: 38.86%, Valid: 36.49%, Test: 36.98%
Run 01:
Highest Train: 38.86
Highest Valid: 36.61
  Final Train: 38.51
   Final Test: 36.88
All runs:
Highest Train: 38.86, nan
Highest Valid: 36.61, nan
  Final Train: 38.51, nan
   Final Test: 36.88, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6112, Train: 12.80%, Valid: 12.85%, Test: 12.75%
Epoch: 25, Loss: 1.5855, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 50, Loss: 1.5676, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 75, Loss: 1.5376, Train: 32.69%, Valid: 32.43%, Test: 32.78%
Epoch: 100, Loss: 1.5133, Train: 34.16%, Valid: 33.92%, Test: 34.14%
Epoch: 125, Loss: 1.5045, Train: 34.45%, Valid: 34.21%, Test: 34.45%
Epoch: 150, Loss: 1.4986, Train: 34.66%, Valid: 34.41%, Test: 34.60%
Epoch: 175, Loss: 1.4948, Train: 34.86%, Valid: 34.64%, Test: 34.77%
Epoch: 200, Loss: 1.4912, Train: 35.14%, Valid: 34.79%, Test: 34.99%
Epoch: 225, Loss: 1.4871, Train: 35.45%, Valid: 34.98%, Test: 35.22%
Epoch: 250, Loss: 1.4826, Train: 35.72%, Valid: 35.12%, Test: 35.48%
Epoch: 275, Loss: 1.4788, Train: 36.09%, Valid: 35.37%, Test: 35.81%
Epoch: 300, Loss: 1.4769, Train: 36.40%, Valid: 35.51%, Test: 36.03%
Epoch: 325, Loss: 1.4723, Train: 36.61%, Valid: 35.64%, Test: 36.17%
Epoch: 350, Loss: 1.4714, Train: 36.78%, Valid: 35.78%, Test: 36.31%
Epoch: 375, Loss: 1.4693, Train: 36.94%, Valid: 35.86%, Test: 36.43%
Epoch: 400, Loss: 1.4659, Train: 37.08%, Valid: 36.00%, Test: 36.51%
Epoch: 425, Loss: 1.4653, Train: 37.20%, Valid: 36.07%, Test: 36.59%
Epoch: 450, Loss: 1.4633, Train: 37.35%, Valid: 36.15%, Test: 36.65%
Epoch: 475, Loss: 1.4620, Train: 37.51%, Valid: 36.22%, Test: 36.75%
Epoch: 500, Loss: 1.4599, Train: 37.62%, Valid: 36.20%, Test: 36.73%
Epoch: 525, Loss: 1.4595, Train: 37.72%, Valid: 36.26%, Test: 36.72%
Epoch: 550, Loss: 1.4581, Train: 37.80%, Valid: 36.26%, Test: 36.71%
Epoch: 575, Loss: 1.4568, Train: 37.90%, Valid: 36.31%, Test: 36.81%
Epoch: 600, Loss: 1.4566, Train: 37.98%, Valid: 36.28%, Test: 36.83%
Epoch: 625, Loss: 1.4557, Train: 38.07%, Valid: 36.31%, Test: 36.82%
Epoch: 650, Loss: 1.4541, Train: 38.17%, Valid: 36.37%, Test: 36.85%
Epoch: 675, Loss: 1.4525, Train: 38.21%, Valid: 36.43%, Test: 36.84%
Epoch: 700, Loss: 1.4521, Train: 38.36%, Valid: 36.46%, Test: 36.85%
Epoch: 725, Loss: 1.4510, Train: 38.42%, Valid: 36.49%, Test: 36.93%
Epoch: 750, Loss: 1.4514, Train: 38.52%, Valid: 36.54%, Test: 36.95%
Epoch: 775, Loss: 1.4494, Train: 38.71%, Valid: 36.80%, Test: 37.27%
Epoch: 800, Loss: 1.4430, Train: 38.91%, Valid: 37.01%, Test: 37.35%
Epoch: 825, Loss: 1.4399, Train: 39.07%, Valid: 37.08%, Test: 37.52%
Epoch: 850, Loss: 1.4386, Train: 39.10%, Valid: 37.07%, Test: 37.38%
Epoch: 875, Loss: 1.4368, Train: 39.24%, Valid: 37.26%, Test: 37.57%
Epoch: 900, Loss: 1.4347, Train: 39.44%, Valid: 37.43%, Test: 37.71%
Epoch: 925, Loss: 1.4333, Train: 39.44%, Valid: 37.42%, Test: 37.68%
Epoch: 950, Loss: 1.4314, Train: 39.60%, Valid: 37.49%, Test: 37.86%
Epoch: 975, Loss: 1.4289, Train: 39.63%, Valid: 37.51%, Test: 37.91%
Run 01:
Highest Train: 39.84
Highest Valid: 37.69
  Final Train: 39.76
   Final Test: 38.02
All runs:
Highest Train: 39.84, nan
Highest Valid: 37.69, nan
  Final Train: 39.76, nan
   Final Test: 38.02, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6146, Train: 25.94%, Valid: 25.82%, Test: 25.60%
Epoch: 25, Loss: 1.5135, Train: 38.13%, Valid: 37.86%, Test: 38.00%
Epoch: 50, Loss: 1.4659, Train: 32.65%, Valid: 32.46%, Test: 32.47%
Epoch: 75, Loss: 1.4490, Train: 37.44%, Valid: 37.26%, Test: 37.54%
Epoch: 100, Loss: 1.4386, Train: 38.00%, Valid: 37.92%, Test: 37.97%
Epoch: 125, Loss: 1.4322, Train: 40.95%, Valid: 40.68%, Test: 40.66%
Epoch: 150, Loss: 1.4344, Train: 41.01%, Valid: 40.78%, Test: 40.70%
Epoch: 175, Loss: 1.4228, Train: 37.43%, Valid: 37.26%, Test: 37.22%
Epoch: 200, Loss: 1.4347, Train: 39.82%, Valid: 39.59%, Test: 39.57%
Epoch: 225, Loss: 1.4287, Train: 39.98%, Valid: 39.80%, Test: 39.68%
Epoch: 250, Loss: 1.4240, Train: 39.05%, Valid: 38.84%, Test: 38.95%
Epoch: 275, Loss: 1.4136, Train: 38.48%, Valid: 38.31%, Test: 38.11%
Epoch: 300, Loss: 1.4146, Train: 39.21%, Valid: 39.06%, Test: 39.05%
Epoch: 325, Loss: 1.4148, Train: 38.05%, Valid: 37.85%, Test: 37.86%
Epoch: 350, Loss: 1.4147, Train: 37.50%, Valid: 37.45%, Test: 37.43%
Epoch: 375, Loss: 1.4115, Train: 40.02%, Valid: 39.75%, Test: 39.77%
Epoch: 400, Loss: 1.4126, Train: 39.72%, Valid: 39.50%, Test: 39.75%
Epoch: 425, Loss: 1.4188, Train: 38.49%, Valid: 38.21%, Test: 38.33%
Epoch: 450, Loss: 1.4152, Train: 40.89%, Valid: 40.63%, Test: 40.86%
Epoch: 475, Loss: 1.4088, Train: 40.78%, Valid: 40.48%, Test: 40.80%
Epoch: 500, Loss: 1.4099, Train: 39.30%, Valid: 38.97%, Test: 39.40%
Epoch: 525, Loss: 1.4001, Train: 40.72%, Valid: 40.46%, Test: 40.82%
Epoch: 550, Loss: 1.4114, Train: 39.91%, Valid: 39.61%, Test: 39.69%
Epoch: 575, Loss: 1.4157, Train: 40.93%, Valid: 40.67%, Test: 40.65%
Epoch: 600, Loss: 1.4062, Train: 41.51%, Valid: 41.15%, Test: 41.28%
Epoch: 625, Loss: 1.4008, Train: 40.03%, Valid: 39.75%, Test: 40.05%
Epoch: 650, Loss: 1.3981, Train: 38.63%, Valid: 38.30%, Test: 38.66%
Epoch: 675, Loss: 1.3917, Train: 38.80%, Valid: 38.44%, Test: 38.91%
Epoch: 700, Loss: 1.3881, Train: 41.05%, Valid: 40.59%, Test: 41.14%
Epoch: 725, Loss: 1.3919, Train: 40.81%, Valid: 40.49%, Test: 40.90%
Epoch: 750, Loss: 1.3883, Train: 38.81%, Valid: 38.53%, Test: 38.79%
Epoch: 775, Loss: 1.3915, Train: 36.88%, Valid: 36.67%, Test: 37.07%
Epoch: 800, Loss: 1.3805, Train: 41.87%, Valid: 41.55%, Test: 41.86%
Epoch: 825, Loss: 1.3979, Train: 41.88%, Valid: 41.60%, Test: 41.89%
Epoch: 850, Loss: 1.3866, Train: 40.58%, Valid: 40.26%, Test: 40.69%
Epoch: 875, Loss: 1.3853, Train: 41.19%, Valid: 40.97%, Test: 41.32%
Epoch: 900, Loss: 1.3948, Train: 38.78%, Valid: 38.48%, Test: 38.71%
Epoch: 925, Loss: 1.3807, Train: 41.32%, Valid: 40.86%, Test: 41.44%
Epoch: 950, Loss: 1.3895, Train: 41.60%, Valid: 41.29%, Test: 41.54%
Epoch: 975, Loss: 1.3770, Train: 42.24%, Valid: 41.85%, Test: 42.15%
Run 01:
Highest Train: 42.28
Highest Valid: 41.99
  Final Train: 42.28
   Final Test: 42.17
All runs:
Highest Train: 42.28, nan
Highest Valid: 41.99, nan
  Final Train: 42.28, nan
   Final Test: 42.17, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6097, Train: 13.56%, Valid: 13.66%, Test: 13.66%
Epoch: 25, Loss: 1.5810, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 50, Loss: 1.5645, Train: 28.73%, Valid: 28.54%, Test: 28.82%
Epoch: 75, Loss: 1.5335, Train: 33.08%, Valid: 32.92%, Test: 33.13%
Epoch: 100, Loss: 1.5116, Train: 34.14%, Valid: 33.96%, Test: 34.17%
Epoch: 125, Loss: 1.5047, Train: 34.41%, Valid: 34.25%, Test: 34.41%
Epoch: 150, Loss: 1.4981, Train: 34.73%, Valid: 34.48%, Test: 34.61%
Epoch: 175, Loss: 1.4940, Train: 34.95%, Valid: 34.64%, Test: 34.78%
Epoch: 200, Loss: 1.4908, Train: 35.20%, Valid: 34.81%, Test: 35.06%
Epoch: 225, Loss: 1.4871, Train: 35.44%, Valid: 34.98%, Test: 35.21%
Epoch: 250, Loss: 1.4845, Train: 35.71%, Valid: 35.16%, Test: 35.56%
Epoch: 275, Loss: 1.4812, Train: 35.97%, Valid: 35.25%, Test: 35.80%
Epoch: 300, Loss: 1.4789, Train: 36.22%, Valid: 35.42%, Test: 35.97%
Epoch: 325, Loss: 1.4760, Train: 36.49%, Valid: 35.59%, Test: 36.08%
Epoch: 350, Loss: 1.4729, Train: 36.73%, Valid: 35.72%, Test: 36.18%
Epoch: 375, Loss: 1.4715, Train: 36.86%, Valid: 35.82%, Test: 36.23%
Epoch: 400, Loss: 1.4700, Train: 36.99%, Valid: 35.88%, Test: 36.33%
Epoch: 425, Loss: 1.4676, Train: 37.10%, Valid: 35.99%, Test: 36.49%
Epoch: 450, Loss: 1.4645, Train: 37.23%, Valid: 36.07%, Test: 36.53%
Epoch: 475, Loss: 1.4649, Train: 37.36%, Valid: 36.14%, Test: 36.58%
Epoch: 500, Loss: 1.4619, Train: 37.47%, Valid: 36.18%, Test: 36.67%
Epoch: 525, Loss: 1.4604, Train: 37.63%, Valid: 36.22%, Test: 36.64%
Epoch: 550, Loss: 1.4587, Train: 37.72%, Valid: 36.23%, Test: 36.66%
Epoch: 575, Loss: 1.4582, Train: 37.87%, Valid: 36.33%, Test: 36.70%
Epoch: 600, Loss: 1.4577, Train: 37.97%, Valid: 36.36%, Test: 36.75%
Epoch: 625, Loss: 1.4557, Train: 38.08%, Valid: 36.36%, Test: 36.76%
Epoch: 650, Loss: 1.4546, Train: 38.11%, Valid: 36.36%, Test: 36.82%
Epoch: 675, Loss: 1.4544, Train: 38.18%, Valid: 36.36%, Test: 36.83%
Epoch: 700, Loss: 1.4525, Train: 38.23%, Valid: 36.40%, Test: 36.81%
Epoch: 725, Loss: 1.4514, Train: 38.32%, Valid: 36.38%, Test: 36.82%
Epoch: 750, Loss: 1.4493, Train: 38.40%, Valid: 36.45%, Test: 36.88%
Epoch: 775, Loss: 1.4506, Train: 38.43%, Valid: 36.41%, Test: 36.89%
Epoch: 800, Loss: 1.4499, Train: 38.46%, Valid: 36.48%, Test: 36.84%
Epoch: 825, Loss: 1.4479, Train: 38.54%, Valid: 36.44%, Test: 36.84%
Epoch: 850, Loss: 1.4477, Train: 38.57%, Valid: 36.45%, Test: 36.88%
Epoch: 875, Loss: 1.4474, Train: 38.57%, Valid: 36.44%, Test: 36.89%
Epoch: 900, Loss: 1.4467, Train: 38.70%, Valid: 36.44%, Test: 36.93%
Epoch: 925, Loss: 1.4450, Train: 38.71%, Valid: 36.46%, Test: 36.86%
Epoch: 950, Loss: 1.4445, Train: 38.79%, Valid: 36.47%, Test: 36.85%
Epoch: 975, Loss: 1.4444, Train: 38.76%, Valid: 36.40%, Test: 36.86%
Run 01:
Highest Train: 38.88
Highest Valid: 36.54
  Final Train: 38.75
   Final Test: 36.88
All runs:
Highest Train: 38.88, nan
Highest Valid: 36.54, nan
  Final Train: 38.75, nan
   Final Test: 36.88, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6107, Train: 17.86%, Valid: 17.99%, Test: 17.85%
Epoch: 25, Loss: 1.5854, Train: 28.72%, Valid: 28.53%, Test: 28.82%
Epoch: 50, Loss: 1.5639, Train: 28.76%, Valid: 28.59%, Test: 28.87%
Epoch: 75, Loss: 1.5274, Train: 33.46%, Valid: 33.37%, Test: 33.56%
Epoch: 100, Loss: 1.5099, Train: 34.22%, Valid: 34.00%, Test: 34.25%
Epoch: 125, Loss: 1.5025, Train: 34.50%, Valid: 34.29%, Test: 34.45%
Epoch: 150, Loss: 1.4971, Train: 34.73%, Valid: 34.43%, Test: 34.58%
Epoch: 175, Loss: 1.4933, Train: 34.96%, Valid: 34.67%, Test: 34.86%
Epoch: 200, Loss: 1.4894, Train: 35.23%, Valid: 34.87%, Test: 35.11%
Epoch: 225, Loss: 1.4862, Train: 35.57%, Valid: 35.01%, Test: 35.37%
Epoch: 250, Loss: 1.4833, Train: 35.84%, Valid: 35.15%, Test: 35.56%
Epoch: 275, Loss: 1.4810, Train: 36.16%, Valid: 35.38%, Test: 35.75%
Epoch: 300, Loss: 1.4782, Train: 36.36%, Valid: 35.59%, Test: 35.99%
Epoch: 325, Loss: 1.4766, Train: 36.55%, Valid: 35.66%, Test: 36.05%
Epoch: 350, Loss: 1.4737, Train: 36.64%, Valid: 35.69%, Test: 36.11%
Epoch: 375, Loss: 1.4722, Train: 36.84%, Valid: 35.81%, Test: 36.18%
Epoch: 400, Loss: 1.4709, Train: 36.92%, Valid: 35.86%, Test: 36.25%
Epoch: 425, Loss: 1.4682, Train: 37.08%, Valid: 35.94%, Test: 36.31%
Epoch: 450, Loss: 1.4662, Train: 37.18%, Valid: 35.89%, Test: 36.34%
Epoch: 475, Loss: 1.4655, Train: 37.20%, Valid: 35.99%, Test: 36.41%
Epoch: 500, Loss: 1.4642, Train: 37.31%, Valid: 36.04%, Test: 36.46%
Epoch: 525, Loss: 1.4635, Train: 37.37%, Valid: 36.06%, Test: 36.50%
Epoch: 550, Loss: 1.4628, Train: 37.49%, Valid: 36.04%, Test: 36.54%
Epoch: 575, Loss: 1.4613, Train: 37.59%, Valid: 36.05%, Test: 36.57%
Epoch: 600, Loss: 1.4608, Train: 37.67%, Valid: 36.14%, Test: 36.57%
Epoch: 625, Loss: 1.4583, Train: 37.73%, Valid: 36.23%, Test: 36.67%
Epoch: 650, Loss: 1.4577, Train: 37.80%, Valid: 36.23%, Test: 36.67%
Epoch: 675, Loss: 1.4565, Train: 37.92%, Valid: 36.35%, Test: 36.68%
Epoch: 700, Loss: 1.4546, Train: 37.99%, Valid: 36.39%, Test: 36.67%
Epoch: 725, Loss: 1.4542, Train: 38.17%, Valid: 36.44%, Test: 36.77%
Epoch: 750, Loss: 1.4532, Train: 38.36%, Valid: 36.63%, Test: 36.90%
Epoch: 775, Loss: 1.4478, Train: 38.56%, Valid: 36.76%, Test: 37.09%
Epoch: 800, Loss: 1.4443, Train: 38.66%, Valid: 36.91%, Test: 37.17%
Epoch: 825, Loss: 1.4439, Train: 38.80%, Valid: 37.08%, Test: 37.22%
Epoch: 850, Loss: 1.4406, Train: 38.95%, Valid: 37.16%, Test: 37.39%
Epoch: 875, Loss: 1.4392, Train: 39.07%, Valid: 37.28%, Test: 37.45%
Epoch: 900, Loss: 1.4371, Train: 39.26%, Valid: 37.35%, Test: 37.53%
Epoch: 925, Loss: 1.4396, Train: 39.00%, Valid: 37.11%, Test: 37.29%
Epoch: 950, Loss: 1.4333, Train: 39.20%, Valid: 37.29%, Test: 37.51%
Epoch: 975, Loss: 1.4327, Train: 39.26%, Valid: 37.26%, Test: 37.44%
Run 01:
Highest Train: 39.63
Highest Valid: 37.58
  Final Train: 39.62
   Final Test: 37.81
All runs:
Highest Train: 39.63, nan
Highest Valid: 37.58, nan
  Final Train: 39.62, nan
   Final Test: 37.81, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6027, Train: 27.28%, Valid: 26.85%, Test: 27.15%
Epoch: 25, Loss: 1.5271, Train: 33.20%, Valid: 33.02%, Test: 33.79%
Epoch: 50, Loss: 1.4831, Train: 33.29%, Valid: 33.32%, Test: 33.90%
Epoch: 75, Loss: 1.4515, Train: 25.12%, Valid: 25.32%, Test: 25.72%
Epoch: 100, Loss: 1.4449, Train: 27.21%, Valid: 27.22%, Test: 27.68%
Epoch: 125, Loss: 1.4346, Train: 28.57%, Valid: 28.76%, Test: 29.19%
Epoch: 150, Loss: 1.4336, Train: 30.23%, Valid: 30.32%, Test: 30.83%
Epoch: 175, Loss: 1.4254, Train: 28.88%, Valid: 28.74%, Test: 29.35%
Epoch: 200, Loss: 1.4316, Train: 28.37%, Valid: 28.51%, Test: 28.98%
Epoch: 225, Loss: 1.4209, Train: 36.72%, Valid: 36.55%, Test: 37.17%
Epoch: 250, Loss: 1.4150, Train: 34.34%, Valid: 34.04%, Test: 34.62%
Epoch: 275, Loss: 1.4215, Train: 36.17%, Valid: 35.72%, Test: 36.73%
Epoch: 300, Loss: 1.4295, Train: 38.59%, Valid: 38.20%, Test: 38.40%
Epoch: 325, Loss: 1.4200, Train: 33.83%, Valid: 33.51%, Test: 34.14%
Epoch: 350, Loss: 1.4112, Train: 32.58%, Valid: 32.74%, Test: 33.27%
Epoch: 375, Loss: 1.4044, Train: 35.81%, Valid: 35.69%, Test: 36.55%
Epoch: 400, Loss: 1.4143, Train: 40.49%, Valid: 40.11%, Test: 40.61%
Epoch: 425, Loss: 1.4096, Train: 39.75%, Valid: 39.28%, Test: 39.79%
Epoch: 450, Loss: 1.4064, Train: 40.71%, Valid: 40.46%, Test: 40.81%
Epoch: 475, Loss: 1.4013, Train: 39.15%, Valid: 39.00%, Test: 39.39%
Epoch: 500, Loss: 1.4012, Train: 42.23%, Valid: 41.87%, Test: 42.20%
Epoch: 525, Loss: 1.4068, Train: 41.86%, Valid: 41.46%, Test: 41.77%
Epoch: 550, Loss: 1.4011, Train: 42.49%, Valid: 42.14%, Test: 42.49%
Epoch: 575, Loss: 1.4171, Train: 42.47%, Valid: 42.18%, Test: 42.54%
Epoch: 600, Loss: 1.4102, Train: 40.79%, Valid: 40.41%, Test: 40.69%
Epoch: 625, Loss: 1.4107, Train: 40.31%, Valid: 39.99%, Test: 40.32%
Epoch: 650, Loss: 1.3931, Train: 42.63%, Valid: 42.43%, Test: 42.68%
Epoch: 675, Loss: 1.4010, Train: 42.39%, Valid: 42.12%, Test: 42.50%
Epoch: 700, Loss: 1.3925, Train: 41.68%, Valid: 41.44%, Test: 41.64%
Epoch: 725, Loss: 1.3957, Train: 42.60%, Valid: 42.37%, Test: 42.53%
Epoch: 750, Loss: 1.3934, Train: 41.10%, Valid: 40.69%, Test: 40.81%
Epoch: 775, Loss: 1.4063, Train: 42.78%, Valid: 42.56%, Test: 42.74%
Epoch: 800, Loss: 1.3891, Train: 42.88%, Valid: 42.70%, Test: 42.98%
Epoch: 825, Loss: 1.3986, Train: 42.16%, Valid: 41.92%, Test: 42.08%
Epoch: 850, Loss: 1.3910, Train: 41.99%, Valid: 41.60%, Test: 42.13%
Epoch: 875, Loss: 1.3911, Train: 42.79%, Valid: 42.52%, Test: 42.82%
Epoch: 900, Loss: 1.3996, Train: 41.30%, Valid: 41.15%, Test: 41.23%
Epoch: 925, Loss: 1.3927, Train: 42.14%, Valid: 42.10%, Test: 42.71%
Epoch: 950, Loss: 1.3874, Train: 42.74%, Valid: 42.57%, Test: 42.80%
Epoch: 975, Loss: 1.3822, Train: 43.50%, Valid: 43.17%, Test: 43.44%
Run 01:
Highest Train: 43.54
Highest Valid: 43.30
  Final Train: 43.52
   Final Test: 43.46
All runs:
Highest Train: 43.54, nan
Highest Valid: 43.30, nan
  Final Train: 43.52, nan
   Final Test: 43.46, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6107, Train: 12.51%, Valid: 12.59%, Test: 12.45%
Epoch: 25, Loss: 1.5885, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 50, Loss: 1.5677, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 75, Loss: 1.5456, Train: 29.97%, Valid: 29.72%, Test: 30.03%
Epoch: 100, Loss: 1.5107, Train: 33.84%, Valid: 33.79%, Test: 33.95%
Epoch: 125, Loss: 1.4992, Train: 34.28%, Valid: 34.12%, Test: 34.34%
Epoch: 150, Loss: 1.4936, Train: 34.53%, Valid: 34.27%, Test: 34.53%
Epoch: 175, Loss: 1.4894, Train: 34.75%, Valid: 34.42%, Test: 34.65%
Epoch: 200, Loss: 1.4852, Train: 35.01%, Valid: 34.61%, Test: 34.78%
Epoch: 225, Loss: 1.4806, Train: 35.31%, Valid: 34.77%, Test: 35.14%
Epoch: 250, Loss: 1.4758, Train: 35.65%, Valid: 34.94%, Test: 35.48%
Epoch: 275, Loss: 1.4705, Train: 35.90%, Valid: 35.25%, Test: 35.59%
Epoch: 300, Loss: 1.4649, Train: 36.20%, Valid: 35.57%, Test: 35.77%
Epoch: 325, Loss: 1.4594, Train: 36.54%, Valid: 35.75%, Test: 35.98%
Epoch: 350, Loss: 1.4544, Train: 36.76%, Valid: 35.90%, Test: 36.07%
Epoch: 375, Loss: 1.4498, Train: 37.04%, Valid: 35.95%, Test: 36.13%
Epoch: 400, Loss: 1.4456, Train: 37.23%, Valid: 35.95%, Test: 36.29%
Epoch: 425, Loss: 1.4420, Train: 37.39%, Valid: 36.01%, Test: 36.25%
Epoch: 450, Loss: 1.4386, Train: 37.58%, Valid: 36.07%, Test: 36.31%
Epoch: 475, Loss: 1.4355, Train: 37.71%, Valid: 36.22%, Test: 36.31%
Epoch: 500, Loss: 1.4323, Train: 37.88%, Valid: 36.25%, Test: 36.35%
Epoch: 525, Loss: 1.4295, Train: 38.03%, Valid: 36.13%, Test: 36.30%
Epoch: 550, Loss: 1.4264, Train: 38.23%, Valid: 36.17%, Test: 36.42%
Epoch: 575, Loss: 1.4237, Train: 38.30%, Valid: 36.09%, Test: 36.42%
Epoch: 600, Loss: 1.4209, Train: 38.44%, Valid: 36.09%, Test: 36.45%
Epoch: 625, Loss: 1.4186, Train: 38.52%, Valid: 36.08%, Test: 36.30%
Epoch: 650, Loss: 1.4160, Train: 38.68%, Valid: 35.88%, Test: 36.45%
Epoch: 675, Loss: 1.4138, Train: 38.83%, Valid: 35.95%, Test: 36.42%
Epoch: 700, Loss: 1.4115, Train: 38.96%, Valid: 35.85%, Test: 36.33%
Epoch: 725, Loss: 1.4096, Train: 39.05%, Valid: 35.90%, Test: 36.29%
Epoch: 750, Loss: 1.4074, Train: 39.21%, Valid: 35.81%, Test: 36.15%
Epoch: 775, Loss: 1.4056, Train: 39.31%, Valid: 35.84%, Test: 36.28%
Epoch: 800, Loss: 1.4048, Train: 39.40%, Valid: 35.77%, Test: 36.30%
Epoch: 825, Loss: 1.4023, Train: 39.53%, Valid: 35.77%, Test: 36.24%
Epoch: 850, Loss: 1.4008, Train: 39.65%, Valid: 35.75%, Test: 36.28%
Epoch: 875, Loss: 1.3992, Train: 39.68%, Valid: 35.76%, Test: 36.30%
Epoch: 900, Loss: 1.3977, Train: 39.78%, Valid: 35.67%, Test: 36.20%
Epoch: 925, Loss: 1.3973, Train: 39.79%, Valid: 35.53%, Test: 36.09%
Epoch: 950, Loss: 1.3949, Train: 39.87%, Valid: 35.65%, Test: 36.15%
Epoch: 975, Loss: 1.3942, Train: 39.99%, Valid: 35.70%, Test: 36.15%
Run 01:
Highest Train: 40.06
Highest Valid: 36.25
  Final Train: 37.88
   Final Test: 36.35
All runs:
Highest Train: 40.06, nan
Highest Valid: 36.25, nan
  Final Train: 37.88, nan
   Final Test: 36.35, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6121, Train: 16.31%, Valid: 15.99%, Test: 16.42%
Epoch: 25, Loss: 1.5987, Train: 31.41%, Valid: 31.20%, Test: 31.95%
Epoch: 50, Loss: 1.5916, Train: 31.37%, Valid: 31.20%, Test: 31.96%
Epoch: 75, Loss: 1.5853, Train: 31.18%, Valid: 31.09%, Test: 31.82%
Epoch: 100, Loss: 1.5766, Train: 30.92%, Valid: 30.86%, Test: 31.51%
Epoch: 125, Loss: 1.5664, Train: 30.88%, Valid: 30.76%, Test: 31.45%
Epoch: 150, Loss: 1.5586, Train: 31.01%, Valid: 30.95%, Test: 31.58%
Epoch: 175, Loss: 1.5487, Train: 31.47%, Valid: 31.31%, Test: 32.10%
Epoch: 200, Loss: 1.5443, Train: 32.06%, Valid: 31.86%, Test: 32.65%
Epoch: 225, Loss: 1.5389, Train: 32.88%, Valid: 32.61%, Test: 33.48%
Epoch: 250, Loss: 1.5363, Train: 33.56%, Valid: 33.33%, Test: 34.14%
Epoch: 275, Loss: 1.5315, Train: 29.12%, Valid: 28.85%, Test: 29.29%
Epoch: 300, Loss: 1.5277, Train: 28.83%, Valid: 28.52%, Test: 28.97%
Epoch: 325, Loss: 1.5240, Train: 28.76%, Valid: 28.44%, Test: 28.86%
Epoch: 350, Loss: 1.5218, Train: 28.75%, Valid: 28.49%, Test: 28.86%
Epoch: 375, Loss: 1.5158, Train: 28.74%, Valid: 28.49%, Test: 28.87%
Epoch: 400, Loss: 1.5135, Train: 28.76%, Valid: 28.48%, Test: 28.88%
Epoch: 425, Loss: 1.5095, Train: 28.72%, Valid: 28.45%, Test: 28.85%
Epoch: 450, Loss: 1.5050, Train: 28.77%, Valid: 28.51%, Test: 28.94%
Epoch: 475, Loss: 1.5000, Train: 28.76%, Valid: 28.45%, Test: 28.90%
Epoch: 500, Loss: 1.5072, Train: 28.83%, Valid: 28.48%, Test: 28.93%
Epoch: 525, Loss: 1.4956, Train: 28.83%, Valid: 28.52%, Test: 28.93%
Epoch: 550, Loss: 1.4902, Train: 29.04%, Valid: 28.78%, Test: 29.06%
Epoch: 575, Loss: 1.4849, Train: 32.06%, Valid: 31.69%, Test: 32.53%
Epoch: 600, Loss: 1.4858, Train: 33.51%, Valid: 33.19%, Test: 34.06%
Epoch: 625, Loss: 1.4769, Train: 33.33%, Valid: 33.01%, Test: 33.86%
Epoch: 650, Loss: 1.4801, Train: 33.20%, Valid: 32.88%, Test: 33.73%
Epoch: 675, Loss: 1.4694, Train: 33.08%, Valid: 32.78%, Test: 33.63%
Epoch: 700, Loss: 1.4864, Train: 33.13%, Valid: 32.82%, Test: 33.60%
Epoch: 725, Loss: 1.4808, Train: 33.24%, Valid: 32.97%, Test: 33.76%
Epoch: 750, Loss: 1.4662, Train: 33.25%, Valid: 32.98%, Test: 33.80%
Epoch: 775, Loss: 1.5585, Train: 29.80%, Valid: 29.30%, Test: 30.00%
Epoch: 800, Loss: 1.4880, Train: 29.62%, Valid: 29.19%, Test: 29.80%
Epoch: 825, Loss: 1.4722, Train: 29.38%, Valid: 29.02%, Test: 29.57%
Epoch: 850, Loss: 1.4645, Train: 29.61%, Valid: 29.25%, Test: 29.91%
Epoch: 875, Loss: 1.4589, Train: 29.67%, Valid: 29.35%, Test: 29.93%
Epoch: 900, Loss: 1.4968, Train: 29.08%, Valid: 28.75%, Test: 29.22%
Epoch: 925, Loss: 1.4654, Train: 29.63%, Valid: 29.27%, Test: 29.74%
Epoch: 950, Loss: 1.4554, Train: 29.91%, Valid: 29.58%, Test: 30.04%
Epoch: 975, Loss: 1.5027, Train: 32.92%, Valid: 32.48%, Test: 33.59%
Run 01:
Highest Train: 33.72
Highest Valid: 33.46
  Final Train: 33.72
   Final Test: 34.31
All runs:
Highest Train: 33.72, nan
Highest Valid: 33.46, nan
  Final Train: 33.72, nan
   Final Test: 34.31, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6519, Train: 19.71%, Valid: 19.82%, Test: 19.96%
Epoch: 25, Loss: 1.5202, Train: 32.80%, Valid: 32.62%, Test: 33.33%
Epoch: 50, Loss: 1.4957, Train: 31.64%, Valid: 31.71%, Test: 32.05%
Epoch: 75, Loss: 1.4818, Train: 31.63%, Valid: 31.61%, Test: 31.98%
Epoch: 100, Loss: 1.4727, Train: 31.36%, Valid: 31.29%, Test: 31.64%
Epoch: 125, Loss: 1.4689, Train: 31.47%, Valid: 31.40%, Test: 31.74%
Epoch: 150, Loss: 1.4866, Train: 31.92%, Valid: 31.66%, Test: 32.23%
Epoch: 175, Loss: 1.4644, Train: 31.89%, Valid: 31.69%, Test: 32.12%
Epoch: 200, Loss: 1.4877, Train: 32.67%, Valid: 32.51%, Test: 33.20%
Epoch: 225, Loss: 1.4769, Train: 32.37%, Valid: 32.29%, Test: 32.75%
Epoch: 250, Loss: 1.4591, Train: 31.28%, Valid: 31.31%, Test: 31.32%
Epoch: 275, Loss: 1.4749, Train: 33.36%, Valid: 33.21%, Test: 33.72%
Epoch: 300, Loss: 1.4605, Train: 33.00%, Valid: 32.82%, Test: 33.49%
Epoch: 325, Loss: 1.4457, Train: 33.09%, Valid: 32.89%, Test: 33.50%
Epoch: 350, Loss: 1.4380, Train: 34.18%, Valid: 34.00%, Test: 34.55%
Epoch: 375, Loss: 1.5219, Train: 33.98%, Valid: 33.76%, Test: 34.43%
Epoch: 400, Loss: 1.4629, Train: 33.93%, Valid: 33.66%, Test: 34.32%
Epoch: 425, Loss: 1.4438, Train: 34.57%, Valid: 34.37%, Test: 34.99%
Epoch: 450, Loss: 1.4323, Train: 35.08%, Valid: 34.99%, Test: 35.47%
Epoch: 475, Loss: 1.4274, Train: 35.22%, Valid: 35.25%, Test: 35.60%
Epoch: 500, Loss: 1.4252, Train: 35.59%, Valid: 35.59%, Test: 35.97%
Epoch: 525, Loss: 1.4296, Train: 35.35%, Valid: 35.29%, Test: 35.68%
Epoch: 550, Loss: 1.4194, Train: 31.26%, Valid: 31.15%, Test: 31.21%
Epoch: 575, Loss: 1.4650, Train: 29.90%, Valid: 29.68%, Test: 29.98%
Epoch: 600, Loss: 1.4328, Train: 30.57%, Valid: 30.41%, Test: 30.62%
Epoch: 625, Loss: 1.5151, Train: 28.72%, Valid: 28.37%, Test: 28.83%
Epoch: 650, Loss: 1.4756, Train: 28.73%, Valid: 28.44%, Test: 28.84%
Epoch: 675, Loss: 1.4560, Train: 28.70%, Valid: 28.51%, Test: 28.80%
Epoch: 700, Loss: 1.4336, Train: 28.78%, Valid: 28.56%, Test: 28.83%
Epoch: 725, Loss: 1.4463, Train: 28.70%, Valid: 28.51%, Test: 28.71%
Epoch: 750, Loss: 1.4337, Train: 29.57%, Valid: 29.35%, Test: 29.58%
Epoch: 775, Loss: 1.5320, Train: 28.82%, Valid: 28.61%, Test: 28.82%
Epoch: 800, Loss: 1.4987, Train: 32.19%, Valid: 31.83%, Test: 31.99%
Epoch: 825, Loss: 1.4836, Train: 32.86%, Valid: 32.53%, Test: 32.73%
Epoch: 850, Loss: 1.4666, Train: 31.82%, Valid: 31.48%, Test: 31.55%
Epoch: 875, Loss: 1.4516, Train: 30.94%, Valid: 30.76%, Test: 30.86%
Epoch: 900, Loss: 1.4375, Train: 30.87%, Valid: 30.64%, Test: 31.03%
Epoch: 925, Loss: 1.4266, Train: 30.65%, Valid: 30.53%, Test: 30.91%
Epoch: 950, Loss: 1.4683, Train: 29.77%, Valid: 29.54%, Test: 29.72%
Epoch: 975, Loss: 1.4531, Train: 30.23%, Valid: 30.00%, Test: 30.20%
Run 01:
Highest Train: 35.59
Highest Valid: 35.64
  Final Train: 35.58
   Final Test: 36.02
All runs:
Highest Train: 35.59, nan
Highest Valid: 35.64, nan
  Final Train: 35.58, nan
   Final Test: 36.02, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6093, Train: 28.71%, Valid: 28.52%, Test: 28.80%
Epoch: 25, Loss: 1.5890, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 50, Loss: 1.5631, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 75, Loss: 1.5329, Train: 32.30%, Valid: 32.12%, Test: 32.52%
Epoch: 100, Loss: 1.5052, Train: 34.08%, Valid: 33.94%, Test: 34.07%
Epoch: 125, Loss: 1.4967, Train: 34.37%, Valid: 34.14%, Test: 34.39%
Epoch: 150, Loss: 1.4916, Train: 34.67%, Valid: 34.36%, Test: 34.61%
Epoch: 175, Loss: 1.4873, Train: 34.96%, Valid: 34.60%, Test: 34.80%
Epoch: 200, Loss: 1.4829, Train: 35.30%, Valid: 34.81%, Test: 35.21%
Epoch: 225, Loss: 1.4786, Train: 35.62%, Valid: 35.04%, Test: 35.47%
Epoch: 250, Loss: 1.4741, Train: 35.80%, Valid: 35.13%, Test: 35.56%
Epoch: 275, Loss: 1.4690, Train: 36.05%, Valid: 35.25%, Test: 35.65%
Epoch: 300, Loss: 1.4635, Train: 36.30%, Valid: 35.41%, Test: 35.97%
Epoch: 325, Loss: 1.4583, Train: 36.61%, Valid: 35.65%, Test: 36.23%
Epoch: 350, Loss: 1.4535, Train: 36.82%, Valid: 35.92%, Test: 36.22%
Epoch: 375, Loss: 1.4493, Train: 37.04%, Valid: 36.05%, Test: 36.30%
Epoch: 400, Loss: 1.4456, Train: 37.18%, Valid: 36.03%, Test: 36.38%
Epoch: 425, Loss: 1.4422, Train: 37.42%, Valid: 36.06%, Test: 36.35%
Epoch: 450, Loss: 1.4392, Train: 37.60%, Valid: 36.08%, Test: 36.36%
Epoch: 475, Loss: 1.4363, Train: 37.78%, Valid: 36.02%, Test: 36.37%
Epoch: 500, Loss: 1.4334, Train: 37.86%, Valid: 35.97%, Test: 36.34%
Epoch: 525, Loss: 1.4308, Train: 38.05%, Valid: 35.92%, Test: 36.30%
Epoch: 550, Loss: 1.4284, Train: 38.20%, Valid: 35.83%, Test: 36.35%
Epoch: 575, Loss: 1.4256, Train: 38.37%, Valid: 35.95%, Test: 36.36%
Epoch: 600, Loss: 1.4231, Train: 38.50%, Valid: 35.94%, Test: 36.33%
Epoch: 625, Loss: 1.4206, Train: 38.64%, Valid: 35.92%, Test: 36.33%
Epoch: 650, Loss: 1.4183, Train: 38.79%, Valid: 35.96%, Test: 36.34%
Epoch: 675, Loss: 1.4158, Train: 38.93%, Valid: 35.84%, Test: 36.35%
Epoch: 700, Loss: 1.4136, Train: 38.98%, Valid: 36.01%, Test: 36.41%
Epoch: 725, Loss: 1.4115, Train: 39.10%, Valid: 35.88%, Test: 36.22%
Epoch: 750, Loss: 1.4091, Train: 39.30%, Valid: 35.78%, Test: 36.20%
Epoch: 775, Loss: 1.4071, Train: 39.46%, Valid: 35.84%, Test: 36.28%
Epoch: 800, Loss: 1.4049, Train: 39.56%, Valid: 35.85%, Test: 36.19%
Epoch: 825, Loss: 1.4033, Train: 39.71%, Valid: 35.84%, Test: 36.22%
Epoch: 850, Loss: 1.4013, Train: 39.78%, Valid: 35.80%, Test: 36.07%
Epoch: 875, Loss: 1.3997, Train: 39.89%, Valid: 35.77%, Test: 36.06%
Epoch: 900, Loss: 1.3987, Train: 39.98%, Valid: 35.74%, Test: 35.91%
Epoch: 925, Loss: 1.3964, Train: 40.16%, Valid: 35.77%, Test: 36.00%
Epoch: 950, Loss: 1.3949, Train: 40.20%, Valid: 35.82%, Test: 36.11%
Epoch: 975, Loss: 1.3939, Train: 40.28%, Valid: 35.87%, Test: 36.16%
Run 01:
Highest Train: 40.42
Highest Valid: 36.13
  Final Train: 37.39
   Final Test: 36.34
All runs:
Highest Train: 40.42, nan
Highest Valid: 36.13, nan
  Final Train: 37.39, nan
   Final Test: 36.34, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6097, Train: 26.46%, Valid: 26.30%, Test: 26.27%
Epoch: 25, Loss: 1.5978, Train: 35.50%, Valid: 35.24%, Test: 35.45%
Epoch: 50, Loss: 1.5910, Train: 36.30%, Valid: 36.02%, Test: 36.14%
Epoch: 75, Loss: 1.5847, Train: 36.64%, Valid: 36.29%, Test: 36.50%
Epoch: 100, Loss: 1.5771, Train: 36.05%, Valid: 35.66%, Test: 36.09%
Epoch: 125, Loss: 1.5694, Train: 33.13%, Valid: 32.80%, Test: 33.70%
Epoch: 150, Loss: 1.5642, Train: 32.81%, Valid: 32.62%, Test: 33.48%
Epoch: 175, Loss: 1.5534, Train: 32.86%, Valid: 32.67%, Test: 33.54%
Epoch: 200, Loss: 1.5429, Train: 32.80%, Valid: 32.56%, Test: 33.49%
Epoch: 225, Loss: 1.5347, Train: 32.81%, Valid: 32.57%, Test: 33.45%
Epoch: 250, Loss: 1.5261, Train: 32.90%, Valid: 32.68%, Test: 33.54%
Epoch: 275, Loss: 1.5192, Train: 33.19%, Valid: 32.90%, Test: 33.72%
Epoch: 300, Loss: 1.5129, Train: 33.37%, Valid: 33.05%, Test: 33.89%
Epoch: 325, Loss: 1.5053, Train: 33.52%, Valid: 33.20%, Test: 34.10%
Epoch: 350, Loss: 1.5026, Train: 33.72%, Valid: 33.42%, Test: 34.25%
Epoch: 375, Loss: 1.4947, Train: 33.41%, Valid: 33.12%, Test: 33.94%
Epoch: 400, Loss: 1.4886, Train: 33.23%, Valid: 32.94%, Test: 33.70%
Epoch: 425, Loss: 1.4855, Train: 37.56%, Valid: 37.32%, Test: 37.53%
Epoch: 450, Loss: 1.4785, Train: 37.88%, Valid: 37.54%, Test: 37.76%
Epoch: 475, Loss: 1.4986, Train: 35.43%, Valid: 35.24%, Test: 35.39%
Epoch: 500, Loss: 1.4779, Train: 36.37%, Valid: 36.09%, Test: 36.13%
Epoch: 525, Loss: 1.4705, Train: 36.47%, Valid: 36.20%, Test: 36.35%
Epoch: 550, Loss: 1.5126, Train: 34.03%, Valid: 33.85%, Test: 34.01%
Epoch: 575, Loss: 1.4833, Train: 35.42%, Valid: 35.23%, Test: 35.31%
Epoch: 600, Loss: 1.4645, Train: 36.29%, Valid: 36.10%, Test: 36.17%
Epoch: 625, Loss: 1.4773, Train: 24.98%, Valid: 25.05%, Test: 24.91%
Epoch: 650, Loss: 1.4864, Train: 36.07%, Valid: 35.87%, Test: 35.90%
Epoch: 675, Loss: 1.4645, Train: 36.64%, Valid: 36.43%, Test: 36.46%
Epoch: 700, Loss: 1.4553, Train: 36.45%, Valid: 36.26%, Test: 36.24%
Epoch: 725, Loss: 1.5157, Train: 35.03%, Valid: 34.78%, Test: 34.92%
Epoch: 750, Loss: 1.4863, Train: 36.27%, Valid: 36.10%, Test: 36.11%
Epoch: 775, Loss: 1.4631, Train: 36.53%, Valid: 36.27%, Test: 36.29%
Epoch: 800, Loss: 1.4556, Train: 36.27%, Valid: 36.07%, Test: 36.07%
Epoch: 825, Loss: 1.4504, Train: 36.35%, Valid: 36.14%, Test: 36.17%
Epoch: 850, Loss: 1.4459, Train: 36.45%, Valid: 36.24%, Test: 36.23%
Epoch: 875, Loss: 1.4418, Train: 36.54%, Valid: 36.35%, Test: 36.32%
Epoch: 900, Loss: 1.4379, Train: 36.60%, Valid: 36.40%, Test: 36.38%
Epoch: 925, Loss: 1.4337, Train: 36.67%, Valid: 36.43%, Test: 36.45%
Epoch: 950, Loss: 1.4277, Train: 36.78%, Valid: 36.51%, Test: 36.52%
Epoch: 975, Loss: 1.4200, Train: 37.32%, Valid: 36.98%, Test: 37.04%
Run 01:
Highest Train: 38.22
Highest Valid: 37.92
  Final Train: 38.22
   Final Test: 37.91
All runs:
Highest Train: 38.22, nan
Highest Valid: 37.92, nan
  Final Train: 38.22, nan
   Final Test: 37.91, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6097, Train: 32.43%, Valid: 32.24%, Test: 32.22%
Epoch: 25, Loss: 1.5064, Train: 28.84%, Valid: 28.63%, Test: 28.95%
Epoch: 50, Loss: 1.4822, Train: 29.25%, Valid: 29.09%, Test: 29.57%
Epoch: 75, Loss: 1.4724, Train: 30.44%, Valid: 30.39%, Test: 30.71%
Epoch: 100, Loss: 1.4675, Train: 31.06%, Valid: 31.11%, Test: 31.31%
Epoch: 125, Loss: 1.4870, Train: 29.35%, Valid: 29.15%, Test: 29.52%
Epoch: 150, Loss: 1.4676, Train: 31.11%, Valid: 30.98%, Test: 31.29%
Epoch: 175, Loss: 1.4602, Train: 30.86%, Valid: 30.83%, Test: 31.10%
Epoch: 200, Loss: 1.4608, Train: 33.75%, Valid: 33.60%, Test: 33.91%
Epoch: 225, Loss: 1.4858, Train: 23.47%, Valid: 23.68%, Test: 24.05%
Epoch: 250, Loss: 1.4699, Train: 30.84%, Valid: 30.84%, Test: 31.03%
Epoch: 275, Loss: 1.4570, Train: 29.22%, Valid: 29.07%, Test: 29.34%
Epoch: 300, Loss: 1.4636, Train: 31.71%, Valid: 31.59%, Test: 31.82%
Epoch: 325, Loss: 1.4523, Train: 30.66%, Valid: 30.71%, Test: 30.86%
Epoch: 350, Loss: 1.4497, Train: 30.22%, Valid: 30.20%, Test: 30.33%
Epoch: 375, Loss: 1.4460, Train: 30.94%, Valid: 31.04%, Test: 31.16%
Epoch: 400, Loss: 1.4410, Train: 30.48%, Valid: 30.53%, Test: 30.62%
Epoch: 425, Loss: 1.4621, Train: 31.37%, Valid: 31.12%, Test: 31.44%
Epoch: 450, Loss: 1.4460, Train: 31.85%, Valid: 31.85%, Test: 31.98%
Epoch: 475, Loss: 1.4652, Train: 33.35%, Valid: 32.97%, Test: 33.24%
Epoch: 500, Loss: 1.4466, Train: 34.34%, Valid: 34.03%, Test: 34.42%
Epoch: 525, Loss: 1.4370, Train: 33.26%, Valid: 33.20%, Test: 33.27%
Epoch: 550, Loss: 1.4753, Train: 31.47%, Valid: 31.51%, Test: 31.78%
Epoch: 575, Loss: 1.4634, Train: 30.17%, Valid: 30.16%, Test: 30.57%
Epoch: 600, Loss: 1.4458, Train: 30.91%, Valid: 30.80%, Test: 31.08%
Epoch: 625, Loss: 1.4391, Train: 31.00%, Valid: 30.89%, Test: 31.15%
Epoch: 650, Loss: 1.4767, Train: 33.85%, Valid: 33.51%, Test: 34.42%
Epoch: 675, Loss: 1.4592, Train: 33.91%, Valid: 33.58%, Test: 34.45%
Epoch: 700, Loss: 1.4465, Train: 33.94%, Valid: 33.61%, Test: 34.41%
Epoch: 725, Loss: 1.4399, Train: 33.81%, Valid: 33.48%, Test: 34.22%
Epoch: 750, Loss: 1.4586, Train: 32.27%, Valid: 32.08%, Test: 32.73%
Epoch: 775, Loss: 1.4466, Train: 32.95%, Valid: 32.74%, Test: 33.29%
Epoch: 800, Loss: 1.4348, Train: 33.95%, Valid: 33.56%, Test: 34.31%
Epoch: 825, Loss: 1.4338, Train: 34.28%, Valid: 34.02%, Test: 34.72%
Epoch: 850, Loss: 1.4273, Train: 34.85%, Valid: 34.77%, Test: 35.25%
Epoch: 875, Loss: 1.4210, Train: 35.63%, Valid: 35.39%, Test: 36.03%
Epoch: 900, Loss: 1.4159, Train: 35.95%, Valid: 35.72%, Test: 36.33%
Epoch: 925, Loss: 1.4119, Train: 36.19%, Valid: 35.87%, Test: 36.55%
Epoch: 950, Loss: 1.4085, Train: 36.36%, Valid: 36.02%, Test: 36.76%
Epoch: 975, Loss: 1.4832, Train: 33.65%, Valid: 33.36%, Test: 34.24%
Run 01:
Highest Train: 36.40
Highest Valid: 36.11
  Final Train: 36.36
   Final Test: 36.81
All runs:
Highest Train: 36.40, nan
Highest Valid: 36.11, nan
  Final Train: 36.36, nan
   Final Test: 36.81, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6076, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.5869, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 50, Loss: 1.5696, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 75, Loss: 1.5444, Train: 31.28%, Valid: 31.03%, Test: 31.44%
Epoch: 100, Loss: 1.5169, Train: 33.98%, Valid: 33.86%, Test: 34.08%
Epoch: 125, Loss: 1.5070, Train: 34.29%, Valid: 34.14%, Test: 34.36%
Epoch: 150, Loss: 1.5014, Train: 34.55%, Valid: 34.31%, Test: 34.45%
Epoch: 175, Loss: 1.4978, Train: 34.79%, Valid: 34.52%, Test: 34.62%
Epoch: 200, Loss: 1.4937, Train: 34.98%, Valid: 34.68%, Test: 34.83%
Epoch: 225, Loss: 1.4901, Train: 35.21%, Valid: 34.87%, Test: 35.05%
Epoch: 250, Loss: 1.4869, Train: 35.45%, Valid: 35.00%, Test: 35.34%
Epoch: 275, Loss: 1.4826, Train: 35.67%, Valid: 35.21%, Test: 35.52%
Epoch: 300, Loss: 1.4802, Train: 35.91%, Valid: 35.36%, Test: 35.77%
Epoch: 325, Loss: 1.4778, Train: 36.20%, Valid: 35.49%, Test: 35.93%
Epoch: 350, Loss: 1.4753, Train: 36.45%, Valid: 35.64%, Test: 36.11%
Epoch: 375, Loss: 1.4720, Train: 36.69%, Valid: 35.79%, Test: 36.19%
Epoch: 400, Loss: 1.4701, Train: 36.84%, Valid: 35.79%, Test: 36.31%
Epoch: 425, Loss: 1.4668, Train: 36.97%, Valid: 35.86%, Test: 36.41%
Epoch: 450, Loss: 1.4659, Train: 37.12%, Valid: 35.90%, Test: 36.41%
Epoch: 475, Loss: 1.4641, Train: 37.26%, Valid: 36.06%, Test: 36.52%
Epoch: 500, Loss: 1.4608, Train: 37.40%, Valid: 36.13%, Test: 36.55%
Epoch: 525, Loss: 1.4606, Train: 37.54%, Valid: 36.23%, Test: 36.62%
Epoch: 550, Loss: 1.4593, Train: 37.69%, Valid: 36.32%, Test: 36.63%
Epoch: 575, Loss: 1.4566, Train: 37.74%, Valid: 36.33%, Test: 36.67%
Epoch: 600, Loss: 1.4561, Train: 37.84%, Valid: 36.35%, Test: 36.71%
Epoch: 625, Loss: 1.4546, Train: 37.94%, Valid: 36.40%, Test: 36.73%
Epoch: 650, Loss: 1.4528, Train: 37.97%, Valid: 36.44%, Test: 36.78%
Epoch: 675, Loss: 1.4516, Train: 38.13%, Valid: 36.45%, Test: 36.71%
Epoch: 700, Loss: 1.4514, Train: 38.22%, Valid: 36.45%, Test: 36.77%
Epoch: 725, Loss: 1.4489, Train: 38.24%, Valid: 36.42%, Test: 36.75%
Epoch: 750, Loss: 1.4498, Train: 38.32%, Valid: 36.38%, Test: 36.75%
Epoch: 775, Loss: 1.4499, Train: 38.38%, Valid: 36.47%, Test: 36.72%
Epoch: 800, Loss: 1.4488, Train: 38.44%, Valid: 36.49%, Test: 36.76%
Epoch: 825, Loss: 1.4471, Train: 38.49%, Valid: 36.48%, Test: 36.78%
Epoch: 850, Loss: 1.4460, Train: 38.56%, Valid: 36.43%, Test: 36.74%
Epoch: 875, Loss: 1.4461, Train: 38.60%, Valid: 36.46%, Test: 36.74%
Epoch: 900, Loss: 1.4454, Train: 38.68%, Valid: 36.49%, Test: 36.79%
Epoch: 925, Loss: 1.4457, Train: 38.68%, Valid: 36.41%, Test: 36.72%
Epoch: 950, Loss: 1.4422, Train: 38.71%, Valid: 36.36%, Test: 36.74%
Epoch: 975, Loss: 1.4445, Train: 38.72%, Valid: 36.46%, Test: 36.81%
Run 01:
Highest Train: 38.78
Highest Valid: 36.51
  Final Train: 38.49
   Final Test: 36.70
All runs:
Highest Train: 38.78, nan
Highest Valid: 36.51, nan
  Final Train: 38.49, nan
   Final Test: 36.70, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6069, Train: 28.27%, Valid: 28.06%, Test: 28.35%
Epoch: 25, Loss: 1.5946, Train: 28.70%, Valid: 28.50%, Test: 28.80%
Epoch: 50, Loss: 1.5712, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 75, Loss: 1.5465, Train: 30.40%, Valid: 30.25%, Test: 30.55%
Epoch: 100, Loss: 1.5172, Train: 33.97%, Valid: 33.89%, Test: 33.98%
Epoch: 125, Loss: 1.5071, Train: 34.29%, Valid: 34.09%, Test: 34.29%
Epoch: 150, Loss: 1.5018, Train: 34.53%, Valid: 34.26%, Test: 34.48%
Epoch: 175, Loss: 1.4982, Train: 34.75%, Valid: 34.49%, Test: 34.64%
Epoch: 200, Loss: 1.4946, Train: 34.92%, Valid: 34.62%, Test: 34.75%
Epoch: 225, Loss: 1.4916, Train: 35.11%, Valid: 34.78%, Test: 34.92%
Epoch: 250, Loss: 1.4891, Train: 35.24%, Valid: 34.86%, Test: 35.09%
Epoch: 275, Loss: 1.4865, Train: 35.44%, Valid: 34.99%, Test: 35.26%
Epoch: 300, Loss: 1.4832, Train: 35.64%, Valid: 35.06%, Test: 35.43%
Epoch: 325, Loss: 1.4813, Train: 35.80%, Valid: 35.19%, Test: 35.64%
Epoch: 350, Loss: 1.4789, Train: 36.00%, Valid: 35.28%, Test: 35.81%
Epoch: 375, Loss: 1.4773, Train: 36.21%, Valid: 35.39%, Test: 35.90%
Epoch: 400, Loss: 1.4740, Train: 36.42%, Valid: 35.50%, Test: 36.07%
Epoch: 425, Loss: 1.4730, Train: 36.68%, Valid: 35.65%, Test: 36.22%
Epoch: 450, Loss: 1.4700, Train: 36.80%, Valid: 35.75%, Test: 36.30%
Epoch: 475, Loss: 1.4677, Train: 36.97%, Valid: 35.80%, Test: 36.43%
Epoch: 500, Loss: 1.4651, Train: 37.17%, Valid: 35.94%, Test: 36.44%
Epoch: 525, Loss: 1.4637, Train: 37.27%, Valid: 36.16%, Test: 36.51%
Epoch: 550, Loss: 1.4631, Train: 37.48%, Valid: 36.23%, Test: 36.58%
Epoch: 575, Loss: 1.4614, Train: 37.60%, Valid: 36.21%, Test: 36.70%
Epoch: 600, Loss: 1.4592, Train: 37.73%, Valid: 36.28%, Test: 36.71%
Epoch: 625, Loss: 1.4572, Train: 37.89%, Valid: 36.33%, Test: 36.80%
Epoch: 650, Loss: 1.4561, Train: 37.90%, Valid: 36.30%, Test: 36.79%
Epoch: 675, Loss: 1.4542, Train: 38.00%, Valid: 36.42%, Test: 36.86%
Epoch: 700, Loss: 1.4522, Train: 38.13%, Valid: 36.37%, Test: 36.90%
Epoch: 725, Loss: 1.4520, Train: 38.18%, Valid: 36.42%, Test: 36.96%
Epoch: 750, Loss: 1.4500, Train: 38.33%, Valid: 36.47%, Test: 36.97%
Epoch: 775, Loss: 1.4446, Train: 38.69%, Valid: 36.83%, Test: 37.37%
Epoch: 800, Loss: 1.4411, Train: 38.88%, Valid: 37.04%, Test: 37.41%
Epoch: 825, Loss: 1.4381, Train: 39.08%, Valid: 37.25%, Test: 37.55%
Epoch: 850, Loss: 1.4343, Train: 39.37%, Valid: 37.37%, Test: 37.82%
Epoch: 875, Loss: 1.4279, Train: 39.53%, Valid: 37.46%, Test: 37.88%
Epoch: 900, Loss: 1.4276, Train: 39.64%, Valid: 37.51%, Test: 37.94%
Epoch: 925, Loss: 1.4268, Train: 39.80%, Valid: 37.63%, Test: 38.04%
Epoch: 950, Loss: 1.4229, Train: 40.09%, Valid: 37.82%, Test: 38.25%
Epoch: 975, Loss: 1.4190, Train: 40.13%, Valid: 37.84%, Test: 38.30%
Run 01:
Highest Train: 40.58
Highest Valid: 38.18
  Final Train: 40.48
   Final Test: 38.47
All runs:
Highest Train: 40.58, nan
Highest Valid: 38.18, nan
  Final Train: 40.48, nan
   Final Test: 38.47, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6153, Train: 28.71%, Valid: 28.54%, Test: 28.80%
Epoch: 25, Loss: 1.5373, Train: 28.68%, Valid: 28.52%, Test: 28.77%
Epoch: 50, Loss: 1.5235, Train: 29.46%, Valid: 29.35%, Test: 29.67%
Epoch: 75, Loss: 1.5196, Train: 29.53%, Valid: 29.32%, Test: 29.61%
Epoch: 100, Loss: 1.5051, Train: 30.45%, Valid: 30.28%, Test: 30.80%
Epoch: 125, Loss: 1.5003, Train: 29.14%, Valid: 29.00%, Test: 29.29%
Epoch: 150, Loss: 1.4902, Train: 29.12%, Valid: 29.01%, Test: 29.37%
Epoch: 175, Loss: 1.4863, Train: 28.72%, Valid: 28.60%, Test: 28.89%
Epoch: 200, Loss: 1.4772, Train: 28.86%, Valid: 28.77%, Test: 29.03%
Epoch: 225, Loss: 1.4904, Train: 28.70%, Valid: 28.56%, Test: 28.83%
Epoch: 250, Loss: 1.4813, Train: 28.66%, Valid: 28.54%, Test: 28.78%
Epoch: 275, Loss: 1.4728, Train: 28.96%, Valid: 28.78%, Test: 29.01%
Epoch: 300, Loss: 1.4751, Train: 28.71%, Valid: 28.52%, Test: 28.81%
Epoch: 325, Loss: 1.4711, Train: 28.65%, Valid: 28.54%, Test: 28.77%
Epoch: 350, Loss: 1.4666, Train: 28.85%, Valid: 28.70%, Test: 29.01%
Epoch: 375, Loss: 1.4671, Train: 28.79%, Valid: 28.54%, Test: 28.85%
Epoch: 400, Loss: 1.4666, Train: 28.63%, Valid: 28.43%, Test: 28.72%
Epoch: 425, Loss: 1.4632, Train: 28.70%, Valid: 28.51%, Test: 28.78%
Epoch: 450, Loss: 1.4606, Train: 28.59%, Valid: 28.38%, Test: 28.67%
Epoch: 475, Loss: 1.4550, Train: 28.58%, Valid: 28.40%, Test: 28.68%
Epoch: 500, Loss: 1.4518, Train: 28.31%, Valid: 28.13%, Test: 28.40%
Epoch: 525, Loss: 1.4624, Train: 29.19%, Valid: 28.87%, Test: 29.31%
Epoch: 550, Loss: 1.4580, Train: 28.98%, Valid: 28.76%, Test: 29.09%
Epoch: 575, Loss: 1.4543, Train: 28.74%, Valid: 28.53%, Test: 28.87%
Epoch: 600, Loss: 1.4539, Train: 28.97%, Valid: 28.70%, Test: 29.10%
Epoch: 625, Loss: 1.4600, Train: 18.92%, Valid: 18.91%, Test: 18.98%
Epoch: 650, Loss: 1.4554, Train: 26.92%, Valid: 26.66%, Test: 26.89%
Epoch: 675, Loss: 1.4467, Train: 29.05%, Valid: 28.85%, Test: 29.14%
Epoch: 700, Loss: 1.4453, Train: 28.71%, Valid: 28.45%, Test: 28.86%
Epoch: 725, Loss: 1.4473, Train: 28.93%, Valid: 28.70%, Test: 29.19%
Epoch: 750, Loss: 1.4487, Train: 27.82%, Valid: 27.46%, Test: 27.82%
Epoch: 775, Loss: 1.4462, Train: 28.11%, Valid: 27.90%, Test: 28.21%
Epoch: 800, Loss: 1.4555, Train: 15.24%, Valid: 15.33%, Test: 15.18%
Epoch: 825, Loss: 1.4440, Train: 14.05%, Valid: 14.02%, Test: 13.91%
Epoch: 850, Loss: 1.4436, Train: 26.57%, Valid: 26.40%, Test: 26.70%
Epoch: 875, Loss: 1.4468, Train: 25.84%, Valid: 25.79%, Test: 26.03%
Epoch: 900, Loss: 1.4562, Train: 16.57%, Valid: 16.54%, Test: 16.46%
Epoch: 925, Loss: 1.4431, Train: 22.13%, Valid: 21.94%, Test: 22.16%
Epoch: 950, Loss: 1.4451, Train: 26.55%, Valid: 26.35%, Test: 26.79%
Epoch: 975, Loss: 1.4425, Train: 27.25%, Valid: 27.11%, Test: 27.52%
Run 01:
Highest Train: 30.68
Highest Valid: 30.46
  Final Train: 30.62
   Final Test: 30.89
All runs:
Highest Train: 30.68, nan
Highest Valid: 30.46, nan
  Final Train: 30.62, nan
   Final Test: 30.89, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6093, Train: 26.78%, Valid: 26.71%, Test: 26.85%
Epoch: 25, Loss: 1.5901, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 50, Loss: 1.5694, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 75, Loss: 1.5433, Train: 31.96%, Valid: 31.79%, Test: 32.20%
Epoch: 100, Loss: 1.5163, Train: 34.04%, Valid: 33.91%, Test: 34.07%
Epoch: 125, Loss: 1.5083, Train: 34.34%, Valid: 34.15%, Test: 34.35%
Epoch: 150, Loss: 1.5025, Train: 34.54%, Valid: 34.31%, Test: 34.50%
Epoch: 175, Loss: 1.4986, Train: 34.75%, Valid: 34.51%, Test: 34.62%
Epoch: 200, Loss: 1.4947, Train: 34.91%, Valid: 34.70%, Test: 34.78%
Epoch: 225, Loss: 1.4913, Train: 35.19%, Valid: 34.78%, Test: 35.02%
Epoch: 250, Loss: 1.4873, Train: 35.40%, Valid: 34.95%, Test: 35.21%
Epoch: 275, Loss: 1.4837, Train: 35.59%, Valid: 35.11%, Test: 35.43%
Epoch: 300, Loss: 1.4811, Train: 35.83%, Valid: 35.28%, Test: 35.61%
Epoch: 325, Loss: 1.4780, Train: 36.10%, Valid: 35.40%, Test: 35.82%
Epoch: 350, Loss: 1.4756, Train: 36.37%, Valid: 35.55%, Test: 36.01%
Epoch: 375, Loss: 1.4731, Train: 36.56%, Valid: 35.70%, Test: 36.14%
Epoch: 400, Loss: 1.4707, Train: 36.77%, Valid: 35.80%, Test: 36.23%
Epoch: 425, Loss: 1.4689, Train: 36.96%, Valid: 35.88%, Test: 36.38%
Epoch: 450, Loss: 1.4667, Train: 37.12%, Valid: 35.94%, Test: 36.40%
Epoch: 475, Loss: 1.4634, Train: 37.24%, Valid: 36.05%, Test: 36.53%
Epoch: 500, Loss: 1.4631, Train: 37.37%, Valid: 36.07%, Test: 36.49%
Epoch: 525, Loss: 1.4607, Train: 37.46%, Valid: 36.13%, Test: 36.57%
Epoch: 550, Loss: 1.4609, Train: 37.62%, Valid: 36.09%, Test: 36.57%
Epoch: 575, Loss: 1.4583, Train: 37.67%, Valid: 36.14%, Test: 36.57%
Epoch: 600, Loss: 1.4575, Train: 37.77%, Valid: 36.18%, Test: 36.64%
Epoch: 625, Loss: 1.4560, Train: 37.86%, Valid: 36.23%, Test: 36.67%
Epoch: 650, Loss: 1.4557, Train: 37.91%, Valid: 36.20%, Test: 36.63%
Epoch: 675, Loss: 1.4546, Train: 38.04%, Valid: 36.19%, Test: 36.71%
Epoch: 700, Loss: 1.4529, Train: 38.14%, Valid: 36.18%, Test: 36.66%
Epoch: 725, Loss: 1.4528, Train: 38.17%, Valid: 36.16%, Test: 36.72%
Epoch: 750, Loss: 1.4518, Train: 38.17%, Valid: 36.23%, Test: 36.72%
Epoch: 775, Loss: 1.4494, Train: 38.24%, Valid: 36.23%, Test: 36.74%
Epoch: 800, Loss: 1.4494, Train: 38.31%, Valid: 36.20%, Test: 36.70%
Epoch: 825, Loss: 1.4478, Train: 38.35%, Valid: 36.20%, Test: 36.71%
Epoch: 850, Loss: 1.4470, Train: 38.44%, Valid: 36.26%, Test: 36.71%
Epoch: 875, Loss: 1.4471, Train: 38.50%, Valid: 36.19%, Test: 36.74%
Epoch: 900, Loss: 1.4464, Train: 38.52%, Valid: 36.13%, Test: 36.67%
Epoch: 925, Loss: 1.4453, Train: 38.64%, Valid: 36.17%, Test: 36.75%
Epoch: 950, Loss: 1.4449, Train: 38.64%, Valid: 36.15%, Test: 36.71%
Epoch: 975, Loss: 1.4437, Train: 38.71%, Valid: 36.16%, Test: 36.76%
Run 01:
Highest Train: 38.79
Highest Valid: 36.31
  Final Train: 38.20
   Final Test: 36.74
All runs:
Highest Train: 38.79, nan
Highest Valid: 36.31, nan
  Final Train: 38.20, nan
   Final Test: 36.74, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6101, Train: 22.47%, Valid: 22.53%, Test: 23.10%
Epoch: 25, Loss: 1.5990, Train: 28.71%, Valid: 28.50%, Test: 28.81%
Epoch: 50, Loss: 1.5741, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 75, Loss: 1.5535, Train: 29.77%, Valid: 29.56%, Test: 29.92%
Epoch: 100, Loss: 1.5222, Train: 33.83%, Valid: 33.71%, Test: 33.92%
Epoch: 125, Loss: 1.5091, Train: 34.31%, Valid: 34.10%, Test: 34.31%
Epoch: 150, Loss: 1.5027, Train: 34.56%, Valid: 34.33%, Test: 34.46%
Epoch: 175, Loss: 1.4979, Train: 34.77%, Valid: 34.57%, Test: 34.67%
Epoch: 200, Loss: 1.4938, Train: 34.96%, Valid: 34.71%, Test: 34.87%
Epoch: 225, Loss: 1.4908, Train: 35.20%, Valid: 34.87%, Test: 35.12%
Epoch: 250, Loss: 1.4874, Train: 35.49%, Valid: 35.02%, Test: 35.36%
Epoch: 275, Loss: 1.4837, Train: 35.72%, Valid: 35.10%, Test: 35.49%
Epoch: 300, Loss: 1.4815, Train: 35.91%, Valid: 35.29%, Test: 35.69%
Epoch: 325, Loss: 1.4789, Train: 36.11%, Valid: 35.46%, Test: 35.82%
Epoch: 350, Loss: 1.4750, Train: 36.30%, Valid: 35.55%, Test: 36.01%
Epoch: 375, Loss: 1.4724, Train: 36.56%, Valid: 35.72%, Test: 36.16%
Epoch: 400, Loss: 1.4697, Train: 36.70%, Valid: 35.80%, Test: 36.24%
Epoch: 425, Loss: 1.4685, Train: 36.92%, Valid: 35.94%, Test: 36.32%
Epoch: 450, Loss: 1.4647, Train: 37.10%, Valid: 36.05%, Test: 36.35%
Epoch: 475, Loss: 1.4639, Train: 37.22%, Valid: 36.08%, Test: 36.42%
Epoch: 500, Loss: 1.4603, Train: 37.35%, Valid: 36.20%, Test: 36.48%
Epoch: 525, Loss: 1.4573, Train: 37.53%, Valid: 36.27%, Test: 36.59%
Epoch: 550, Loss: 1.4561, Train: 37.67%, Valid: 36.36%, Test: 36.60%
Epoch: 575, Loss: 1.4557, Train: 37.84%, Valid: 36.40%, Test: 36.69%
Epoch: 600, Loss: 1.4527, Train: 37.90%, Valid: 36.47%, Test: 36.71%
Epoch: 625, Loss: 1.4518, Train: 38.03%, Valid: 36.42%, Test: 36.72%
Epoch: 650, Loss: 1.4496, Train: 38.10%, Valid: 36.44%, Test: 36.78%
Epoch: 675, Loss: 1.4493, Train: 38.27%, Valid: 36.51%, Test: 36.83%
Epoch: 700, Loss: 1.4466, Train: 38.38%, Valid: 36.51%, Test: 36.88%
Epoch: 725, Loss: 1.4400, Train: 38.78%, Valid: 36.89%, Test: 37.16%
Epoch: 750, Loss: 1.4357, Train: 38.99%, Valid: 37.05%, Test: 37.29%
Epoch: 775, Loss: 1.4336, Train: 39.12%, Valid: 37.15%, Test: 37.45%
Epoch: 800, Loss: 1.4300, Train: 39.40%, Valid: 37.28%, Test: 37.59%
Epoch: 825, Loss: 1.4266, Train: 39.48%, Valid: 37.36%, Test: 37.69%
Epoch: 850, Loss: 1.4277, Train: 39.58%, Valid: 37.39%, Test: 37.84%
Epoch: 875, Loss: 1.4241, Train: 39.66%, Valid: 37.46%, Test: 37.92%
Epoch: 900, Loss: 1.4198, Train: 39.88%, Valid: 37.47%, Test: 37.97%
Epoch: 925, Loss: 1.4201, Train: 39.95%, Valid: 37.56%, Test: 37.95%
Epoch: 950, Loss: 1.4183, Train: 39.90%, Valid: 37.60%, Test: 37.96%
Epoch: 975, Loss: 1.4175, Train: 39.76%, Valid: 37.56%, Test: 37.88%
Run 01:
Highest Train: 40.36
Highest Valid: 37.80
  Final Train: 40.29
   Final Test: 38.25
All runs:
Highest Train: 40.36, nan
Highest Valid: 37.80, nan
  Final Train: 40.29, nan
   Final Test: 38.25, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.1, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6096, Train: 22.07%, Valid: 22.01%, Test: 22.55%
Epoch: 25, Loss: 1.5544, Train: 17.63%, Valid: 17.66%, Test: 17.64%
Epoch: 50, Loss: 1.5411, Train: 18.16%, Valid: 18.11%, Test: 17.98%
Epoch: 75, Loss: 1.5336, Train: 17.97%, Valid: 18.05%, Test: 17.68%
Epoch: 100, Loss: 1.5201, Train: 19.99%, Valid: 19.92%, Test: 19.95%
Epoch: 125, Loss: 1.5099, Train: 21.67%, Valid: 21.57%, Test: 21.67%
Epoch: 150, Loss: 1.5079, Train: 33.25%, Valid: 32.50%, Test: 33.41%
Epoch: 175, Loss: 1.5003, Train: 21.63%, Valid: 21.46%, Test: 21.91%
Epoch: 200, Loss: 1.4947, Train: 21.67%, Valid: 21.49%, Test: 21.97%
Epoch: 225, Loss: 1.4923, Train: 32.54%, Valid: 31.90%, Test: 32.73%
Epoch: 250, Loss: 1.4990, Train: 32.48%, Valid: 31.80%, Test: 32.59%
Epoch: 275, Loss: 1.4877, Train: 32.75%, Valid: 32.05%, Test: 32.73%
Epoch: 300, Loss: 1.4840, Train: 30.86%, Valid: 30.58%, Test: 31.07%
Epoch: 325, Loss: 1.4850, Train: 30.83%, Valid: 30.48%, Test: 31.15%
Epoch: 350, Loss: 1.4826, Train: 31.26%, Valid: 30.99%, Test: 31.50%
Epoch: 375, Loss: 1.4804, Train: 31.10%, Valid: 30.76%, Test: 31.32%
Epoch: 400, Loss: 1.4891, Train: 31.86%, Valid: 31.33%, Test: 32.01%
Epoch: 425, Loss: 1.4803, Train: 30.34%, Valid: 30.16%, Test: 30.51%
Epoch: 450, Loss: 1.4766, Train: 30.60%, Valid: 30.35%, Test: 30.75%
Epoch: 475, Loss: 1.4721, Train: 31.61%, Valid: 31.16%, Test: 31.66%
Epoch: 500, Loss: 1.4664, Train: 31.25%, Valid: 30.88%, Test: 31.45%
Epoch: 525, Loss: 1.4681, Train: 20.05%, Valid: 19.96%, Test: 19.98%
Epoch: 550, Loss: 1.4684, Train: 30.41%, Valid: 30.22%, Test: 30.64%
Epoch: 575, Loss: 1.4628, Train: 20.14%, Valid: 20.16%, Test: 20.21%
Epoch: 600, Loss: 1.4630, Train: 28.65%, Valid: 28.41%, Test: 28.64%
Epoch: 625, Loss: 1.4594, Train: 31.57%, Valid: 31.32%, Test: 31.75%
Epoch: 650, Loss: 1.4588, Train: 20.33%, Valid: 20.36%, Test: 20.36%
Epoch: 675, Loss: 1.4541, Train: 26.92%, Valid: 26.85%, Test: 27.18%
Epoch: 700, Loss: 1.4559, Train: 26.94%, Valid: 26.77%, Test: 27.05%
Epoch: 725, Loss: 1.4561, Train: 20.47%, Valid: 20.45%, Test: 20.62%
Epoch: 750, Loss: 1.4553, Train: 28.04%, Valid: 28.09%, Test: 28.14%
Epoch: 775, Loss: 1.4509, Train: 21.20%, Valid: 21.33%, Test: 21.60%
Epoch: 800, Loss: 1.4510, Train: 21.34%, Valid: 21.49%, Test: 21.99%
Epoch: 825, Loss: 1.4563, Train: 23.36%, Valid: 23.46%, Test: 23.60%
Epoch: 850, Loss: 1.4496, Train: 23.34%, Valid: 23.32%, Test: 23.71%
Epoch: 875, Loss: 1.4487, Train: 21.95%, Valid: 22.02%, Test: 22.61%
Epoch: 900, Loss: 1.4486, Train: 21.82%, Valid: 21.92%, Test: 22.40%
Epoch: 925, Loss: 1.4529, Train: 27.68%, Valid: 27.90%, Test: 28.10%
Epoch: 950, Loss: 1.4455, Train: 21.63%, Valid: 21.78%, Test: 22.27%
Epoch: 975, Loss: 1.4467, Train: 21.48%, Valid: 21.60%, Test: 22.09%
Run 01:
Highest Train: 33.76
Highest Valid: 33.62
  Final Train: 33.72
   Final Test: 34.26
All runs:
Highest Train: 33.76, nan
Highest Valid: 33.62, nan
  Final Train: 33.72, nan
   Final Test: 34.26, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6317, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.5657, Train: 28.73%, Valid: 28.54%, Test: 28.82%
Epoch: 50, Loss: 1.5197, Train: 33.44%, Valid: 33.28%, Test: 33.59%
Epoch: 75, Loss: 1.4969, Train: 34.35%, Valid: 34.19%, Test: 34.50%
Epoch: 100, Loss: 1.4853, Train: 35.13%, Valid: 34.62%, Test: 35.02%
Epoch: 125, Loss: 1.4747, Train: 35.65%, Valid: 35.03%, Test: 35.49%
Epoch: 150, Loss: 1.4644, Train: 36.17%, Valid: 35.48%, Test: 35.86%
Epoch: 175, Loss: 1.4558, Train: 36.54%, Valid: 35.82%, Test: 36.24%
Epoch: 200, Loss: 1.4491, Train: 36.97%, Valid: 35.98%, Test: 36.40%
Epoch: 225, Loss: 1.4438, Train: 37.25%, Valid: 36.04%, Test: 36.49%
Epoch: 250, Loss: 1.4393, Train: 37.50%, Valid: 36.09%, Test: 36.58%
Epoch: 275, Loss: 1.4350, Train: 37.76%, Valid: 36.17%, Test: 36.59%
Epoch: 300, Loss: 1.4310, Train: 37.95%, Valid: 36.16%, Test: 36.57%
Epoch: 325, Loss: 1.4272, Train: 38.09%, Valid: 36.07%, Test: 36.63%
Epoch: 350, Loss: 1.4246, Train: 38.28%, Valid: 36.17%, Test: 36.57%
Epoch: 375, Loss: 1.4221, Train: 38.32%, Valid: 36.04%, Test: 36.57%
Epoch: 400, Loss: 1.4198, Train: 38.52%, Valid: 36.02%, Test: 36.55%
Epoch: 425, Loss: 1.4175, Train: 38.68%, Valid: 36.09%, Test: 36.45%
Epoch: 450, Loss: 1.4155, Train: 38.85%, Valid: 35.93%, Test: 36.46%
Epoch: 475, Loss: 1.4130, Train: 38.94%, Valid: 35.97%, Test: 36.42%
Epoch: 500, Loss: 1.4106, Train: 39.06%, Valid: 35.96%, Test: 36.48%
Epoch: 525, Loss: 1.4085, Train: 39.20%, Valid: 35.89%, Test: 36.41%
Epoch: 550, Loss: 1.4069, Train: 39.25%, Valid: 35.84%, Test: 36.32%
Epoch: 575, Loss: 1.4050, Train: 39.36%, Valid: 35.92%, Test: 36.35%
Epoch: 600, Loss: 1.4031, Train: 39.53%, Valid: 35.90%, Test: 36.38%
Epoch: 625, Loss: 1.4015, Train: 39.59%, Valid: 35.92%, Test: 36.34%
Epoch: 650, Loss: 1.3999, Train: 39.68%, Valid: 35.84%, Test: 36.36%
Epoch: 675, Loss: 1.3987, Train: 39.73%, Valid: 35.77%, Test: 36.17%
Epoch: 700, Loss: 1.3969, Train: 39.83%, Valid: 35.81%, Test: 36.29%
Epoch: 725, Loss: 1.3955, Train: 39.93%, Valid: 35.75%, Test: 36.20%
Epoch: 750, Loss: 1.3947, Train: 39.94%, Valid: 35.63%, Test: 36.14%
Epoch: 775, Loss: 1.3926, Train: 40.10%, Valid: 35.64%, Test: 36.20%
Epoch: 800, Loss: 1.3910, Train: 40.13%, Valid: 35.51%, Test: 35.98%
Epoch: 825, Loss: 1.3899, Train: 40.20%, Valid: 35.56%, Test: 36.00%
Epoch: 850, Loss: 1.3881, Train: 40.22%, Valid: 35.49%, Test: 35.94%
Epoch: 875, Loss: 1.3868, Train: 40.34%, Valid: 35.56%, Test: 35.85%
Epoch: 900, Loss: 1.3854, Train: 40.40%, Valid: 35.51%, Test: 35.89%
Epoch: 925, Loss: 1.3840, Train: 40.48%, Valid: 35.48%, Test: 35.81%
Epoch: 950, Loss: 1.3828, Train: 40.55%, Valid: 35.40%, Test: 35.69%
Epoch: 975, Loss: 1.3819, Train: 40.53%, Valid: 35.34%, Test: 35.73%
Run 01:
Highest Train: 40.68
Highest Valid: 36.28
  Final Train: 38.22
   Final Test: 36.48
All runs:
Highest Train: 40.68, nan
Highest Valid: 36.28, nan
  Final Train: 38.22, nan
   Final Test: 36.48, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6174, Train: 22.17%, Valid: 22.17%, Test: 22.73%
Epoch: 25, Loss: 1.5578, Train: 28.74%, Valid: 28.56%, Test: 28.85%
Epoch: 50, Loss: 1.5088, Train: 33.73%, Valid: 33.60%, Test: 33.82%
Epoch: 75, Loss: 1.4915, Train: 34.55%, Valid: 34.32%, Test: 34.54%
Epoch: 100, Loss: 1.4808, Train: 35.19%, Valid: 34.86%, Test: 35.11%
Epoch: 125, Loss: 1.4696, Train: 35.77%, Valid: 35.27%, Test: 35.63%
Epoch: 150, Loss: 1.4563, Train: 36.42%, Valid: 35.55%, Test: 36.13%
Epoch: 175, Loss: 1.4422, Train: 37.17%, Valid: 36.37%, Test: 36.77%
Epoch: 200, Loss: 1.4295, Train: 38.05%, Valid: 37.12%, Test: 37.48%
Epoch: 225, Loss: 1.4190, Train: 38.67%, Valid: 37.60%, Test: 37.88%
Epoch: 250, Loss: 1.4097, Train: 39.16%, Valid: 37.96%, Test: 38.21%
Epoch: 275, Loss: 1.4018, Train: 39.52%, Valid: 38.18%, Test: 38.46%
Epoch: 300, Loss: 1.3951, Train: 39.76%, Valid: 38.31%, Test: 38.61%
Epoch: 325, Loss: 1.3889, Train: 40.10%, Valid: 38.57%, Test: 38.71%
Epoch: 350, Loss: 1.3837, Train: 40.32%, Valid: 38.58%, Test: 38.82%
Epoch: 375, Loss: 1.3788, Train: 40.48%, Valid: 38.73%, Test: 38.92%
Epoch: 400, Loss: 1.3742, Train: 40.74%, Valid: 38.72%, Test: 38.98%
Epoch: 425, Loss: 1.3699, Train: 40.87%, Valid: 38.78%, Test: 39.07%
Epoch: 450, Loss: 1.3658, Train: 41.14%, Valid: 39.00%, Test: 39.22%
Epoch: 475, Loss: 1.3618, Train: 41.24%, Valid: 38.91%, Test: 39.26%
Epoch: 500, Loss: 1.3581, Train: 41.43%, Valid: 39.08%, Test: 39.26%
Epoch: 525, Loss: 1.3545, Train: 41.61%, Valid: 39.21%, Test: 39.32%
Epoch: 550, Loss: 1.3510, Train: 41.71%, Valid: 39.27%, Test: 39.41%
Epoch: 575, Loss: 1.3475, Train: 41.93%, Valid: 39.28%, Test: 39.43%
Epoch: 600, Loss: 1.3444, Train: 42.06%, Valid: 39.35%, Test: 39.57%
Epoch: 625, Loss: 1.3416, Train: 42.22%, Valid: 39.33%, Test: 39.66%
Epoch: 650, Loss: 1.3435, Train: 42.08%, Valid: 39.12%, Test: 39.61%
Epoch: 675, Loss: 1.3376, Train: 42.48%, Valid: 39.48%, Test: 39.78%
Epoch: 700, Loss: 1.3346, Train: 42.60%, Valid: 39.52%, Test: 39.88%
Epoch: 725, Loss: 1.3322, Train: 42.70%, Valid: 39.57%, Test: 39.91%
Epoch: 750, Loss: 1.3299, Train: 42.78%, Valid: 39.64%, Test: 40.04%
Epoch: 775, Loss: 1.3276, Train: 42.90%, Valid: 39.68%, Test: 40.12%
Epoch: 800, Loss: 1.3253, Train: 42.97%, Valid: 39.73%, Test: 40.16%
Epoch: 825, Loss: 1.3230, Train: 43.03%, Valid: 39.75%, Test: 40.17%
Epoch: 850, Loss: 1.3207, Train: 43.19%, Valid: 39.65%, Test: 40.20%
Epoch: 875, Loss: 1.3192, Train: 43.08%, Valid: 39.62%, Test: 40.19%
Epoch: 900, Loss: 1.3172, Train: 43.35%, Valid: 39.91%, Test: 40.52%
Epoch: 925, Loss: 1.3564, Train: 41.35%, Valid: 37.90%, Test: 38.55%
Epoch: 950, Loss: 1.3324, Train: 42.67%, Valid: 39.32%, Test: 39.86%
Epoch: 975, Loss: 1.3215, Train: 43.21%, Valid: 39.69%, Test: 40.24%
Run 01:
Highest Train: 43.48
Highest Valid: 40.09
  Final Train: 43.47
   Final Test: 40.54
All runs:
Highest Train: 43.48, nan
Highest Valid: 40.09, nan
  Final Train: 43.47, nan
   Final Test: 40.54, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.8821, Train: 20.89%, Valid: 21.28%, Test: 20.98%
Epoch: 25, Loss: 1.4937, Train: 26.18%, Valid: 26.57%, Test: 26.33%
Epoch: 50, Loss: 1.4635, Train: 33.15%, Valid: 33.10%, Test: 33.81%
Epoch: 75, Loss: 1.4408, Train: 33.62%, Valid: 33.47%, Test: 34.27%
Epoch: 100, Loss: 1.4244, Train: 35.58%, Valid: 34.96%, Test: 35.84%
Epoch: 125, Loss: 1.4134, Train: 38.59%, Valid: 38.16%, Test: 38.30%
Epoch: 150, Loss: 1.4053, Train: 39.58%, Valid: 39.29%, Test: 39.32%
Epoch: 175, Loss: 1.4036, Train: 39.31%, Valid: 39.09%, Test: 39.41%
Epoch: 200, Loss: 1.3930, Train: 39.70%, Valid: 39.60%, Test: 39.80%
Epoch: 225, Loss: 1.3891, Train: 39.80%, Valid: 39.67%, Test: 39.87%
Epoch: 250, Loss: 1.3843, Train: 39.98%, Valid: 39.94%, Test: 39.99%
Epoch: 275, Loss: 1.3919, Train: 39.85%, Valid: 39.65%, Test: 39.87%
Epoch: 300, Loss: 1.3787, Train: 40.23%, Valid: 40.06%, Test: 40.22%
Epoch: 325, Loss: 1.3750, Train: 40.25%, Valid: 40.17%, Test: 40.27%
Epoch: 350, Loss: 1.3728, Train: 40.34%, Valid: 40.13%, Test: 40.34%
Epoch: 375, Loss: 1.3749, Train: 40.22%, Valid: 40.04%, Test: 40.33%
Epoch: 400, Loss: 1.3688, Train: 40.48%, Valid: 40.42%, Test: 40.64%
Epoch: 425, Loss: 1.3666, Train: 40.53%, Valid: 40.34%, Test: 40.58%
Epoch: 450, Loss: 1.3658, Train: 40.48%, Valid: 40.36%, Test: 40.51%
Epoch: 475, Loss: 1.3644, Train: 40.65%, Valid: 40.54%, Test: 40.76%
Epoch: 500, Loss: 1.3607, Train: 40.73%, Valid: 40.62%, Test: 40.83%
Epoch: 525, Loss: 1.3588, Train: 40.75%, Valid: 40.59%, Test: 40.88%
Epoch: 550, Loss: 1.3573, Train: 40.68%, Valid: 40.46%, Test: 40.89%
Epoch: 575, Loss: 1.3555, Train: 40.78%, Valid: 40.65%, Test: 40.94%
Epoch: 600, Loss: 1.3659, Train: 40.73%, Valid: 40.57%, Test: 40.73%
Epoch: 625, Loss: 1.3905, Train: 39.84%, Valid: 39.69%, Test: 39.89%
Epoch: 650, Loss: 1.3706, Train: 40.22%, Valid: 40.21%, Test: 40.35%
Epoch: 675, Loss: 1.3658, Train: 40.42%, Valid: 40.45%, Test: 40.55%
Epoch: 700, Loss: 1.3628, Train: 40.63%, Valid: 40.66%, Test: 40.75%
Epoch: 725, Loss: 1.3606, Train: 40.71%, Valid: 40.71%, Test: 40.85%
Epoch: 750, Loss: 1.3587, Train: 40.82%, Valid: 40.78%, Test: 40.92%
Epoch: 775, Loss: 1.3571, Train: 40.90%, Valid: 40.83%, Test: 41.02%
Epoch: 800, Loss: 1.3555, Train: 40.99%, Valid: 40.86%, Test: 41.09%
Epoch: 825, Loss: 1.3541, Train: 41.03%, Valid: 40.91%, Test: 41.16%
Epoch: 850, Loss: 1.3527, Train: 41.10%, Valid: 40.97%, Test: 41.18%
Epoch: 875, Loss: 1.3513, Train: 41.18%, Valid: 40.97%, Test: 41.24%
Epoch: 900, Loss: 1.3502, Train: 41.19%, Valid: 40.98%, Test: 41.22%
Epoch: 925, Loss: 1.3487, Train: 41.21%, Valid: 40.99%, Test: 41.31%
Epoch: 950, Loss: 1.3478, Train: 41.17%, Valid: 40.96%, Test: 41.28%
Epoch: 975, Loss: 1.3474, Train: 41.14%, Valid: 40.88%, Test: 41.26%
Run 01:
Highest Train: 41.36
Highest Valid: 41.21
  Final Train: 41.35
   Final Test: 41.42
All runs:
Highest Train: 41.36, nan
Highest Valid: 41.21, nan
  Final Train: 41.35, nan
   Final Test: 41.42, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6050, Train: 25.19%, Valid: 25.45%, Test: 25.75%
Epoch: 25, Loss: 1.5515, Train: 29.96%, Valid: 29.70%, Test: 30.14%
Epoch: 50, Loss: 1.5047, Train: 34.16%, Valid: 33.93%, Test: 34.17%
Epoch: 75, Loss: 1.4915, Train: 34.60%, Valid: 34.46%, Test: 34.64%
Epoch: 100, Loss: 1.4814, Train: 35.25%, Valid: 34.76%, Test: 35.21%
Epoch: 125, Loss: 1.4721, Train: 35.75%, Valid: 35.16%, Test: 35.70%
Epoch: 150, Loss: 1.4632, Train: 36.23%, Valid: 35.51%, Test: 36.07%
Epoch: 175, Loss: 1.4552, Train: 36.62%, Valid: 35.75%, Test: 36.31%
Epoch: 200, Loss: 1.4483, Train: 37.11%, Valid: 35.97%, Test: 36.38%
Epoch: 225, Loss: 1.4429, Train: 37.37%, Valid: 36.15%, Test: 36.42%
Epoch: 250, Loss: 1.4382, Train: 37.64%, Valid: 36.25%, Test: 36.45%
Epoch: 275, Loss: 1.4342, Train: 37.85%, Valid: 36.27%, Test: 36.42%
Epoch: 300, Loss: 1.4304, Train: 38.04%, Valid: 36.26%, Test: 36.50%
Epoch: 325, Loss: 1.4270, Train: 38.27%, Valid: 36.25%, Test: 36.52%
Epoch: 350, Loss: 1.4236, Train: 38.45%, Valid: 36.25%, Test: 36.58%
Epoch: 375, Loss: 1.4205, Train: 38.67%, Valid: 36.25%, Test: 36.59%
Epoch: 400, Loss: 1.4175, Train: 38.79%, Valid: 36.20%, Test: 36.62%
Epoch: 425, Loss: 1.4147, Train: 38.99%, Valid: 36.13%, Test: 36.62%
Epoch: 450, Loss: 1.4119, Train: 39.10%, Valid: 36.13%, Test: 36.64%
Epoch: 475, Loss: 1.4094, Train: 39.27%, Valid: 36.05%, Test: 36.56%
Epoch: 500, Loss: 1.4070, Train: 39.43%, Valid: 36.07%, Test: 36.51%
Epoch: 525, Loss: 1.4048, Train: 39.53%, Valid: 36.02%, Test: 36.41%
Epoch: 550, Loss: 1.4025, Train: 39.64%, Valid: 36.02%, Test: 36.48%
Epoch: 575, Loss: 1.4016, Train: 39.65%, Valid: 35.84%, Test: 36.35%
Epoch: 600, Loss: 1.3985, Train: 39.81%, Valid: 35.93%, Test: 36.53%
Epoch: 625, Loss: 1.3967, Train: 39.95%, Valid: 35.94%, Test: 36.46%
Epoch: 650, Loss: 1.3957, Train: 40.00%, Valid: 35.91%, Test: 36.40%
Epoch: 675, Loss: 1.3934, Train: 40.12%, Valid: 35.73%, Test: 36.35%
Epoch: 700, Loss: 1.3915, Train: 40.16%, Valid: 35.86%, Test: 36.40%
Epoch: 725, Loss: 1.3901, Train: 40.22%, Valid: 35.79%, Test: 36.35%
Epoch: 750, Loss: 1.3886, Train: 40.31%, Valid: 35.81%, Test: 36.36%
Epoch: 775, Loss: 1.3876, Train: 40.43%, Valid: 35.70%, Test: 36.25%
Epoch: 800, Loss: 1.3858, Train: 40.47%, Valid: 35.77%, Test: 36.27%
Epoch: 825, Loss: 1.3848, Train: 40.56%, Valid: 35.61%, Test: 36.28%
Epoch: 850, Loss: 1.3832, Train: 40.66%, Valid: 35.67%, Test: 36.18%
Epoch: 875, Loss: 1.3822, Train: 40.64%, Valid: 35.72%, Test: 36.20%
Epoch: 900, Loss: 1.3809, Train: 40.72%, Valid: 35.62%, Test: 36.23%
Epoch: 925, Loss: 1.3805, Train: 40.76%, Valid: 35.59%, Test: 36.06%
Epoch: 950, Loss: 1.3788, Train: 40.87%, Valid: 35.45%, Test: 36.11%
Epoch: 975, Loss: 1.3781, Train: 40.89%, Valid: 35.43%, Test: 36.16%
Run 01:
Highest Train: 40.99
Highest Valid: 36.31
  Final Train: 37.84
   Final Test: 36.44
All runs:
Highest Train: 40.99, nan
Highest Valid: 36.31, nan
  Final Train: 37.84, nan
   Final Test: 36.44, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6031, Train: 22.20%, Valid: 22.20%, Test: 22.76%
Epoch: 25, Loss: 1.5546, Train: 28.73%, Valid: 28.56%, Test: 28.84%
Epoch: 50, Loss: 1.5054, Train: 33.97%, Valid: 33.88%, Test: 34.02%
Epoch: 75, Loss: 1.4892, Train: 34.65%, Valid: 34.38%, Test: 34.59%
Epoch: 100, Loss: 1.4791, Train: 35.09%, Valid: 34.83%, Test: 35.06%
Epoch: 125, Loss: 1.4697, Train: 35.63%, Valid: 35.21%, Test: 35.48%
Epoch: 150, Loss: 1.4591, Train: 36.22%, Valid: 35.59%, Test: 35.94%
Epoch: 175, Loss: 1.4480, Train: 36.72%, Valid: 36.23%, Test: 36.44%
Epoch: 200, Loss: 1.4381, Train: 37.35%, Valid: 36.71%, Test: 36.85%
Epoch: 225, Loss: 1.4296, Train: 37.93%, Valid: 37.13%, Test: 37.35%
Epoch: 250, Loss: 1.4218, Train: 38.43%, Valid: 37.35%, Test: 37.70%
Epoch: 275, Loss: 1.4146, Train: 38.81%, Valid: 37.52%, Test: 37.90%
Epoch: 300, Loss: 1.4076, Train: 39.01%, Valid: 37.72%, Test: 38.07%
Epoch: 325, Loss: 1.4012, Train: 39.40%, Valid: 38.01%, Test: 38.35%
Epoch: 350, Loss: 1.3948, Train: 39.64%, Valid: 38.14%, Test: 38.42%
Epoch: 375, Loss: 1.3890, Train: 39.95%, Valid: 38.18%, Test: 38.51%
Epoch: 400, Loss: 1.3840, Train: 40.11%, Valid: 38.25%, Test: 38.64%
Epoch: 425, Loss: 1.3790, Train: 40.34%, Valid: 38.38%, Test: 38.82%
Epoch: 450, Loss: 1.3748, Train: 40.59%, Valid: 38.56%, Test: 39.06%
Epoch: 475, Loss: 1.3705, Train: 40.79%, Valid: 38.52%, Test: 39.15%
Epoch: 500, Loss: 1.3666, Train: 40.97%, Valid: 38.65%, Test: 39.23%
Epoch: 525, Loss: 1.3662, Train: 40.78%, Valid: 38.33%, Test: 38.92%
Epoch: 550, Loss: 1.3606, Train: 41.19%, Valid: 38.74%, Test: 39.31%
Epoch: 575, Loss: 1.3573, Train: 41.31%, Valid: 38.77%, Test: 39.34%
Epoch: 600, Loss: 1.3543, Train: 41.44%, Valid: 38.87%, Test: 39.41%
Epoch: 625, Loss: 1.3513, Train: 41.61%, Valid: 38.90%, Test: 39.45%
Epoch: 650, Loss: 1.3481, Train: 41.74%, Valid: 39.03%, Test: 39.58%
Epoch: 675, Loss: 1.3450, Train: 41.83%, Valid: 39.10%, Test: 39.65%
Epoch: 700, Loss: 1.3418, Train: 41.96%, Valid: 39.21%, Test: 39.71%
Epoch: 725, Loss: 1.4050, Train: 39.56%, Valid: 37.05%, Test: 37.69%
Epoch: 750, Loss: 1.3529, Train: 41.24%, Valid: 38.83%, Test: 39.27%
Epoch: 775, Loss: 1.3410, Train: 41.88%, Valid: 39.36%, Test: 39.80%
Epoch: 800, Loss: 1.3361, Train: 42.21%, Valid: 39.50%, Test: 39.96%
Epoch: 825, Loss: 1.3327, Train: 42.35%, Valid: 39.55%, Test: 40.02%
Epoch: 850, Loss: 1.3298, Train: 42.53%, Valid: 39.67%, Test: 40.14%
Epoch: 875, Loss: 1.3273, Train: 42.66%, Valid: 39.83%, Test: 40.31%
Epoch: 900, Loss: 1.3249, Train: 42.80%, Valid: 39.99%, Test: 40.48%
Epoch: 925, Loss: 1.3225, Train: 42.94%, Valid: 40.10%, Test: 40.58%
Epoch: 950, Loss: 1.3203, Train: 43.06%, Valid: 40.18%, Test: 40.72%
Epoch: 975, Loss: 1.3180, Train: 43.17%, Valid: 40.27%, Test: 40.83%
Run 01:
Highest Train: 43.30
Highest Valid: 40.40
  Final Train: 43.30
   Final Test: 41.05
All runs:
Highest Train: 43.30, nan
Highest Valid: 40.40, nan
  Final Train: 43.30, nan
   Final Test: 41.05, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6912, Train: 23.83%, Valid: 23.99%, Test: 23.81%
Epoch: 25, Loss: 1.4707, Train: 28.27%, Valid: 27.99%, Test: 28.41%
Epoch: 50, Loss: 1.4411, Train: 34.87%, Valid: 34.59%, Test: 34.73%
Epoch: 75, Loss: 1.4173, Train: 39.20%, Valid: 38.79%, Test: 38.71%
Epoch: 100, Loss: 1.4054, Train: 38.67%, Valid: 38.22%, Test: 38.81%
Epoch: 125, Loss: 1.3972, Train: 39.63%, Valid: 39.15%, Test: 39.56%
Epoch: 150, Loss: 1.3936, Train: 39.64%, Valid: 39.18%, Test: 39.63%
Epoch: 175, Loss: 1.3876, Train: 40.41%, Valid: 40.07%, Test: 40.39%
Epoch: 200, Loss: 1.3850, Train: 40.98%, Valid: 40.70%, Test: 41.04%
Epoch: 225, Loss: 1.3813, Train: 41.18%, Valid: 41.08%, Test: 41.31%
Epoch: 250, Loss: 1.3775, Train: 41.34%, Valid: 41.18%, Test: 41.42%
Epoch: 275, Loss: 1.3747, Train: 41.17%, Valid: 41.01%, Test: 41.19%
Epoch: 300, Loss: 1.3714, Train: 40.89%, Valid: 40.72%, Test: 41.06%
Epoch: 325, Loss: 1.3692, Train: 41.19%, Valid: 41.10%, Test: 41.20%
Epoch: 350, Loss: 1.3671, Train: 40.91%, Valid: 40.74%, Test: 41.00%
Epoch: 375, Loss: 1.3630, Train: 41.02%, Valid: 40.91%, Test: 41.08%
Epoch: 400, Loss: 1.3610, Train: 41.02%, Valid: 40.97%, Test: 41.10%
Epoch: 425, Loss: 1.3588, Train: 40.87%, Valid: 40.69%, Test: 40.99%
Epoch: 450, Loss: 1.3566, Train: 41.02%, Valid: 40.92%, Test: 41.12%
Epoch: 475, Loss: 1.3551, Train: 40.76%, Valid: 40.42%, Test: 40.86%
Epoch: 500, Loss: 1.3570, Train: 40.80%, Valid: 40.64%, Test: 41.02%
Epoch: 525, Loss: 1.3515, Train: 41.22%, Valid: 41.11%, Test: 41.32%
Epoch: 550, Loss: 1.3543, Train: 40.95%, Valid: 40.71%, Test: 41.04%
Epoch: 575, Loss: 1.3473, Train: 41.29%, Valid: 41.11%, Test: 41.39%
Epoch: 600, Loss: 1.3459, Train: 41.45%, Valid: 41.29%, Test: 41.57%
Epoch: 625, Loss: 1.3447, Train: 41.22%, Valid: 41.00%, Test: 41.43%
Epoch: 650, Loss: 1.3442, Train: 41.29%, Valid: 41.12%, Test: 41.39%
Epoch: 675, Loss: 1.3413, Train: 41.37%, Valid: 41.16%, Test: 41.50%
Epoch: 700, Loss: 1.3411, Train: 40.93%, Valid: 40.77%, Test: 41.15%
Epoch: 725, Loss: 1.3430, Train: 41.40%, Valid: 41.29%, Test: 41.52%
Epoch: 750, Loss: 1.3476, Train: 41.25%, Valid: 41.02%, Test: 41.42%
Epoch: 775, Loss: 1.4219, Train: 37.93%, Valid: 37.56%, Test: 38.00%
Epoch: 800, Loss: 1.3917, Train: 39.70%, Valid: 39.38%, Test: 39.60%
Epoch: 825, Loss: 1.3780, Train: 40.54%, Valid: 40.28%, Test: 40.51%
Epoch: 850, Loss: 1.3701, Train: 40.95%, Valid: 40.75%, Test: 40.82%
Epoch: 875, Loss: 1.3643, Train: 41.04%, Valid: 40.91%, Test: 41.15%
Epoch: 900, Loss: 1.3590, Train: 41.22%, Valid: 41.03%, Test: 41.31%
Epoch: 925, Loss: 1.3546, Train: 41.30%, Valid: 41.24%, Test: 41.49%
Epoch: 950, Loss: 1.3509, Train: 41.39%, Valid: 41.28%, Test: 41.50%
Epoch: 975, Loss: 1.3481, Train: 41.43%, Valid: 41.28%, Test: 41.47%
Run 01:
Highest Train: 41.75
Highest Valid: 41.63
  Final Train: 41.73
   Final Test: 41.77
All runs:
Highest Train: 41.75, nan
Highest Valid: 41.63, nan
  Final Train: 41.73, nan
   Final Test: 41.77, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6071, Train: 18.95%, Valid: 18.92%, Test: 18.69%
Epoch: 25, Loss: 1.5648, Train: 28.94%, Valid: 28.74%, Test: 28.99%
Epoch: 50, Loss: 1.5243, Train: 33.96%, Valid: 33.78%, Test: 34.00%
Epoch: 75, Loss: 1.5074, Train: 34.46%, Valid: 34.23%, Test: 34.54%
Epoch: 100, Loss: 1.4966, Train: 34.97%, Valid: 34.63%, Test: 34.88%
Epoch: 125, Loss: 1.4904, Train: 35.43%, Valid: 34.92%, Test: 35.28%
Epoch: 150, Loss: 1.4849, Train: 35.77%, Valid: 35.12%, Test: 35.59%
Epoch: 175, Loss: 1.4797, Train: 36.07%, Valid: 35.39%, Test: 35.86%
Epoch: 200, Loss: 1.4752, Train: 36.39%, Valid: 35.60%, Test: 36.11%
Epoch: 225, Loss: 1.4729, Train: 36.69%, Valid: 35.78%, Test: 36.30%
Epoch: 250, Loss: 1.4689, Train: 36.86%, Valid: 35.89%, Test: 36.41%
Epoch: 275, Loss: 1.4656, Train: 37.07%, Valid: 36.04%, Test: 36.44%
Epoch: 300, Loss: 1.4641, Train: 37.29%, Valid: 36.18%, Test: 36.54%
Epoch: 325, Loss: 1.4604, Train: 37.50%, Valid: 36.27%, Test: 36.56%
Epoch: 350, Loss: 1.4596, Train: 37.60%, Valid: 36.31%, Test: 36.62%
Epoch: 375, Loss: 1.4578, Train: 37.65%, Valid: 36.37%, Test: 36.63%
Epoch: 400, Loss: 1.4564, Train: 37.83%, Valid: 36.40%, Test: 36.62%
Epoch: 425, Loss: 1.4541, Train: 37.93%, Valid: 36.39%, Test: 36.66%
Epoch: 450, Loss: 1.4536, Train: 38.08%, Valid: 36.45%, Test: 36.77%
Epoch: 475, Loss: 1.4518, Train: 38.10%, Valid: 36.42%, Test: 36.77%
Epoch: 500, Loss: 1.4517, Train: 38.17%, Valid: 36.44%, Test: 36.76%
Epoch: 525, Loss: 1.4488, Train: 38.22%, Valid: 36.46%, Test: 36.80%
Epoch: 550, Loss: 1.4486, Train: 38.32%, Valid: 36.45%, Test: 36.75%
Epoch: 575, Loss: 1.4505, Train: 38.37%, Valid: 36.52%, Test: 36.82%
Epoch: 600, Loss: 1.4466, Train: 38.47%, Valid: 36.55%, Test: 36.89%
Epoch: 625, Loss: 1.4467, Train: 38.49%, Valid: 36.45%, Test: 36.79%
Epoch: 650, Loss: 1.4450, Train: 38.61%, Valid: 36.49%, Test: 36.88%
Epoch: 675, Loss: 1.4435, Train: 38.63%, Valid: 36.48%, Test: 36.88%
Epoch: 700, Loss: 1.4442, Train: 38.65%, Valid: 36.53%, Test: 36.90%
Epoch: 725, Loss: 1.4432, Train: 38.68%, Valid: 36.48%, Test: 36.87%
Epoch: 750, Loss: 1.4426, Train: 38.72%, Valid: 36.52%, Test: 36.84%
Epoch: 775, Loss: 1.4425, Train: 38.79%, Valid: 36.48%, Test: 36.93%
Epoch: 800, Loss: 1.4409, Train: 38.79%, Valid: 36.51%, Test: 36.88%
Epoch: 825, Loss: 1.4412, Train: 38.87%, Valid: 36.58%, Test: 36.90%
Epoch: 850, Loss: 1.4402, Train: 38.92%, Valid: 36.53%, Test: 36.92%
Epoch: 875, Loss: 1.4407, Train: 38.92%, Valid: 36.47%, Test: 36.86%
Epoch: 900, Loss: 1.4395, Train: 38.93%, Valid: 36.47%, Test: 36.93%
Epoch: 925, Loss: 1.4400, Train: 38.99%, Valid: 36.48%, Test: 36.91%
Epoch: 950, Loss: 1.4381, Train: 39.03%, Valid: 36.58%, Test: 36.88%
Epoch: 975, Loss: 1.4367, Train: 39.07%, Valid: 36.51%, Test: 36.89%
Run 01:
Highest Train: 39.10
Highest Valid: 36.61
  Final Train: 38.75
   Final Test: 36.90
All runs:
Highest Train: 39.10, nan
Highest Valid: 36.61, nan
  Final Train: 38.75, nan
   Final Test: 36.90, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6176, Train: 17.17%, Valid: 17.34%, Test: 17.00%
Epoch: 25, Loss: 1.5704, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 50, Loss: 1.5318, Train: 33.35%, Valid: 33.28%, Test: 33.48%
Epoch: 75, Loss: 1.5099, Train: 34.26%, Valid: 34.05%, Test: 34.28%
Epoch: 100, Loss: 1.5000, Train: 34.66%, Valid: 34.42%, Test: 34.50%
Epoch: 125, Loss: 1.4952, Train: 34.99%, Valid: 34.63%, Test: 34.82%
Epoch: 150, Loss: 1.4893, Train: 35.24%, Valid: 34.78%, Test: 35.13%
Epoch: 175, Loss: 1.4833, Train: 35.67%, Valid: 35.04%, Test: 35.51%
Epoch: 200, Loss: 1.4801, Train: 36.07%, Valid: 35.33%, Test: 35.81%
Epoch: 225, Loss: 1.4757, Train: 36.47%, Valid: 35.56%, Test: 36.06%
Epoch: 250, Loss: 1.4721, Train: 36.75%, Valid: 35.81%, Test: 36.18%
Epoch: 275, Loss: 1.4690, Train: 36.97%, Valid: 35.93%, Test: 36.36%
Epoch: 300, Loss: 1.4657, Train: 37.15%, Valid: 36.06%, Test: 36.42%
Epoch: 325, Loss: 1.4626, Train: 37.38%, Valid: 36.15%, Test: 36.51%
Epoch: 350, Loss: 1.4557, Train: 37.60%, Valid: 36.33%, Test: 36.69%
Epoch: 375, Loss: 1.4495, Train: 37.81%, Valid: 36.46%, Test: 36.81%
Epoch: 400, Loss: 1.4445, Train: 38.02%, Valid: 36.63%, Test: 36.99%
Epoch: 425, Loss: 1.4393, Train: 38.20%, Valid: 36.70%, Test: 37.07%
Epoch: 450, Loss: 1.4363, Train: 38.43%, Valid: 36.99%, Test: 37.23%
Epoch: 475, Loss: 1.4353, Train: 38.72%, Valid: 37.18%, Test: 37.43%
Epoch: 500, Loss: 1.4314, Train: 38.93%, Valid: 37.39%, Test: 37.62%
Epoch: 525, Loss: 1.4258, Train: 39.23%, Valid: 37.60%, Test: 37.89%
Epoch: 550, Loss: 1.4255, Train: 39.62%, Valid: 37.84%, Test: 38.14%
Epoch: 575, Loss: 1.4173, Train: 39.63%, Valid: 37.81%, Test: 38.18%
Epoch: 600, Loss: 1.4181, Train: 39.71%, Valid: 37.97%, Test: 38.35%
Epoch: 625, Loss: 1.4129, Train: 39.92%, Valid: 38.01%, Test: 38.41%
Epoch: 650, Loss: 1.4096, Train: 40.04%, Valid: 38.16%, Test: 38.52%
Epoch: 675, Loss: 1.4047, Train: 40.27%, Valid: 38.24%, Test: 38.64%
Epoch: 700, Loss: 1.3997, Train: 40.51%, Valid: 38.47%, Test: 38.88%
Epoch: 725, Loss: 1.4014, Train: 40.70%, Valid: 38.64%, Test: 39.06%
Epoch: 750, Loss: 1.3970, Train: 40.59%, Valid: 38.49%, Test: 38.88%
Epoch: 775, Loss: 1.3955, Train: 41.00%, Valid: 38.92%, Test: 39.31%
Epoch: 800, Loss: 1.3939, Train: 40.97%, Valid: 39.10%, Test: 39.36%
Epoch: 825, Loss: 1.3878, Train: 41.09%, Valid: 39.01%, Test: 39.34%
Epoch: 850, Loss: 1.3836, Train: 41.12%, Valid: 38.97%, Test: 39.36%
Epoch: 875, Loss: 1.3802, Train: 41.42%, Valid: 39.19%, Test: 39.54%
Epoch: 900, Loss: 1.3888, Train: 41.24%, Valid: 39.01%, Test: 39.37%
Epoch: 925, Loss: 1.3792, Train: 41.42%, Valid: 39.32%, Test: 39.61%
Epoch: 950, Loss: 1.3781, Train: 41.69%, Valid: 39.52%, Test: 39.86%
Epoch: 975, Loss: 1.3758, Train: 41.56%, Valid: 39.31%, Test: 39.65%
Run 01:
Highest Train: 41.91
Highest Valid: 39.68
  Final Train: 41.87
   Final Test: 39.96
All runs:
Highest Train: 41.91, nan
Highest Valid: 39.68, nan
  Final Train: 41.87, nan
   Final Test: 39.96, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.5456, Train: 27.26%, Valid: 27.03%, Test: 27.29%
Epoch: 25, Loss: 1.4865, Train: 30.10%, Valid: 29.85%, Test: 30.13%
Epoch: 50, Loss: 1.4677, Train: 24.10%, Valid: 23.91%, Test: 24.23%
Epoch: 75, Loss: 1.4500, Train: 26.18%, Valid: 25.80%, Test: 26.25%
Epoch: 100, Loss: 1.4370, Train: 24.06%, Valid: 23.94%, Test: 24.11%
Epoch: 125, Loss: 1.4347, Train: 24.68%, Valid: 24.51%, Test: 24.71%
Epoch: 150, Loss: 1.4316, Train: 23.91%, Valid: 23.92%, Test: 23.81%
Epoch: 175, Loss: 1.4178, Train: 25.94%, Valid: 25.95%, Test: 26.11%
Epoch: 200, Loss: 1.4379, Train: 29.68%, Valid: 29.53%, Test: 29.41%
Epoch: 225, Loss: 1.4190, Train: 28.09%, Valid: 27.95%, Test: 28.18%
Epoch: 250, Loss: 1.4180, Train: 30.80%, Valid: 30.74%, Test: 30.77%
Epoch: 275, Loss: 1.4163, Train: 33.11%, Valid: 32.93%, Test: 32.92%
Epoch: 300, Loss: 1.4271, Train: 33.32%, Valid: 33.28%, Test: 33.36%
Epoch: 325, Loss: 1.4093, Train: 36.03%, Valid: 35.78%, Test: 35.91%
Epoch: 350, Loss: 1.4068, Train: 34.88%, Valid: 34.57%, Test: 34.78%
Epoch: 375, Loss: 1.4054, Train: 36.79%, Valid: 36.42%, Test: 36.53%
Epoch: 400, Loss: 1.3967, Train: 34.80%, Valid: 34.60%, Test: 34.62%
Epoch: 425, Loss: 1.4072, Train: 37.27%, Valid: 37.00%, Test: 37.34%
Epoch: 450, Loss: 1.4072, Train: 37.71%, Valid: 37.43%, Test: 37.74%
Epoch: 475, Loss: 1.3939, Train: 36.61%, Valid: 36.15%, Test: 36.58%
Epoch: 500, Loss: 1.4120, Train: 39.37%, Valid: 38.88%, Test: 39.29%
Epoch: 525, Loss: 1.4153, Train: 37.91%, Valid: 37.49%, Test: 37.72%
Epoch: 550, Loss: 1.4235, Train: 39.66%, Valid: 39.28%, Test: 39.50%
Epoch: 575, Loss: 1.3998, Train: 38.30%, Valid: 37.95%, Test: 38.47%
Epoch: 600, Loss: 1.3956, Train: 38.32%, Valid: 38.03%, Test: 38.41%
Epoch: 625, Loss: 1.4092, Train: 38.02%, Valid: 37.63%, Test: 37.94%
Epoch: 650, Loss: 1.4030, Train: 34.72%, Valid: 34.54%, Test: 34.58%
Epoch: 675, Loss: 1.3910, Train: 36.57%, Valid: 36.26%, Test: 36.45%
Epoch: 700, Loss: 1.4015, Train: 39.67%, Valid: 39.28%, Test: 39.53%
Epoch: 725, Loss: 1.3839, Train: 37.94%, Valid: 37.62%, Test: 37.95%
Epoch: 750, Loss: 1.3958, Train: 39.78%, Valid: 39.50%, Test: 39.70%
Epoch: 775, Loss: 1.4037, Train: 40.05%, Valid: 39.74%, Test: 39.77%
Epoch: 800, Loss: 1.3836, Train: 38.79%, Valid: 38.50%, Test: 38.61%
Epoch: 825, Loss: 1.3794, Train: 38.41%, Valid: 38.10%, Test: 38.18%
Epoch: 850, Loss: 1.3827, Train: 37.47%, Valid: 37.25%, Test: 37.43%
Epoch: 875, Loss: 1.3778, Train: 37.52%, Valid: 37.18%, Test: 37.36%
Epoch: 900, Loss: 1.3828, Train: 38.19%, Valid: 37.90%, Test: 37.92%
Epoch: 925, Loss: 1.3738, Train: 38.69%, Valid: 38.41%, Test: 38.52%
Epoch: 950, Loss: 1.3735, Train: 37.02%, Valid: 36.72%, Test: 36.84%
Epoch: 975, Loss: 1.3895, Train: 34.28%, Valid: 34.15%, Test: 34.06%
Run 01:
Highest Train: 40.75
Highest Valid: 40.52
  Final Train: 40.74
   Final Test: 40.68
All runs:
Highest Train: 40.75, nan
Highest Valid: 40.52, nan
  Final Train: 40.74, nan
   Final Test: 40.68, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6023, Train: 28.69%, Valid: 28.48%, Test: 28.81%
Epoch: 25, Loss: 1.5658, Train: 28.74%, Valid: 28.59%, Test: 28.85%
Epoch: 50, Loss: 1.5230, Train: 33.91%, Valid: 33.77%, Test: 34.04%
Epoch: 75, Loss: 1.5066, Train: 34.51%, Valid: 34.31%, Test: 34.55%
Epoch: 100, Loss: 1.4976, Train: 34.91%, Valid: 34.65%, Test: 34.81%
Epoch: 125, Loss: 1.4908, Train: 35.27%, Valid: 34.94%, Test: 35.23%
Epoch: 150, Loss: 1.4844, Train: 35.70%, Valid: 35.18%, Test: 35.64%
Epoch: 175, Loss: 1.4812, Train: 36.10%, Valid: 35.37%, Test: 36.02%
Epoch: 200, Loss: 1.4767, Train: 36.42%, Valid: 35.55%, Test: 36.18%
Epoch: 225, Loss: 1.4726, Train: 36.61%, Valid: 35.67%, Test: 36.37%
Epoch: 250, Loss: 1.4700, Train: 36.87%, Valid: 35.93%, Test: 36.32%
Epoch: 275, Loss: 1.4672, Train: 37.04%, Valid: 35.99%, Test: 36.42%
Epoch: 300, Loss: 1.4666, Train: 37.21%, Valid: 36.05%, Test: 36.56%
Epoch: 325, Loss: 1.4622, Train: 37.41%, Valid: 36.13%, Test: 36.56%
Epoch: 350, Loss: 1.4625, Train: 37.54%, Valid: 36.19%, Test: 36.67%
Epoch: 375, Loss: 1.4593, Train: 37.62%, Valid: 36.15%, Test: 36.65%
Epoch: 400, Loss: 1.4587, Train: 37.78%, Valid: 36.21%, Test: 36.71%
Epoch: 425, Loss: 1.4568, Train: 37.83%, Valid: 36.30%, Test: 36.73%
Epoch: 450, Loss: 1.4554, Train: 37.96%, Valid: 36.32%, Test: 36.75%
Epoch: 475, Loss: 1.4541, Train: 38.08%, Valid: 36.30%, Test: 36.70%
Epoch: 500, Loss: 1.4523, Train: 38.16%, Valid: 36.37%, Test: 36.75%
Epoch: 525, Loss: 1.4507, Train: 38.24%, Valid: 36.37%, Test: 36.78%
Epoch: 550, Loss: 1.4501, Train: 38.30%, Valid: 36.32%, Test: 36.76%
Epoch: 575, Loss: 1.4505, Train: 38.39%, Valid: 36.39%, Test: 36.71%
Epoch: 600, Loss: 1.4485, Train: 38.46%, Valid: 36.39%, Test: 36.78%
Epoch: 625, Loss: 1.4477, Train: 38.50%, Valid: 36.39%, Test: 36.79%
Epoch: 650, Loss: 1.4468, Train: 38.57%, Valid: 36.43%, Test: 36.87%
Epoch: 675, Loss: 1.4468, Train: 38.63%, Valid: 36.46%, Test: 36.86%
Epoch: 700, Loss: 1.4469, Train: 38.68%, Valid: 36.50%, Test: 36.88%
Epoch: 725, Loss: 1.4462, Train: 38.69%, Valid: 36.40%, Test: 36.91%
Epoch: 750, Loss: 1.4436, Train: 38.73%, Valid: 36.40%, Test: 36.91%
Epoch: 775, Loss: 1.4426, Train: 38.78%, Valid: 36.42%, Test: 36.99%
Epoch: 800, Loss: 1.4428, Train: 38.85%, Valid: 36.41%, Test: 36.95%
Epoch: 825, Loss: 1.4436, Train: 38.89%, Valid: 36.45%, Test: 36.94%
Epoch: 850, Loss: 1.4416, Train: 38.92%, Valid: 36.44%, Test: 36.90%
Epoch: 875, Loss: 1.4435, Train: 38.96%, Valid: 36.46%, Test: 36.90%
Epoch: 900, Loss: 1.4421, Train: 38.95%, Valid: 36.46%, Test: 36.92%
Epoch: 925, Loss: 1.4416, Train: 38.98%, Valid: 36.47%, Test: 36.94%
Epoch: 950, Loss: 1.4402, Train: 39.01%, Valid: 36.46%, Test: 36.91%
Epoch: 975, Loss: 1.4404, Train: 39.05%, Valid: 36.46%, Test: 36.90%
Run 01:
Highest Train: 39.12
Highest Valid: 36.53
  Final Train: 38.91
   Final Test: 36.86
All runs:
Highest Train: 39.12, nan
Highest Valid: 36.53, nan
  Final Train: 38.91, nan
   Final Test: 36.86, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.5986, Train: 28.70%, Valid: 28.52%, Test: 28.83%
Epoch: 25, Loss: 1.5651, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 50, Loss: 1.5228, Train: 33.91%, Valid: 33.80%, Test: 33.98%
Epoch: 75, Loss: 1.5054, Train: 34.53%, Valid: 34.34%, Test: 34.53%
Epoch: 100, Loss: 1.4962, Train: 34.97%, Valid: 34.73%, Test: 34.89%
Epoch: 125, Loss: 1.4880, Train: 35.47%, Valid: 35.02%, Test: 35.37%
Epoch: 150, Loss: 1.4840, Train: 35.92%, Valid: 35.21%, Test: 35.74%
Epoch: 175, Loss: 1.4782, Train: 36.46%, Valid: 35.65%, Test: 36.07%
Epoch: 200, Loss: 1.4729, Train: 36.72%, Valid: 35.87%, Test: 36.33%
Epoch: 225, Loss: 1.4688, Train: 37.02%, Valid: 36.12%, Test: 36.52%
Epoch: 250, Loss: 1.4615, Train: 37.26%, Valid: 36.35%, Test: 36.61%
Epoch: 275, Loss: 1.4551, Train: 37.59%, Valid: 36.51%, Test: 36.72%
Epoch: 300, Loss: 1.4510, Train: 37.70%, Valid: 36.63%, Test: 36.87%
Epoch: 325, Loss: 1.4446, Train: 38.06%, Valid: 36.84%, Test: 37.06%
Epoch: 350, Loss: 1.4382, Train: 38.38%, Valid: 37.13%, Test: 37.36%
Epoch: 375, Loss: 1.4327, Train: 38.69%, Valid: 37.44%, Test: 37.62%
Epoch: 400, Loss: 1.4303, Train: 38.80%, Valid: 37.46%, Test: 37.71%
Epoch: 425, Loss: 1.4249, Train: 39.09%, Valid: 37.69%, Test: 37.95%
Epoch: 450, Loss: 1.4252, Train: 39.46%, Valid: 38.06%, Test: 38.38%
Epoch: 475, Loss: 1.4172, Train: 39.75%, Valid: 38.26%, Test: 38.63%
Epoch: 500, Loss: 1.4049, Train: 39.87%, Valid: 38.39%, Test: 38.77%
Epoch: 525, Loss: 1.4084, Train: 40.33%, Valid: 38.72%, Test: 38.94%
Epoch: 550, Loss: 1.4213, Train: 40.18%, Valid: 38.66%, Test: 39.01%
Epoch: 575, Loss: 1.4022, Train: 40.62%, Valid: 38.87%, Test: 39.34%
Epoch: 600, Loss: 1.3886, Train: 40.74%, Valid: 38.97%, Test: 39.47%
Epoch: 625, Loss: 1.3899, Train: 41.10%, Valid: 39.27%, Test: 39.68%
Epoch: 650, Loss: 1.3836, Train: 41.05%, Valid: 39.25%, Test: 39.64%
Epoch: 675, Loss: 1.3880, Train: 41.13%, Valid: 39.33%, Test: 39.74%
Epoch: 700, Loss: 1.3866, Train: 41.23%, Valid: 39.39%, Test: 39.75%
Epoch: 725, Loss: 1.3857, Train: 41.42%, Valid: 39.61%, Test: 39.85%
Epoch: 750, Loss: 1.3938, Train: 41.19%, Valid: 39.16%, Test: 39.48%
Epoch: 775, Loss: 1.3817, Train: 41.65%, Valid: 39.77%, Test: 40.02%
Epoch: 800, Loss: 1.3911, Train: 41.68%, Valid: 39.77%, Test: 40.04%
Epoch: 825, Loss: 1.3821, Train: 41.75%, Valid: 39.85%, Test: 40.13%
Epoch: 850, Loss: 1.3725, Train: 41.65%, Valid: 39.66%, Test: 39.94%
Epoch: 875, Loss: 1.3724, Train: 41.81%, Valid: 39.86%, Test: 40.03%
Epoch: 900, Loss: 1.3702, Train: 41.93%, Valid: 39.93%, Test: 40.27%
Epoch: 925, Loss: 1.3662, Train: 42.18%, Valid: 40.18%, Test: 40.40%
Epoch: 950, Loss: 1.3639, Train: 41.90%, Valid: 39.74%, Test: 40.23%
Epoch: 975, Loss: 1.3923, Train: 42.20%, Valid: 40.00%, Test: 40.43%
Run 01:
Highest Train: 42.25
Highest Valid: 40.22
  Final Train: 42.23
   Final Test: 40.47
All runs:
Highest Train: 42.25, nan
Highest Valid: 40.22, nan
  Final Train: 42.23, nan
   Final Test: 40.47, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.7059, Train: 18.35%, Valid: 18.56%, Test: 17.96%
Epoch: 25, Loss: 1.5151, Train: 27.36%, Valid: 27.37%, Test: 27.20%
Epoch: 50, Loss: 1.4913, Train: 35.21%, Valid: 35.08%, Test: 35.17%
Epoch: 75, Loss: 1.4630, Train: 35.51%, Valid: 35.30%, Test: 35.41%
Epoch: 100, Loss: 1.4453, Train: 36.25%, Valid: 36.06%, Test: 36.12%
Epoch: 125, Loss: 1.4358, Train: 37.29%, Valid: 37.07%, Test: 37.12%
Epoch: 150, Loss: 1.4285, Train: 37.59%, Valid: 37.24%, Test: 37.46%
Epoch: 175, Loss: 1.4291, Train: 37.95%, Valid: 37.52%, Test: 37.73%
Epoch: 200, Loss: 1.4382, Train: 38.62%, Valid: 38.31%, Test: 38.66%
Epoch: 225, Loss: 1.4143, Train: 38.17%, Valid: 37.78%, Test: 38.00%
Epoch: 250, Loss: 1.4129, Train: 37.53%, Valid: 37.14%, Test: 37.44%
Epoch: 275, Loss: 1.4157, Train: 36.36%, Valid: 36.02%, Test: 36.23%
Epoch: 300, Loss: 1.4094, Train: 37.08%, Valid: 36.66%, Test: 37.14%
Epoch: 325, Loss: 1.4095, Train: 39.49%, Valid: 39.14%, Test: 39.33%
Epoch: 350, Loss: 1.4103, Train: 38.28%, Valid: 37.99%, Test: 38.23%
Epoch: 375, Loss: 1.4011, Train: 36.87%, Valid: 36.40%, Test: 36.64%
Epoch: 400, Loss: 1.3989, Train: 37.72%, Valid: 37.42%, Test: 37.74%
Epoch: 425, Loss: 1.4008, Train: 38.58%, Valid: 38.23%, Test: 38.62%
Epoch: 450, Loss: 1.3974, Train: 38.19%, Valid: 37.77%, Test: 38.20%
Epoch: 475, Loss: 1.4001, Train: 38.02%, Valid: 37.61%, Test: 38.05%
Epoch: 500, Loss: 1.3956, Train: 37.86%, Valid: 37.52%, Test: 37.82%
Epoch: 525, Loss: 1.4305, Train: 38.79%, Valid: 38.34%, Test: 38.76%
Epoch: 550, Loss: 1.3920, Train: 37.14%, Valid: 36.97%, Test: 37.23%
Epoch: 575, Loss: 1.3930, Train: 36.78%, Valid: 36.66%, Test: 36.94%
Epoch: 600, Loss: 1.3906, Train: 36.73%, Valid: 36.51%, Test: 36.70%
Epoch: 625, Loss: 1.3981, Train: 38.52%, Valid: 38.20%, Test: 38.53%
Epoch: 650, Loss: 1.4119, Train: 38.25%, Valid: 37.90%, Test: 38.16%
Epoch: 675, Loss: 1.3845, Train: 39.27%, Valid: 39.08%, Test: 39.35%
Epoch: 700, Loss: 1.3944, Train: 40.54%, Valid: 40.17%, Test: 40.43%
Epoch: 725, Loss: 1.3924, Train: 35.75%, Valid: 35.59%, Test: 35.69%
Epoch: 750, Loss: 1.3821, Train: 36.97%, Valid: 36.74%, Test: 36.92%
Epoch: 775, Loss: 1.3877, Train: 37.35%, Valid: 37.19%, Test: 37.42%
Epoch: 800, Loss: 1.3870, Train: 40.68%, Valid: 40.48%, Test: 40.69%
Epoch: 825, Loss: 1.3794, Train: 37.61%, Valid: 37.27%, Test: 37.58%
Epoch: 850, Loss: 1.3795, Train: 41.09%, Valid: 40.88%, Test: 41.07%
Epoch: 875, Loss: 1.3806, Train: 38.78%, Valid: 38.52%, Test: 38.76%
Epoch: 900, Loss: 1.3875, Train: 38.65%, Valid: 38.38%, Test: 38.64%
Epoch: 925, Loss: 1.3972, Train: 35.95%, Valid: 35.81%, Test: 35.89%
Epoch: 950, Loss: 1.3777, Train: 37.65%, Valid: 37.30%, Test: 37.64%
Epoch: 975, Loss: 1.3748, Train: 38.38%, Valid: 38.21%, Test: 38.34%
Run 01:
Highest Train: 41.18
Highest Valid: 40.93
  Final Train: 41.18
   Final Test: 40.99
All runs:
Highest Train: 41.18, nan
Highest Valid: 40.93, nan
  Final Train: 41.18, nan
   Final Test: 40.99, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6087, Train: 22.16%, Valid: 22.17%, Test: 22.74%
Epoch: 25, Loss: 1.5594, Train: 28.73%, Valid: 28.54%, Test: 28.83%
Epoch: 50, Loss: 1.5072, Train: 33.96%, Valid: 33.92%, Test: 34.02%
Epoch: 75, Loss: 1.4924, Train: 34.67%, Valid: 34.36%, Test: 34.71%
Epoch: 100, Loss: 1.4842, Train: 35.16%, Valid: 34.68%, Test: 35.02%
Epoch: 125, Loss: 1.4775, Train: 35.54%, Valid: 34.91%, Test: 35.48%
Epoch: 150, Loss: 1.4695, Train: 35.98%, Valid: 35.33%, Test: 35.77%
Epoch: 175, Loss: 1.4608, Train: 36.44%, Valid: 35.67%, Test: 36.11%
Epoch: 200, Loss: 1.4533, Train: 36.76%, Valid: 35.97%, Test: 36.21%
Epoch: 225, Loss: 1.4474, Train: 37.10%, Valid: 36.14%, Test: 36.37%
Epoch: 250, Loss: 1.4423, Train: 37.39%, Valid: 36.26%, Test: 36.56%
Epoch: 275, Loss: 1.4375, Train: 37.66%, Valid: 36.40%, Test: 36.63%
Epoch: 300, Loss: 1.4334, Train: 37.86%, Valid: 36.47%, Test: 36.66%
Epoch: 325, Loss: 1.4295, Train: 38.12%, Valid: 36.40%, Test: 36.75%
Epoch: 350, Loss: 1.4265, Train: 38.17%, Valid: 36.32%, Test: 36.71%
Epoch: 375, Loss: 1.4233, Train: 38.40%, Valid: 36.26%, Test: 36.76%
Epoch: 400, Loss: 1.4211, Train: 38.57%, Valid: 36.47%, Test: 36.74%
Epoch: 425, Loss: 1.4181, Train: 38.71%, Valid: 36.41%, Test: 36.72%
Epoch: 450, Loss: 1.4158, Train: 38.78%, Valid: 36.38%, Test: 36.68%
Epoch: 475, Loss: 1.4134, Train: 38.94%, Valid: 36.39%, Test: 36.64%
Epoch: 500, Loss: 1.4110, Train: 39.02%, Valid: 36.37%, Test: 36.55%
Epoch: 525, Loss: 1.4084, Train: 39.11%, Valid: 36.40%, Test: 36.58%
Epoch: 550, Loss: 1.4061, Train: 39.23%, Valid: 36.35%, Test: 36.50%
Epoch: 575, Loss: 1.4039, Train: 39.34%, Valid: 36.43%, Test: 36.52%
Epoch: 600, Loss: 1.4024, Train: 39.50%, Valid: 36.32%, Test: 36.45%
Epoch: 625, Loss: 1.3997, Train: 39.56%, Valid: 36.43%, Test: 36.42%
Epoch: 650, Loss: 1.3977, Train: 39.72%, Valid: 36.38%, Test: 36.37%
Epoch: 675, Loss: 1.3964, Train: 39.85%, Valid: 36.35%, Test: 36.33%
Epoch: 700, Loss: 1.3942, Train: 39.88%, Valid: 36.28%, Test: 36.24%
Epoch: 725, Loss: 1.3937, Train: 39.94%, Valid: 36.25%, Test: 36.19%
Epoch: 750, Loss: 1.3906, Train: 40.13%, Valid: 36.13%, Test: 36.08%
Epoch: 775, Loss: 1.3891, Train: 40.28%, Valid: 36.14%, Test: 36.09%
Epoch: 800, Loss: 1.3883, Train: 40.33%, Valid: 36.06%, Test: 35.96%
Epoch: 825, Loss: 1.3864, Train: 40.43%, Valid: 36.08%, Test: 36.12%
Epoch: 850, Loss: 1.3848, Train: 40.51%, Valid: 36.11%, Test: 36.05%
Epoch: 875, Loss: 1.3838, Train: 40.49%, Valid: 36.02%, Test: 35.92%
Epoch: 900, Loss: 1.3818, Train: 40.69%, Valid: 35.91%, Test: 35.83%
Epoch: 925, Loss: 1.3808, Train: 40.79%, Valid: 35.86%, Test: 35.77%
Epoch: 950, Loss: 1.3795, Train: 40.83%, Valid: 35.86%, Test: 35.77%
Epoch: 975, Loss: 1.3787, Train: 40.88%, Valid: 35.91%, Test: 35.76%
Run 01:
Highest Train: 40.98
Highest Valid: 36.51
  Final Train: 39.42
   Final Test: 36.44
All runs:
Highest Train: 40.98, nan
Highest Valid: 36.51, nan
  Final Train: 39.42, nan
   Final Test: 36.44, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6308, Train: 17.42%, Valid: 17.59%, Test: 17.08%
Epoch: 25, Loss: 1.5628, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 50, Loss: 1.5195, Train: 33.58%, Valid: 33.40%, Test: 33.67%
Epoch: 75, Loss: 1.4957, Train: 34.52%, Valid: 34.32%, Test: 34.59%
Epoch: 100, Loss: 1.4874, Train: 35.09%, Valid: 34.64%, Test: 34.96%
Epoch: 125, Loss: 1.4799, Train: 35.60%, Valid: 35.05%, Test: 35.30%
Epoch: 150, Loss: 1.4708, Train: 36.03%, Valid: 35.33%, Test: 35.60%
Epoch: 175, Loss: 1.4602, Train: 36.42%, Valid: 35.61%, Test: 35.99%
Epoch: 200, Loss: 1.4479, Train: 36.90%, Valid: 36.00%, Test: 36.30%
Epoch: 225, Loss: 1.4352, Train: 37.57%, Valid: 36.53%, Test: 36.84%
Epoch: 250, Loss: 1.4241, Train: 38.18%, Valid: 37.01%, Test: 37.15%
Epoch: 275, Loss: 1.4146, Train: 38.72%, Valid: 37.25%, Test: 37.50%
Epoch: 300, Loss: 1.4067, Train: 39.03%, Valid: 37.51%, Test: 37.76%
Epoch: 325, Loss: 1.3997, Train: 39.40%, Valid: 37.69%, Test: 38.01%
Epoch: 350, Loss: 1.3945, Train: 39.63%, Valid: 37.83%, Test: 38.07%
Epoch: 375, Loss: 1.3886, Train: 39.95%, Valid: 38.09%, Test: 38.44%
Epoch: 400, Loss: 1.3829, Train: 40.17%, Valid: 38.07%, Test: 38.47%
Epoch: 425, Loss: 1.3775, Train: 40.40%, Valid: 38.24%, Test: 38.53%
Epoch: 450, Loss: 1.3724, Train: 40.59%, Valid: 38.40%, Test: 38.70%
Epoch: 475, Loss: 1.3738, Train: 40.89%, Valid: 38.63%, Test: 38.83%
Epoch: 500, Loss: 1.3667, Train: 41.01%, Valid: 38.75%, Test: 38.96%
Epoch: 525, Loss: 1.3605, Train: 41.19%, Valid: 38.97%, Test: 39.07%
Epoch: 550, Loss: 1.3562, Train: 41.42%, Valid: 39.13%, Test: 39.13%
Epoch: 575, Loss: 1.3523, Train: 41.61%, Valid: 39.20%, Test: 39.32%
Epoch: 600, Loss: 1.3483, Train: 41.83%, Valid: 39.35%, Test: 39.41%
Epoch: 625, Loss: 1.3441, Train: 42.01%, Valid: 39.46%, Test: 39.53%
Epoch: 650, Loss: 1.3397, Train: 42.21%, Valid: 39.48%, Test: 39.66%
Epoch: 675, Loss: 1.3352, Train: 42.39%, Valid: 39.56%, Test: 39.65%
Epoch: 700, Loss: 1.3308, Train: 42.65%, Valid: 39.77%, Test: 39.82%
Epoch: 725, Loss: 1.3265, Train: 42.77%, Valid: 39.74%, Test: 39.88%
Epoch: 750, Loss: 1.3220, Train: 42.98%, Valid: 39.87%, Test: 40.05%
Epoch: 775, Loss: 1.3181, Train: 43.09%, Valid: 39.97%, Test: 39.99%
Epoch: 800, Loss: 1.3140, Train: 43.34%, Valid: 40.21%, Test: 40.20%
Epoch: 825, Loss: 1.3838, Train: 39.35%, Valid: 36.46%, Test: 36.76%
Epoch: 850, Loss: 1.3441, Train: 41.76%, Valid: 39.02%, Test: 38.92%
Epoch: 875, Loss: 1.3191, Train: 43.09%, Valid: 40.12%, Test: 40.21%
Epoch: 900, Loss: 1.3101, Train: 43.52%, Valid: 40.53%, Test: 40.48%
Epoch: 925, Loss: 1.3056, Train: 43.69%, Valid: 40.51%, Test: 40.55%
Epoch: 950, Loss: 1.3025, Train: 43.82%, Valid: 40.61%, Test: 40.58%
Epoch: 975, Loss: 1.2998, Train: 43.98%, Valid: 40.66%, Test: 40.63%
Run 01:
Highest Train: 44.06
Highest Valid: 40.73
  Final Train: 44.04
   Final Test: 40.68
All runs:
Highest Train: 44.06, nan
Highest Valid: 40.73, nan
  Final Train: 44.04, nan
   Final Test: 40.68, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.5958, Train: 33.06%, Valid: 33.00%, Test: 33.92%
Epoch: 25, Loss: 1.4879, Train: 30.09%, Valid: 30.01%, Test: 30.20%
Epoch: 50, Loss: 1.4697, Train: 33.65%, Valid: 33.33%, Test: 34.10%
Epoch: 75, Loss: 1.4604, Train: 33.63%, Valid: 33.33%, Test: 34.02%
Epoch: 100, Loss: 1.4996, Train: 29.82%, Valid: 29.65%, Test: 29.88%
Epoch: 125, Loss: 1.4579, Train: 29.35%, Valid: 29.15%, Test: 29.44%
Epoch: 150, Loss: 1.4510, Train: 31.41%, Valid: 31.29%, Test: 31.54%
Epoch: 175, Loss: 1.4408, Train: 31.10%, Valid: 30.93%, Test: 31.11%
Epoch: 200, Loss: 1.4521, Train: 30.68%, Valid: 30.49%, Test: 30.73%
Epoch: 225, Loss: 1.4420, Train: 30.97%, Valid: 30.79%, Test: 31.09%
Epoch: 250, Loss: 1.4377, Train: 31.10%, Valid: 30.96%, Test: 31.18%
Epoch: 275, Loss: 1.4343, Train: 31.15%, Valid: 30.98%, Test: 31.22%
Epoch: 300, Loss: 1.4648, Train: 31.11%, Valid: 30.88%, Test: 31.22%
Epoch: 325, Loss: 1.4342, Train: 31.83%, Valid: 31.70%, Test: 32.05%
Epoch: 350, Loss: 1.4269, Train: 32.41%, Valid: 32.26%, Test: 32.61%
Epoch: 375, Loss: 1.4228, Train: 32.70%, Valid: 32.52%, Test: 32.89%
Epoch: 400, Loss: 1.4188, Train: 33.56%, Valid: 33.30%, Test: 33.64%
Epoch: 425, Loss: 1.4151, Train: 35.04%, Valid: 34.70%, Test: 35.01%
Epoch: 450, Loss: 1.4707, Train: 29.38%, Valid: 28.99%, Test: 29.45%
Epoch: 475, Loss: 1.4408, Train: 31.73%, Valid: 31.46%, Test: 31.90%
Epoch: 500, Loss: 1.4262, Train: 32.16%, Valid: 31.88%, Test: 32.32%
Epoch: 525, Loss: 1.4197, Train: 32.00%, Valid: 31.65%, Test: 32.21%
Epoch: 550, Loss: 1.4242, Train: 31.42%, Valid: 31.09%, Test: 31.49%
Epoch: 575, Loss: 1.4157, Train: 32.09%, Valid: 31.83%, Test: 32.22%
Epoch: 600, Loss: 1.4117, Train: 33.10%, Valid: 32.81%, Test: 33.03%
Epoch: 625, Loss: 1.4092, Train: 34.70%, Valid: 34.43%, Test: 34.43%
Epoch: 650, Loss: 1.4070, Train: 36.61%, Valid: 36.38%, Test: 36.45%
Epoch: 675, Loss: 1.4049, Train: 38.33%, Valid: 37.96%, Test: 38.15%
Epoch: 700, Loss: 1.4033, Train: 39.64%, Valid: 39.26%, Test: 39.40%
Epoch: 725, Loss: 1.4008, Train: 40.43%, Valid: 40.02%, Test: 40.30%
Epoch: 750, Loss: 1.3988, Train: 40.94%, Valid: 40.55%, Test: 40.69%
Epoch: 775, Loss: 1.3970, Train: 41.26%, Valid: 40.80%, Test: 41.03%
Epoch: 800, Loss: 1.4065, Train: 40.39%, Valid: 39.86%, Test: 40.17%
Epoch: 825, Loss: 1.3985, Train: 41.04%, Valid: 40.62%, Test: 40.81%
Epoch: 850, Loss: 1.3936, Train: 41.39%, Valid: 41.13%, Test: 41.20%
Epoch: 875, Loss: 1.3909, Train: 41.51%, Valid: 41.16%, Test: 41.41%
Epoch: 900, Loss: 1.3884, Train: 41.62%, Valid: 41.31%, Test: 41.55%
Epoch: 925, Loss: 1.4399, Train: 30.79%, Valid: 30.59%, Test: 30.84%
Epoch: 950, Loss: 1.4184, Train: 30.76%, Valid: 30.54%, Test: 30.79%
Epoch: 975, Loss: 1.4125, Train: 36.86%, Valid: 36.56%, Test: 36.71%
Run 01:
Highest Train: 41.65
Highest Valid: 41.35
  Final Train: 41.62
   Final Test: 41.56
All runs:
Highest Train: 41.65, nan
Highest Valid: 41.35, nan
  Final Train: 41.62, nan
   Final Test: 41.56, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.5960, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.5521, Train: 28.94%, Valid: 28.73%, Test: 29.04%
Epoch: 50, Loss: 1.5043, Train: 34.14%, Valid: 33.96%, Test: 34.15%
Epoch: 75, Loss: 1.4897, Train: 34.85%, Valid: 34.39%, Test: 34.82%
Epoch: 100, Loss: 1.4805, Train: 35.31%, Valid: 34.79%, Test: 35.22%
Epoch: 125, Loss: 1.4719, Train: 35.80%, Valid: 35.20%, Test: 35.64%
Epoch: 150, Loss: 1.4633, Train: 36.16%, Valid: 35.51%, Test: 35.86%
Epoch: 175, Loss: 1.4549, Train: 36.55%, Valid: 35.76%, Test: 36.31%
Epoch: 200, Loss: 1.4480, Train: 37.06%, Valid: 35.94%, Test: 36.39%
Epoch: 225, Loss: 1.4421, Train: 37.37%, Valid: 36.15%, Test: 36.44%
Epoch: 250, Loss: 1.4371, Train: 37.66%, Valid: 36.16%, Test: 36.57%
Epoch: 275, Loss: 1.4329, Train: 37.91%, Valid: 36.34%, Test: 36.53%
Epoch: 300, Loss: 1.4292, Train: 38.09%, Valid: 36.49%, Test: 36.59%
Epoch: 325, Loss: 1.4255, Train: 38.20%, Valid: 36.44%, Test: 36.63%
Epoch: 350, Loss: 1.4223, Train: 38.50%, Valid: 36.40%, Test: 36.58%
Epoch: 375, Loss: 1.4193, Train: 38.63%, Valid: 36.41%, Test: 36.56%
Epoch: 400, Loss: 1.4166, Train: 38.68%, Valid: 36.37%, Test: 36.51%
Epoch: 425, Loss: 1.4143, Train: 38.91%, Valid: 36.40%, Test: 36.46%
Epoch: 450, Loss: 1.4116, Train: 39.03%, Valid: 36.34%, Test: 36.46%
Epoch: 475, Loss: 1.4098, Train: 39.09%, Valid: 36.26%, Test: 36.52%
Epoch: 500, Loss: 1.4074, Train: 39.21%, Valid: 36.29%, Test: 36.54%
Epoch: 525, Loss: 1.4055, Train: 39.26%, Valid: 36.26%, Test: 36.35%
Epoch: 550, Loss: 1.4030, Train: 39.39%, Valid: 36.21%, Test: 36.37%
Epoch: 575, Loss: 1.4011, Train: 39.54%, Valid: 36.23%, Test: 36.41%
Epoch: 600, Loss: 1.3989, Train: 39.63%, Valid: 36.18%, Test: 36.39%
Epoch: 625, Loss: 1.3972, Train: 39.73%, Valid: 36.27%, Test: 36.39%
Epoch: 650, Loss: 1.3949, Train: 39.81%, Valid: 36.20%, Test: 36.42%
Epoch: 675, Loss: 1.3928, Train: 39.94%, Valid: 36.29%, Test: 36.38%
Epoch: 700, Loss: 1.3912, Train: 40.07%, Valid: 36.21%, Test: 36.30%
Epoch: 725, Loss: 1.3896, Train: 40.05%, Valid: 36.06%, Test: 36.17%
Epoch: 750, Loss: 1.3878, Train: 40.24%, Valid: 36.04%, Test: 36.16%
Epoch: 775, Loss: 1.3864, Train: 40.27%, Valid: 36.02%, Test: 36.11%
Epoch: 800, Loss: 1.3844, Train: 40.43%, Valid: 36.06%, Test: 36.14%
Epoch: 825, Loss: 1.3831, Train: 40.47%, Valid: 35.95%, Test: 36.12%
Epoch: 850, Loss: 1.3815, Train: 40.56%, Valid: 35.99%, Test: 36.03%
Epoch: 875, Loss: 1.3806, Train: 40.67%, Valid: 35.89%, Test: 36.00%
Epoch: 900, Loss: 1.3792, Train: 40.65%, Valid: 35.85%, Test: 36.00%
Epoch: 925, Loss: 1.3776, Train: 40.75%, Valid: 35.92%, Test: 35.98%
Epoch: 950, Loss: 1.3765, Train: 40.78%, Valid: 35.78%, Test: 35.94%
Epoch: 975, Loss: 1.3789, Train: 40.88%, Valid: 35.66%, Test: 35.91%
Run 01:
Highest Train: 40.95
Highest Valid: 36.50
  Final Train: 38.29
   Final Test: 36.65
All runs:
Highest Train: 40.95, nan
Highest Valid: 36.50, nan
  Final Train: 38.29, nan
   Final Test: 36.65, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.5949, Train: 28.72%, Valid: 28.53%, Test: 28.81%
Epoch: 25, Loss: 1.5484, Train: 29.51%, Valid: 29.30%, Test: 29.65%
Epoch: 50, Loss: 1.5020, Train: 34.24%, Valid: 34.12%, Test: 34.27%
Epoch: 75, Loss: 1.4896, Train: 34.86%, Valid: 34.52%, Test: 34.81%
Epoch: 100, Loss: 1.4812, Train: 35.24%, Valid: 34.81%, Test: 35.17%
Epoch: 125, Loss: 1.4730, Train: 35.74%, Valid: 35.09%, Test: 35.52%
Epoch: 150, Loss: 1.4634, Train: 36.20%, Valid: 35.53%, Test: 35.79%
Epoch: 175, Loss: 1.4532, Train: 36.75%, Valid: 35.93%, Test: 36.34%
Epoch: 200, Loss: 1.4437, Train: 37.18%, Valid: 36.22%, Test: 36.55%
Epoch: 225, Loss: 1.4332, Train: 37.70%, Valid: 36.52%, Test: 36.93%
Epoch: 250, Loss: 1.4228, Train: 38.15%, Valid: 36.94%, Test: 37.28%
Epoch: 275, Loss: 1.4136, Train: 38.65%, Valid: 37.36%, Test: 37.70%
Epoch: 300, Loss: 1.4049, Train: 39.13%, Valid: 37.65%, Test: 37.99%
Epoch: 325, Loss: 1.3969, Train: 39.49%, Valid: 37.77%, Test: 38.26%
Epoch: 350, Loss: 1.3895, Train: 39.86%, Valid: 38.17%, Test: 38.48%
Epoch: 375, Loss: 1.3827, Train: 40.10%, Valid: 38.27%, Test: 38.52%
Epoch: 400, Loss: 1.3762, Train: 40.44%, Valid: 38.52%, Test: 38.72%
Epoch: 425, Loss: 1.3702, Train: 40.76%, Valid: 38.72%, Test: 38.99%
Epoch: 450, Loss: 1.3659, Train: 40.92%, Valid: 38.89%, Test: 39.13%
Epoch: 475, Loss: 1.3606, Train: 41.07%, Valid: 38.97%, Test: 39.22%
Epoch: 500, Loss: 1.3560, Train: 41.31%, Valid: 39.22%, Test: 39.43%
Epoch: 525, Loss: 1.3514, Train: 41.53%, Valid: 39.30%, Test: 39.55%
Epoch: 550, Loss: 1.3469, Train: 41.78%, Valid: 39.51%, Test: 39.74%
Epoch: 575, Loss: 1.3427, Train: 41.93%, Valid: 39.65%, Test: 39.78%
Epoch: 600, Loss: 1.3388, Train: 42.19%, Valid: 39.91%, Test: 40.06%
Epoch: 625, Loss: 1.3347, Train: 42.26%, Valid: 39.87%, Test: 39.88%
Epoch: 650, Loss: 1.3306, Train: 42.56%, Valid: 39.99%, Test: 40.35%
Epoch: 675, Loss: 1.3262, Train: 42.67%, Valid: 40.09%, Test: 40.13%
Epoch: 700, Loss: 1.3314, Train: 42.53%, Valid: 39.98%, Test: 40.33%
Epoch: 725, Loss: 1.3227, Train: 42.98%, Valid: 40.20%, Test: 40.60%
Epoch: 750, Loss: 1.3179, Train: 43.14%, Valid: 40.36%, Test: 40.74%
Epoch: 775, Loss: 1.3125, Train: 43.37%, Valid: 40.52%, Test: 40.89%
Epoch: 800, Loss: 1.3093, Train: 43.48%, Valid: 40.63%, Test: 41.11%
Epoch: 825, Loss: 1.3618, Train: 41.30%, Valid: 39.11%, Test: 39.47%
Epoch: 850, Loss: 1.3394, Train: 42.30%, Valid: 39.79%, Test: 40.16%
Epoch: 875, Loss: 1.3287, Train: 42.78%, Valid: 40.16%, Test: 40.42%
Epoch: 900, Loss: 1.3180, Train: 43.10%, Valid: 40.65%, Test: 40.77%
Epoch: 925, Loss: 1.3100, Train: 43.48%, Valid: 40.94%, Test: 41.12%
Epoch: 950, Loss: 1.3061, Train: 43.59%, Valid: 41.07%, Test: 41.13%
Epoch: 975, Loss: 1.3031, Train: 43.72%, Valid: 41.09%, Test: 41.26%
Run 01:
Highest Train: 43.82
Highest Valid: 41.18
  Final Train: 43.82
   Final Test: 41.36
All runs:
Highest Train: 43.82, nan
Highest Valid: 41.18, nan
  Final Train: 43.82, nan
   Final Test: 41.36, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.0, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.5969, Train: 26.31%, Valid: 26.23%, Test: 26.19%
Epoch: 25, Loss: 1.4892, Train: 30.63%, Valid: 30.77%, Test: 30.98%
Epoch: 50, Loss: 1.4829, Train: 30.48%, Valid: 30.36%, Test: 30.62%
Epoch: 75, Loss: 1.4667, Train: 32.77%, Valid: 32.79%, Test: 33.15%
Epoch: 100, Loss: 1.4650, Train: 35.00%, Valid: 35.35%, Test: 35.22%
Epoch: 125, Loss: 1.4543, Train: 33.62%, Valid: 33.40%, Test: 34.05%
Epoch: 150, Loss: 1.4629, Train: 33.83%, Valid: 33.72%, Test: 34.16%
Epoch: 175, Loss: 1.4430, Train: 34.45%, Valid: 34.22%, Test: 34.93%
Epoch: 200, Loss: 1.4341, Train: 34.63%, Valid: 34.44%, Test: 35.12%
Epoch: 225, Loss: 1.4966, Train: 33.94%, Valid: 33.74%, Test: 34.36%
Epoch: 250, Loss: 1.4539, Train: 32.92%, Valid: 32.78%, Test: 33.61%
Epoch: 275, Loss: 1.4325, Train: 33.89%, Valid: 33.68%, Test: 34.35%
Epoch: 300, Loss: 1.4241, Train: 34.49%, Valid: 34.21%, Test: 34.84%
Epoch: 325, Loss: 1.4186, Train: 34.54%, Valid: 34.23%, Test: 34.76%
Epoch: 350, Loss: 1.5003, Train: 28.50%, Valid: 28.31%, Test: 28.61%
Epoch: 375, Loss: 1.4596, Train: 28.58%, Valid: 28.38%, Test: 28.64%
Epoch: 400, Loss: 1.4336, Train: 28.77%, Valid: 28.54%, Test: 28.83%
Epoch: 425, Loss: 1.4283, Train: 29.27%, Valid: 29.02%, Test: 29.38%
Epoch: 450, Loss: 1.4212, Train: 30.17%, Valid: 29.94%, Test: 30.32%
Epoch: 475, Loss: 1.4166, Train: 30.88%, Valid: 30.63%, Test: 31.05%
Epoch: 500, Loss: 1.4290, Train: 30.79%, Valid: 30.57%, Test: 30.95%
Epoch: 525, Loss: 1.4137, Train: 32.41%, Valid: 32.22%, Test: 32.62%
Epoch: 550, Loss: 1.4092, Train: 32.65%, Valid: 32.40%, Test: 32.90%
Epoch: 575, Loss: 1.4064, Train: 33.28%, Valid: 32.93%, Test: 33.46%
Epoch: 600, Loss: 1.4039, Train: 33.78%, Valid: 33.59%, Test: 33.97%
Epoch: 625, Loss: 1.4018, Train: 34.32%, Valid: 34.14%, Test: 34.51%
Epoch: 650, Loss: 1.4583, Train: 30.54%, Valid: 30.23%, Test: 30.44%
Epoch: 675, Loss: 1.4167, Train: 33.44%, Valid: 33.13%, Test: 33.73%
Epoch: 700, Loss: 1.4051, Train: 34.36%, Valid: 34.04%, Test: 34.55%
Epoch: 725, Loss: 1.4002, Train: 35.18%, Valid: 34.78%, Test: 35.42%
Epoch: 750, Loss: 1.3970, Train: 35.64%, Valid: 35.21%, Test: 35.88%
Epoch: 775, Loss: 1.3940, Train: 35.91%, Valid: 35.49%, Test: 36.09%
Epoch: 800, Loss: 1.3924, Train: 32.45%, Valid: 32.15%, Test: 33.02%
Epoch: 825, Loss: 1.4405, Train: 28.88%, Valid: 28.61%, Test: 28.84%
Epoch: 850, Loss: 1.4185, Train: 29.69%, Valid: 29.29%, Test: 29.60%
Epoch: 875, Loss: 1.4078, Train: 31.09%, Valid: 30.83%, Test: 31.31%
Epoch: 900, Loss: 1.3993, Train: 35.62%, Valid: 35.64%, Test: 35.98%
Epoch: 925, Loss: 1.3922, Train: 39.46%, Valid: 39.39%, Test: 39.47%
Epoch: 950, Loss: 1.3856, Train: 40.82%, Valid: 40.63%, Test: 40.64%
Epoch: 975, Loss: 1.3794, Train: 41.15%, Valid: 40.86%, Test: 40.89%
Run 01:
Highest Train: 41.31
Highest Valid: 41.01
  Final Train: 41.31
   Final Test: 41.09
All runs:
Highest Train: 41.31, nan
Highest Valid: 41.01, nan
  Final Train: 41.31, nan
   Final Test: 41.09, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6247, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.5707, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 50, Loss: 1.5287, Train: 33.46%, Valid: 33.34%, Test: 33.59%
Epoch: 75, Loss: 1.5096, Train: 34.28%, Valid: 34.11%, Test: 34.28%
Epoch: 100, Loss: 1.5007, Train: 34.66%, Valid: 34.32%, Test: 34.64%
Epoch: 125, Loss: 1.4947, Train: 34.97%, Valid: 34.61%, Test: 34.87%
Epoch: 150, Loss: 1.4901, Train: 35.44%, Valid: 34.94%, Test: 35.28%
Epoch: 175, Loss: 1.4848, Train: 35.85%, Valid: 35.17%, Test: 35.66%
Epoch: 200, Loss: 1.4803, Train: 36.18%, Valid: 35.44%, Test: 35.94%
Epoch: 225, Loss: 1.4789, Train: 36.47%, Valid: 35.54%, Test: 36.05%
Epoch: 250, Loss: 1.4751, Train: 36.71%, Valid: 35.71%, Test: 36.20%
Epoch: 275, Loss: 1.4725, Train: 36.90%, Valid: 35.81%, Test: 36.30%
Epoch: 300, Loss: 1.4706, Train: 37.09%, Valid: 35.88%, Test: 36.36%
Epoch: 325, Loss: 1.4678, Train: 37.19%, Valid: 35.91%, Test: 36.39%
Epoch: 350, Loss: 1.4666, Train: 37.31%, Valid: 36.00%, Test: 36.44%
Epoch: 375, Loss: 1.4641, Train: 37.45%, Valid: 36.06%, Test: 36.43%
Epoch: 400, Loss: 1.4619, Train: 37.54%, Valid: 36.09%, Test: 36.45%
Epoch: 425, Loss: 1.4605, Train: 37.72%, Valid: 36.22%, Test: 36.57%
Epoch: 450, Loss: 1.4581, Train: 37.80%, Valid: 36.28%, Test: 36.60%
Epoch: 475, Loss: 1.4582, Train: 37.81%, Valid: 36.34%, Test: 36.65%
Epoch: 500, Loss: 1.4564, Train: 37.95%, Valid: 36.38%, Test: 36.71%
Epoch: 525, Loss: 1.4543, Train: 38.05%, Valid: 36.40%, Test: 36.75%
Epoch: 550, Loss: 1.4545, Train: 38.15%, Valid: 36.37%, Test: 36.76%
Epoch: 575, Loss: 1.4528, Train: 38.23%, Valid: 36.41%, Test: 36.76%
Epoch: 600, Loss: 1.4516, Train: 38.28%, Valid: 36.48%, Test: 36.83%
Epoch: 625, Loss: 1.4501, Train: 38.38%, Valid: 36.49%, Test: 36.83%
Epoch: 650, Loss: 1.4501, Train: 38.40%, Valid: 36.50%, Test: 36.77%
Epoch: 675, Loss: 1.4494, Train: 38.45%, Valid: 36.50%, Test: 36.75%
Epoch: 700, Loss: 1.4481, Train: 38.53%, Valid: 36.56%, Test: 36.72%
Epoch: 725, Loss: 1.4476, Train: 38.60%, Valid: 36.59%, Test: 36.74%
Epoch: 750, Loss: 1.4466, Train: 38.62%, Valid: 36.52%, Test: 36.74%
Epoch: 775, Loss: 1.4440, Train: 38.65%, Valid: 36.56%, Test: 36.71%
Epoch: 800, Loss: 1.4440, Train: 38.73%, Valid: 36.62%, Test: 36.72%
Epoch: 825, Loss: 1.4438, Train: 38.78%, Valid: 36.56%, Test: 36.75%
Epoch: 850, Loss: 1.4453, Train: 38.77%, Valid: 36.51%, Test: 36.72%
Epoch: 875, Loss: 1.4434, Train: 38.82%, Valid: 36.58%, Test: 36.81%
Epoch: 900, Loss: 1.4430, Train: 38.82%, Valid: 36.56%, Test: 36.69%
Epoch: 925, Loss: 1.4428, Train: 38.93%, Valid: 36.57%, Test: 36.76%
Epoch: 950, Loss: 1.4414, Train: 38.90%, Valid: 36.58%, Test: 36.72%
Epoch: 975, Loss: 1.4413, Train: 38.97%, Valid: 36.55%, Test: 36.71%
Run 01:
Highest Train: 39.00
Highest Valid: 36.64
  Final Train: 38.84
   Final Test: 36.77
All runs:
Highest Train: 39.00, nan
Highest Valid: 36.64, nan
  Final Train: 38.84, nan
   Final Test: 36.77, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6044, Train: 25.51%, Valid: 25.47%, Test: 25.12%
Epoch: 25, Loss: 1.5613, Train: 28.80%, Valid: 28.61%, Test: 28.90%
Epoch: 50, Loss: 1.5215, Train: 34.00%, Valid: 33.78%, Test: 34.05%
Epoch: 75, Loss: 1.5062, Train: 34.51%, Valid: 34.31%, Test: 34.52%
Epoch: 100, Loss: 1.4977, Train: 34.89%, Valid: 34.71%, Test: 34.79%
Epoch: 125, Loss: 1.4912, Train: 35.28%, Valid: 34.92%, Test: 35.18%
Epoch: 150, Loss: 1.4865, Train: 35.66%, Valid: 35.13%, Test: 35.49%
Epoch: 175, Loss: 1.4819, Train: 36.02%, Valid: 35.47%, Test: 35.74%
Epoch: 200, Loss: 1.4771, Train: 36.29%, Valid: 35.67%, Test: 36.05%
Epoch: 225, Loss: 1.4732, Train: 36.58%, Valid: 35.85%, Test: 36.30%
Epoch: 250, Loss: 1.4692, Train: 36.85%, Valid: 35.99%, Test: 36.42%
Epoch: 275, Loss: 1.4654, Train: 37.07%, Valid: 36.20%, Test: 36.54%
Epoch: 300, Loss: 1.4607, Train: 37.28%, Valid: 36.38%, Test: 36.65%
Epoch: 325, Loss: 1.4575, Train: 37.51%, Valid: 36.41%, Test: 36.75%
Epoch: 350, Loss: 1.4471, Train: 37.85%, Valid: 36.66%, Test: 36.98%
Epoch: 375, Loss: 1.4389, Train: 38.10%, Valid: 36.94%, Test: 37.19%
Epoch: 400, Loss: 1.4318, Train: 38.54%, Valid: 37.30%, Test: 37.60%
Epoch: 425, Loss: 1.4313, Train: 39.02%, Valid: 37.66%, Test: 37.91%
Epoch: 450, Loss: 1.4235, Train: 39.27%, Valid: 37.82%, Test: 38.08%
Epoch: 475, Loss: 1.4197, Train: 39.58%, Valid: 38.15%, Test: 38.28%
Epoch: 500, Loss: 1.4073, Train: 39.95%, Valid: 38.46%, Test: 38.68%
Epoch: 525, Loss: 1.4100, Train: 40.27%, Valid: 38.77%, Test: 39.02%
Epoch: 550, Loss: 1.3959, Train: 40.41%, Valid: 38.85%, Test: 39.11%
Epoch: 575, Loss: 1.4047, Train: 40.63%, Valid: 38.97%, Test: 39.39%
Epoch: 600, Loss: 1.3916, Train: 40.76%, Valid: 39.09%, Test: 39.48%
Epoch: 625, Loss: 1.3894, Train: 40.93%, Valid: 39.19%, Test: 39.69%
Epoch: 650, Loss: 1.3818, Train: 40.93%, Valid: 39.23%, Test: 39.67%
Epoch: 675, Loss: 1.3820, Train: 41.25%, Valid: 39.48%, Test: 39.94%
Epoch: 700, Loss: 1.3927, Train: 41.12%, Valid: 39.30%, Test: 39.74%
Epoch: 725, Loss: 1.3875, Train: 41.32%, Valid: 39.54%, Test: 39.87%
Epoch: 750, Loss: 1.3856, Train: 41.51%, Valid: 39.64%, Test: 40.02%
Epoch: 775, Loss: 1.3721, Train: 41.38%, Valid: 39.44%, Test: 39.92%
Epoch: 800, Loss: 1.3748, Train: 41.87%, Valid: 39.83%, Test: 40.12%
Epoch: 825, Loss: 1.3704, Train: 41.62%, Valid: 39.72%, Test: 40.05%
Epoch: 850, Loss: 1.3682, Train: 41.90%, Valid: 39.81%, Test: 40.35%
Epoch: 875, Loss: 1.3761, Train: 41.91%, Valid: 39.85%, Test: 40.30%
Epoch: 900, Loss: 1.3632, Train: 42.29%, Valid: 40.15%, Test: 40.57%
Epoch: 925, Loss: 1.3772, Train: 41.90%, Valid: 39.83%, Test: 40.17%
Epoch   951: reducing learning rate of group 0 to 9.0000e-04.
Epoch: 950, Loss: 1.3713, Train: 41.84%, Valid: 39.78%, Test: 40.14%
Epoch: 975, Loss: 1.3626, Train: 42.12%, Valid: 39.98%, Test: 40.34%
Run 01:
Highest Train: 42.45
Highest Valid: 40.38
  Final Train: 42.29
   Final Test: 40.49
All runs:
Highest Train: 42.45, nan
Highest Valid: 40.38, nan
  Final Train: 42.29, nan
   Final Test: 40.49, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=0.1, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=5, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.7001, Train: 12.61%, Valid: 12.64%, Test: 12.48%
Epoch: 25, Loss: 1.5494, Train: 28.66%, Valid: 28.47%, Test: 28.81%
Epoch: 50, Loss: 1.5368, Train: 28.62%, Valid: 28.44%, Test: 28.76%
Epoch: 75, Loss: 1.5232, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 100, Loss: 1.5043, Train: 28.87%, Valid: 28.68%, Test: 28.99%
Epoch: 125, Loss: 1.4958, Train: 29.25%, Valid: 28.96%, Test: 29.27%
Epoch: 150, Loss: 1.4903, Train: 29.99%, Valid: 29.71%, Test: 30.04%
Epoch: 175, Loss: 1.4848, Train: 32.35%, Valid: 32.28%, Test: 32.75%
Epoch: 200, Loss: 1.4711, Train: 33.30%, Valid: 32.98%, Test: 33.55%
Epoch: 225, Loss: 1.4685, Train: 34.75%, Valid: 34.55%, Test: 34.87%
Epoch: 250, Loss: 1.4786, Train: 35.21%, Valid: 34.83%, Test: 35.05%
Epoch: 275, Loss: 1.4621, Train: 35.80%, Valid: 35.48%, Test: 35.64%
