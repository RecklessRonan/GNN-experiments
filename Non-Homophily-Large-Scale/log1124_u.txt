nohup: ignoring input
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 2.0794, Train: 84.61%, Valid: 84.43%, Test: 84.71%
Epoch: 25, Loss: 0.3625, Train: 87.56%, Valid: 87.59%, Test: 87.59%
Epoch: 50, Loss: 0.3397, Train: 86.86%, Valid: 86.79%, Test: 86.90%
Epoch: 75, Loss: 0.5323, Train: 85.10%, Valid: 85.09%, Test: 85.22%
Epoch: 100, Loss: 0.3469, Train: 85.66%, Valid: 85.67%, Test: 85.75%
Epoch: 125, Loss: 0.3333, Train: 85.29%, Valid: 85.32%, Test: 85.42%
Epoch: 150, Loss: 0.3283, Train: 85.64%, Valid: 85.66%, Test: 85.74%
Epoch: 175, Loss: 0.3253, Train: 85.81%, Valid: 85.84%, Test: 85.90%
Run 01:
Highest Train: 87.97
Highest Valid: 87.98
  Final Train: 87.97
   Final Test: 88.00
All runs:
Highest Train: 87.97, nan
Highest Valid: 87.98, nan
  Final Train: 87.97, nan
   Final Test: 88.00, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 2.5460, Train: 85.11%, Valid: 85.10%, Test: 85.19%
Epoch: 25, Loss: 0.3822, Train: 85.54%, Valid: 85.40%, Test: 85.60%
Epoch: 50, Loss: 0.3646, Train: 85.91%, Valid: 85.78%, Test: 85.94%
Epoch: 75, Loss: 0.3548, Train: 86.60%, Valid: 86.43%, Test: 86.64%
Epoch: 100, Loss: 0.3435, Train: 85.70%, Valid: 85.52%, Test: 85.75%
Epoch: 125, Loss: 0.3385, Train: 86.53%, Valid: 86.42%, Test: 86.58%
Epoch: 150, Loss: 2.1308, Train: 86.20%, Valid: 86.17%, Test: 86.32%
Epoch: 175, Loss: 1.7567, Train: 85.44%, Valid: 85.24%, Test: 85.53%
Run 01:
Highest Train: 87.58
Highest Valid: 87.46
  Final Train: 87.58
   Final Test: 87.60
All runs:
Highest Train: 87.58, nan
Highest Valid: 87.46, nan
  Final Train: 87.58, nan
   Final Test: 87.60, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 126877.5391, Train: 34.81%, Valid: 35.05%, Test: 35.08%
Epoch: 25, Loss: 1056.6899, Train: 18.25%, Valid: 18.40%, Test: 18.30%
Epoch: 50, Loss: 411.3820, Train: 16.52%, Valid: 16.62%, Test: 16.43%
Epoch: 75, Loss: 176.5313, Train: 16.64%, Valid: 16.68%, Test: 16.56%
Epoch: 100, Loss: 3.0900, Train: 84.07%, Valid: 83.95%, Test: 84.13%
Epoch: 125, Loss: 3.2030, Train: 84.02%, Valid: 83.92%, Test: 84.09%
Epoch: 150, Loss: 2.5441, Train: 84.22%, Valid: 84.11%, Test: 84.29%
Epoch: 175, Loss: 2.1463, Train: 84.35%, Valid: 84.25%, Test: 84.43%
Run 01:
Highest Train: 84.97
Highest Valid: 85.01
  Final Train: 84.97
   Final Test: 85.10
All runs:
Highest Train: 84.97, nan
Highest Valid: 85.01, nan
  Final Train: 84.97, nan
   Final Test: 85.10, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 16.8570, Train: 80.99%, Valid: 81.03%, Test: 81.06%
Epoch: 25, Loss: 0.5579, Train: 84.35%, Valid: 84.16%, Test: 84.47%
Epoch: 50, Loss: 0.3725, Train: 84.08%, Valid: 83.87%, Test: 84.19%
Epoch: 75, Loss: 0.3630, Train: 84.10%, Valid: 83.88%, Test: 84.20%
Epoch: 100, Loss: 0.3539, Train: 85.85%, Valid: 85.63%, Test: 85.91%
Epoch: 125, Loss: 0.3457, Train: 85.38%, Valid: 85.41%, Test: 85.50%
Epoch: 150, Loss: 0.3449, Train: 85.69%, Valid: 85.72%, Test: 85.77%
Epoch: 175, Loss: 0.3373, Train: 85.68%, Valid: 85.71%, Test: 85.76%
Run 01:
Highest Train: 88.06
Highest Valid: 88.12
  Final Train: 88.06
   Final Test: 88.10
All runs:
Highest Train: 88.06, nan
Highest Valid: 88.12, nan
  Final Train: 88.06, nan
   Final Test: 88.10, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 13.9844, Train: 84.78%, Valid: 84.82%, Test: 84.97%
Epoch: 25, Loss: 0.6426, Train: 86.12%, Valid: 86.05%, Test: 86.12%
Epoch: 50, Loss: 0.4145, Train: 86.79%, Valid: 86.74%, Test: 86.92%
Epoch: 75, Loss: 0.3679, Train: 85.53%, Valid: 85.35%, Test: 85.61%
Epoch: 100, Loss: 0.3559, Train: 86.37%, Valid: 86.29%, Test: 86.45%
Epoch: 125, Loss: 0.3481, Train: 85.38%, Valid: 85.19%, Test: 85.47%
Epoch: 150, Loss: 0.3399, Train: 85.65%, Valid: 85.42%, Test: 85.74%
Epoch: 175, Loss: 0.3368, Train: 85.61%, Valid: 85.41%, Test: 85.68%
Run 01:
Highest Train: 87.10
Highest Valid: 86.99
  Final Train: 87.10
   Final Test: 87.25
All runs:
Highest Train: 87.10, nan
Highest Valid: 86.99, nan
  Final Train: 87.10, nan
   Final Test: 87.25, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 72.9199, Train: 73.26%, Valid: 73.09%, Test: 73.10%
Epoch: 25, Loss: 1.4853, Train: 83.42%, Valid: 83.34%, Test: 83.46%
Epoch: 50, Loss: 0.8907, Train: 80.90%, Valid: 80.98%, Test: 81.28%
Epoch: 75, Loss: 0.7465, Train: 85.33%, Valid: 85.26%, Test: 85.35%
Epoch: 100, Loss: 0.7223, Train: 85.09%, Valid: 85.03%, Test: 85.10%
Epoch: 125, Loss: 0.6927, Train: 86.15%, Valid: 86.01%, Test: 86.11%
Epoch: 150, Loss: 0.4384, Train: 84.18%, Valid: 84.07%, Test: 84.30%
Epoch: 175, Loss: 0.6832, Train: 75.87%, Valid: 75.98%, Test: 76.21%
Run 01:
Highest Train: 87.80
Highest Valid: 87.60
  Final Train: 87.77
   Final Test: 87.70
All runs:
Highest Train: 87.80, nan
Highest Valid: 87.60, nan
  Final Train: 87.77, nan
   Final Test: 87.70, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 8.8015, Train: 84.63%, Valid: 84.47%, Test: 84.71%
Epoch: 25, Loss: 0.6036, Train: 84.92%, Valid: 84.88%, Test: 85.07%
Epoch: 50, Loss: 0.4027, Train: 85.24%, Valid: 85.28%, Test: 85.36%
Epoch: 75, Loss: 0.3745, Train: 85.19%, Valid: 85.19%, Test: 85.29%
Epoch: 100, Loss: 0.3663, Train: 85.92%, Valid: 85.80%, Test: 85.98%
Epoch: 125, Loss: 0.3545, Train: 85.80%, Valid: 85.69%, Test: 85.86%
Epoch: 150, Loss: 0.3496, Train: 85.51%, Valid: 85.50%, Test: 85.59%
Epoch: 175, Loss: 0.3466, Train: 85.67%, Valid: 85.65%, Test: 85.75%
Run 01:
Highest Train: 88.12
Highest Valid: 88.17
  Final Train: 88.12
   Final Test: 88.15
All runs:
Highest Train: 88.12, nan
Highest Valid: 88.17, nan
  Final Train: 88.12, nan
   Final Test: 88.15, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 12.6920, Train: 83.72%, Valid: 83.58%, Test: 83.78%
Epoch: 25, Loss: 0.4707, Train: 86.23%, Valid: 86.20%, Test: 86.37%
Epoch: 50, Loss: 0.3791, Train: 86.95%, Valid: 86.88%, Test: 86.98%
Epoch: 75, Loss: 0.3584, Train: 87.20%, Valid: 87.13%, Test: 87.23%
Epoch: 100, Loss: 0.3508, Train: 86.67%, Valid: 86.65%, Test: 86.69%
Epoch: 125, Loss: 0.3476, Train: 88.05%, Valid: 88.02%, Test: 88.07%
Epoch: 150, Loss: 0.3419, Train: 86.68%, Valid: 86.55%, Test: 86.74%
Epoch: 175, Loss: 0.3409, Train: 86.62%, Valid: 86.46%, Test: 86.67%
Run 01:
Highest Train: 88.47
Highest Valid: 88.35
  Final Train: 88.47
   Final Test: 88.40
All runs:
Highest Train: 88.47, nan
Highest Valid: 88.35, nan
  Final Train: 88.47, nan
   Final Test: 88.40, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 618.8034, Train: 66.50%, Valid: 66.42%, Test: 66.16%
Epoch: 25, Loss: 6.1745, Train: 82.96%, Valid: 82.88%, Test: 82.99%
Epoch: 50, Loss: 1.8553, Train: 83.64%, Valid: 83.57%, Test: 83.69%
Epoch: 75, Loss: 1.5908, Train: 83.06%, Valid: 82.99%, Test: 83.10%
Epoch: 100, Loss: 1.7283, Train: 83.31%, Valid: 83.22%, Test: 83.37%
Epoch: 125, Loss: 1.1848, Train: 83.06%, Valid: 82.97%, Test: 83.10%
Epoch: 150, Loss: 1.5088, Train: 83.03%, Valid: 82.95%, Test: 83.07%
Epoch: 175, Loss: 1.4495, Train: 83.15%, Valid: 83.07%, Test: 83.21%
Run 01:
Highest Train: 83.76
Highest Valid: 83.68
  Final Train: 83.76
   Final Test: 83.83
All runs:
Highest Train: 83.76, nan
Highest Valid: 83.68, nan
  Final Train: 83.76, nan
   Final Test: 83.83, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 9.5586, Train: 84.47%, Valid: 84.43%, Test: 84.53%
Epoch: 25, Loss: 0.5347, Train: 86.81%, Valid: 86.85%, Test: 86.91%
Epoch: 50, Loss: 0.3970, Train: 86.70%, Valid: 86.72%, Test: 86.73%
Epoch: 75, Loss: 0.3773, Train: 85.66%, Valid: 85.72%, Test: 85.79%
Epoch: 100, Loss: 0.3622, Train: 85.58%, Valid: 85.58%, Test: 85.66%
Epoch: 125, Loss: 0.3562, Train: 85.36%, Valid: 85.34%, Test: 85.47%
Epoch: 150, Loss: 0.3511, Train: 85.66%, Valid: 85.67%, Test: 85.76%
Epoch: 175, Loss: 0.3489, Train: 85.53%, Valid: 85.53%, Test: 85.64%
Run 01:
Highest Train: 87.92
Highest Valid: 88.03
  Final Train: 87.92
   Final Test: 88.00
All runs:
Highest Train: 87.92, nan
Highest Valid: 88.03, nan
  Final Train: 87.92, nan
   Final Test: 88.00, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 9.0903, Train: 85.21%, Valid: 85.14%, Test: 85.29%
Epoch: 25, Loss: 0.5981, Train: 85.22%, Valid: 85.05%, Test: 85.32%
Epoch: 50, Loss: 0.3871, Train: 86.65%, Valid: 86.56%, Test: 86.63%
Epoch: 75, Loss: 0.3713, Train: 86.40%, Valid: 86.29%, Test: 86.46%
Epoch: 100, Loss: 0.3613, Train: 86.08%, Valid: 85.95%, Test: 86.10%
Epoch: 125, Loss: 0.3569, Train: 85.12%, Valid: 85.08%, Test: 85.23%
Epoch: 150, Loss: 0.3531, Train: 85.23%, Valid: 85.15%, Test: 85.29%
Epoch: 175, Loss: 0.3501, Train: 87.91%, Valid: 87.77%, Test: 87.90%
Run 01:
Highest Train: 87.99
Highest Valid: 87.85
  Final Train: 87.98
   Final Test: 87.98
All runs:
Highest Train: 87.99, nan
Highest Valid: 87.85, nan
  Final Train: 87.98, nan
   Final Test: 87.98, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 288.0507, Train: 62.62%, Valid: 62.90%, Test: 63.20%
Epoch: 25, Loss: 1.9959, Train: 86.54%, Valid: 86.37%, Test: 86.59%
Epoch: 50, Loss: 1.6198, Train: 70.23%, Valid: 70.40%, Test: 70.61%
Epoch: 75, Loss: 1.6581, Train: 66.60%, Valid: 66.78%, Test: 66.94%
Epoch: 100, Loss: 1.5343, Train: 65.74%, Valid: 65.89%, Test: 66.08%
Epoch: 125, Loss: 1.2630, Train: 66.86%, Valid: 67.05%, Test: 67.17%
Epoch: 150, Loss: 1.3447, Train: 68.64%, Valid: 68.83%, Test: 69.01%
Epoch: 175, Loss: 1.1153, Train: 69.65%, Valid: 69.77%, Test: 70.00%
Run 01:
Highest Train: 86.54
Highest Valid: 86.37
  Final Train: 86.54
   Final Test: 86.59
All runs:
Highest Train: 86.54, nan
Highest Valid: 86.37, nan
  Final Train: 86.54, nan
   Final Test: 86.59, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 0.4557, Train: 85.22%, Valid: 85.22%, Test: 85.36%
Epoch: 25, Loss: 0.3606, Train: 85.41%, Valid: 85.40%, Test: 85.54%
Epoch: 50, Loss: 0.3471, Train: 85.34%, Valid: 85.40%, Test: 85.38%
Epoch: 75, Loss: 0.5109, Train: 86.03%, Valid: 85.87%, Test: 86.13%
Epoch: 100, Loss: 0.3735, Train: 85.43%, Valid: 85.37%, Test: 85.59%
Epoch: 125, Loss: 0.3451, Train: 87.81%, Valid: 87.87%, Test: 87.90%
Epoch: 150, Loss: 0.3350, Train: 87.31%, Valid: 87.08%, Test: 87.33%
Epoch: 175, Loss: 0.3287, Train: 87.24%, Valid: 87.02%, Test: 87.28%
Run 01:
Highest Train: 88.46
Highest Valid: 88.49
  Final Train: 88.46
   Final Test: 88.52
All runs:
Highest Train: 88.46, nan
Highest Valid: 88.49, nan
  Final Train: 88.46, nan
   Final Test: 88.52, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 0.7885, Train: 85.75%, Valid: 85.65%, Test: 85.78%
Epoch: 25, Loss: 28.9179, Train: 85.11%, Valid: 84.90%, Test: 85.21%
Epoch: 50, Loss: 37.7348, Train: 15.64%, Valid: 15.71%, Test: 15.67%
Epoch: 75, Loss: 1.4926, Train: 85.67%, Valid: 85.67%, Test: 85.82%
Epoch: 100, Loss: 1.4808, Train: 86.28%, Valid: 86.09%, Test: 86.28%
Epoch: 125, Loss: 1.3120, Train: 86.24%, Valid: 86.08%, Test: 86.25%
Epoch: 150, Loss: 1.1451, Train: 86.21%, Valid: 86.05%, Test: 86.22%
Epoch: 175, Loss: 0.9893, Train: 86.18%, Valid: 86.03%, Test: 86.20%
Run 01:
Highest Train: 87.68
Highest Valid: 87.67
  Final Train: 87.68
   Final Test: 87.69
All runs:
Highest Train: 87.68, nan
Highest Valid: 87.67, nan
  Final Train: 87.68, nan
   Final Test: 87.69, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 28388.6016, Train: 48.77%, Valid: 48.74%, Test: 48.85%
Epoch: 25, Loss: 29759.1934, Train: 46.14%, Valid: 45.71%, Test: 45.50%
Epoch: 50, Loss: 149844.7969, Train: 34.51%, Valid: 34.68%, Test: 34.76%
Epoch: 75, Loss: 12846.6729, Train: 28.35%, Valid: 28.48%, Test: 28.62%
Epoch: 100, Loss: 18529.6465, Train: 28.16%, Valid: 28.29%, Test: 28.42%
Epoch: 125, Loss: 3522.0273, Train: 33.89%, Valid: 33.98%, Test: 34.26%
Epoch: 150, Loss: 1225.1168, Train: 28.88%, Valid: 29.00%, Test: 29.18%
Epoch: 175, Loss: 21785.5801, Train: 24.04%, Valid: 24.19%, Test: 24.31%
Run 01:
Highest Train: 62.49
Highest Valid: 62.39
  Final Train: 62.49
   Final Test: 61.92
All runs:
Highest Train: 62.49, nan
Highest Valid: 62.39, nan
  Final Train: 62.49, nan
   Final Test: 61.92, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 3.3517, Train: 84.98%, Valid: 84.85%, Test: 85.01%
Epoch: 25, Loss: 0.4053, Train: 85.65%, Valid: 85.69%, Test: 85.73%
Epoch: 50, Loss: 0.3620, Train: 85.62%, Valid: 85.63%, Test: 85.73%
Epoch: 75, Loss: 0.3546, Train: 85.41%, Valid: 85.45%, Test: 85.57%
Epoch: 100, Loss: 0.3465, Train: 85.32%, Valid: 85.36%, Test: 85.42%
Epoch: 125, Loss: 0.3384, Train: 85.75%, Valid: 85.77%, Test: 85.87%
Epoch: 150, Loss: 0.3352, Train: 86.01%, Valid: 86.03%, Test: 86.10%
Epoch: 175, Loss: 0.3321, Train: 85.62%, Valid: 85.62%, Test: 85.73%
Run 01:
Highest Train: 87.62
Highest Valid: 87.66
  Final Train: 87.62
   Final Test: 87.63
All runs:
Highest Train: 87.62, nan
Highest Valid: 87.66, nan
  Final Train: 87.62, nan
   Final Test: 87.63, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 2.2291, Train: 85.26%, Valid: 85.07%, Test: 85.34%
Epoch: 25, Loss: 0.4473, Train: 86.67%, Valid: 86.71%, Test: 86.74%
Epoch: 50, Loss: 0.3756, Train: 85.15%, Valid: 85.08%, Test: 85.27%
Epoch: 75, Loss: 0.3584, Train: 86.57%, Valid: 86.44%, Test: 86.62%
Epoch: 100, Loss: 0.3494, Train: 86.51%, Valid: 86.34%, Test: 86.55%
Epoch: 125, Loss: 0.3414, Train: 86.29%, Valid: 86.15%, Test: 86.39%
Epoch: 150, Loss: 0.3372, Train: 86.65%, Valid: 86.64%, Test: 86.76%
Epoch: 175, Loss: 0.3347, Train: 86.86%, Valid: 86.86%, Test: 86.97%
Run 01:
Highest Train: 86.89
Highest Valid: 86.88
  Final Train: 86.88
   Final Test: 86.91
All runs:
Highest Train: 86.89, nan
Highest Valid: 86.88, nan
  Final Train: 86.88, nan
   Final Test: 86.91, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 17.2065, Train: 41.07%, Valid: 41.30%, Test: 41.44%
Epoch: 25, Loss: 18.4888, Train: 73.69%, Valid: 73.51%, Test: 73.47%
Epoch: 50, Loss: 79.8401, Train: 79.06%, Valid: 78.86%, Test: 78.97%
Epoch: 75, Loss: 31.3408, Train: 81.20%, Valid: 81.06%, Test: 81.10%
Epoch: 100, Loss: 2.6203, Train: 84.13%, Valid: 84.02%, Test: 84.16%
Epoch: 125, Loss: 1.4664, Train: 85.03%, Valid: 84.89%, Test: 85.09%
Epoch: 150, Loss: 0.7271, Train: 85.15%, Valid: 84.98%, Test: 85.21%
Epoch: 175, Loss: 0.7342, Train: 85.15%, Valid: 84.98%, Test: 85.21%
Run 01:
Highest Train: 86.24
Highest Valid: 86.16
  Final Train: 86.24
   Final Test: 86.23
All runs:
Highest Train: 86.24, nan
Highest Valid: 86.16, nan
  Final Train: 86.24, nan
   Final Test: 86.23, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 0.4927, Train: 86.45%, Valid: 86.43%, Test: 86.46%
Epoch: 25, Loss: 0.3722, Train: 86.59%, Valid: 86.48%, Test: 86.61%
Epoch: 50, Loss: 0.3627, Train: 85.34%, Valid: 85.32%, Test: 85.49%
Epoch: 75, Loss: 0.3566, Train: 85.72%, Valid: 85.73%, Test: 85.91%
Epoch: 100, Loss: 0.3477, Train: 85.13%, Valid: 85.13%, Test: 85.26%
Epoch: 125, Loss: 0.3446, Train: 84.81%, Valid: 84.84%, Test: 84.96%
Epoch: 150, Loss: 0.3385, Train: 87.28%, Valid: 87.19%, Test: 87.31%
Epoch: 175, Loss: 0.3345, Train: 87.71%, Valid: 87.61%, Test: 87.76%
Run 01:
Highest Train: 87.88
Highest Valid: 87.75
  Final Train: 87.86
   Final Test: 87.90
All runs:
Highest Train: 87.88, nan
Highest Valid: 87.75, nan
  Final Train: 87.86, nan
   Final Test: 87.90, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 1.2240, Train: 85.44%, Valid: 85.27%, Test: 85.50%
Epoch: 25, Loss: 0.3799, Train: 86.18%, Valid: 86.05%, Test: 86.24%
Epoch: 50, Loss: 0.3591, Train: 86.23%, Valid: 86.08%, Test: 86.25%
Epoch: 75, Loss: 0.3478, Train: 86.20%, Valid: 86.16%, Test: 86.25%
Epoch: 100, Loss: 0.3394, Train: 86.53%, Valid: 86.52%, Test: 86.59%
Epoch: 125, Loss: 0.3353, Train: 85.22%, Valid: 85.24%, Test: 85.31%
Epoch: 150, Loss: 0.3328, Train: 85.32%, Valid: 85.29%, Test: 85.42%
Epoch: 175, Loss: 0.3313, Train: 86.17%, Valid: 86.03%, Test: 86.29%
Run 01:
Highest Train: 87.49
Highest Valid: 87.50
  Final Train: 87.49
   Final Test: 87.49
All runs:
Highest Train: 87.49, nan
Highest Valid: 87.50, nan
  Final Train: 87.49, nan
   Final Test: 87.49, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 28.1126, Train: 51.86%, Valid: 52.20%, Test: 52.26%
Epoch: 25, Loss: 12.3778, Train: 81.65%, Valid: 81.54%, Test: 81.59%
Epoch: 50, Loss: 4.2551, Train: 72.16%, Valid: 72.32%, Test: 72.55%
Epoch: 75, Loss: 1.7196, Train: 75.17%, Valid: 75.27%, Test: 75.50%
Epoch: 100, Loss: 1.1491, Train: 82.57%, Valid: 82.60%, Test: 82.81%
Epoch: 125, Loss: 0.9506, Train: 81.48%, Valid: 81.59%, Test: 81.74%
Epoch: 150, Loss: 0.9536, Train: 82.01%, Valid: 82.10%, Test: 82.27%
Epoch: 175, Loss: 0.9457, Train: 82.44%, Valid: 82.55%, Test: 82.70%
Run 01:
Highest Train: 85.41
Highest Valid: 85.26
  Final Train: 85.41
   Final Test: 85.49
All runs:
Highest Train: 85.41, nan
Highest Valid: 85.26, nan
  Final Train: 85.41, nan
   Final Test: 85.49, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 4.5318, Train: 83.92%, Valid: 83.70%, Test: 84.02%
Epoch: 25, Loss: 0.5225, Train: 86.84%, Valid: 86.77%, Test: 86.85%
Epoch: 50, Loss: 0.3835, Train: 85.22%, Valid: 85.17%, Test: 85.32%
Epoch: 75, Loss: 0.3749, Train: 85.38%, Valid: 85.32%, Test: 85.48%
Epoch: 100, Loss: 0.3591, Train: 85.37%, Valid: 85.33%, Test: 85.48%
Epoch: 125, Loss: 0.3560, Train: 85.41%, Valid: 85.39%, Test: 85.53%
Epoch: 150, Loss: 0.3527, Train: 85.35%, Valid: 85.34%, Test: 85.49%
Epoch: 175, Loss: 0.3487, Train: 85.43%, Valid: 85.40%, Test: 85.56%
Run 01:
Highest Train: 88.42
Highest Valid: 88.47
  Final Train: 88.42
   Final Test: 88.47
All runs:
Highest Train: 88.42, nan
Highest Valid: 88.47, nan
  Final Train: 88.42, nan
   Final Test: 88.47, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 3.7196, Train: 84.36%, Valid: 84.48%, Test: 84.53%
Epoch: 25, Loss: 0.4270, Train: 85.65%, Valid: 85.49%, Test: 85.72%
Epoch: 50, Loss: 0.3899, Train: 85.78%, Valid: 85.68%, Test: 85.93%
Epoch: 75, Loss: 0.3707, Train: 84.35%, Valid: 84.33%, Test: 84.49%
Epoch: 100, Loss: 0.3613, Train: 86.19%, Valid: 86.05%, Test: 86.21%
Epoch: 125, Loss: 0.3590, Train: 85.48%, Valid: 85.35%, Test: 85.60%
Epoch: 150, Loss: 0.3500, Train: 84.26%, Valid: 84.36%, Test: 84.45%
Epoch: 175, Loss: 0.3467, Train: 86.43%, Valid: 86.25%, Test: 86.48%
Run 01:
Highest Train: 87.19
Highest Valid: 87.13
  Final Train: 87.15
   Final Test: 87.19
All runs:
Highest Train: 87.19, nan
Highest Valid: 87.13, nan
  Final Train: 87.15, nan
   Final Test: 87.19, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 244.0938, Train: 61.28%, Valid: 61.05%, Test: 61.03%
Epoch: 25, Loss: 21.2941, Train: 83.74%, Valid: 83.66%, Test: 83.81%
Epoch: 50, Loss: 2.0667, Train: 80.53%, Valid: 80.63%, Test: 80.93%
Epoch: 75, Loss: 1.8264, Train: 79.83%, Valid: 79.92%, Test: 80.21%
Epoch: 100, Loss: 1.5301, Train: 81.61%, Valid: 81.69%, Test: 81.90%
Epoch: 125, Loss: 1.2028, Train: 79.89%, Valid: 79.97%, Test: 80.28%
Epoch: 150, Loss: 0.7507, Train: 80.35%, Valid: 80.49%, Test: 80.76%
Epoch: 175, Loss: 0.6722, Train: 88.21%, Valid: 88.02%, Test: 88.11%
Run 01:
Highest Train: 88.21
Highest Valid: 88.02
  Final Train: 88.21
   Final Test: 88.11
All runs:
Highest Train: 88.21, nan
Highest Valid: 88.02, nan
  Final Train: 88.21, nan
   Final Test: 88.11, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 2.6039, Train: 84.05%, Valid: 83.86%, Test: 84.16%
Epoch: 25, Loss: 0.5731, Train: 85.08%, Valid: 85.08%, Test: 85.22%
Epoch: 50, Loss: 0.3614, Train: 85.80%, Valid: 85.82%, Test: 85.91%
Epoch: 75, Loss: 0.3407, Train: 85.78%, Valid: 85.78%, Test: 85.89%
Epoch: 100, Loss: 0.3402, Train: 85.69%, Valid: 85.72%, Test: 85.78%
Epoch: 125, Loss: 0.9914, Train: 85.69%, Valid: 85.76%, Test: 85.82%
Epoch: 150, Loss: 1.0577, Train: 86.96%, Valid: 86.95%, Test: 87.01%
Epoch: 175, Loss: 0.6855, Train: 85.94%, Valid: 85.94%, Test: 85.97%
Run 01:
Highest Train: 87.47
Highest Valid: 87.30
  Final Train: 87.47
   Final Test: 87.48
All runs:
Highest Train: 87.47, nan
Highest Valid: 87.30, nan
  Final Train: 87.47, nan
   Final Test: 87.48, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 0.7051, Train: 80.57%, Valid: 80.67%, Test: 80.82%
Epoch: 25, Loss: 0.6659, Train: 85.40%, Valid: 85.20%, Test: 85.45%
Epoch: 50, Loss: 0.4573, Train: 86.43%, Valid: 86.48%, Test: 86.56%
Epoch: 75, Loss: 0.3670, Train: 85.40%, Valid: 85.21%, Test: 85.47%
Epoch: 100, Loss: 0.3517, Train: 85.77%, Valid: 85.59%, Test: 85.86%
Epoch: 125, Loss: 0.3428, Train: 86.04%, Valid: 85.84%, Test: 86.11%
Epoch: 150, Loss: 0.3377, Train: 87.03%, Valid: 87.02%, Test: 87.14%
Epoch: 175, Loss: 0.3308, Train: 86.98%, Valid: 86.93%, Test: 87.01%
Run 01:
Highest Train: 87.13
Highest Valid: 87.12
  Final Train: 87.13
   Final Test: 87.22
All runs:
Highest Train: 87.13, nan
Highest Valid: 87.12, nan
  Final Train: 87.13, nan
   Final Test: 87.22, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 1278426275119104.0000, Train: 48.73%, Valid: 48.94%, Test: 48.68%
Epoch: 25, Loss: 2121208.5000, Train: 63.70%, Valid: 63.97%, Test: 64.18%
Epoch: 50, Loss: 51283.6172, Train: 49.00%, Valid: 49.28%, Test: 49.34%
Epoch: 75, Loss: 124544264.0000, Train: 66.43%, Valid: 66.19%, Test: 66.07%
Epoch: 100, Loss: 790034.8125, Train: 47.91%, Valid: 47.88%, Test: 47.92%
Epoch: 125, Loss: 128154.1250, Train: 58.41%, Valid: 58.24%, Test: 58.11%
Epoch: 150, Loss: 46705.9609, Train: 48.31%, Valid: 48.33%, Test: 48.26%
Epoch: 175, Loss: 1554460.2500, Train: 76.66%, Valid: 76.48%, Test: 76.57%
Run 01:
Highest Train: 82.90
Highest Valid: 82.84
  Final Train: 82.90
   Final Test: 82.92
All runs:
Highest Train: 82.90, nan
Highest Valid: 82.84, nan
  Final Train: 82.90, nan
   Final Test: 82.92, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 0.5692, Train: 86.78%, Valid: 86.72%, Test: 86.82%
Epoch: 25, Loss: 0.3611, Train: 85.88%, Valid: 85.96%, Test: 85.97%
Epoch: 50, Loss: 0.3486, Train: 85.44%, Valid: 85.49%, Test: 85.56%
Epoch: 75, Loss: 0.3406, Train: 85.09%, Valid: 85.15%, Test: 85.21%
Epoch: 100, Loss: 0.3333, Train: 85.54%, Valid: 85.53%, Test: 85.66%
Epoch: 125, Loss: 0.3310, Train: 85.86%, Valid: 85.86%, Test: 85.94%
Epoch: 150, Loss: 0.3295, Train: 86.14%, Valid: 86.02%, Test: 86.25%
Epoch: 175, Loss: 0.3296, Train: 87.48%, Valid: 87.28%, Test: 87.56%
Run 01:
Highest Train: 87.83
Highest Valid: 87.66
  Final Train: 87.83
   Final Test: 87.90
All runs:
Highest Train: 87.83, nan
Highest Valid: 87.66, nan
  Final Train: 87.83, nan
   Final Test: 87.90, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 1.0726, Train: 84.97%, Valid: 84.79%, Test: 85.06%
Epoch: 25, Loss: 0.3753, Train: 84.34%, Valid: 84.35%, Test: 84.46%
Epoch: 50, Loss: 0.3560, Train: 83.81%, Valid: 83.88%, Test: 84.01%
Epoch: 75, Loss: 0.3428, Train: 85.91%, Valid: 85.78%, Test: 85.96%
Epoch: 100, Loss: 0.3381, Train: 85.79%, Valid: 85.61%, Test: 85.83%
Epoch: 125, Loss: 0.3342, Train: 85.77%, Valid: 85.58%, Test: 85.84%
Epoch: 150, Loss: 0.3326, Train: 86.86%, Valid: 86.79%, Test: 86.98%
Epoch: 175, Loss: 0.3309, Train: 86.95%, Valid: 86.94%, Test: 87.00%
Run 01:
Highest Train: 87.92
Highest Valid: 87.74
  Final Train: 87.92
   Final Test: 87.91
All runs:
Highest Train: 87.92, nan
Highest Valid: 87.74, nan
  Final Train: 87.92, nan
   Final Test: 87.91, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 21.1451, Train: 80.01%, Valid: 79.83%, Test: 79.94%
Epoch: 25, Loss: 7.9374, Train: 81.31%, Valid: 81.18%, Test: 81.24%
Epoch: 50, Loss: 5.3772, Train: 83.79%, Valid: 83.71%, Test: 83.83%
Epoch: 75, Loss: 0.9246, Train: 87.55%, Valid: 87.27%, Test: 87.52%
Epoch: 100, Loss: 0.7376, Train: 85.30%, Valid: 85.16%, Test: 85.39%
Epoch: 125, Loss: 0.6387, Train: 85.22%, Valid: 85.08%, Test: 85.30%
Epoch: 150, Loss: 0.5794, Train: 85.43%, Valid: 85.27%, Test: 85.51%
Epoch: 175, Loss: 0.5795, Train: 87.75%, Valid: 87.54%, Test: 87.72%
Run 01:
Highest Train: 87.79
Highest Valid: 87.55
  Final Train: 87.76
   Final Test: 87.73
All runs:
Highest Train: 87.79, nan
Highest Valid: 87.55, nan
  Final Train: 87.76, nan
   Final Test: 87.73, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 2.4584, Train: 86.43%, Valid: 86.41%, Test: 86.55%
Epoch: 25, Loss: 0.3963, Train: 86.16%, Valid: 86.24%, Test: 86.25%
Epoch: 50, Loss: 0.3701, Train: 85.27%, Valid: 85.24%, Test: 85.37%
Epoch: 75, Loss: 0.3613, Train: 85.51%, Valid: 85.54%, Test: 85.59%
Epoch: 100, Loss: 0.3565, Train: 85.25%, Valid: 85.28%, Test: 85.41%
Epoch: 125, Loss: 0.3508, Train: 85.29%, Valid: 85.35%, Test: 85.46%
Epoch: 150, Loss: 0.3452, Train: 85.43%, Valid: 85.47%, Test: 85.60%
Epoch: 175, Loss: 0.3435, Train: 85.82%, Valid: 85.85%, Test: 86.00%
Run 01:
Highest Train: 86.68
Highest Valid: 86.68
  Final Train: 86.68
   Final Test: 86.75
All runs:
Highest Train: 86.68, nan
Highest Valid: 86.68, nan
  Final Train: 86.68, nan
   Final Test: 86.75, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 6.1186, Train: 82.45%, Valid: 82.55%, Test: 82.67%
Epoch: 25, Loss: 0.4308, Train: 85.24%, Valid: 85.07%, Test: 85.32%
Epoch: 50, Loss: 0.3763, Train: 85.40%, Valid: 85.46%, Test: 85.48%
Epoch: 75, Loss: 0.3638, Train: 85.35%, Valid: 85.39%, Test: 85.45%
Epoch: 100, Loss: 0.3588, Train: 86.88%, Valid: 86.90%, Test: 86.92%
Epoch: 125, Loss: 0.3512, Train: 85.35%, Valid: 85.34%, Test: 85.42%
Epoch: 150, Loss: 0.3462, Train: 86.43%, Valid: 86.41%, Test: 86.50%
Epoch: 175, Loss: 0.3419, Train: 86.82%, Valid: 86.77%, Test: 86.85%
Run 01:
Highest Train: 87.50
Highest Valid: 87.50
  Final Train: 87.47
   Final Test: 87.50
All runs:
Highest Train: 87.50, nan
Highest Valid: 87.50, nan
  Final Train: 87.47, nan
   Final Test: 87.50, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 101.9401, Train: 49.83%, Valid: 49.95%, Test: 49.82%
Epoch: 25, Loss: 29.9282, Train: 82.36%, Valid: 82.28%, Test: 82.35%
Epoch: 50, Loss: 4.0811, Train: 75.97%, Valid: 76.08%, Test: 76.32%
Epoch: 75, Loss: 1.9528, Train: 85.00%, Valid: 84.85%, Test: 85.10%
Epoch: 100, Loss: 1.0393, Train: 82.55%, Valid: 82.55%, Test: 82.82%
Epoch: 125, Loss: 0.8036, Train: 81.05%, Valid: 81.24%, Test: 81.46%
Epoch: 150, Loss: 0.5735, Train: 85.92%, Valid: 85.76%, Test: 85.94%
Epoch: 175, Loss: 2.4358, Train: 47.69%, Valid: 47.67%, Test: 47.65%
Run 01:
Highest Train: 88.20
Highest Valid: 87.95
  Final Train: 88.20
   Final Test: 88.09
All runs:
Highest Train: 88.20, nan
Highest Valid: 87.95, nan
  Final Train: 88.20, nan
   Final Test: 88.09, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 10.3626, Train: 85.19%, Valid: 85.13%, Test: 85.34%
Epoch: 25, Loss: 0.6122, Train: 83.98%, Valid: 83.80%, Test: 84.13%
Epoch: 50, Loss: 0.4167, Train: 85.33%, Valid: 85.25%, Test: 85.39%
Epoch: 75, Loss: 0.3891, Train: 85.44%, Valid: 85.43%, Test: 85.56%
Epoch: 100, Loss: 0.3723, Train: 85.49%, Valid: 85.44%, Test: 85.63%
Epoch: 125, Loss: 0.3596, Train: 85.68%, Valid: 85.59%, Test: 85.81%
Epoch: 150, Loss: 0.3525, Train: 85.60%, Valid: 85.57%, Test: 85.74%
Epoch: 175, Loss: 0.3485, Train: 85.65%, Valid: 85.60%, Test: 85.79%
Run 01:
Highest Train: 86.10
Highest Valid: 85.99
  Final Train: 86.10
   Final Test: 86.16
All runs:
Highest Train: 86.10, nan
Highest Valid: 85.99, nan
  Final Train: 86.10, nan
   Final Test: 86.16, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 5.0137, Train: 85.25%, Valid: 85.17%, Test: 85.29%
Epoch: 25, Loss: 0.5889, Train: 85.98%, Valid: 85.84%, Test: 86.05%
Epoch: 50, Loss: 0.3861, Train: 85.46%, Valid: 85.24%, Test: 85.52%
Epoch: 75, Loss: 0.3731, Train: 86.32%, Valid: 86.32%, Test: 86.34%
Epoch: 100, Loss: 0.3642, Train: 86.46%, Valid: 86.31%, Test: 86.48%
Epoch: 125, Loss: 0.3600, Train: 86.42%, Valid: 86.34%, Test: 86.47%
Epoch: 150, Loss: 0.3566, Train: 86.32%, Valid: 86.16%, Test: 86.36%
Epoch: 175, Loss: 0.3536, Train: 86.36%, Valid: 86.18%, Test: 86.38%
Run 01:
Highest Train: 86.81
Highest Valid: 86.81
  Final Train: 86.81
   Final Test: 86.88
All runs:
Highest Train: 86.81, nan
Highest Valid: 86.81, nan
  Final Train: 86.81, nan
   Final Test: 86.88, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 256.9613, Train: 78.61%, Valid: 78.49%, Test: 78.37%
Epoch: 25, Loss: 8.0831, Train: 81.30%, Valid: 81.17%, Test: 81.33%
Epoch: 50, Loss: 8.9315, Train: 60.29%, Valid: 60.50%, Test: 60.81%
Epoch: 75, Loss: 7.7437, Train: 83.08%, Valid: 83.03%, Test: 83.17%
Epoch: 100, Loss: 7.8103, Train: 79.60%, Valid: 79.48%, Test: 79.76%
Epoch: 125, Loss: 7.2032, Train: 76.12%, Valid: 76.03%, Test: 76.29%
Epoch: 150, Loss: 30.1907, Train: 76.27%, Valid: 76.13%, Test: 76.59%
Epoch: 175, Loss: 13.3037, Train: 65.93%, Valid: 65.84%, Test: 66.02%
Run 01:
Highest Train: 83.99
Highest Valid: 83.88
  Final Train: 83.99
   Final Test: 84.04
All runs:
Highest Train: 83.99, nan
Highest Valid: 83.88, nan
  Final Train: 83.99, nan
   Final Test: 84.04, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.9002, Train: 87.40%, Valid: 87.45%, Test: 87.47%
Epoch: 25, Loss: 0.9306, Train: 87.25%, Valid: 87.31%, Test: 87.40%
Epoch: 50, Loss: 0.9232, Train: 85.22%, Valid: 85.27%, Test: 85.31%
Epoch: 75, Loss: 0.3553, Train: 85.72%, Valid: 85.71%, Test: 85.83%
Epoch: 100, Loss: 0.3370, Train: 85.60%, Valid: 85.62%, Test: 85.71%
Epoch: 125, Loss: 0.3485, Train: 85.41%, Valid: 85.36%, Test: 85.53%
Epoch: 150, Loss: 0.3371, Train: 86.11%, Valid: 86.04%, Test: 86.15%
Epoch: 175, Loss: 0.3316, Train: 85.03%, Valid: 85.02%, Test: 85.16%
Run 01:
Highest Train: 87.68
Highest Valid: 87.78
  Final Train: 87.68
   Final Test: 87.76
All runs:
Highest Train: 87.68, nan
Highest Valid: 87.78, nan
  Final Train: 87.68, nan
   Final Test: 87.76, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 7.5862, Train: 85.55%, Valid: 85.42%, Test: 85.62%
Epoch: 25, Loss: 0.6123, Train: 85.57%, Valid: 85.40%, Test: 85.58%
Epoch: 50, Loss: 3.1644, Train: 85.70%, Valid: 85.52%, Test: 85.69%
Epoch: 75, Loss: 3.2787, Train: 85.41%, Valid: 85.24%, Test: 85.49%
Epoch: 100, Loss: 8.3907, Train: 84.95%, Valid: 84.73%, Test: 85.04%
Epoch: 125, Loss: 136.0856, Train: 86.48%, Valid: 86.24%, Test: 86.51%
Epoch: 150, Loss: 10.7006, Train: 86.05%, Valid: 85.91%, Test: 86.09%
Epoch: 175, Loss: 10.9546, Train: 85.99%, Valid: 85.89%, Test: 86.04%
Run 01:
Highest Train: 87.20
Highest Valid: 86.97
  Final Train: 87.20
   Final Test: 87.19
All runs:
Highest Train: 87.20, nan
Highest Valid: 86.97, nan
  Final Train: 87.20, nan
   Final Test: 87.19, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 12048.5137, Train: 62.12%, Valid: 61.95%, Test: 61.87%
Epoch: 25, Loss: 51.0298, Train: 41.85%, Valid: 41.59%, Test: 41.31%
Epoch: 50, Loss: 28.4883, Train: 48.01%, Valid: 47.87%, Test: 47.55%
Epoch: 75, Loss: 25.7588, Train: 81.81%, Valid: 81.73%, Test: 81.77%
Epoch: 100, Loss: 23.2181, Train: 82.57%, Valid: 82.52%, Test: 82.57%
Epoch: 125, Loss: 13.0558, Train: 83.13%, Valid: 83.07%, Test: 83.14%
Epoch: 150, Loss: 6.2726, Train: 83.31%, Valid: 83.24%, Test: 83.34%
Epoch: 175, Loss: 3.4536, Train: 83.49%, Valid: 83.42%, Test: 83.52%
Run 01:
Highest Train: 83.67
Highest Valid: 83.59
  Final Train: 83.67
   Final Test: 83.71
All runs:
Highest Train: 83.67, nan
Highest Valid: 83.59, nan
  Final Train: 83.67, nan
   Final Test: 83.71, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 13.9665, Train: 84.78%, Valid: 84.78%, Test: 84.91%
Epoch: 25, Loss: 0.5324, Train: 85.10%, Valid: 84.97%, Test: 85.22%
Epoch: 50, Loss: 0.3763, Train: 85.44%, Valid: 85.42%, Test: 85.50%
Epoch: 75, Loss: 0.3624, Train: 87.31%, Valid: 87.31%, Test: 87.36%
Epoch: 100, Loss: 0.3586, Train: 84.84%, Valid: 84.77%, Test: 84.96%
Epoch: 125, Loss: 0.3464, Train: 85.08%, Valid: 85.05%, Test: 85.18%
Epoch: 150, Loss: 0.3406, Train: 87.13%, Valid: 86.93%, Test: 87.17%
Epoch: 175, Loss: 0.3459, Train: 87.65%, Valid: 87.44%, Test: 87.67%
Run 01:
Highest Train: 87.66
Highest Valid: 87.48
  Final Train: 87.66
   Final Test: 87.70
All runs:
Highest Train: 87.66, nan
Highest Valid: 87.48, nan
  Final Train: 87.66, nan
   Final Test: 87.70, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 9.8031, Train: 85.24%, Valid: 85.06%, Test: 85.29%
Epoch: 25, Loss: 0.6853, Train: 85.28%, Valid: 85.11%, Test: 85.34%
Epoch: 50, Loss: 0.4559, Train: 85.95%, Valid: 85.75%, Test: 85.99%
Epoch: 75, Loss: 0.4113, Train: 86.18%, Valid: 85.99%, Test: 86.24%
Epoch: 100, Loss: 0.3600, Train: 85.62%, Valid: 85.40%, Test: 85.69%
Epoch: 125, Loss: 0.3420, Train: 86.75%, Valid: 86.68%, Test: 86.83%
Epoch: 150, Loss: 0.3389, Train: 85.76%, Valid: 85.56%, Test: 85.85%
Epoch: 175, Loss: 0.3385, Train: 85.80%, Valid: 85.62%, Test: 85.88%
Run 01:
Highest Train: 87.12
Highest Valid: 87.01
  Final Train: 87.12
   Final Test: 87.18
All runs:
Highest Train: 87.12, nan
Highest Valid: 87.01, nan
  Final Train: 87.12, nan
   Final Test: 87.18, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1864.4590, Train: 37.03%, Valid: 36.77%, Test: 36.75%
Epoch: 25, Loss: 9.5055, Train: 84.00%, Valid: 83.89%, Test: 84.03%
Epoch: 50, Loss: 7.7870, Train: 76.06%, Valid: 76.17%, Test: 76.40%
Epoch: 75, Loss: 3.7766, Train: 74.23%, Valid: 74.30%, Test: 74.61%
Epoch: 100, Loss: 5.5634, Train: 72.31%, Valid: 72.47%, Test: 72.74%
Epoch: 125, Loss: 2.5831, Train: 85.32%, Valid: 85.18%, Test: 85.40%
Epoch: 150, Loss: 2.3547, Train: 85.43%, Valid: 85.28%, Test: 85.51%
Epoch: 175, Loss: 2.0645, Train: 85.34%, Valid: 85.19%, Test: 85.41%
Run 01:
Highest Train: 85.80
Highest Valid: 85.76
  Final Train: 85.80
   Final Test: 85.94
All runs:
Highest Train: 85.80, nan
Highest Valid: 85.76, nan
  Final Train: 85.80, nan
   Final Test: 85.94, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 3.2780, Train: 84.35%, Valid: 84.18%, Test: 84.44%
Epoch: 25, Loss: 0.4650, Train: 85.00%, Valid: 85.06%, Test: 85.13%
Epoch: 50, Loss: 0.3760, Train: 85.92%, Valid: 85.84%, Test: 85.98%
Epoch: 75, Loss: 0.3569, Train: 85.82%, Valid: 85.67%, Test: 85.85%
Epoch: 100, Loss: 0.3451, Train: 85.34%, Valid: 85.31%, Test: 85.46%
Epoch: 125, Loss: 0.3401, Train: 85.65%, Valid: 85.61%, Test: 85.71%
Epoch: 150, Loss: 0.3348, Train: 85.82%, Valid: 85.83%, Test: 85.89%
Epoch: 175, Loss: 0.3387, Train: 85.83%, Valid: 85.84%, Test: 85.92%
Run 01:
Highest Train: 87.04
Highest Valid: 86.98
  Final Train: 87.04
   Final Test: 87.09
All runs:
Highest Train: 87.04, nan
Highest Valid: 86.98, nan
  Final Train: 87.04, nan
   Final Test: 87.09, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 3.8319, Train: 85.86%, Valid: 85.75%, Test: 85.89%
Epoch: 25, Loss: 0.5662, Train: 86.35%, Valid: 86.34%, Test: 86.43%
Epoch: 50, Loss: 0.3694, Train: 85.56%, Valid: 85.36%, Test: 85.61%
Epoch: 75, Loss: 0.3494, Train: 87.48%, Valid: 87.52%, Test: 87.52%
Epoch: 100, Loss: 0.3454, Train: 86.89%, Valid: 86.84%, Test: 86.91%
Epoch: 125, Loss: 0.3394, Train: 86.77%, Valid: 86.75%, Test: 86.87%
Epoch: 150, Loss: 0.3366, Train: 87.16%, Valid: 87.10%, Test: 87.24%
Epoch: 175, Loss: 0.3333, Train: 85.77%, Valid: 85.57%, Test: 85.86%
Run 01:
Highest Train: 87.70
Highest Valid: 87.67
  Final Train: 87.70
   Final Test: 87.78
All runs:
Highest Train: 87.70, nan
Highest Valid: 87.67, nan
  Final Train: 87.70, nan
   Final Test: 87.78, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 184.5338, Train: 79.63%, Valid: 79.50%, Test: 79.53%
Epoch: 25, Loss: 3.2158, Train: 85.25%, Valid: 85.11%, Test: 85.32%
Epoch: 50, Loss: 2.9728, Train: 84.80%, Valid: 84.67%, Test: 84.86%
Epoch: 75, Loss: 2.6086, Train: 84.64%, Valid: 84.52%, Test: 84.70%
Epoch: 100, Loss: 2.0582, Train: 84.53%, Valid: 84.41%, Test: 84.57%
Epoch: 125, Loss: 1.7559, Train: 76.89%, Valid: 76.97%, Test: 77.17%
Epoch: 150, Loss: 1.4812, Train: 84.34%, Valid: 84.23%, Test: 84.38%
Epoch: 175, Loss: 1.0734, Train: 79.93%, Valid: 79.91%, Test: 80.14%
Run 01:
Highest Train: 86.58
Highest Valid: 86.45
  Final Train: 86.58
   Final Test: 86.64
All runs:
Highest Train: 86.58, nan
Highest Valid: 86.45, nan
  Final Train: 86.58, nan
   Final Test: 86.64, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 10.4278, Train: 87.23%, Valid: 87.17%, Test: 87.24%
Epoch: 25, Loss: 0.6802, Train: 85.86%, Valid: 85.74%, Test: 85.92%
Epoch: 50, Loss: 0.4446, Train: 85.82%, Valid: 85.85%, Test: 85.97%
Epoch: 75, Loss: 0.3713, Train: 86.58%, Valid: 86.50%, Test: 86.64%
Epoch: 100, Loss: 0.3615, Train: 87.40%, Valid: 87.23%, Test: 87.42%
Epoch: 125, Loss: 0.3549, Train: 87.68%, Valid: 87.53%, Test: 87.72%
Epoch: 150, Loss: 0.3487, Train: 87.67%, Valid: 87.48%, Test: 87.68%
Epoch: 175, Loss: 0.3432, Train: 87.90%, Valid: 87.72%, Test: 87.91%
Run 01:
Highest Train: 87.91
Highest Valid: 87.72
  Final Train: 87.90
   Final Test: 87.91
All runs:
Highest Train: 87.91, nan
Highest Valid: 87.72, nan
  Final Train: 87.90, nan
   Final Test: 87.91, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 2.7324, Train: 85.87%, Valid: 85.85%, Test: 86.06%
Epoch: 25, Loss: 0.4252, Train: 85.73%, Valid: 85.49%, Test: 85.78%
Epoch: 50, Loss: 0.3724, Train: 85.67%, Valid: 85.52%, Test: 85.76%
Epoch: 75, Loss: 0.3602, Train: 85.74%, Valid: 85.57%, Test: 85.81%
Epoch: 100, Loss: 0.3510, Train: 86.29%, Valid: 86.15%, Test: 86.37%
Epoch: 125, Loss: 0.3454, Train: 86.70%, Valid: 86.66%, Test: 86.76%
Epoch: 150, Loss: 0.3409, Train: 86.68%, Valid: 86.59%, Test: 86.74%
Epoch: 175, Loss: 0.3382, Train: 87.14%, Valid: 87.15%, Test: 87.22%
Run 01:
Highest Train: 87.44
Highest Valid: 87.40
  Final Train: 87.44
   Final Test: 87.53
All runs:
Highest Train: 87.44, nan
Highest Valid: 87.40, nan
  Final Train: 87.44, nan
   Final Test: 87.53, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 219.9061, Train: 47.03%, Valid: 47.22%, Test: 47.45%
Epoch: 25, Loss: 2.0786, Train: 83.74%, Valid: 83.65%, Test: 83.79%
Epoch: 50, Loss: 2.2271, Train: 77.83%, Valid: 77.99%, Test: 78.24%
Epoch: 75, Loss: 1.9436, Train: 84.66%, Valid: 84.54%, Test: 84.72%
Epoch: 100, Loss: 1.9174, Train: 84.66%, Valid: 84.54%, Test: 84.70%
Epoch: 125, Loss: 1.8546, Train: 84.80%, Valid: 84.67%, Test: 84.86%
Epoch: 150, Loss: 1.5399, Train: 84.63%, Valid: 84.51%, Test: 84.67%
Epoch: 175, Loss: 1.1270, Train: 84.59%, Valid: 84.49%, Test: 84.66%
Run 01:
Highest Train: 86.67
Highest Valid: 86.48
  Final Train: 86.67
   Final Test: 86.58
All runs:
Highest Train: 86.67, nan
Highest Valid: 86.48, nan
  Final Train: 86.67, nan
   Final Test: 86.58, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 11.5845, Train: 88.43%, Valid: 88.48%, Test: 88.53%
Epoch: 25, Loss: 10.8557, Train: 84.12%, Valid: 83.96%, Test: 84.27%
Epoch: 50, Loss: 62.7246, Train: 85.48%, Valid: 85.27%, Test: 85.58%
Epoch: 75, Loss: 5.0260, Train: 84.60%, Valid: 84.40%, Test: 84.70%
Epoch: 100, Loss: 2.9222, Train: 84.52%, Valid: 84.32%, Test: 84.62%
Epoch: 125, Loss: 2.9256, Train: 84.52%, Valid: 84.32%, Test: 84.62%
Epoch: 150, Loss: 2.9223, Train: 84.53%, Valid: 84.33%, Test: 84.62%
Epoch: 175, Loss: 2.9192, Train: 84.53%, Valid: 84.33%, Test: 84.63%
Run 01:
Highest Train: 88.43
Highest Valid: 88.48
  Final Train: 88.43
   Final Test: 88.53
All runs:
Highest Train: 88.43, nan
Highest Valid: 88.48, nan
  Final Train: 88.43, nan
   Final Test: 88.53, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 8.0216, Train: 86.38%, Valid: 86.26%, Test: 86.38%
Epoch: 25, Loss: 1.1314, Train: 85.59%, Valid: 85.41%, Test: 85.64%
Epoch: 50, Loss: 0.7201, Train: 85.57%, Valid: 85.39%, Test: 85.63%
Epoch: 75, Loss: 0.3709, Train: 87.37%, Valid: 87.39%, Test: 87.44%
Epoch: 100, Loss: 0.3430, Train: 86.43%, Valid: 86.37%, Test: 86.49%
Epoch: 125, Loss: 0.3351, Train: 87.24%, Valid: 87.19%, Test: 87.27%
Epoch: 150, Loss: 0.3263, Train: 87.69%, Valid: 87.70%, Test: 87.74%
Epoch: 175, Loss: 0.3371, Train: 87.37%, Valid: 87.29%, Test: 87.40%
Run 01:
Highest Train: 87.73
Highest Valid: 87.76
  Final Train: 87.73
   Final Test: 87.79
All runs:
Highest Train: 87.73, nan
Highest Valid: 87.76, nan
  Final Train: 87.73, nan
   Final Test: 87.79, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 523778.6562, Train: 43.03%, Valid: 43.20%, Test: 43.36%
Epoch: 25, Loss: 208.9612, Train: 31.42%, Valid: 31.37%, Test: 31.22%
Epoch: 50, Loss: 564.0703, Train: 26.97%, Valid: 27.05%, Test: 27.08%
Epoch: 75, Loss: 214.9365, Train: 25.12%, Valid: 25.15%, Test: 25.17%
Epoch: 100, Loss: 183.6555, Train: 21.18%, Valid: 21.18%, Test: 21.04%
Epoch: 125, Loss: 175.4659, Train: 22.84%, Valid: 22.80%, Test: 22.66%
Epoch: 150, Loss: 171.3434, Train: 24.84%, Valid: 24.84%, Test: 24.59%
Epoch: 175, Loss: 167.6603, Train: 26.08%, Valid: 25.95%, Test: 25.75%
Run 01:
Highest Train: 65.82
Highest Valid: 65.66
  Final Train: 65.82
   Final Test: 65.51
All runs:
Highest Train: 65.82, nan
Highest Valid: 65.66, nan
  Final Train: 65.82, nan
   Final Test: 65.51, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 8.3102, Train: 86.07%, Valid: 85.97%, Test: 86.12%
Epoch: 25, Loss: 0.4512, Train: 85.55%, Valid: 85.44%, Test: 85.59%
Epoch: 50, Loss: 0.3689, Train: 87.14%, Valid: 87.15%, Test: 87.19%
Epoch: 75, Loss: 0.3525, Train: 85.80%, Valid: 85.81%, Test: 85.91%
Epoch: 100, Loss: 0.3420, Train: 87.74%, Valid: 87.70%, Test: 87.83%
Epoch: 125, Loss: 0.3349, Train: 85.72%, Valid: 85.67%, Test: 85.84%
Epoch: 150, Loss: 0.3334, Train: 85.90%, Valid: 85.93%, Test: 86.03%
Epoch: 175, Loss: 0.3325, Train: 86.05%, Valid: 86.03%, Test: 86.16%
Run 01:
Highest Train: 87.74
Highest Valid: 87.70
  Final Train: 87.74
   Final Test: 87.83
All runs:
Highest Train: 87.74, nan
Highest Valid: 87.70, nan
  Final Train: 87.74, nan
   Final Test: 87.83, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 2.4826, Train: 84.29%, Valid: 84.29%, Test: 84.42%
Epoch: 25, Loss: 0.6075, Train: 86.12%, Valid: 86.12%, Test: 86.19%
Epoch: 50, Loss: 0.3676, Train: 86.11%, Valid: 86.15%, Test: 86.17%
Epoch: 75, Loss: 0.3491, Train: 86.61%, Valid: 86.45%, Test: 86.65%
Epoch: 100, Loss: 0.3416, Train: 86.05%, Valid: 85.87%, Test: 86.11%
Epoch: 125, Loss: 0.3342, Train: 86.30%, Valid: 86.14%, Test: 86.36%
Epoch: 150, Loss: 0.3305, Train: 86.48%, Valid: 86.31%, Test: 86.55%
Epoch: 175, Loss: 0.3368, Train: 86.41%, Valid: 86.24%, Test: 86.49%
Run 01:
Highest Train: 87.49
Highest Valid: 87.44
  Final Train: 87.49
   Final Test: 87.51
All runs:
Highest Train: 87.49, nan
Highest Valid: 87.44, nan
  Final Train: 87.49, nan
   Final Test: 87.51, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 17.5890, Train: 56.12%, Valid: 56.48%, Test: 56.61%
Epoch: 25, Loss: 22.0808, Train: 83.32%, Valid: 83.23%, Test: 83.38%
Epoch: 50, Loss: 2.2486, Train: 85.30%, Valid: 85.13%, Test: 85.38%
Epoch: 75, Loss: 1.4563, Train: 82.07%, Valid: 82.17%, Test: 82.35%
Epoch: 100, Loss: 1.2401, Train: 84.01%, Valid: 84.07%, Test: 84.15%
Epoch: 125, Loss: 1.0392, Train: 83.57%, Valid: 83.63%, Test: 83.77%
Epoch: 150, Loss: 0.9991, Train: 83.26%, Valid: 83.21%, Test: 83.46%
Epoch: 175, Loss: 0.8592, Train: 81.82%, Valid: 81.84%, Test: 82.10%
Run 01:
Highest Train: 85.46
Highest Valid: 85.31
  Final Train: 85.46
   Final Test: 85.56
All runs:
Highest Train: 85.46, nan
Highest Valid: 85.31, nan
  Final Train: 85.46, nan
   Final Test: 85.56, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 9.0031, Train: 86.58%, Valid: 86.55%, Test: 86.73%
Epoch: 25, Loss: 0.6850, Train: 85.79%, Valid: 85.69%, Test: 85.82%
Epoch: 50, Loss: 0.4219, Train: 85.56%, Valid: 85.61%, Test: 85.66%
Epoch: 75, Loss: 0.3666, Train: 85.76%, Valid: 85.77%, Test: 85.85%
Epoch: 100, Loss: 0.3558, Train: 87.63%, Valid: 87.46%, Test: 87.66%
Epoch: 125, Loss: 0.3502, Train: 86.83%, Valid: 86.67%, Test: 86.94%
Epoch: 150, Loss: 0.3454, Train: 85.83%, Valid: 85.82%, Test: 85.91%
Epoch: 175, Loss: 0.3441, Train: 85.82%, Valid: 85.82%, Test: 85.92%
Run 01:
Highest Train: 88.15
Highest Valid: 88.24
  Final Train: 88.15
   Final Test: 88.17
All runs:
Highest Train: 88.15, nan
Highest Valid: 88.24, nan
  Final Train: 88.15, nan
   Final Test: 88.17, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 0.8145, Train: 85.66%, Valid: 85.47%, Test: 85.72%
Epoch: 25, Loss: 0.3787, Train: 85.42%, Valid: 85.25%, Test: 85.48%
Epoch: 50, Loss: 0.3583, Train: 86.31%, Valid: 86.18%, Test: 86.36%
Epoch: 75, Loss: 0.3468, Train: 85.41%, Valid: 85.24%, Test: 85.47%
Epoch: 100, Loss: 0.3376, Train: 87.19%, Valid: 87.20%, Test: 87.26%
Epoch: 125, Loss: 0.3344, Train: 87.21%, Valid: 87.21%, Test: 87.26%
Epoch: 150, Loss: 0.3305, Train: 87.22%, Valid: 87.21%, Test: 87.27%
Epoch: 175, Loss: 0.3306, Train: 87.29%, Valid: 87.29%, Test: 87.35%
Run 01:
Highest Train: 87.56
Highest Valid: 87.60
  Final Train: 87.56
   Final Test: 87.54
All runs:
Highest Train: 87.56, nan
Highest Valid: 87.60, nan
  Final Train: 87.56, nan
   Final Test: 87.54, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 224.2018, Train: 45.39%, Valid: 45.37%, Test: 45.39%
Epoch: 25, Loss: 8.7657, Train: 84.27%, Valid: 84.15%, Test: 84.32%
Epoch: 50, Loss: 5.3296, Train: 81.72%, Valid: 81.83%, Test: 82.01%
Epoch: 75, Loss: 1.5189, Train: 83.57%, Valid: 83.69%, Test: 83.81%
Epoch: 100, Loss: 1.3039, Train: 82.89%, Valid: 82.99%, Test: 83.15%
Epoch: 125, Loss: 1.0681, Train: 83.04%, Valid: 83.16%, Test: 83.29%
Epoch: 150, Loss: 0.9471, Train: 84.46%, Valid: 84.48%, Test: 84.61%
Epoch: 175, Loss: 0.8929, Train: 84.94%, Valid: 84.95%, Test: 85.03%
Run 01:
Highest Train: 86.87
Highest Valid: 86.83
  Final Train: 86.87
   Final Test: 86.89
All runs:
Highest Train: 86.87, nan
Highest Valid: 86.83, nan
  Final Train: 86.87, nan
   Final Test: 86.89, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 7.5655, Train: 84.95%, Valid: 84.81%, Test: 85.04%
Epoch: 25, Loss: 0.5254, Train: 86.43%, Valid: 86.48%, Test: 86.56%
Epoch: 50, Loss: 0.3825, Train: 86.26%, Valid: 86.14%, Test: 86.26%
Epoch: 75, Loss: 0.3641, Train: 85.42%, Valid: 85.39%, Test: 85.44%
Epoch: 100, Loss: 0.3482, Train: 86.57%, Valid: 86.58%, Test: 86.68%
Epoch: 125, Loss: 0.3459, Train: 87.03%, Valid: 86.82%, Test: 87.10%
Epoch: 150, Loss: 0.3395, Train: 87.32%, Valid: 87.17%, Test: 87.37%
Epoch: 175, Loss: 0.3383, Train: 87.66%, Valid: 87.45%, Test: 87.71%
Run 01:
Highest Train: 88.10
Highest Valid: 88.21
  Final Train: 88.10
   Final Test: 88.21
All runs:
Highest Train: 88.10, nan
Highest Valid: 88.21, nan
  Final Train: 88.10, nan
   Final Test: 88.21, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 7.6495, Train: 86.63%, Valid: 86.63%, Test: 86.68%
Epoch: 25, Loss: 0.6109, Train: 86.08%, Valid: 85.91%, Test: 86.08%
Epoch: 50, Loss: 0.3829, Train: 86.62%, Valid: 86.64%, Test: 86.80%
Epoch: 75, Loss: 0.3657, Train: 86.58%, Valid: 86.43%, Test: 86.65%
Epoch: 100, Loss: 0.3524, Train: 86.42%, Valid: 86.43%, Test: 86.50%
Epoch: 125, Loss: 0.3446, Train: 86.60%, Valid: 86.57%, Test: 86.67%
Epoch: 150, Loss: 0.3405, Train: 86.85%, Valid: 86.81%, Test: 86.85%
Epoch: 175, Loss: 0.3381, Train: 85.90%, Valid: 85.84%, Test: 85.94%
Run 01:
Highest Train: 88.43
Highest Valid: 88.29
  Final Train: 88.41
   Final Test: 88.37
All runs:
Highest Train: 88.43, nan
Highest Valid: 88.29, nan
  Final Train: 88.41, nan
   Final Test: 88.37, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 483.0558, Train: 55.83%, Valid: 55.72%, Test: 55.76%
Epoch: 25, Loss: 7.1807, Train: 74.52%, Valid: 74.63%, Test: 74.87%
Epoch: 50, Loss: 2.7368, Train: 80.51%, Valid: 80.61%, Test: 80.89%
Epoch: 75, Loss: 2.7070, Train: 80.39%, Valid: 80.51%, Test: 80.76%
Epoch: 100, Loss: 3.3115, Train: 79.97%, Valid: 80.07%, Test: 80.33%
Epoch: 125, Loss: 1.7211, Train: 80.73%, Valid: 80.82%, Test: 81.06%
Epoch: 150, Loss: 1.4845, Train: 81.73%, Valid: 81.80%, Test: 82.00%
Epoch: 175, Loss: 1.3990, Train: 84.62%, Valid: 84.61%, Test: 84.74%
Run 01:
Highest Train: 84.78
Highest Valid: 84.81
  Final Train: 84.78
   Final Test: 84.92
All runs:
Highest Train: 84.78, nan
Highest Valid: 84.81, nan
  Final Train: 84.78, nan
   Final Test: 84.92, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 5.8062, Train: 83.91%, Valid: 83.74%, Test: 84.02%
Epoch: 25, Loss: 1.2186, Train: 85.83%, Valid: 85.92%, Test: 85.93%
Epoch: 50, Loss: 0.7737, Train: 85.50%, Valid: 85.52%, Test: 85.59%
Epoch: 75, Loss: 0.3563, Train: 85.85%, Valid: 85.84%, Test: 85.94%
Epoch: 100, Loss: 0.3335, Train: 85.79%, Valid: 85.78%, Test: 85.88%
Epoch: 125, Loss: 0.8797, Train: 86.62%, Valid: 86.55%, Test: 86.69%
Epoch: 150, Loss: 0.4065, Train: 85.88%, Valid: 85.77%, Test: 85.93%
Epoch: 175, Loss: 0.3386, Train: 86.02%, Valid: 86.01%, Test: 86.08%
Run 01:
Highest Train: 88.34
Highest Valid: 88.39
  Final Train: 88.34
   Final Test: 88.40
All runs:
Highest Train: 88.34, nan
Highest Valid: 88.39, nan
  Final Train: 88.34, nan
   Final Test: 88.40, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 17.2787, Train: 84.46%, Valid: 84.42%, Test: 84.56%
Epoch: 25, Loss: 0.4048, Train: 85.57%, Valid: 85.45%, Test: 85.62%
Epoch: 50, Loss: 0.3602, Train: 85.73%, Valid: 85.57%, Test: 85.79%
Epoch: 75, Loss: 0.3497, Train: 85.75%, Valid: 85.59%, Test: 85.82%
Epoch: 100, Loss: 0.3357, Train: 85.29%, Valid: 85.10%, Test: 85.36%
Epoch: 125, Loss: 0.3409, Train: 85.73%, Valid: 85.53%, Test: 85.80%
Epoch: 150, Loss: 0.3327, Train: 85.57%, Valid: 85.36%, Test: 85.64%
Epoch: 175, Loss: 0.3564, Train: 85.92%, Valid: 85.74%, Test: 85.98%
Run 01:
Highest Train: 86.89
Highest Valid: 86.85
  Final Train: 86.89
   Final Test: 86.92
All runs:
Highest Train: 86.89, nan
Highest Valid: 86.85, nan
  Final Train: 86.89, nan
   Final Test: 86.92, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 216118390647947264.0000, Train: 41.86%, Valid: 41.84%, Test: 41.91%
Epoch: 25, Loss: 94.4690, Train: 35.45%, Valid: 35.15%, Test: 34.90%
Epoch: 50, Loss: 111.5604, Train: 34.91%, Valid: 34.65%, Test: 34.37%
Epoch: 75, Loss: 95.9839, Train: 34.29%, Valid: 34.02%, Test: 33.79%
Epoch: 100, Loss: 94.4174, Train: 30.37%, Valid: 30.28%, Test: 30.01%
Epoch: 125, Loss: 100.0066, Train: 32.81%, Valid: 32.63%, Test: 32.47%
Epoch: 150, Loss: 94.0877, Train: 32.27%, Valid: 32.11%, Test: 31.90%
Epoch: 175, Loss: 90.4937, Train: 34.26%, Valid: 33.99%, Test: 33.76%
Run 01:
Highest Train: 61.52
Highest Valid: 61.49
  Final Train: 61.52
   Final Test: 61.30
All runs:
Highest Train: 61.52, nan
Highest Valid: 61.49, nan
  Final Train: 61.52, nan
   Final Test: 61.30, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 2.5468, Train: 86.24%, Valid: 86.10%, Test: 86.21%
Epoch: 25, Loss: 0.6142, Train: 86.56%, Valid: 86.61%, Test: 86.60%
Epoch: 50, Loss: 0.3714, Train: 85.68%, Valid: 85.72%, Test: 85.75%
Epoch: 75, Loss: 0.3511, Train: 85.97%, Valid: 85.87%, Test: 86.02%
Epoch: 100, Loss: 0.3384, Train: 85.88%, Valid: 85.88%, Test: 85.95%
Epoch: 125, Loss: 0.3345, Train: 86.11%, Valid: 86.10%, Test: 86.21%
Epoch: 150, Loss: 0.3336, Train: 85.85%, Valid: 85.85%, Test: 85.92%
Epoch: 175, Loss: 0.3287, Train: 85.89%, Valid: 85.91%, Test: 85.96%
Run 01:
Highest Train: 87.58
Highest Valid: 87.65
  Final Train: 87.58
   Final Test: 87.64
All runs:
Highest Train: 87.58, nan
Highest Valid: 87.65, nan
  Final Train: 87.58, nan
   Final Test: 87.64, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.2985, Train: 85.43%, Valid: 85.42%, Test: 85.51%
Epoch: 25, Loss: 0.4062, Train: 85.37%, Valid: 85.15%, Test: 85.42%
Epoch: 50, Loss: 0.3603, Train: 86.25%, Valid: 86.19%, Test: 86.30%
Epoch: 75, Loss: 0.3468, Train: 87.03%, Valid: 87.08%, Test: 87.10%
Epoch: 100, Loss: 0.3367, Train: 85.34%, Valid: 85.15%, Test: 85.44%
Epoch: 125, Loss: 0.3405, Train: 87.32%, Valid: 87.34%, Test: 87.40%
Epoch: 150, Loss: 0.3353, Train: 86.86%, Valid: 86.83%, Test: 86.93%
Epoch: 175, Loss: 0.3344, Train: 87.29%, Valid: 87.33%, Test: 87.37%
Run 01:
Highest Train: 87.73
Highest Valid: 87.61
  Final Train: 87.73
   Final Test: 87.76
All runs:
Highest Train: 87.73, nan
Highest Valid: 87.61, nan
  Final Train: 87.73, nan
   Final Test: 87.76, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 277.8682, Train: 41.81%, Valid: 41.90%, Test: 42.07%
Epoch: 25, Loss: 10.4313, Train: 83.50%, Valid: 83.42%, Test: 83.51%
Epoch: 50, Loss: 5.3385, Train: 84.37%, Valid: 84.26%, Test: 84.41%
Epoch: 75, Loss: 8.0734, Train: 84.48%, Valid: 84.36%, Test: 84.53%
Epoch: 100, Loss: 18.5188, Train: 84.08%, Valid: 83.95%, Test: 84.16%
Epoch: 125, Loss: 0.9297, Train: 85.66%, Valid: 85.59%, Test: 85.73%
Epoch: 150, Loss: 22.6560, Train: 85.48%, Valid: 85.43%, Test: 85.49%
Epoch: 175, Loss: 30.0042, Train: 84.87%, Valid: 84.86%, Test: 84.93%
Run 01:
Highest Train: 86.14
Highest Valid: 86.07
  Final Train: 86.14
   Final Test: 86.15
All runs:
Highest Train: 86.14, nan
Highest Valid: 86.07, nan
  Final Train: 86.14, nan
   Final Test: 86.15, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 6.7615, Train: 86.80%, Valid: 86.81%, Test: 86.97%
Epoch: 25, Loss: 0.6137, Train: 86.22%, Valid: 86.12%, Test: 86.23%
Epoch: 50, Loss: 0.3998, Train: 85.95%, Valid: 85.83%, Test: 86.09%
Epoch: 75, Loss: 0.3778, Train: 85.84%, Valid: 85.79%, Test: 85.96%
Epoch: 100, Loss: 0.3581, Train: 85.48%, Valid: 85.44%, Test: 85.65%
Epoch: 125, Loss: 0.3521, Train: 85.51%, Valid: 85.27%, Test: 85.59%
Epoch: 150, Loss: 0.3417, Train: 85.63%, Valid: 85.60%, Test: 85.69%
Epoch: 175, Loss: 0.3402, Train: 86.61%, Valid: 86.55%, Test: 86.64%
Run 01:
Highest Train: 87.87
Highest Valid: 87.94
  Final Train: 87.87
   Final Test: 87.90
All runs:
Highest Train: 87.87, nan
Highest Valid: 87.94, nan
  Final Train: 87.87, nan
   Final Test: 87.90, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 0.9122, Train: 86.11%, Valid: 86.14%, Test: 86.15%
Epoch: 25, Loss: 0.3735, Train: 85.83%, Valid: 85.65%, Test: 85.90%
Epoch: 50, Loss: 0.3590, Train: 86.04%, Valid: 85.91%, Test: 86.12%
Epoch: 75, Loss: 0.3447, Train: 85.50%, Valid: 85.34%, Test: 85.56%
Epoch: 100, Loss: 0.3381, Train: 85.64%, Valid: 85.46%, Test: 85.70%
Epoch: 125, Loss: 0.3354, Train: 87.45%, Valid: 87.45%, Test: 87.52%
Epoch: 150, Loss: 0.3317, Train: 87.50%, Valid: 87.50%, Test: 87.55%
Epoch: 175, Loss: 0.3312, Train: 87.45%, Valid: 87.46%, Test: 87.49%
Run 01:
Highest Train: 87.54
Highest Valid: 87.54
  Final Train: 87.54
   Final Test: 87.56
All runs:
Highest Train: 87.54, nan
Highest Valid: 87.54, nan
  Final Train: 87.54, nan
   Final Test: 87.56, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 175.3721, Train: 48.21%, Valid: 48.14%, Test: 48.25%
Epoch: 25, Loss: 20.0262, Train: 84.38%, Valid: 84.27%, Test: 84.47%
Epoch: 50, Loss: 6.0032, Train: 84.45%, Valid: 84.32%, Test: 84.49%
Epoch: 75, Loss: 1.4109, Train: 84.98%, Valid: 84.84%, Test: 85.04%
Epoch: 100, Loss: 1.0564, Train: 85.30%, Valid: 85.15%, Test: 85.37%
Epoch: 125, Loss: 0.9464, Train: 85.45%, Valid: 85.30%, Test: 85.52%
Epoch: 150, Loss: 0.9964, Train: 86.19%, Valid: 86.00%, Test: 86.20%
Epoch: 175, Loss: 1.0280, Train: 87.78%, Valid: 87.59%, Test: 87.75%
Run 01:
Highest Train: 87.87
Highest Valid: 87.71
  Final Train: 87.82
   Final Test: 87.85
All runs:
Highest Train: 87.87, nan
Highest Valid: 87.71, nan
  Final Train: 87.82, nan
   Final Test: 87.85, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 13.8331, Train: 86.35%, Valid: 86.38%, Test: 86.38%
Epoch: 25, Loss: 0.7146, Train: 86.67%, Valid: 86.76%, Test: 86.72%
Epoch: 50, Loss: 0.3862, Train: 85.85%, Valid: 85.90%, Test: 85.97%
Epoch: 75, Loss: 0.3711, Train: 85.43%, Valid: 85.45%, Test: 85.55%
Epoch: 100, Loss: 0.3605, Train: 85.56%, Valid: 85.52%, Test: 85.68%
Epoch: 125, Loss: 0.3500, Train: 85.49%, Valid: 85.45%, Test: 85.62%
Epoch: 150, Loss: 0.3472, Train: 85.44%, Valid: 85.43%, Test: 85.57%
Epoch: 175, Loss: 0.3423, Train: 85.37%, Valid: 85.33%, Test: 85.49%
Run 01:
Highest Train: 86.97
Highest Valid: 87.00
  Final Train: 86.97
   Final Test: 87.01
All runs:
Highest Train: 86.97, nan
Highest Valid: 87.00, nan
  Final Train: 86.97, nan
   Final Test: 87.01, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 8.0189, Train: 85.40%, Valid: 85.38%, Test: 85.47%
Epoch: 25, Loss: 0.5924, Train: 85.95%, Valid: 85.79%, Test: 86.03%
Epoch: 50, Loss: 0.3998, Train: 85.44%, Valid: 85.20%, Test: 85.50%
Epoch: 75, Loss: 0.3703, Train: 85.90%, Valid: 85.74%, Test: 85.99%
Epoch: 100, Loss: 0.3598, Train: 86.18%, Valid: 86.04%, Test: 86.25%
Epoch: 125, Loss: 0.3488, Train: 86.11%, Valid: 85.95%, Test: 86.19%
Epoch: 150, Loss: 0.3437, Train: 86.40%, Valid: 86.24%, Test: 86.46%
Epoch: 175, Loss: 0.3374, Train: 87.10%, Valid: 87.09%, Test: 87.14%
Run 01:
Highest Train: 87.19
Highest Valid: 87.18
  Final Train: 87.19
   Final Test: 87.24
All runs:
Highest Train: 87.19, nan
Highest Valid: 87.18, nan
  Final Train: 87.19, nan
   Final Test: 87.24, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 47.3921, Train: 53.21%, Valid: 53.22%, Test: 53.18%
Epoch: 25, Loss: 89.8596, Train: 84.32%, Valid: 84.19%, Test: 84.36%
Epoch: 50, Loss: 2.6568, Train: 84.49%, Valid: 84.39%, Test: 84.53%
Epoch: 75, Loss: 1.7242, Train: 85.28%, Valid: 85.14%, Test: 85.35%
Epoch: 100, Loss: 1.3033, Train: 84.77%, Valid: 84.65%, Test: 84.86%
Epoch: 125, Loss: 0.9933, Train: 85.32%, Valid: 85.17%, Test: 85.39%
Epoch: 150, Loss: 0.9302, Train: 85.19%, Valid: 85.04%, Test: 85.26%
Epoch: 175, Loss: 0.6664, Train: 85.04%, Valid: 84.89%, Test: 85.10%
Run 01:
Highest Train: 87.73
Highest Valid: 87.60
  Final Train: 87.68
   Final Test: 87.74
All runs:
Highest Train: 87.73, nan
Highest Valid: 87.60, nan
  Final Train: 87.68, nan
   Final Test: 87.74, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 9.2544, Train: 88.13%, Valid: 88.16%, Test: 88.15%
Epoch: 25, Loss: 5016.3901, Train: 16.04%, Valid: 16.20%, Test: 15.92%
Epoch: 50, Loss: 2.7323, Train: 87.14%, Valid: 87.24%, Test: 87.21%
Epoch: 75, Loss: 2.6845, Train: 87.05%, Valid: 87.13%, Test: 87.13%
Epoch: 100, Loss: 2.8215, Train: 87.05%, Valid: 87.13%, Test: 87.13%
Epoch: 125, Loss: 2.7789, Train: 87.05%, Valid: 87.13%, Test: 87.13%
Epoch: 150, Loss: 2.6666, Train: 87.05%, Valid: 87.13%, Test: 87.13%
Epoch: 175, Loss: 2.8228, Train: 87.05%, Valid: 87.13%, Test: 87.13%
Run 01:
Highest Train: 88.39
Highest Valid: 88.46
  Final Train: 88.39
   Final Test: 88.40
All runs:
Highest Train: 88.39, nan
Highest Valid: 88.46, nan
  Final Train: 88.39, nan
   Final Test: 88.40, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 15.1302, Train: 86.00%, Valid: 85.88%, Test: 86.05%
Epoch: 25, Loss: 5.1168, Train: 85.49%, Valid: 85.33%, Test: 85.53%
Epoch: 50, Loss: 9.3813, Train: 16.34%, Valid: 16.35%, Test: 16.23%
Epoch: 75, Loss: 10954.2969, Train: 85.32%, Valid: 85.12%, Test: 85.39%
Epoch: 100, Loss: 1262.0767, Train: 85.75%, Valid: 85.61%, Test: 85.80%
Epoch: 125, Loss: 728.1652, Train: 87.39%, Valid: 87.46%, Test: 87.44%
Epoch: 150, Loss: 896.6033, Train: 87.23%, Valid: 87.27%, Test: 87.29%
Epoch: 175, Loss: 949.5709, Train: 86.25%, Valid: 86.13%, Test: 86.31%
Run 01:
Highest Train: 87.99
Highest Valid: 88.04
  Final Train: 87.99
   Final Test: 87.96
All runs:
Highest Train: 87.99, nan
Highest Valid: 88.04, nan
  Final Train: 87.99, nan
   Final Test: 87.96, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 26739.5449, Train: 73.19%, Valid: 73.03%, Test: 72.99%
Epoch: 25, Loss: 10.9759, Train: 84.95%, Valid: 85.01%, Test: 85.14%
Epoch: 50, Loss: 5.5930, Train: 85.22%, Valid: 85.28%, Test: 85.40%
Epoch: 75, Loss: 4.1606, Train: 85.32%, Valid: 85.36%, Test: 85.50%
Epoch: 100, Loss: 3.6430, Train: 85.53%, Valid: 85.52%, Test: 85.70%
Epoch: 125, Loss: 3.3731, Train: 85.59%, Valid: 85.55%, Test: 85.71%
Epoch: 150, Loss: 3.1930, Train: 85.54%, Valid: 85.54%, Test: 85.67%
Epoch: 175, Loss: 3.0543, Train: 85.46%, Valid: 85.39%, Test: 85.53%
Run 01:
Highest Train: 85.60
Highest Valid: 85.57
  Final Train: 85.59
   Final Test: 85.71
All runs:
Highest Train: 85.60, nan
Highest Valid: 85.57, nan
  Final Train: 85.59, nan
   Final Test: 85.71, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 3.0135, Train: 85.06%, Valid: 85.03%, Test: 85.21%
Epoch: 25, Loss: 0.6099, Train: 85.46%, Valid: 85.51%, Test: 85.57%
Epoch: 50, Loss: 0.3840, Train: 85.50%, Valid: 85.28%, Test: 85.59%
Epoch: 75, Loss: 0.3495, Train: 86.60%, Valid: 86.56%, Test: 86.64%
Epoch: 100, Loss: 0.3399, Train: 85.77%, Valid: 85.54%, Test: 85.85%
Epoch: 125, Loss: 0.3341, Train: 86.26%, Valid: 86.05%, Test: 86.32%
Epoch: 150, Loss: 0.3310, Train: 86.97%, Valid: 86.83%, Test: 87.04%
Epoch: 175, Loss: 0.3325, Train: 86.82%, Valid: 86.69%, Test: 86.93%
Run 01:
Highest Train: 87.92
Highest Valid: 88.01
  Final Train: 87.92
   Final Test: 88.05
All runs:
Highest Train: 87.92, nan
Highest Valid: 88.01, nan
  Final Train: 87.92, nan
   Final Test: 88.05, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 19.8060, Train: 86.07%, Valid: 86.01%, Test: 86.19%
Epoch: 25, Loss: 0.5682, Train: 85.32%, Valid: 85.20%, Test: 85.39%
Epoch: 50, Loss: 0.4379, Train: 86.13%, Valid: 86.00%, Test: 86.19%
Epoch: 75, Loss: 0.3792, Train: 86.26%, Valid: 86.09%, Test: 86.32%
Epoch: 100, Loss: 0.3585, Train: 86.45%, Valid: 86.29%, Test: 86.52%
Epoch: 125, Loss: 0.3404, Train: 87.01%, Valid: 86.86%, Test: 87.04%
Epoch: 150, Loss: 0.3346, Train: 86.47%, Valid: 86.32%, Test: 86.53%
Epoch: 175, Loss: 0.3319, Train: 86.75%, Valid: 86.71%, Test: 86.80%
Run 01:
Highest Train: 88.10
Highest Valid: 87.94
  Final Train: 88.10
   Final Test: 88.06
All runs:
Highest Train: 88.10, nan
Highest Valid: 87.94, nan
  Final Train: 88.10, nan
   Final Test: 88.06, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 308.0962, Train: 69.73%, Valid: 69.55%, Test: 69.40%
Epoch: 25, Loss: 4.9623, Train: 83.37%, Valid: 83.45%, Test: 83.57%
Epoch: 50, Loss: 4.5549, Train: 77.48%, Valid: 77.64%, Test: 77.89%
Epoch: 75, Loss: 4.6061, Train: 75.99%, Valid: 76.10%, Test: 76.31%
Epoch: 100, Loss: 2.6202, Train: 84.70%, Valid: 84.57%, Test: 84.76%
Epoch: 125, Loss: 2.0092, Train: 84.42%, Valid: 84.29%, Test: 84.46%
Epoch: 150, Loss: 1.8620, Train: 74.65%, Valid: 74.70%, Test: 74.93%
Epoch: 175, Loss: 1.6943, Train: 74.12%, Valid: 74.18%, Test: 74.44%
Run 01:
Highest Train: 85.53
Highest Valid: 85.45
  Final Train: 85.49
   Final Test: 85.53
All runs:
Highest Train: 85.53, nan
Highest Valid: 85.45, nan
  Final Train: 85.49, nan
   Final Test: 85.53, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.9669, Train: 85.95%, Valid: 85.74%, Test: 86.00%
Epoch: 25, Loss: 0.4670, Train: 85.67%, Valid: 85.59%, Test: 85.75%
Epoch: 50, Loss: 0.3679, Train: 87.16%, Valid: 87.12%, Test: 87.20%
Epoch: 75, Loss: 0.3463, Train: 85.47%, Valid: 85.46%, Test: 85.59%
Epoch: 100, Loss: 0.3396, Train: 85.62%, Valid: 85.61%, Test: 85.71%
Epoch: 125, Loss: 0.3364, Train: 85.84%, Valid: 85.80%, Test: 85.93%
Epoch: 150, Loss: 0.3332, Train: 85.79%, Valid: 85.77%, Test: 85.88%
Epoch: 175, Loss: 0.3315, Train: 85.85%, Valid: 85.82%, Test: 85.93%
Run 01:
Highest Train: 88.51
Highest Valid: 88.55
  Final Train: 88.51
   Final Test: 88.55
All runs:
Highest Train: 88.51, nan
Highest Valid: 88.55, nan
  Final Train: 88.51, nan
   Final Test: 88.55, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 8.3993, Train: 84.87%, Valid: 84.84%, Test: 84.93%
Epoch: 25, Loss: 0.8729, Train: 86.25%, Valid: 86.15%, Test: 86.31%
Epoch: 50, Loss: 0.4183, Train: 85.27%, Valid: 85.08%, Test: 85.36%
Epoch: 75, Loss: 0.3602, Train: 86.45%, Valid: 86.29%, Test: 86.53%
Epoch: 100, Loss: 0.3442, Train: 86.43%, Valid: 86.29%, Test: 86.50%
Epoch: 125, Loss: 0.3397, Train: 85.94%, Valid: 85.80%, Test: 86.01%
Epoch: 150, Loss: 0.3393, Train: 86.47%, Valid: 86.33%, Test: 86.53%
Epoch: 175, Loss: 0.3322, Train: 86.70%, Valid: 86.57%, Test: 86.76%
Run 01:
Highest Train: 87.44
Highest Valid: 87.43
  Final Train: 87.44
   Final Test: 87.50
All runs:
Highest Train: 87.44, nan
Highest Valid: 87.43, nan
  Final Train: 87.44, nan
   Final Test: 87.50, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 83.8880, Train: 76.04%, Valid: 75.86%, Test: 75.92%
Epoch: 25, Loss: 2.6930, Train: 77.68%, Valid: 77.84%, Test: 78.09%
Epoch: 50, Loss: 2.2865, Train: 85.21%, Valid: 85.08%, Test: 85.28%
Epoch: 75, Loss: 1.9081, Train: 85.41%, Valid: 85.27%, Test: 85.47%
Epoch: 100, Loss: 1.5217, Train: 84.94%, Valid: 84.81%, Test: 85.00%
Epoch: 125, Loss: 0.8633, Train: 74.02%, Valid: 74.10%, Test: 74.34%
Epoch: 150, Loss: 0.4919, Train: 77.21%, Valid: 77.30%, Test: 77.57%
Epoch: 175, Loss: 0.5266, Train: 77.74%, Valid: 77.80%, Test: 78.08%
Run 01:
Highest Train: 86.33
Highest Valid: 86.22
  Final Train: 86.33
   Final Test: 86.40
All runs:
Highest Train: 86.33, nan
Highest Valid: 86.22, nan
  Final Train: 86.33, nan
   Final Test: 86.40, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 18.1147, Train: 88.40%, Valid: 88.44%, Test: 88.43%
Epoch: 25, Loss: 0.8296, Train: 86.71%, Valid: 86.63%, Test: 86.75%
Epoch: 50, Loss: 0.3910, Train: 87.21%, Valid: 87.00%, Test: 87.26%
Epoch: 75, Loss: 0.3585, Train: 85.71%, Valid: 85.52%, Test: 85.78%
Epoch: 100, Loss: 0.3545, Train: 87.64%, Valid: 87.46%, Test: 87.69%
Epoch: 125, Loss: 0.3455, Train: 88.00%, Valid: 87.80%, Test: 88.01%
Epoch: 150, Loss: 0.3400, Train: 87.77%, Valid: 87.57%, Test: 87.80%
Epoch: 175, Loss: 0.3380, Train: 87.40%, Valid: 87.18%, Test: 87.40%
Run 01:
Highest Train: 88.40
Highest Valid: 88.44
  Final Train: 88.40
   Final Test: 88.43
All runs:
Highest Train: 88.40, nan
Highest Valid: 88.44, nan
  Final Train: 88.40, nan
   Final Test: 88.43, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 5.6503, Train: 85.52%, Valid: 85.39%, Test: 85.52%
Epoch: 25, Loss: 0.7210, Train: 86.95%, Valid: 86.93%, Test: 86.98%
Epoch: 50, Loss: 0.3950, Train: 85.62%, Valid: 85.43%, Test: 85.65%
Epoch: 75, Loss: 0.3683, Train: 86.05%, Valid: 85.93%, Test: 86.21%
Epoch: 100, Loss: 0.3542, Train: 86.55%, Valid: 86.39%, Test: 86.62%
Epoch: 125, Loss: 0.3471, Train: 86.38%, Valid: 86.23%, Test: 86.46%
Epoch: 150, Loss: 0.3404, Train: 86.89%, Valid: 86.85%, Test: 86.95%
Epoch: 175, Loss: 0.3389, Train: 86.36%, Valid: 86.19%, Test: 86.42%
Run 01:
Highest Train: 87.91
Highest Valid: 87.78
  Final Train: 87.91
   Final Test: 87.86
All runs:
Highest Train: 87.91, nan
Highest Valid: 87.78, nan
  Final Train: 87.91, nan
   Final Test: 87.86, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 112.8279, Train: 62.18%, Valid: 62.38%, Test: 62.68%
Epoch: 25, Loss: 4.2470, Train: 84.95%, Valid: 84.82%, Test: 85.02%
Epoch: 50, Loss: 2.6726, Train: 84.84%, Valid: 84.73%, Test: 84.90%
Epoch: 75, Loss: 1.9548, Train: 84.85%, Valid: 84.75%, Test: 84.92%
Epoch: 100, Loss: 1.3528, Train: 84.42%, Valid: 84.29%, Test: 84.43%
Epoch: 125, Loss: 0.9474, Train: 76.06%, Valid: 76.12%, Test: 76.39%
Epoch: 150, Loss: 0.7025, Train: 84.45%, Valid: 84.46%, Test: 84.60%
Epoch: 175, Loss: 0.4738, Train: 84.48%, Valid: 84.42%, Test: 84.64%
Run 01:
Highest Train: 87.65
Highest Valid: 87.49
  Final Train: 87.65
   Final Test: 87.62
All runs:
Highest Train: 87.65, nan
Highest Valid: 87.49, nan
  Final Train: 87.65, nan
   Final Test: 87.62, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.2653, Train: 87.05%, Valid: 86.98%, Test: 87.08%
Epoch: 25, Loss: 11.8853, Train: 84.44%, Valid: 84.29%, Test: 84.59%
Epoch: 50, Loss: 2.4022, Train: 84.30%, Valid: 84.11%, Test: 84.40%
Epoch: 75, Loss: 2.2980, Train: 85.36%, Valid: 85.36%, Test: 85.46%
Epoch: 100, Loss: 2.1187, Train: 86.40%, Valid: 86.46%, Test: 86.43%
Epoch: 125, Loss: 1.7407, Train: 85.74%, Valid: 85.79%, Test: 85.79%
Epoch: 150, Loss: 1.1510, Train: 85.62%, Valid: 85.66%, Test: 85.67%
Epoch: 175, Loss: 0.5012, Train: 86.80%, Valid: 86.70%, Test: 86.82%
Run 01:
Highest Train: 87.05
Highest Valid: 87.05
  Final Train: 87.00
   Final Test: 87.03
All runs:
Highest Train: 87.05, nan
Highest Valid: 87.05, nan
  Final Train: 87.00, nan
   Final Test: 87.03, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.2197, Train: 84.91%, Valid: 84.96%, Test: 85.00%
Epoch: 25, Loss: 1.7085, Train: 85.86%, Valid: 85.67%, Test: 85.90%
Epoch: 50, Loss: 1.2510, Train: 86.17%, Valid: 86.04%, Test: 86.23%
Epoch: 75, Loss: 0.6098, Train: 86.18%, Valid: 86.05%, Test: 86.23%
Epoch: 100, Loss: 0.3660, Train: 86.62%, Valid: 86.61%, Test: 86.76%
Epoch: 125, Loss: 0.3506, Train: 86.17%, Valid: 86.10%, Test: 86.28%
Epoch: 150, Loss: 0.3552, Train: 86.83%, Valid: 86.73%, Test: 86.86%
Epoch: 175, Loss: 0.3394, Train: 86.97%, Valid: 86.90%, Test: 87.10%
Run 01:
Highest Train: 87.07
Highest Valid: 86.98
  Final Train: 87.07
   Final Test: 87.16
All runs:
Highest Train: 87.07, nan
Highest Valid: 86.98, nan
  Final Train: 87.07, nan
   Final Test: 87.16, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 75924256.0000, Train: 57.90%, Valid: 57.80%, Test: 57.62%
Epoch: 25, Loss: 1720.1018, Train: 15.18%, Valid: 15.36%, Test: 15.08%
Epoch: 50, Loss: 1619.3666, Train: 15.14%, Valid: 15.32%, Test: 15.04%
Epoch: 75, Loss: 1537.4624, Train: 15.13%, Valid: 15.32%, Test: 15.03%
Epoch: 100, Loss: 1580.7815, Train: 15.14%, Valid: 15.32%, Test: 15.03%
Epoch: 125, Loss: 1572.9067, Train: 15.13%, Valid: 15.32%, Test: 15.02%
Epoch: 150, Loss: 1642.1091, Train: 15.14%, Valid: 15.33%, Test: 15.04%
Epoch: 175, Loss: 1551.9792, Train: 15.14%, Valid: 15.32%, Test: 15.03%
Run 01:
Highest Train: 60.80
Highest Valid: 60.66
  Final Train: 60.80
   Final Test: 60.57
All runs:
Highest Train: 60.80, nan
Highest Valid: 60.66, nan
  Final Train: 60.80, nan
   Final Test: 60.57, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.5059, Train: 86.54%, Valid: 86.56%, Test: 86.58%
Epoch: 25, Loss: 0.4656, Train: 86.94%, Valid: 86.83%, Test: 86.91%
Epoch: 50, Loss: 0.3644, Train: 88.09%, Valid: 88.18%, Test: 88.15%
Epoch: 75, Loss: 0.3515, Train: 86.10%, Valid: 86.12%, Test: 86.20%
Epoch: 100, Loss: 0.3449, Train: 86.05%, Valid: 86.04%, Test: 86.14%
Epoch: 125, Loss: 0.3354, Train: 85.92%, Valid: 85.90%, Test: 86.01%
Epoch: 150, Loss: 0.3341, Train: 85.98%, Valid: 86.00%, Test: 86.09%
Epoch: 175, Loss: 0.3324, Train: 87.97%, Valid: 87.78%, Test: 88.01%
Run 01:
Highest Train: 88.10
Highest Valid: 88.18
  Final Train: 88.09
   Final Test: 88.15
All runs:
Highest Train: 88.10, nan
Highest Valid: 88.18, nan
  Final Train: 88.09, nan
   Final Test: 88.15, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 9.0380, Train: 84.17%, Valid: 84.18%, Test: 84.27%
Epoch: 25, Loss: 1.1061, Train: 86.81%, Valid: 86.88%, Test: 86.90%
Epoch: 50, Loss: 0.3846, Train: 86.57%, Valid: 86.54%, Test: 86.66%
Epoch: 75, Loss: 0.3533, Train: 85.92%, Valid: 85.71%, Test: 85.98%
Epoch: 100, Loss: 0.3432, Train: 85.52%, Valid: 85.32%, Test: 85.62%
Epoch: 125, Loss: 0.3364, Train: 85.82%, Valid: 85.61%, Test: 85.88%
Epoch: 150, Loss: 0.3342, Train: 85.72%, Valid: 85.51%, Test: 85.78%
Epoch: 175, Loss: 0.3313, Train: 87.45%, Valid: 87.46%, Test: 87.51%
Run 01:
Highest Train: 87.53
Highest Valid: 87.54
  Final Train: 87.53
   Final Test: 87.57
All runs:
Highest Train: 87.53, nan
Highest Valid: 87.54, nan
  Final Train: 87.53, nan
   Final Test: 87.57, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 429.3821, Train: 60.93%, Valid: 60.73%, Test: 60.66%
Epoch: 25, Loss: 9.9660, Train: 84.79%, Valid: 84.66%, Test: 84.86%
Epoch: 50, Loss: 2.4313, Train: 76.68%, Valid: 76.81%, Test: 77.05%
Epoch: 75, Loss: 1.7877, Train: 79.03%, Valid: 79.13%, Test: 79.43%
Epoch: 100, Loss: 1.7359, Train: 80.34%, Valid: 80.44%, Test: 80.74%
Epoch: 125, Loss: 1.4864, Train: 81.22%, Valid: 81.32%, Test: 81.62%
Epoch: 150, Loss: 1.3984, Train: 82.33%, Valid: 82.42%, Test: 82.62%
Epoch: 175, Loss: 1.5302, Train: 85.08%, Valid: 85.03%, Test: 85.21%
Run 01:
Highest Train: 86.10
Highest Valid: 86.11
  Final Train: 86.10
   Final Test: 86.20
All runs:
Highest Train: 86.10, nan
Highest Valid: 86.11, nan
  Final Train: 86.10, nan
   Final Test: 86.20, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 6.1793, Train: 88.06%, Valid: 88.13%, Test: 88.11%
Epoch: 25, Loss: 0.5883, Train: 85.90%, Valid: 85.82%, Test: 85.94%
Epoch: 50, Loss: 0.4220, Train: 85.61%, Valid: 85.68%, Test: 85.68%
Epoch: 75, Loss: 0.3547, Train: 85.74%, Valid: 85.75%, Test: 85.83%
Epoch: 100, Loss: 0.3489, Train: 86.74%, Valid: 86.58%, Test: 86.85%
Epoch: 125, Loss: 0.3411, Train: 86.98%, Valid: 86.79%, Test: 87.00%
Epoch: 150, Loss: 0.3340, Train: 86.45%, Valid: 86.44%, Test: 86.50%
Epoch: 175, Loss: 0.3331, Train: 86.32%, Valid: 86.16%, Test: 86.36%
Run 01:
Highest Train: 88.23
Highest Valid: 88.30
  Final Train: 88.23
   Final Test: 88.22
All runs:
Highest Train: 88.23, nan
Highest Valid: 88.30, nan
  Final Train: 88.23, nan
   Final Test: 88.22, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.7568, Train: 85.84%, Valid: 85.83%, Test: 85.91%
Epoch: 25, Loss: 0.3850, Train: 85.58%, Valid: 85.43%, Test: 85.66%
Epoch: 50, Loss: 0.3566, Train: 85.63%, Valid: 85.47%, Test: 85.69%
Epoch: 75, Loss: 0.3447, Train: 85.73%, Valid: 85.56%, Test: 85.81%
Epoch: 100, Loss: 0.3364, Train: 87.17%, Valid: 87.18%, Test: 87.25%
Epoch: 125, Loss: 0.3326, Train: 87.27%, Valid: 87.18%, Test: 87.40%
Epoch: 150, Loss: 0.3315, Train: 87.12%, Valid: 87.10%, Test: 87.21%
Epoch: 175, Loss: 0.3301, Train: 87.40%, Valid: 87.41%, Test: 87.48%
Run 01:
Highest Train: 87.69
Highest Valid: 87.70
  Final Train: 87.69
   Final Test: 87.77
All runs:
Highest Train: 87.69, nan
Highest Valid: 87.70, nan
  Final Train: 87.69, nan
   Final Test: 87.77, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 359.4836, Train: 38.69%, Valid: 38.81%, Test: 38.65%
Epoch: 25, Loss: 13.8193, Train: 74.84%, Valid: 74.92%, Test: 75.19%
Epoch: 50, Loss: 5.4944, Train: 84.57%, Valid: 84.45%, Test: 84.61%
Epoch: 75, Loss: 2.8817, Train: 84.67%, Valid: 84.55%, Test: 84.71%
Epoch: 100, Loss: 2.7831, Train: 84.72%, Valid: 84.60%, Test: 84.76%
Epoch: 125, Loss: 2.6658, Train: 84.83%, Valid: 84.70%, Test: 84.87%
Epoch: 150, Loss: 2.4622, Train: 85.01%, Valid: 84.86%, Test: 85.04%
Epoch: 175, Loss: 1.7029, Train: 85.01%, Valid: 84.86%, Test: 85.04%
Run 01:
Highest Train: 87.32
Highest Valid: 87.09
  Final Train: 87.32
   Final Test: 87.26
All runs:
Highest Train: 87.32, nan
Highest Valid: 87.09, nan
  Final Train: 87.32, nan
   Final Test: 87.26, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 14.6124, Train: 85.33%, Valid: 85.29%, Test: 85.51%
Epoch: 25, Loss: 1.1085, Train: 85.06%, Valid: 85.08%, Test: 85.18%
Epoch: 50, Loss: 0.4151, Train: 85.32%, Valid: 85.33%, Test: 85.40%
Epoch: 75, Loss: 0.3705, Train: 85.82%, Valid: 85.86%, Test: 85.90%
Epoch: 100, Loss: 0.3611, Train: 85.27%, Valid: 85.29%, Test: 85.43%
Epoch: 125, Loss: 0.3510, Train: 85.80%, Valid: 85.81%, Test: 85.95%
Epoch: 150, Loss: 0.3482, Train: 85.64%, Valid: 85.55%, Test: 85.76%
Epoch: 175, Loss: 0.3409, Train: 86.31%, Valid: 86.09%, Test: 86.37%
Run 01:
Highest Train: 87.58
Highest Valid: 87.39
  Final Train: 87.58
   Final Test: 87.58
All runs:
Highest Train: 87.58, nan
Highest Valid: 87.39, nan
  Final Train: 87.58, nan
   Final Test: 87.58, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.2910, Train: 86.51%, Valid: 86.52%, Test: 86.59%
Epoch: 25, Loss: 0.3817, Train: 86.41%, Valid: 86.38%, Test: 86.55%
Epoch: 50, Loss: 0.3584, Train: 85.69%, Valid: 85.53%, Test: 85.76%
Epoch: 75, Loss: 0.3452, Train: 85.52%, Valid: 85.35%, Test: 85.60%
Epoch: 100, Loss: 0.3377, Train: 87.16%, Valid: 87.16%, Test: 87.24%
Epoch: 125, Loss: 0.3341, Train: 87.56%, Valid: 87.58%, Test: 87.63%
Epoch: 150, Loss: 0.3346, Train: 85.91%, Valid: 85.71%, Test: 86.00%
Epoch: 175, Loss: 0.3301, Train: 87.26%, Valid: 87.26%, Test: 87.32%
Run 01:
Highest Train: 87.69
Highest Valid: 87.70
  Final Train: 87.69
   Final Test: 87.75
All runs:
Highest Train: 87.69, nan
Highest Valid: 87.70, nan
  Final Train: 87.69, nan
   Final Test: 87.75, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 412.9005, Train: 82.42%, Valid: 82.33%, Test: 82.46%
Epoch: 25, Loss: 8.0839, Train: 85.03%, Valid: 84.88%, Test: 85.08%
Epoch: 50, Loss: 5.1284, Train: 85.28%, Valid: 85.12%, Test: 85.35%
Epoch: 75, Loss: 3.6261, Train: 82.83%, Valid: 82.96%, Test: 83.07%
Epoch: 100, Loss: 2.9136, Train: 85.80%, Valid: 85.67%, Test: 85.86%
Epoch: 125, Loss: 2.4377, Train: 82.99%, Valid: 83.05%, Test: 83.18%
Epoch: 150, Loss: 1.7493, Train: 81.10%, Valid: 81.14%, Test: 81.38%
Epoch: 175, Loss: 1.6228, Train: 85.16%, Valid: 85.02%, Test: 85.24%
Run 01:
Highest Train: 85.80
Highest Valid: 85.67
  Final Train: 85.80
   Final Test: 85.86
All runs:
Highest Train: 85.80, nan
Highest Valid: 85.67, nan
  Final Train: 85.80, nan
   Final Test: 85.86, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 16.0668, Train: 85.34%, Valid: 85.19%, Test: 85.47%
Epoch: 25, Loss: 7.7757, Train: 87.54%, Valid: 87.59%, Test: 87.54%
Epoch: 50, Loss: 5.3890, Train: 84.47%, Valid: 84.29%, Test: 84.59%
Epoch: 75, Loss: 6.9951, Train: 84.51%, Valid: 84.32%, Test: 84.60%
Epoch: 100, Loss: 7.1540, Train: 84.51%, Valid: 84.32%, Test: 84.61%
Epoch: 125, Loss: 7.1648, Train: 84.51%, Valid: 84.32%, Test: 84.61%
Epoch: 150, Loss: 7.1661, Train: 84.51%, Valid: 84.32%, Test: 84.61%
Epoch: 175, Loss: 7.1641, Train: 84.51%, Valid: 84.32%, Test: 84.61%
Run 01:
Highest Train: 88.19
Highest Valid: 88.24
  Final Train: 88.19
   Final Test: 88.22
All runs:
Highest Train: 88.19, nan
Highest Valid: 88.24, nan
  Final Train: 88.19, nan
   Final Test: 88.22, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 11.0981, Train: 84.24%, Valid: 84.30%, Test: 84.31%
Epoch: 25, Loss: 4.0192, Train: 85.33%, Valid: 85.11%, Test: 85.40%
Epoch: 50, Loss: 1.2551, Train: 86.56%, Valid: 86.65%, Test: 86.69%
Epoch: 75, Loss: 2.5157, Train: 87.14%, Valid: 87.15%, Test: 87.20%
Epoch: 100, Loss: 2.0608, Train: 86.90%, Valid: 86.95%, Test: 87.00%
Epoch: 125, Loss: 2.8059, Train: 86.75%, Valid: 86.69%, Test: 86.77%
Epoch: 150, Loss: 0.6630, Train: 83.99%, Valid: 83.83%, Test: 84.10%
Epoch: 175, Loss: 0.4144, Train: 85.82%, Valid: 85.63%, Test: 85.89%
Run 01:
Highest Train: 87.97
Highest Valid: 87.88
  Final Train: 87.97
   Final Test: 88.04
All runs:
Highest Train: 87.97, nan
Highest Valid: 87.88, nan
  Final Train: 87.97, nan
   Final Test: 88.04, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 6150357734391808.0000, Train: 39.46%, Valid: 39.61%, Test: 39.56%
Epoch: 25, Loss: 113.7320, Train: 84.92%, Valid: 84.82%, Test: 85.00%
Epoch: 50, Loss: 107.3961, Train: 84.99%, Valid: 84.86%, Test: 85.07%
Epoch: 75, Loss: 63.6540, Train: 84.91%, Valid: 84.78%, Test: 84.99%
Epoch: 100, Loss: 8.4001, Train: 84.91%, Valid: 84.79%, Test: 84.97%
Epoch: 125, Loss: 7.0550, Train: 84.90%, Valid: 84.79%, Test: 84.95%
Epoch: 150, Loss: 7.0393, Train: 84.89%, Valid: 84.78%, Test: 84.95%
Epoch: 175, Loss: 6.9871, Train: 84.89%, Valid: 84.78%, Test: 84.94%
Run 01:
Highest Train: 85.01
Highest Valid: 84.88
  Final Train: 85.01
   Final Test: 85.09
All runs:
Highest Train: 85.01, nan
Highest Valid: 84.88, nan
  Final Train: 85.01, nan
   Final Test: 85.09, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 7.4815, Train: 88.21%, Valid: 88.27%, Test: 88.27%
Epoch: 25, Loss: 1.8870, Train: 86.79%, Valid: 86.64%, Test: 86.83%
Epoch: 50, Loss: 0.7037, Train: 86.51%, Valid: 86.62%, Test: 86.61%
Epoch: 75, Loss: 0.4521, Train: 86.21%, Valid: 86.27%, Test: 86.27%
Epoch: 100, Loss: 0.3949, Train: 85.90%, Valid: 85.93%, Test: 85.99%
Epoch: 125, Loss: 0.3549, Train: 85.95%, Valid: 85.96%, Test: 86.04%
Epoch: 150, Loss: 0.3456, Train: 85.95%, Valid: 85.96%, Test: 86.05%
Epoch: 175, Loss: 0.3394, Train: 85.89%, Valid: 85.91%, Test: 85.95%
Run 01:
Highest Train: 88.21
Highest Valid: 88.27
  Final Train: 88.21
   Final Test: 88.27
All runs:
Highest Train: 88.21, nan
Highest Valid: 88.27, nan
  Final Train: 88.21, nan
   Final Test: 88.27, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.1881, Train: 85.73%, Valid: 85.72%, Test: 85.80%
Epoch: 25, Loss: 0.5347, Train: 85.85%, Valid: 85.85%, Test: 85.96%
Epoch: 50, Loss: 0.3605, Train: 85.86%, Valid: 85.71%, Test: 85.90%
Epoch: 75, Loss: 0.3475, Train: 86.84%, Valid: 86.64%, Test: 86.93%
Epoch: 100, Loss: 0.3384, Train: 85.48%, Valid: 85.28%, Test: 85.57%
Epoch: 125, Loss: 0.3345, Train: 87.33%, Valid: 87.30%, Test: 87.38%
Epoch: 150, Loss: 0.3317, Train: 86.95%, Valid: 86.93%, Test: 87.00%
Epoch: 175, Loss: 0.3317, Train: 86.80%, Valid: 86.72%, Test: 86.87%
Run 01:
Highest Train: 87.64
Highest Valid: 87.50
  Final Train: 87.64
   Final Test: 87.72
All runs:
Highest Train: 87.64, nan
Highest Valid: 87.50, nan
  Final Train: 87.64, nan
   Final Test: 87.72, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 9.5610, Train: 53.09%, Valid: 53.21%, Test: 53.08%
Epoch: 25, Loss: 4.1142, Train: 85.49%, Valid: 85.34%, Test: 85.56%
Epoch: 50, Loss: 4.1812, Train: 86.71%, Valid: 86.67%, Test: 86.74%
Epoch: 75, Loss: 3.1516, Train: 82.50%, Valid: 82.61%, Test: 82.80%
Epoch: 100, Loss: 2.0771, Train: 86.17%, Valid: 86.07%, Test: 86.26%
Epoch: 125, Loss: 1.6442, Train: 83.21%, Valid: 83.27%, Test: 83.41%
Epoch: 150, Loss: 1.2522, Train: 85.45%, Valid: 85.29%, Test: 85.51%
Epoch: 175, Loss: 0.7037, Train: 85.22%, Valid: 85.08%, Test: 85.26%
Run 01:
Highest Train: 86.85
Highest Valid: 86.81
  Final Train: 86.85
   Final Test: 86.87
All runs:
Highest Train: 86.85, nan
Highest Valid: 86.81, nan
  Final Train: 86.85, nan
   Final Test: 86.87, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 13.8151, Train: 86.65%, Valid: 86.66%, Test: 86.80%
Epoch: 25, Loss: 0.7926, Train: 86.22%, Valid: 86.27%, Test: 86.26%
Epoch: 50, Loss: 0.4019, Train: 86.43%, Valid: 86.35%, Test: 86.47%
Epoch: 75, Loss: 0.3617, Train: 87.27%, Valid: 87.13%, Test: 87.38%
Epoch: 100, Loss: 0.3451, Train: 87.01%, Valid: 86.78%, Test: 87.04%
Epoch: 125, Loss: 0.3411, Train: 85.90%, Valid: 85.71%, Test: 85.95%
Epoch: 150, Loss: 0.3357, Train: 87.56%, Valid: 87.41%, Test: 87.56%
Epoch: 175, Loss: 0.3317, Train: 87.48%, Valid: 87.29%, Test: 87.50%
Run 01:
Highest Train: 87.76
Highest Valid: 87.73
  Final Train: 87.76
   Final Test: 87.73
All runs:
Highest Train: 87.76, nan
Highest Valid: 87.73, nan
  Final Train: 87.76, nan
   Final Test: 87.73, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 9.1292, Train: 86.26%, Valid: 86.14%, Test: 86.35%
Epoch: 25, Loss: 0.6757, Train: 85.49%, Valid: 85.32%, Test: 85.55%
Epoch: 50, Loss: 0.3986, Train: 85.89%, Valid: 85.71%, Test: 85.96%
Epoch: 75, Loss: 0.3554, Train: 83.82%, Valid: 83.83%, Test: 84.00%
Epoch: 100, Loss: 0.3446, Train: 85.61%, Valid: 85.41%, Test: 85.70%
Epoch: 125, Loss: 0.3412, Train: 86.57%, Valid: 86.54%, Test: 86.72%
Epoch: 150, Loss: 0.3367, Train: 85.82%, Valid: 85.72%, Test: 85.86%
Epoch: 175, Loss: 0.3381, Train: 85.55%, Valid: 85.51%, Test: 85.68%
Run 01:
Highest Train: 87.40
Highest Valid: 87.40
  Final Train: 87.40
   Final Test: 87.49
All runs:
Highest Train: 87.40, nan
Highest Valid: 87.40, nan
  Final Train: 87.40, nan
   Final Test: 87.49, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 70.5387, Train: 73.34%, Valid: 73.51%, Test: 73.70%
Epoch: 25, Loss: 6.2350, Train: 84.14%, Valid: 84.16%, Test: 84.26%
Epoch: 50, Loss: 6.1524, Train: 82.09%, Valid: 82.20%, Test: 82.40%
Epoch: 75, Loss: 4.8970, Train: 81.59%, Valid: 81.70%, Test: 81.86%
Epoch: 100, Loss: 3.8141, Train: 85.55%, Valid: 85.46%, Test: 85.65%
Epoch: 125, Loss: 2.6848, Train: 86.38%, Valid: 86.32%, Test: 86.48%
Epoch: 150, Loss: 2.2859, Train: 86.63%, Valid: 86.63%, Test: 86.68%
Epoch: 175, Loss: 1.9912, Train: 84.90%, Valid: 84.92%, Test: 85.09%
Run 01:
Highest Train: 87.20
Highest Valid: 87.19
  Final Train: 87.20
   Final Test: 87.28
All runs:
Highest Train: 87.20, nan
Highest Valid: 87.19, nan
  Final Train: 87.20, nan
   Final Test: 87.28, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 5.2782, Train: 85.65%, Valid: 85.56%, Test: 85.67%
Epoch: 25, Loss: 0.4752, Train: 85.71%, Valid: 85.60%, Test: 85.73%
Epoch: 50, Loss: 0.3799, Train: 85.87%, Valid: 85.71%, Test: 85.86%
Epoch: 75, Loss: 0.3575, Train: 84.94%, Valid: 84.69%, Test: 85.06%
Epoch: 100, Loss: 0.3504, Train: 86.05%, Valid: 85.89%, Test: 86.06%
Epoch: 125, Loss: 0.3524, Train: 85.60%, Valid: 85.55%, Test: 85.74%
Epoch: 150, Loss: 0.3426, Train: 85.55%, Valid: 85.54%, Test: 85.63%
Epoch: 175, Loss: 0.3393, Train: 85.71%, Valid: 85.74%, Test: 85.84%
Run 01:
Highest Train: 87.67
Highest Valid: 87.47
  Final Train: 87.67
   Final Test: 87.71
All runs:
Highest Train: 87.67, nan
Highest Valid: 87.47, nan
  Final Train: 87.67, nan
   Final Test: 87.71, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 7.7423, Train: 86.11%, Valid: 85.93%, Test: 86.16%
Epoch: 25, Loss: 0.6592, Train: 85.94%, Valid: 85.79%, Test: 86.00%
Epoch: 50, Loss: 0.3945, Train: 85.57%, Valid: 85.37%, Test: 85.64%
Epoch: 75, Loss: 0.3623, Train: 86.16%, Valid: 86.16%, Test: 86.25%
Epoch: 100, Loss: 0.3541, Train: 86.52%, Valid: 86.35%, Test: 86.58%
Epoch: 125, Loss: 0.3462, Train: 86.49%, Valid: 86.33%, Test: 86.55%
Epoch: 150, Loss: 0.3425, Train: 86.45%, Valid: 86.35%, Test: 86.46%
Epoch: 175, Loss: 0.3395, Train: 87.01%, Valid: 86.98%, Test: 87.06%
Run 01:
Highest Train: 87.22
Highest Valid: 87.16
  Final Train: 87.22
   Final Test: 87.27
All runs:
Highest Train: 87.22, nan
Highest Valid: 87.16, nan
  Final Train: 87.22, nan
   Final Test: 87.27, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 8.0893, Train: 50.36%, Valid: 50.63%, Test: 50.70%
Epoch: 25, Loss: 4.8125, Train: 85.13%, Valid: 84.99%, Test: 85.19%
Epoch: 50, Loss: 3.1934, Train: 83.74%, Valid: 83.82%, Test: 83.94%
Epoch: 75, Loss: 2.5209, Train: 82.39%, Valid: 82.49%, Test: 82.67%
Epoch: 100, Loss: 1.7597, Train: 82.08%, Valid: 82.16%, Test: 82.35%
Epoch: 125, Loss: 1.4572, Train: 81.92%, Valid: 82.01%, Test: 82.22%
Epoch: 150, Loss: 1.0628, Train: 82.31%, Valid: 82.38%, Test: 82.56%
Epoch: 175, Loss: 0.7203, Train: 78.62%, Valid: 78.71%, Test: 79.01%
Run 01:
Highest Train: 87.11
Highest Valid: 87.09
  Final Train: 87.11
   Final Test: 87.16
All runs:
Highest Train: 87.11, nan
Highest Valid: 87.09, nan
  Final Train: 87.11, nan
   Final Test: 87.16, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.7946, Train: 84.45%, Valid: 84.24%, Test: 84.55%
Epoch: 25, Loss: 183958.3125, Train: 85.14%, Valid: 85.14%, Test: 85.28%
Epoch: 50, Loss: 45301.6367, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 256.4538, Train: 84.53%, Valid: 84.54%, Test: 84.67%
Epoch: 100, Loss: 37.3894, Train: 84.56%, Valid: 84.57%, Test: 84.69%
Epoch: 125, Loss: 1170.7533, Train: 84.46%, Valid: 84.48%, Test: 84.60%
Epoch: 150, Loss: 41.3889, Train: 84.51%, Valid: 84.52%, Test: 84.65%
Epoch: 175, Loss: 41.9448, Train: 84.49%, Valid: 84.50%, Test: 84.62%
Run 01:
Highest Train: 88.13
Highest Valid: 88.19
  Final Train: 88.12
   Final Test: 88.16
All runs:
Highest Train: 88.13, nan
Highest Valid: 88.19, nan
  Final Train: 88.12, nan
   Final Test: 88.16, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 12.2109, Train: 86.17%, Valid: 86.15%, Test: 86.17%
Epoch: 25, Loss: 1.8904, Train: 85.79%, Valid: 85.66%, Test: 85.85%
Epoch: 50, Loss: 10.6777, Train: 85.72%, Valid: 85.49%, Test: 85.73%
Epoch: 75, Loss: 16.4758, Train: 84.93%, Valid: 84.75%, Test: 84.99%
Epoch: 100, Loss: 13.6365, Train: 85.70%, Valid: 85.52%, Test: 85.70%
Epoch: 125, Loss: 7.3962, Train: 84.72%, Valid: 84.54%, Test: 84.76%
Epoch: 150, Loss: 5.3222, Train: 85.66%, Valid: 85.49%, Test: 85.72%
Epoch: 175, Loss: 4.9594, Train: 86.59%, Valid: 86.47%, Test: 86.60%
Run 01:
Highest Train: 87.96
Highest Valid: 87.86
  Final Train: 87.96
   Final Test: 87.96
All runs:
Highest Train: 87.96, nan
Highest Valid: 87.86, nan
  Final Train: 87.96, nan
   Final Test: 87.96, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 32982.0039, Train: 19.79%, Valid: 19.95%, Test: 19.92%
Epoch: 25, Loss: 35.3252, Train: 80.41%, Valid: 80.31%, Test: 80.32%
Epoch: 50, Loss: 12.1805, Train: 85.14%, Valid: 85.13%, Test: 85.28%
Epoch: 75, Loss: 7.7659, Train: 85.57%, Valid: 85.57%, Test: 85.82%
Epoch: 100, Loss: 7.2727, Train: 85.85%, Valid: 85.81%, Test: 86.02%
Epoch: 125, Loss: 6.5748, Train: 85.77%, Valid: 85.69%, Test: 85.93%
Epoch: 150, Loss: 5.8195, Train: 85.63%, Valid: 85.51%, Test: 85.75%
Epoch: 175, Loss: 5.0463, Train: 85.60%, Valid: 85.46%, Test: 85.69%
Run 01:
Highest Train: 85.88
Highest Valid: 85.84
  Final Train: 85.88
   Final Test: 86.02
All runs:
Highest Train: 85.88, nan
Highest Valid: 85.84, nan
  Final Train: 85.88, nan
   Final Test: 86.02, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 18.8050, Train: 86.35%, Valid: 86.38%, Test: 86.44%
Epoch: 25, Loss: 5.7959, Train: 84.79%, Valid: 84.79%, Test: 84.91%
Epoch: 50, Loss: 0.5311, Train: 87.20%, Valid: 87.08%, Test: 87.23%
Epoch: 75, Loss: 0.3579, Train: 86.09%, Valid: 85.94%, Test: 86.14%
Epoch: 100, Loss: 0.3491, Train: 85.41%, Valid: 85.43%, Test: 85.51%
Epoch: 125, Loss: 0.3377, Train: 85.65%, Valid: 85.65%, Test: 85.74%
Epoch: 150, Loss: 0.3361, Train: 85.90%, Valid: 85.88%, Test: 85.98%
Epoch: 175, Loss: 0.3331, Train: 85.73%, Valid: 85.71%, Test: 85.81%
Run 01:
Highest Train: 88.35
Highest Valid: 88.36
  Final Train: 88.35
   Final Test: 88.41
All runs:
Highest Train: 88.35, nan
Highest Valid: 88.36, nan
  Final Train: 88.35, nan
   Final Test: 88.41, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 6.8551, Train: 85.75%, Valid: 85.58%, Test: 85.80%
Epoch: 25, Loss: 3.4490, Train: 85.40%, Valid: 85.15%, Test: 85.42%
Epoch: 50, Loss: 0.3730, Train: 87.59%, Valid: 87.60%, Test: 87.67%
Epoch: 75, Loss: 0.3539, Train: 86.45%, Valid: 86.28%, Test: 86.53%
Epoch: 100, Loss: 0.3458, Train: 85.50%, Valid: 85.30%, Test: 85.58%
Epoch: 125, Loss: 0.3371, Train: 85.68%, Valid: 85.48%, Test: 85.75%
Epoch: 150, Loss: 0.3340, Train: 85.72%, Valid: 85.53%, Test: 85.78%
Epoch: 175, Loss: 0.3315, Train: 86.21%, Valid: 86.12%, Test: 86.18%
Run 01:
Highest Train: 88.16
Highest Valid: 88.03
  Final Train: 88.16
   Final Test: 88.15
All runs:
Highest Train: 88.16, nan
Highest Valid: 88.03, nan
  Final Train: 88.16, nan
   Final Test: 88.15, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 39.4635, Train: 56.56%, Valid: 56.92%, Test: 57.01%
Epoch: 25, Loss: 8.9131, Train: 84.99%, Valid: 84.86%, Test: 85.06%
Epoch: 50, Loss: 3.9498, Train: 85.08%, Valid: 84.96%, Test: 85.15%
Epoch: 75, Loss: 3.1870, Train: 83.49%, Valid: 83.52%, Test: 83.57%
Epoch: 100, Loss: 2.9077, Train: 82.50%, Valid: 82.61%, Test: 82.75%
Epoch: 125, Loss: 2.3071, Train: 85.16%, Valid: 85.02%, Test: 85.22%
Epoch: 150, Loss: 1.9347, Train: 85.57%, Valid: 85.40%, Test: 85.64%
Epoch: 175, Loss: 1.4774, Train: 86.29%, Valid: 86.12%, Test: 86.33%
Run 01:
Highest Train: 87.94
Highest Valid: 87.83
  Final Train: 87.94
   Final Test: 87.89
All runs:
Highest Train: 87.94, nan
Highest Valid: 87.83, nan
  Final Train: 87.94, nan
   Final Test: 87.89, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 3.7291, Train: 85.73%, Valid: 85.53%, Test: 85.84%
Epoch: 25, Loss: 5.4230, Train: 15.24%, Valid: 15.31%, Test: 15.09%
Epoch: 50, Loss: 0.5107, Train: 87.45%, Valid: 87.30%, Test: 87.48%
Epoch: 75, Loss: 0.3574, Train: 85.86%, Valid: 85.80%, Test: 85.94%
Epoch: 100, Loss: 0.3407, Train: 86.00%, Valid: 86.00%, Test: 86.10%
Epoch: 125, Loss: 0.3363, Train: 87.40%, Valid: 87.38%, Test: 87.45%
Epoch: 150, Loss: 0.3324, Train: 87.41%, Valid: 87.43%, Test: 87.47%
Epoch: 175, Loss: 0.3319, Train: 86.59%, Valid: 86.53%, Test: 86.60%
Run 01:
Highest Train: 88.65
Highest Valid: 88.67
  Final Train: 88.65
   Final Test: 88.69
All runs:
Highest Train: 88.65, nan
Highest Valid: 88.67, nan
  Final Train: 88.65, nan
   Final Test: 88.69, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 4.4511, Train: 85.31%, Valid: 85.35%, Test: 85.49%
Epoch: 25, Loss: 1.9024, Train: 87.01%, Valid: 87.07%, Test: 87.09%
Epoch: 50, Loss: 0.4308, Train: 86.22%, Valid: 86.17%, Test: 86.32%
Epoch: 75, Loss: 0.3572, Train: 87.46%, Valid: 87.50%, Test: 87.56%
Epoch: 100, Loss: 0.3431, Train: 85.59%, Valid: 85.36%, Test: 85.67%
Epoch: 125, Loss: 0.3375, Train: 87.23%, Valid: 87.18%, Test: 87.30%
Epoch: 150, Loss: 0.3344, Train: 85.93%, Valid: 85.74%, Test: 85.98%
Epoch: 175, Loss: 0.3325, Train: 87.63%, Valid: 87.64%, Test: 87.66%
Run 01:
Highest Train: 88.01
Highest Valid: 87.92
  Final Train: 88.01
   Final Test: 88.03
All runs:
Highest Train: 88.01, nan
Highest Valid: 87.92, nan
  Final Train: 88.01, nan
   Final Test: 88.03, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 17.6140, Train: 53.06%, Valid: 53.33%, Test: 53.32%
Epoch: 25, Loss: 16.1486, Train: 84.81%, Valid: 84.63%, Test: 84.92%
Epoch: 50, Loss: 3.8020, Train: 84.36%, Valid: 84.29%, Test: 84.40%
Epoch: 75, Loss: 2.3949, Train: 84.93%, Valid: 84.81%, Test: 85.00%
Epoch: 100, Loss: 2.4293, Train: 84.93%, Valid: 84.80%, Test: 84.99%
Epoch: 125, Loss: 1.5886, Train: 85.05%, Valid: 84.92%, Test: 85.11%
Epoch: 150, Loss: 1.7815, Train: 84.43%, Valid: 84.29%, Test: 84.46%
Epoch: 175, Loss: 1.4300, Train: 85.01%, Valid: 84.87%, Test: 85.06%
Run 01:
Highest Train: 85.68
Highest Valid: 85.55
  Final Train: 85.68
   Final Test: 85.76
All runs:
Highest Train: 85.68, nan
Highest Valid: 85.55, nan
  Final Train: 85.68, nan
   Final Test: 85.76, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 26.8963, Train: 85.58%, Valid: 85.36%, Test: 85.65%
Epoch: 25, Loss: 2.5369, Train: 85.06%, Valid: 85.10%, Test: 85.17%
Epoch: 50, Loss: 0.3991, Train: 87.00%, Valid: 87.01%, Test: 87.06%
Epoch: 75, Loss: 0.3676, Train: 85.54%, Valid: 85.53%, Test: 85.62%
Epoch: 100, Loss: 0.3565, Train: 85.50%, Valid: 85.47%, Test: 85.64%
Epoch: 125, Loss: 0.3489, Train: 85.47%, Valid: 85.41%, Test: 85.57%
Epoch: 150, Loss: 0.3424, Train: 85.83%, Valid: 85.84%, Test: 85.93%
Epoch: 175, Loss: 0.3381, Train: 85.88%, Valid: 85.86%, Test: 85.96%
Run 01:
Highest Train: 88.14
Highest Valid: 88.19
  Final Train: 88.09
   Final Test: 88.14
All runs:
Highest Train: 88.14, nan
Highest Valid: 88.19, nan
  Final Train: 88.09, nan
   Final Test: 88.14, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 6.9900, Train: 85.73%, Valid: 85.69%, Test: 85.81%
Epoch: 25, Loss: 2.9544, Train: 85.60%, Valid: 85.39%, Test: 85.63%
Epoch: 50, Loss: 0.3751, Train: 85.78%, Valid: 85.61%, Test: 85.85%
Epoch: 75, Loss: 0.3538, Train: 85.93%, Valid: 85.76%, Test: 86.00%
Epoch: 100, Loss: 0.3416, Train: 85.91%, Valid: 85.88%, Test: 86.02%
Epoch: 125, Loss: 0.3387, Train: 86.79%, Valid: 86.75%, Test: 86.90%
Epoch: 150, Loss: 0.3339, Train: 87.00%, Valid: 86.86%, Test: 87.11%
Epoch: 175, Loss: 0.3324, Train: 87.11%, Valid: 86.94%, Test: 87.23%
Run 01:
Highest Train: 87.40
Highest Valid: 87.41
  Final Train: 87.37
   Final Test: 87.44
All runs:
Highest Train: 87.40, nan
Highest Valid: 87.41, nan
  Final Train: 87.37, nan
   Final Test: 87.44, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 213.7020, Train: 83.72%, Valid: 83.63%, Test: 83.76%
Epoch: 25, Loss: 15.8324, Train: 84.72%, Valid: 84.55%, Test: 84.83%
Epoch: 50, Loss: 5.8468, Train: 84.38%, Valid: 84.28%, Test: 84.40%
Epoch: 75, Loss: 4.4196, Train: 84.38%, Valid: 84.28%, Test: 84.41%
Epoch: 100, Loss: 2.3736, Train: 84.24%, Valid: 84.15%, Test: 84.32%
Epoch: 125, Loss: 1.8813, Train: 77.27%, Valid: 77.31%, Test: 77.57%
Epoch: 150, Loss: 1.9691, Train: 84.89%, Valid: 84.77%, Test: 84.97%
Epoch: 175, Loss: 1.2628, Train: 84.71%, Valid: 84.61%, Test: 84.79%
Run 01:
Highest Train: 87.02
Highest Valid: 86.89
  Final Train: 87.02
   Final Test: 86.92
All runs:
Highest Train: 87.02, nan
Highest Valid: 86.89, nan
  Final Train: 87.02, nan
   Final Test: 86.92, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 20.4355, Train: 85.10%, Valid: 84.93%, Test: 85.25%
Epoch: 25, Loss: 468.0527, Train: 16.40%, Valid: 16.37%, Test: 16.34%
Epoch: 50, Loss: 701.1146, Train: 83.02%, Valid: 82.89%, Test: 83.11%
Epoch: 75, Loss: 676.7261, Train: 83.34%, Valid: 83.22%, Test: 83.47%
Epoch: 100, Loss: 860.7717, Train: 83.72%, Valid: 83.56%, Test: 83.84%
Epoch: 125, Loss: 907.9088, Train: 83.42%, Valid: 83.29%, Test: 83.53%
Epoch: 150, Loss: 802.9147, Train: 16.87%, Valid: 16.80%, Test: 16.79%
Epoch: 175, Loss: 888.6423, Train: 83.58%, Valid: 83.45%, Test: 83.67%
Run 01:
Highest Train: 87.98
Highest Valid: 87.98
  Final Train: 87.98
   Final Test: 88.03
All runs:
Highest Train: 87.98, nan
Highest Valid: 87.98, nan
  Final Train: 87.98, nan
   Final Test: 88.03, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 14.6336, Train: 85.82%, Valid: 85.62%, Test: 85.86%
Epoch: 25, Loss: 30.2168, Train: 87.28%, Valid: 87.38%, Test: 87.34%
Epoch: 50, Loss: 11.4006, Train: 87.27%, Valid: 87.38%, Test: 87.34%
Epoch: 75, Loss: 11.5777, Train: 87.33%, Valid: 87.44%, Test: 87.40%
Epoch: 100, Loss: 11.5953, Train: 87.35%, Valid: 87.45%, Test: 87.41%
Epoch: 125, Loss: 11.5749, Train: 87.34%, Valid: 87.45%, Test: 87.41%
Epoch: 150, Loss: 11.5820, Train: 87.34%, Valid: 87.45%, Test: 87.41%
Epoch: 175, Loss: 11.5488, Train: 87.34%, Valid: 87.44%, Test: 87.40%
Run 01:
Highest Train: 87.35
Highest Valid: 87.45
  Final Train: 87.35
   Final Test: 87.41
All runs:
Highest Train: 87.35, nan
Highest Valid: 87.45, nan
  Final Train: 87.35, nan
   Final Test: 87.41, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 11013.4590, Train: 67.36%, Valid: 67.12%, Test: 67.03%
Epoch: 25, Loss: 2587.7095, Train: 82.95%, Valid: 82.69%, Test: 82.95%
Epoch: 50, Loss: 114639002140672.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 4953.8384, Train: 15.31%, Valid: 15.43%, Test: 15.21%
Epoch: 100, Loss: 27388.8301, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 56404.0547, Train: 50.90%, Valid: 50.90%, Test: 50.93%
Epoch: 150, Loss: 766160.8125, Train: 50.24%, Valid: 50.23%, Test: 50.27%
Epoch: 175, Loss: 3535619.0000, Train: 50.26%, Valid: 50.25%, Test: 50.29%
Run 01:
Highest Train: 86.56
Highest Valid: 86.40
  Final Train: 86.56
   Final Test: 86.46
All runs:
Highest Train: 86.56, nan
Highest Valid: 86.40, nan
  Final Train: 86.56, nan
   Final Test: 86.46, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 19.7232, Train: 16.46%, Valid: 16.63%, Test: 16.30%
Epoch: 25, Loss: 3.9388, Train: 85.95%, Valid: 86.00%, Test: 86.02%
Epoch: 50, Loss: 1.6020, Train: 86.40%, Valid: 86.27%, Test: 86.36%
Epoch: 75, Loss: 0.4344, Train: 86.78%, Valid: 86.54%, Test: 86.85%
Epoch: 100, Loss: 0.3467, Train: 86.21%, Valid: 86.22%, Test: 86.30%
Epoch: 125, Loss: 0.3378, Train: 85.62%, Valid: 85.57%, Test: 85.63%
Epoch: 150, Loss: 0.3375, Train: 86.20%, Valid: 86.19%, Test: 86.18%
Epoch: 175, Loss: 0.3306, Train: 86.77%, Valid: 86.76%, Test: 86.81%
Run 01:
Highest Train: 87.90
Highest Valid: 87.96
  Final Train: 87.90
   Final Test: 87.99
All runs:
Highest Train: 87.90, nan
Highest Valid: 87.96, nan
  Final Train: 87.90, nan
   Final Test: 87.99, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 5.8933, Train: 86.56%, Valid: 86.44%, Test: 86.64%
Epoch: 25, Loss: 6.1304, Train: 15.15%, Valid: 15.38%, Test: 15.04%
Epoch: 50, Loss: 0.8858, Train: 85.24%, Valid: 85.04%, Test: 85.31%
Epoch: 75, Loss: 0.3800, Train: 85.96%, Valid: 85.78%, Test: 86.03%
Epoch: 100, Loss: 0.3569, Train: 85.78%, Valid: 85.58%, Test: 85.84%
Epoch: 125, Loss: 0.3452, Train: 85.71%, Valid: 85.50%, Test: 85.77%
Epoch: 150, Loss: 0.3406, Train: 86.34%, Valid: 86.18%, Test: 86.35%
Epoch: 175, Loss: 0.3336, Train: 86.11%, Valid: 86.03%, Test: 86.07%
Run 01:
Highest Train: 87.43
Highest Valid: 87.43
  Final Train: 87.43
   Final Test: 87.47
All runs:
Highest Train: 87.43, nan
Highest Valid: 87.43, nan
  Final Train: 87.43, nan
   Final Test: 87.47, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 49.2690, Train: 68.28%, Valid: 68.45%, Test: 68.62%
Epoch: 25, Loss: 11.4028, Train: 85.34%, Valid: 85.21%, Test: 85.41%
Epoch: 50, Loss: 5.3727, Train: 85.62%, Valid: 85.46%, Test: 85.67%
Epoch: 75, Loss: 3.7093, Train: 84.61%, Valid: 84.64%, Test: 84.71%
Epoch: 100, Loss: 3.3279, Train: 83.99%, Valid: 84.06%, Test: 84.14%
Epoch: 125, Loss: 2.5280, Train: 84.75%, Valid: 84.73%, Test: 84.83%
Epoch: 150, Loss: 2.2621, Train: 84.09%, Valid: 84.11%, Test: 84.27%
Epoch: 175, Loss: 1.9670, Train: 84.04%, Valid: 84.01%, Test: 84.16%
Run 01:
Highest Train: 86.44
Highest Valid: 86.32
  Final Train: 86.44
   Final Test: 86.50
All runs:
Highest Train: 86.44, nan
Highest Valid: 86.32, nan
  Final Train: 86.44, nan
   Final Test: 86.50, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 8.1418, Train: 85.49%, Valid: 85.49%, Test: 85.62%
Epoch: 25, Loss: 3.2174, Train: 86.12%, Valid: 86.15%, Test: 86.17%
Epoch: 50, Loss: 0.3881, Train: 86.56%, Valid: 86.44%, Test: 86.65%
Epoch: 75, Loss: 0.3573, Train: 85.85%, Valid: 85.78%, Test: 85.90%
Epoch: 100, Loss: 0.3442, Train: 85.69%, Valid: 85.64%, Test: 85.77%
Epoch: 125, Loss: 0.3377, Train: 86.11%, Valid: 86.08%, Test: 86.18%
Epoch: 150, Loss: 0.3349, Train: 86.15%, Valid: 86.12%, Test: 86.23%
Epoch: 175, Loss: 0.3327, Train: 86.19%, Valid: 86.16%, Test: 86.27%
Run 01:
Highest Train: 88.13
Highest Valid: 88.14
  Final Train: 88.13
   Final Test: 88.18
All runs:
Highest Train: 88.13, nan
Highest Valid: 88.14, nan
  Final Train: 88.13, nan
   Final Test: 88.18, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 6.6179, Train: 85.86%, Valid: 85.74%, Test: 85.87%
Epoch: 25, Loss: 4.9805, Train: 84.76%, Valid: 84.72%, Test: 84.98%
Epoch: 50, Loss: 0.5173, Train: 86.16%, Valid: 86.02%, Test: 86.22%
Epoch: 75, Loss: 0.3587, Train: 85.79%, Valid: 85.61%, Test: 85.85%
Epoch: 100, Loss: 0.3479, Train: 85.74%, Valid: 85.56%, Test: 85.81%
Epoch: 125, Loss: 0.3416, Train: 85.63%, Valid: 85.43%, Test: 85.71%
Epoch: 150, Loss: 0.3374, Train: 87.38%, Valid: 87.35%, Test: 87.42%
Epoch: 175, Loss: 0.3356, Train: 86.82%, Valid: 86.78%, Test: 86.86%
Run 01:
Highest Train: 87.38
Highest Valid: 87.35
  Final Train: 87.38
   Final Test: 87.42
All runs:
Highest Train: 87.38, nan
Highest Valid: 87.35, nan
  Final Train: 87.38, nan
   Final Test: 87.42, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 84.6924, Train: 70.96%, Valid: 71.06%, Test: 71.36%
Epoch: 25, Loss: 9.3549, Train: 85.40%, Valid: 85.26%, Test: 85.47%
Epoch: 50, Loss: 7.4250, Train: 85.57%, Valid: 85.43%, Test: 85.65%
Epoch: 75, Loss: 3.5250, Train: 84.81%, Valid: 84.70%, Test: 84.88%
Epoch: 100, Loss: 3.3053, Train: 85.50%, Valid: 85.36%, Test: 85.52%
Epoch: 125, Loss: 2.6496, Train: 84.97%, Valid: 84.83%, Test: 85.05%
Epoch: 150, Loss: 2.6700, Train: 85.45%, Valid: 85.31%, Test: 85.52%
Epoch: 175, Loss: 1.3151, Train: 85.40%, Valid: 85.25%, Test: 85.46%
Run 01:
Highest Train: 87.01
Highest Valid: 86.91
  Final Train: 87.01
   Final Test: 87.19
All runs:
Highest Train: 87.01, nan
Highest Valid: 86.91, nan
  Final Train: 87.01, nan
   Final Test: 87.19, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 14.3686, Train: 87.63%, Valid: 87.66%, Test: 87.61%
Epoch: 25, Loss: 1.6352, Train: 87.97%, Valid: 88.05%, Test: 88.00%
Epoch: 50, Loss: 0.3882, Train: 85.67%, Valid: 85.71%, Test: 85.73%
Epoch: 75, Loss: 0.3686, Train: 85.92%, Valid: 85.94%, Test: 86.00%
Epoch: 100, Loss: 0.3565, Train: 85.83%, Valid: 85.84%, Test: 85.93%
Epoch: 125, Loss: 0.3483, Train: 85.82%, Valid: 85.84%, Test: 85.92%
Epoch: 150, Loss: 0.3393, Train: 85.82%, Valid: 85.78%, Test: 85.91%
Epoch: 175, Loss: 0.3388, Train: 85.92%, Valid: 85.85%, Test: 85.97%
Run 01:
Highest Train: 88.01
Highest Valid: 88.07
  Final Train: 88.01
   Final Test: 88.05
All runs:
Highest Train: 88.01, nan
Highest Valid: 88.07, nan
  Final Train: 88.01, nan
   Final Test: 88.05, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 7.8351, Train: 86.15%, Valid: 86.02%, Test: 86.20%
Epoch: 25, Loss: 2.1856, Train: 85.44%, Valid: 85.23%, Test: 85.48%
Epoch: 50, Loss: 0.3944, Train: 86.35%, Valid: 86.21%, Test: 86.38%
Epoch: 75, Loss: 0.3557, Train: 84.90%, Valid: 84.85%, Test: 85.08%
Epoch: 100, Loss: 0.3490, Train: 85.66%, Valid: 85.48%, Test: 85.75%
Epoch: 125, Loss: 0.3429, Train: 86.81%, Valid: 86.66%, Test: 86.85%
Epoch: 150, Loss: 0.3407, Train: 86.05%, Valid: 85.88%, Test: 86.13%
Epoch: 175, Loss: 0.3366, Train: 86.66%, Valid: 86.56%, Test: 86.72%
Run 01:
Highest Train: 88.14
Highest Valid: 88.02
  Final Train: 88.14
   Final Test: 88.19
All runs:
Highest Train: 88.14, nan
Highest Valid: 88.02, nan
  Final Train: 88.14, nan
   Final Test: 88.19, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 346.9710, Train: 69.42%, Valid: 69.56%, Test: 69.73%
Epoch: 25, Loss: 5.8550, Train: 78.91%, Valid: 79.02%, Test: 79.30%
Epoch: 50, Loss: 5.2036, Train: 79.36%, Valid: 79.44%, Test: 79.69%
Epoch: 75, Loss: 3.8372, Train: 82.19%, Valid: 82.24%, Test: 82.43%
Epoch: 100, Loss: 2.8893, Train: 79.33%, Valid: 79.31%, Test: 79.61%
Epoch: 125, Loss: 2.8824, Train: 85.36%, Valid: 85.24%, Test: 85.45%
Epoch: 150, Loss: 2.1197, Train: 85.63%, Valid: 85.50%, Test: 85.71%
Epoch: 175, Loss: 1.4730, Train: 82.75%, Valid: 82.80%, Test: 83.07%
Run 01:
Highest Train: 87.25
Highest Valid: 87.13
  Final Train: 87.25
   Final Test: 87.31
All runs:
Highest Train: 87.25, nan
Highest Valid: 87.13, nan
  Final Train: 87.25, nan
   Final Test: 87.31, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.6217, Train: 86.29%, Valid: 86.26%, Test: 86.42%
Epoch: 25, Loss: 7.0444, Train: 85.95%, Valid: 86.00%, Test: 86.03%
Epoch: 50, Loss: 12.0905, Train: 84.40%, Valid: 84.22%, Test: 84.54%
Epoch: 75, Loss: 12.4588, Train: 84.40%, Valid: 84.22%, Test: 84.52%
Epoch: 100, Loss: 12.5707, Train: 84.46%, Valid: 84.25%, Test: 84.58%
Epoch: 125, Loss: 12.4850, Train: 84.46%, Valid: 84.26%, Test: 84.57%
Epoch: 150, Loss: 12.5232, Train: 84.39%, Valid: 84.22%, Test: 84.53%
Epoch: 175, Loss: 12.5140, Train: 84.40%, Valid: 84.21%, Test: 84.53%
Run 01:
Highest Train: 87.59
Highest Valid: 87.59
  Final Train: 87.59
   Final Test: 87.70
All runs:
Highest Train: 87.59, nan
Highest Valid: 87.59, nan
  Final Train: 87.59, nan
   Final Test: 87.70, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 21.1742, Train: 86.63%, Valid: 86.72%, Test: 86.71%
Epoch: 25, Loss: 2654462720.0000, Train: 49.99%, Valid: 49.99%, Test: 50.00%
Epoch: 50, Loss: 37709408956922763614988997230592.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 99170923713658880.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 13695853568.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 471419584512.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 38818601066259546112.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 650898995173916672.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 87.56
Highest Valid: 87.53
  Final Train: 87.56
   Final Test: 87.59
All runs:
Highest Train: 87.56, nan
Highest Valid: 87.53, nan
  Final Train: 87.56, nan
   Final Test: 87.59, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2045.8391, Train: 84.10%, Valid: 83.98%, Test: 84.15%
Epoch: 25, Loss: 1616.5132, Train: 15.18%, Valid: 15.36%, Test: 15.04%
Epoch: 50, Loss: 8227.0654, Train: 15.11%, Valid: 15.29%, Test: 14.99%
Epoch: 75, Loss: 1481.9015, Train: 15.14%, Valid: 15.32%, Test: 15.02%
Epoch: 100, Loss: 1826.1653, Train: 15.12%, Valid: 15.30%, Test: 14.99%
Epoch: 125, Loss: 1863.7432, Train: 15.11%, Valid: 15.29%, Test: 14.98%
Epoch: 150, Loss: 1531.3245, Train: 72.66%, Valid: 72.80%, Test: 72.98%
Epoch: 175, Loss: 2571.7634, Train: 15.21%, Valid: 15.38%, Test: 15.09%
Run 01:
Highest Train: 85.09
Highest Valid: 84.92
  Final Train: 85.08
   Final Test: 85.08
All runs:
Highest Train: 85.09, nan
Highest Valid: 84.92, nan
  Final Train: 85.08, nan
   Final Test: 85.08, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.7663, Train: 85.53%, Valid: 85.36%, Test: 85.67%
Epoch: 25, Loss: 4.8091, Train: 86.94%, Valid: 86.80%, Test: 87.00%
Epoch: 50, Loss: 0.4898, Train: 86.40%, Valid: 86.45%, Test: 86.46%
Epoch: 75, Loss: 0.3664, Train: 86.69%, Valid: 86.76%, Test: 86.81%
Epoch: 100, Loss: 0.3584, Train: 85.82%, Valid: 85.82%, Test: 85.90%
Epoch: 125, Loss: 0.3475, Train: 85.66%, Valid: 85.63%, Test: 85.75%
Epoch: 150, Loss: 0.3385, Train: 85.57%, Valid: 85.56%, Test: 85.67%
Epoch: 175, Loss: 0.3317, Train: 86.04%, Valid: 86.03%, Test: 86.12%
Run 01:
Highest Train: 88.04
Highest Valid: 88.02
  Final Train: 87.97
   Final Test: 88.01
All runs:
Highest Train: 88.04, nan
Highest Valid: 88.02, nan
  Final Train: 87.97, nan
   Final Test: 88.01, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 4.1198, Train: 86.57%, Valid: 86.61%, Test: 86.61%
Epoch: 25, Loss: 3.8601, Train: 86.12%, Valid: 85.97%, Test: 86.20%
Epoch: 50, Loss: 0.4326, Train: 86.24%, Valid: 86.07%, Test: 86.30%
Epoch: 75, Loss: 0.3579, Train: 86.24%, Valid: 86.10%, Test: 86.31%
Epoch: 100, Loss: 0.3455, Train: 85.56%, Valid: 85.37%, Test: 85.65%
Epoch: 125, Loss: 0.3372, Train: 85.78%, Valid: 85.59%, Test: 85.84%
Epoch: 150, Loss: 0.3319, Train: 87.56%, Valid: 87.56%, Test: 87.63%
Epoch: 175, Loss: 0.3334, Train: 86.84%, Valid: 86.80%, Test: 86.93%
Run 01:
Highest Train: 87.61
Highest Valid: 87.63
  Final Train: 87.61
   Final Test: 87.68
All runs:
Highest Train: 87.61, nan
Highest Valid: 87.63, nan
  Final Train: 87.61, nan
   Final Test: 87.68, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 20.6846, Train: 79.56%, Valid: 79.54%, Test: 79.72%
Epoch: 25, Loss: 15.2023, Train: 82.29%, Valid: 82.26%, Test: 82.44%
Epoch: 50, Loss: 8.3837, Train: 83.59%, Valid: 83.65%, Test: 83.77%
Epoch: 75, Loss: 2.9707, Train: 82.12%, Valid: 82.22%, Test: 82.37%
Epoch: 100, Loss: 2.0404, Train: 82.97%, Valid: 83.03%, Test: 83.18%
Epoch: 125, Loss: 1.3180, Train: 82.71%, Valid: 82.79%, Test: 82.97%
Epoch: 150, Loss: 0.8456, Train: 85.59%, Valid: 85.58%, Test: 85.67%
Epoch: 175, Loss: 2.8813, Train: 85.65%, Valid: 85.51%, Test: 85.73%
Run 01:
Highest Train: 88.16
Highest Valid: 87.94
  Final Train: 88.16
   Final Test: 88.07
All runs:
Highest Train: 88.16, nan
Highest Valid: 87.94, nan
  Final Train: 88.16, nan
   Final Test: 88.07, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 5.7411, Train: 86.52%, Valid: 86.52%, Test: 86.66%
Epoch: 25, Loss: 2.1088, Train: 85.37%, Valid: 85.42%, Test: 85.46%
Epoch: 50, Loss: 0.3892, Train: 85.79%, Valid: 85.77%, Test: 85.80%
Epoch: 75, Loss: 0.3527, Train: 86.67%, Valid: 86.62%, Test: 86.78%
Epoch: 100, Loss: 0.3408, Train: 86.37%, Valid: 86.39%, Test: 86.44%
Epoch: 125, Loss: 0.3359, Train: 85.59%, Valid: 85.63%, Test: 85.70%
Epoch: 150, Loss: 0.3314, Train: 86.86%, Valid: 86.73%, Test: 86.93%
Epoch: 175, Loss: 0.3293, Train: 86.06%, Valid: 86.01%, Test: 86.11%
Run 01:
Highest Train: 88.24
Highest Valid: 88.30
  Final Train: 88.24
   Final Test: 88.28
All runs:
Highest Train: 88.24, nan
Highest Valid: 88.30, nan
  Final Train: 88.24, nan
   Final Test: 88.28, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 22.4599, Train: 87.84%, Valid: 87.79%, Test: 87.89%
Epoch: 25, Loss: 1.9129, Train: 85.87%, Valid: 85.82%, Test: 85.99%
Epoch: 50, Loss: 0.4914, Train: 86.18%, Valid: 86.18%, Test: 86.25%
Epoch: 75, Loss: 0.3676, Train: 86.50%, Valid: 86.37%, Test: 86.43%
Epoch: 100, Loss: 0.3509, Train: 85.95%, Valid: 85.76%, Test: 86.01%
Epoch: 125, Loss: 0.3459, Train: 86.16%, Valid: 85.97%, Test: 86.22%
Epoch: 150, Loss: 0.3420, Train: 86.88%, Valid: 86.84%, Test: 87.01%
Epoch: 175, Loss: 0.3397, Train: 86.62%, Valid: 86.52%, Test: 86.72%
Run 01:
Highest Train: 87.93
Highest Valid: 87.91
  Final Train: 87.93
   Final Test: 87.96
All runs:
Highest Train: 87.93, nan
Highest Valid: 87.91, nan
  Final Train: 87.93, nan
   Final Test: 87.96, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 94.2362, Train: 56.95%, Valid: 56.78%, Test: 56.61%
Epoch: 25, Loss: 5.9942, Train: 85.46%, Valid: 85.32%, Test: 85.54%
Epoch: 50, Loss: 5.8788, Train: 85.64%, Valid: 85.48%, Test: 85.70%
Epoch: 75, Loss: 4.0537, Train: 83.07%, Valid: 83.18%, Test: 83.30%
Epoch: 100, Loss: 3.7042, Train: 83.77%, Valid: 83.75%, Test: 83.90%
Epoch: 125, Loss: 3.2982, Train: 83.19%, Valid: 83.28%, Test: 83.38%
Epoch: 150, Loss: 3.1071, Train: 84.02%, Valid: 84.06%, Test: 84.22%
Epoch: 175, Loss: 2.7279, Train: 82.96%, Valid: 83.06%, Test: 83.20%
Run 01:
Highest Train: 85.89
Highest Valid: 85.80
  Final Train: 85.89
   Final Test: 85.99
All runs:
Highest Train: 85.89, nan
Highest Valid: 85.80, nan
  Final Train: 85.89, nan
   Final Test: 85.99, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 4.0217, Train: 87.84%, Valid: 87.80%, Test: 87.85%
Epoch: 25, Loss: 1.2299, Train: 85.80%, Valid: 85.67%, Test: 85.84%
Epoch: 50, Loss: 0.3766, Train: 86.13%, Valid: 86.12%, Test: 86.18%
Epoch: 75, Loss: 0.3518, Train: 85.85%, Valid: 85.88%, Test: 85.97%
Epoch: 100, Loss: 0.3443, Train: 85.66%, Valid: 85.67%, Test: 85.80%
Epoch: 125, Loss: 0.3371, Train: 85.68%, Valid: 85.67%, Test: 85.80%
Epoch: 150, Loss: 0.3353, Train: 85.55%, Valid: 85.57%, Test: 85.68%
Epoch: 175, Loss: 0.3339, Train: 85.54%, Valid: 85.54%, Test: 85.65%
Run 01:
Highest Train: 88.11
Highest Valid: 88.19
  Final Train: 88.11
   Final Test: 88.18
All runs:
Highest Train: 88.11, nan
Highest Valid: 88.19, nan
  Final Train: 88.11, nan
   Final Test: 88.18, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 11.9241, Train: 86.37%, Valid: 86.37%, Test: 86.47%
Epoch: 25, Loss: 1.9930, Train: 85.90%, Valid: 85.72%, Test: 85.97%
Epoch: 50, Loss: 0.3835, Train: 85.74%, Valid: 85.59%, Test: 85.80%
Epoch: 75, Loss: 0.3649, Train: 85.70%, Valid: 85.53%, Test: 85.77%
Epoch: 100, Loss: 0.3525, Train: 85.81%, Valid: 85.64%, Test: 85.87%
Epoch: 125, Loss: 0.3423, Train: 85.81%, Valid: 85.62%, Test: 85.87%
Epoch: 150, Loss: 0.3430, Train: 85.87%, Valid: 85.67%, Test: 85.92%
Epoch: 175, Loss: 0.3396, Train: 86.06%, Valid: 85.85%, Test: 86.11%
Run 01:
Highest Train: 87.63
Highest Valid: 87.65
  Final Train: 87.63
   Final Test: 87.69
All runs:
Highest Train: 87.63, nan
Highest Valid: 87.65, nan
  Final Train: 87.63, nan
   Final Test: 87.69, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.01, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 304.1087, Train: 23.60%, Valid: 23.59%, Test: 23.38%
Epoch: 25, Loss: 6.9054, Train: 85.47%, Valid: 85.33%, Test: 85.54%
Epoch: 50, Loss: 4.9394, Train: 83.50%, Valid: 83.58%, Test: 83.70%
Epoch: 75, Loss: 3.6452, Train: 82.81%, Valid: 82.90%, Test: 83.07%
Epoch: 100, Loss: 2.8731, Train: 82.59%, Valid: 82.68%, Test: 82.85%
Epoch: 125, Loss: 2.2502, Train: 82.89%, Valid: 82.91%, Test: 83.14%
Epoch: 150, Loss: 1.7298, Train: 83.94%, Valid: 83.83%, Test: 84.11%
Epoch: 175, Loss: 1.7633, Train: 82.70%, Valid: 82.71%, Test: 82.92%
Run 01:
Highest Train: 86.06
Highest Valid: 85.95
  Final Train: 86.06
   Final Test: 86.10
All runs:
Highest Train: 86.06, nan
Highest Valid: 85.95, nan
  Final Train: 86.06, nan
   Final Test: 86.10, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 10.3154, Train: 85.57%, Valid: 85.59%, Test: 85.69%
Epoch: 25, Loss: 0.5435, Train: 85.79%, Valid: 85.69%, Test: 85.80%
Epoch: 50, Loss: 0.4781, Train: 86.70%, Valid: 86.77%, Test: 86.74%
Epoch: 75, Loss: 0.3872, Train: 86.47%, Valid: 86.52%, Test: 86.54%
Epoch: 100, Loss: 0.3537, Train: 85.59%, Valid: 85.57%, Test: 85.68%
Epoch: 125, Loss: 0.3477, Train: 86.03%, Valid: 85.85%, Test: 86.10%
Epoch: 150, Loss: 0.3348, Train: 85.70%, Valid: 85.68%, Test: 85.79%
Epoch: 175, Loss: 0.3346, Train: 86.06%, Valid: 85.90%, Test: 86.14%
Run 01:
Highest Train: 87.22
Highest Valid: 87.14
  Final Train: 87.22
   Final Test: 87.27
All runs:
Highest Train: 87.22, nan
Highest Valid: 87.14, nan
  Final Train: 87.22, nan
   Final Test: 87.27, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 7.2953, Train: 83.99%, Valid: 83.88%, Test: 84.07%
Epoch: 25, Loss: 0.6012, Train: 86.31%, Valid: 86.25%, Test: 86.34%
Epoch: 50, Loss: 0.3726, Train: 85.61%, Valid: 85.49%, Test: 85.67%
Epoch: 75, Loss: 0.3610, Train: 85.57%, Valid: 85.42%, Test: 85.63%
Epoch: 100, Loss: 0.3539, Train: 85.55%, Valid: 85.39%, Test: 85.62%
Epoch: 125, Loss: 0.3453, Train: 85.51%, Valid: 85.35%, Test: 85.58%
Epoch: 150, Loss: 0.3347, Train: 85.37%, Valid: 85.17%, Test: 85.45%
Epoch: 175, Loss: 0.8176, Train: 85.54%, Valid: 85.37%, Test: 85.62%
Run 01:
Highest Train: 87.08
Highest Valid: 87.00
  Final Train: 87.08
   Final Test: 87.08
All runs:
Highest Train: 87.08, nan
Highest Valid: 87.00, nan
  Final Train: 87.08, nan
   Final Test: 87.08, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 96574.5234, Train: 30.28%, Valid: 30.55%, Test: 30.51%
Epoch: 25, Loss: 433.0208, Train: 26.40%, Valid: 26.46%, Test: 26.46%
Epoch: 50, Loss: 82.1676, Train: 37.33%, Valid: 37.33%, Test: 37.19%
Epoch: 75, Loss: 80.3755, Train: 39.50%, Valid: 39.51%, Test: 39.32%
Epoch: 100, Loss: 63.4458, Train: 39.00%, Valid: 39.07%, Test: 38.83%
Epoch: 125, Loss: 41.8844, Train: 43.47%, Valid: 43.44%, Test: 43.33%
Epoch: 150, Loss: 18.2865, Train: 83.77%, Valid: 83.82%, Test: 83.97%
Epoch: 175, Loss: 12.5905, Train: 84.54%, Valid: 84.55%, Test: 84.70%
Run 01:
Highest Train: 84.59
Highest Valid: 84.60
  Final Train: 84.59
   Final Test: 84.76
All runs:
Highest Train: 84.59, nan
Highest Valid: 84.60, nan
  Final Train: 84.59, nan
   Final Test: 84.76, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 16.6403, Train: 16.87%, Valid: 17.04%, Test: 16.78%
Epoch: 25, Loss: 0.6122, Train: 87.23%, Valid: 87.08%, Test: 87.30%
Epoch: 50, Loss: 0.4896, Train: 87.44%, Valid: 87.33%, Test: 87.48%
Epoch: 75, Loss: 0.4039, Train: 87.69%, Valid: 87.72%, Test: 87.73%
Epoch: 100, Loss: 0.3717, Train: 86.99%, Valid: 87.04%, Test: 87.08%
Epoch: 125, Loss: 0.3646, Train: 85.42%, Valid: 85.40%, Test: 85.49%
Epoch: 150, Loss: 0.3628, Train: 85.52%, Valid: 85.47%, Test: 85.60%
Epoch: 175, Loss: 0.3576, Train: 85.50%, Valid: 85.49%, Test: 85.59%
Run 01:
Highest Train: 88.30
Highest Valid: 88.39
  Final Train: 88.30
   Final Test: 88.35
All runs:
Highest Train: 88.30, nan
Highest Valid: 88.39, nan
  Final Train: 88.30, nan
   Final Test: 88.35, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 8.8063, Train: 15.82%, Valid: 15.77%, Test: 15.77%
Epoch: 25, Loss: 0.4090, Train: 85.34%, Valid: 85.16%, Test: 85.40%
Epoch: 50, Loss: 0.3720, Train: 85.49%, Valid: 85.35%, Test: 85.56%
Epoch: 75, Loss: 0.3594, Train: 85.58%, Valid: 85.43%, Test: 85.65%
Epoch: 100, Loss: 0.3553, Train: 85.84%, Valid: 85.65%, Test: 85.89%
Epoch: 125, Loss: 0.3461, Train: 85.56%, Valid: 85.40%, Test: 85.63%
Epoch: 150, Loss: 0.3419, Train: 85.73%, Valid: 85.56%, Test: 85.80%
Epoch: 175, Loss: 0.3360, Train: 85.62%, Valid: 85.41%, Test: 85.69%
Run 01:
Highest Train: 87.51
Highest Valid: 87.45
  Final Train: 87.51
   Final Test: 87.57
All runs:
Highest Train: 87.51, nan
Highest Valid: 87.45, nan
  Final Train: 87.51, nan
   Final Test: 87.57, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 21.6298, Train: 46.48%, Valid: 46.71%, Test: 46.82%
Epoch: 25, Loss: 4.1312, Train: 83.36%, Valid: 83.38%, Test: 83.50%
Epoch: 50, Loss: 2.2357, Train: 70.56%, Valid: 70.67%, Test: 70.94%
Epoch: 75, Loss: 1.6058, Train: 73.17%, Valid: 73.29%, Test: 73.53%
Epoch: 100, Loss: 0.8520, Train: 76.60%, Valid: 76.71%, Test: 76.94%
Epoch: 125, Loss: 0.5797, Train: 82.25%, Valid: 82.34%, Test: 82.52%
Epoch: 150, Loss: 0.5370, Train: 81.90%, Valid: 81.98%, Test: 82.21%
Epoch: 175, Loss: 0.5259, Train: 82.54%, Valid: 82.51%, Test: 82.77%
Run 01:
Highest Train: 85.24
Highest Valid: 85.12
  Final Train: 85.24
   Final Test: 85.36
All runs:
Highest Train: 85.24, nan
Highest Valid: 85.12, nan
  Final Train: 85.24, nan
   Final Test: 85.36, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 10.7593, Train: 13.66%, Valid: 13.56%, Test: 13.62%
Epoch: 25, Loss: 0.5116, Train: 86.20%, Valid: 86.24%, Test: 86.30%
Epoch: 50, Loss: 0.4188, Train: 86.00%, Valid: 86.10%, Test: 86.12%
Epoch: 75, Loss: 0.3833, Train: 85.39%, Valid: 85.41%, Test: 85.49%
Epoch: 100, Loss: 0.3713, Train: 85.39%, Valid: 85.40%, Test: 85.48%
Epoch: 125, Loss: 0.3601, Train: 85.39%, Valid: 85.42%, Test: 85.49%
Epoch: 150, Loss: 0.3528, Train: 85.59%, Valid: 85.61%, Test: 85.70%
Epoch: 175, Loss: 0.3471, Train: 85.63%, Valid: 85.66%, Test: 85.74%
Run 01:
Highest Train: 87.76
Highest Valid: 87.77
  Final Train: 87.76
   Final Test: 87.74
All runs:
Highest Train: 87.76, nan
Highest Valid: 87.77, nan
  Final Train: 87.76, nan
   Final Test: 87.74, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 4.6761, Train: 85.24%, Valid: 85.23%, Test: 85.43%
Epoch: 25, Loss: 0.5774, Train: 87.74%, Valid: 87.61%, Test: 87.66%
Epoch: 50, Loss: 0.4223, Train: 85.83%, Valid: 85.67%, Test: 85.85%
Epoch: 75, Loss: 0.3810, Train: 85.45%, Valid: 85.26%, Test: 85.50%
Epoch: 100, Loss: 0.3717, Train: 85.48%, Valid: 85.31%, Test: 85.55%
Epoch: 125, Loss: 0.3675, Train: 85.37%, Valid: 85.19%, Test: 85.43%
Epoch: 150, Loss: 0.3609, Train: 85.31%, Valid: 85.11%, Test: 85.38%
Epoch: 175, Loss: 0.3564, Train: 85.29%, Valid: 85.10%, Test: 85.39%
Run 01:
Highest Train: 87.90
Highest Valid: 87.76
  Final Train: 87.90
   Final Test: 87.82
All runs:
Highest Train: 87.90, nan
Highest Valid: 87.76, nan
  Final Train: 87.90, nan
   Final Test: 87.82, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 1257.6595, Train: 61.67%, Valid: 61.61%, Test: 61.31%
Epoch: 25, Loss: 99.3192, Train: 57.80%, Valid: 57.64%, Test: 57.49%
Epoch: 50, Loss: 10.8117, Train: 83.29%, Valid: 83.21%, Test: 83.33%
Epoch: 75, Loss: 1.8549, Train: 80.61%, Valid: 80.70%, Test: 80.98%
Epoch: 100, Loss: 1.9118, Train: 73.97%, Valid: 74.02%, Test: 74.30%
Epoch: 125, Loss: 1.8239, Train: 73.48%, Valid: 73.58%, Test: 73.82%
Epoch: 150, Loss: 2.0331, Train: 74.05%, Valid: 74.12%, Test: 74.36%
Epoch: 175, Loss: 1.9311, Train: 73.77%, Valid: 73.82%, Test: 74.09%
Run 01:
Highest Train: 85.85
Highest Valid: 85.69
  Final Train: 85.85
   Final Test: 85.89
All runs:
Highest Train: 85.85, nan
Highest Valid: 85.69, nan
  Final Train: 85.85, nan
   Final Test: 85.89, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 3.1343, Train: 86.68%, Valid: 86.80%, Test: 86.69%
Epoch: 25, Loss: 0.4380, Train: 86.08%, Valid: 85.94%, Test: 86.13%
Epoch: 50, Loss: 0.3979, Train: 86.15%, Valid: 86.06%, Test: 86.22%
Epoch: 75, Loss: 0.3794, Train: 87.20%, Valid: 87.11%, Test: 87.24%
Epoch: 100, Loss: 0.3669, Train: 87.10%, Valid: 87.00%, Test: 87.14%
Epoch: 125, Loss: 0.3574, Train: 87.52%, Valid: 87.37%, Test: 87.56%
Epoch: 150, Loss: 0.3586, Train: 87.61%, Valid: 87.48%, Test: 87.64%
Epoch: 175, Loss: 0.3527, Train: 87.54%, Valid: 87.38%, Test: 87.58%
Run 01:
Highest Train: 87.67
Highest Valid: 87.79
  Final Train: 87.67
   Final Test: 87.74
All runs:
Highest Train: 87.67, nan
Highest Valid: 87.79, nan
  Final Train: 87.67, nan
   Final Test: 87.74, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 15.2380, Train: 75.57%, Valid: 75.69%, Test: 75.98%
Epoch: 25, Loss: 0.5764, Train: 85.89%, Valid: 85.78%, Test: 85.92%
Epoch: 50, Loss: 0.5050, Train: 86.38%, Valid: 86.35%, Test: 86.47%
Epoch: 75, Loss: 0.4218, Train: 86.66%, Valid: 86.56%, Test: 86.78%
Epoch: 100, Loss: 0.3918, Train: 85.66%, Valid: 85.47%, Test: 85.72%
Epoch: 125, Loss: 0.3839, Train: 85.53%, Valid: 85.34%, Test: 85.60%
Epoch: 150, Loss: 0.3659, Train: 85.38%, Valid: 85.17%, Test: 85.45%
Epoch: 175, Loss: 0.3601, Train: 85.41%, Valid: 85.21%, Test: 85.50%
Run 01:
Highest Train: 86.98
Highest Valid: 86.77
  Final Train: 86.98
   Final Test: 86.97
All runs:
Highest Train: 86.98, nan
Highest Valid: 86.77, nan
  Final Train: 86.98, nan
   Final Test: 86.97, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 16.7201, Train: 72.28%, Valid: 72.09%, Test: 72.03%
Epoch: 25, Loss: 4.9679, Train: 81.09%, Valid: 80.94%, Test: 81.00%
Epoch: 50, Loss: 1.4137, Train: 84.83%, Valid: 84.70%, Test: 84.88%
Epoch: 75, Loss: 0.8474, Train: 85.68%, Valid: 85.51%, Test: 85.76%
Epoch: 100, Loss: 0.7171, Train: 85.51%, Valid: 85.35%, Test: 85.60%
Epoch: 125, Loss: 0.7033, Train: 85.09%, Valid: 85.07%, Test: 85.18%
Epoch: 150, Loss: 0.6520, Train: 85.36%, Valid: 85.34%, Test: 85.40%
Epoch: 175, Loss: 0.7574, Train: 85.29%, Valid: 85.16%, Test: 85.37%
Run 01:
Highest Train: 87.70
Highest Valid: 87.48
  Final Train: 87.70
   Final Test: 87.74
All runs:
Highest Train: 87.70, nan
Highest Valid: 87.48, nan
  Final Train: 87.70, nan
   Final Test: 87.74, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 5.1718, Train: 83.05%, Valid: 83.11%, Test: 83.20%
Epoch: 25, Loss: 0.4202, Train: 85.53%, Valid: 85.58%, Test: 85.65%
Epoch: 50, Loss: 0.3813, Train: 85.80%, Valid: 85.86%, Test: 85.89%
Epoch: 75, Loss: 0.3602, Train: 85.67%, Valid: 85.71%, Test: 85.76%
Epoch: 100, Loss: 0.3501, Train: 85.77%, Valid: 85.80%, Test: 85.86%
Epoch: 125, Loss: 0.3415, Train: 85.74%, Valid: 85.77%, Test: 85.86%
Epoch: 150, Loss: 0.3290, Train: 85.40%, Valid: 85.44%, Test: 85.53%
Epoch: 175, Loss: 0.3377, Train: 85.58%, Valid: 85.54%, Test: 85.64%
Run 01:
Highest Train: 87.19
Highest Valid: 87.06
  Final Train: 87.19
   Final Test: 87.13
All runs:
Highest Train: 87.19, nan
Highest Valid: 87.06, nan
  Final Train: 87.19, nan
   Final Test: 87.13, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 0.5124, Train: 85.29%, Valid: 85.20%, Test: 85.34%
Epoch: 25, Loss: 0.3872, Train: 85.46%, Valid: 85.32%, Test: 85.53%
Epoch: 50, Loss: 0.3687, Train: 85.55%, Valid: 85.40%, Test: 85.62%
Epoch: 75, Loss: 0.3549, Train: 85.66%, Valid: 85.49%, Test: 85.73%
Epoch: 100, Loss: 0.3376, Train: 85.47%, Valid: 85.30%, Test: 85.55%
Epoch: 125, Loss: 0.3399, Train: 85.88%, Valid: 85.68%, Test: 85.94%
Epoch: 150, Loss: 0.3327, Train: 85.77%, Valid: 85.56%, Test: 85.83%
Epoch: 175, Loss: 0.3286, Train: 85.69%, Valid: 85.48%, Test: 85.76%
Run 01:
Highest Train: 87.20
Highest Valid: 87.04
  Final Train: 87.20
   Final Test: 87.20
All runs:
Highest Train: 87.20, nan
Highest Valid: 87.04, nan
  Final Train: 87.20, nan
   Final Test: 87.20, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 2770100.5000, Train: 50.96%, Valid: 50.95%, Test: 50.90%
Epoch: 25, Loss: 70576144.0000, Train: 49.50%, Valid: 49.50%, Test: 49.54%
Epoch: 50, Loss: 12674174.0000, Train: 53.24%, Valid: 53.17%, Test: 53.09%
Epoch: 75, Loss: 54345256.0000, Train: 49.25%, Valid: 49.16%, Test: 49.30%
Epoch: 100, Loss: 208897472.0000, Train: 53.21%, Valid: 53.16%, Test: 53.08%
Epoch: 125, Loss: 260648256.0000, Train: 50.66%, Valid: 50.64%, Test: 50.69%
Epoch: 150, Loss: 222059568.0000, Train: 51.19%, Valid: 51.21%, Test: 51.21%
Epoch: 175, Loss: 53015124.0000, Train: 49.76%, Valid: 49.76%, Test: 49.77%
Run 01:
Highest Train: 64.95
Highest Valid: 64.68
  Final Train: 64.95
   Final Test: 64.60
All runs:
Highest Train: 64.95, nan
Highest Valid: 64.68, nan
  Final Train: 64.95, nan
   Final Test: 64.60, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 7.9787, Train: 87.12%, Valid: 87.17%, Test: 87.11%
Epoch: 25, Loss: 0.4915, Train: 86.13%, Valid: 86.21%, Test: 86.19%
Epoch: 50, Loss: 0.4329, Train: 85.72%, Valid: 85.77%, Test: 85.78%
Epoch: 75, Loss: 0.3846, Train: 85.35%, Valid: 85.29%, Test: 85.44%
Epoch: 100, Loss: 0.3650, Train: 86.38%, Valid: 86.26%, Test: 86.43%
Epoch: 125, Loss: 0.3551, Train: 86.44%, Valid: 86.31%, Test: 86.50%
Epoch: 150, Loss: 0.3480, Train: 86.84%, Valid: 86.65%, Test: 86.89%
Epoch: 175, Loss: 0.3405, Train: 85.83%, Valid: 85.81%, Test: 85.92%
Run 01:
Highest Train: 87.52
Highest Valid: 87.37
  Final Train: 87.52
   Final Test: 87.56
All runs:
Highest Train: 87.52, nan
Highest Valid: 87.37, nan
  Final Train: 87.52, nan
   Final Test: 87.56, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 4.7976, Train: 83.10%, Valid: 83.24%, Test: 83.31%
Epoch: 25, Loss: 0.4798, Train: 85.25%, Valid: 85.06%, Test: 85.32%
Epoch: 50, Loss: 0.3881, Train: 85.43%, Valid: 85.51%, Test: 85.51%
Epoch: 75, Loss: 0.3695, Train: 86.62%, Valid: 86.54%, Test: 86.65%
Epoch: 100, Loss: 0.3623, Train: 86.47%, Valid: 86.34%, Test: 86.49%
Epoch: 125, Loss: 0.3518, Train: 86.41%, Valid: 86.25%, Test: 86.42%
Epoch: 150, Loss: 0.3451, Train: 86.35%, Valid: 86.22%, Test: 86.42%
Epoch: 175, Loss: 0.3438, Train: 86.26%, Valid: 86.08%, Test: 86.36%
Run 01:
Highest Train: 87.46
Highest Valid: 87.34
  Final Train: 87.46
   Final Test: 87.53
All runs:
Highest Train: 87.46, nan
Highest Valid: 87.34, nan
  Final Train: 87.46, nan
   Final Test: 87.53, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 79.6966, Train: 48.48%, Valid: 48.51%, Test: 48.56%
Epoch: 25, Loss: 169.9534, Train: 77.85%, Valid: 77.66%, Test: 77.72%
Epoch: 50, Loss: 9.4230, Train: 78.78%, Valid: 78.60%, Test: 78.67%
Epoch: 75, Loss: 6.4822, Train: 81.43%, Valid: 81.31%, Test: 81.36%
Epoch: 100, Loss: 6.6440, Train: 81.79%, Valid: 81.68%, Test: 81.74%
Epoch: 125, Loss: 5.6305, Train: 82.53%, Valid: 82.45%, Test: 82.52%
Epoch: 150, Loss: 4.3356, Train: 83.13%, Valid: 83.05%, Test: 83.14%
Epoch: 175, Loss: 3.4048, Train: 83.34%, Valid: 83.25%, Test: 83.37%
Run 01:
Highest Train: 83.68
Highest Valid: 83.59
  Final Train: 83.68
   Final Test: 83.73
All runs:
Highest Train: 83.68, nan
Highest Valid: 83.59, nan
  Final Train: 83.68, nan
   Final Test: 83.73, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 6.3169, Train: 88.24%, Valid: 88.31%, Test: 88.23%
Epoch: 25, Loss: 0.5071, Train: 87.28%, Valid: 87.13%, Test: 87.30%
Epoch: 50, Loss: 0.4049, Train: 87.30%, Valid: 87.16%, Test: 87.30%
Epoch: 75, Loss: 0.3833, Train: 85.83%, Valid: 85.68%, Test: 85.87%
Epoch: 100, Loss: 0.3673, Train: 86.04%, Valid: 86.12%, Test: 86.12%
Epoch: 125, Loss: 0.3636, Train: 86.57%, Valid: 86.55%, Test: 86.77%
Epoch: 150, Loss: 0.3531, Train: 85.32%, Valid: 85.31%, Test: 85.33%
Epoch: 175, Loss: 0.3501, Train: 85.53%, Valid: 85.51%, Test: 85.55%
Run 01:
Highest Train: 88.24
Highest Valid: 88.31
  Final Train: 88.24
   Final Test: 88.23
All runs:
Highest Train: 88.24, nan
Highest Valid: 88.31, nan
  Final Train: 88.24, nan
   Final Test: 88.23, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 2.6384, Train: 85.19%, Valid: 85.02%, Test: 85.25%
Epoch: 25, Loss: 0.4988, Train: 85.19%, Valid: 84.99%, Test: 85.32%
Epoch: 50, Loss: 0.3810, Train: 85.94%, Valid: 85.74%, Test: 86.00%
Epoch: 75, Loss: 0.3724, Train: 85.62%, Valid: 85.40%, Test: 85.66%
Epoch: 100, Loss: 0.3670, Train: 85.67%, Valid: 85.50%, Test: 85.72%
Epoch: 125, Loss: 0.3576, Train: 85.62%, Valid: 85.43%, Test: 85.67%
Epoch: 150, Loss: 0.3520, Train: 85.57%, Valid: 85.37%, Test: 85.61%
Epoch: 175, Loss: 0.3489, Train: 86.87%, Valid: 86.91%, Test: 86.94%
Run 01:
Highest Train: 87.45
Highest Valid: 87.41
  Final Train: 87.45
   Final Test: 87.49
All runs:
Highest Train: 87.45, nan
Highest Valid: 87.41, nan
  Final Train: 87.45, nan
   Final Test: 87.49, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 319.6962, Train: 58.96%, Valid: 59.27%, Test: 59.32%
Epoch: 25, Loss: 13.1423, Train: 42.27%, Valid: 42.47%, Test: 42.52%
Epoch: 50, Loss: 10.7719, Train: 76.36%, Valid: 76.16%, Test: 76.25%
Epoch: 75, Loss: 7.7052, Train: 81.28%, Valid: 81.17%, Test: 81.20%
Epoch: 100, Loss: 11.3977, Train: 78.49%, Valid: 78.52%, Test: 78.85%
Epoch: 125, Loss: 5.5138, Train: 80.72%, Valid: 80.70%, Test: 80.88%
Epoch: 150, Loss: 5.5701, Train: 73.84%, Valid: 73.78%, Test: 74.15%
Epoch: 175, Loss: 4.7463, Train: 73.79%, Valid: 73.80%, Test: 74.15%
Run 01:
Highest Train: 83.30
Highest Valid: 83.22
  Final Train: 83.30
   Final Test: 83.37
All runs:
Highest Train: 83.30, nan
Highest Valid: 83.22, nan
  Final Train: 83.30, nan
   Final Test: 83.37, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 9.4742, Train: 83.87%, Valid: 83.84%, Test: 84.02%
Epoch: 25, Loss: 0.6200, Train: 83.85%, Valid: 83.67%, Test: 83.99%
Epoch: 50, Loss: 0.4427, Train: 85.14%, Valid: 85.15%, Test: 85.30%
Epoch: 75, Loss: 0.3846, Train: 85.78%, Valid: 85.83%, Test: 85.82%
Epoch: 100, Loss: 0.3769, Train: 85.65%, Valid: 85.66%, Test: 85.71%
Epoch: 125, Loss: 0.3729, Train: 85.84%, Valid: 85.86%, Test: 85.90%
Epoch: 150, Loss: 0.3718, Train: 85.96%, Valid: 86.01%, Test: 86.00%
Epoch: 175, Loss: 0.3637, Train: 86.41%, Valid: 86.45%, Test: 86.40%
Run 01:
Highest Train: 86.88
Highest Valid: 86.88
  Final Train: 86.88
   Final Test: 86.86
All runs:
Highest Train: 86.88, nan
Highest Valid: 86.88, nan
  Final Train: 86.88, nan
   Final Test: 86.86, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 3.8128, Train: 85.33%, Valid: 85.25%, Test: 85.41%
Epoch: 25, Loss: 0.4987, Train: 86.09%, Valid: 86.01%, Test: 86.16%
Epoch: 50, Loss: 0.3874, Train: 87.63%, Valid: 87.53%, Test: 87.62%
Epoch: 75, Loss: 0.3727, Train: 87.19%, Valid: 87.09%, Test: 87.24%
Epoch: 100, Loss: 0.3645, Train: 86.25%, Valid: 86.10%, Test: 86.27%
Epoch: 125, Loss: 0.3606, Train: 86.33%, Valid: 86.21%, Test: 86.39%
Epoch: 150, Loss: 0.3541, Train: 86.21%, Valid: 86.16%, Test: 86.27%
Epoch: 175, Loss: 0.3494, Train: 86.40%, Valid: 86.25%, Test: 86.43%
Run 01:
Highest Train: 87.96
Highest Valid: 87.82
  Final Train: 87.96
   Final Test: 87.89
All runs:
Highest Train: 87.96, nan
Highest Valid: 87.82, nan
  Final Train: 87.96, nan
   Final Test: 87.89, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 378.8091, Train: 46.36%, Valid: 46.36%, Test: 46.46%
Epoch: 25, Loss: 10.5197, Train: 83.34%, Valid: 83.24%, Test: 83.40%
Epoch: 50, Loss: 7.7107, Train: 83.26%, Valid: 83.17%, Test: 83.34%
Epoch: 75, Loss: 5.2523, Train: 83.81%, Valid: 83.72%, Test: 83.89%
Epoch: 100, Loss: 4.2349, Train: 84.02%, Valid: 83.91%, Test: 84.08%
Epoch: 125, Loss: 2.1739, Train: 76.39%, Valid: 76.55%, Test: 76.76%
Epoch: 150, Loss: 1.8558, Train: 81.73%, Valid: 81.73%, Test: 81.94%
Epoch: 175, Loss: 1.5108, Train: 82.40%, Valid: 82.39%, Test: 82.61%
Run 01:
Highest Train: 84.40
Highest Valid: 84.28
  Final Train: 84.40
   Final Test: 84.48
All runs:
Highest Train: 84.40, nan
Highest Valid: 84.28, nan
  Final Train: 84.40, nan
   Final Test: 84.48, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 11.3103, Train: 15.37%, Valid: 15.27%, Test: 15.28%
Epoch: 25, Loss: 0.5036, Train: 85.65%, Valid: 85.49%, Test: 85.67%
Epoch: 50, Loss: 0.4454, Train: 85.71%, Valid: 85.60%, Test: 85.74%
Epoch: 75, Loss: 0.3882, Train: 85.91%, Valid: 85.78%, Test: 85.97%
Epoch: 100, Loss: 0.3614, Train: 85.93%, Valid: 85.81%, Test: 86.02%
Epoch: 125, Loss: 0.3493, Train: 86.09%, Valid: 86.12%, Test: 86.18%
Epoch: 150, Loss: 0.3345, Train: 85.89%, Valid: 85.90%, Test: 86.00%
Epoch: 175, Loss: 0.3265, Train: 85.77%, Valid: 85.80%, Test: 85.86%
Run 01:
Highest Train: 87.72
Highest Valid: 87.48
  Final Train: 87.72
   Final Test: 87.75
All runs:
Highest Train: 87.72, nan
Highest Valid: 87.48, nan
  Final Train: 87.72, nan
   Final Test: 87.75, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 0.5407, Train: 84.46%, Valid: 84.43%, Test: 84.56%
Epoch: 25, Loss: 0.4296, Train: 85.87%, Valid: 85.72%, Test: 85.95%
Epoch: 50, Loss: 0.3653, Train: 85.68%, Valid: 85.51%, Test: 85.73%
Epoch: 75, Loss: 0.3567, Train: 85.70%, Valid: 85.52%, Test: 85.76%
Epoch: 100, Loss: 0.3488, Train: 85.74%, Valid: 85.54%, Test: 85.79%
Epoch: 125, Loss: 0.3393, Train: 85.80%, Valid: 85.59%, Test: 85.86%
Epoch: 150, Loss: 0.4794, Train: 86.16%, Valid: 86.11%, Test: 86.20%
Epoch: 175, Loss: 0.3425, Train: 85.92%, Valid: 85.72%, Test: 85.98%
Run 01:
Highest Train: 87.10
Highest Valid: 86.90
  Final Train: 87.10
   Final Test: 87.13
All runs:
Highest Train: 87.10, nan
Highest Valid: 86.90, nan
  Final Train: 87.10, nan
   Final Test: 87.13, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 227115946467131392.0000, Train: 49.51%, Valid: 49.64%, Test: 49.35%
Epoch: 25, Loss: 87665393522442240.0000, Train: 50.01%, Valid: 50.00%, Test: 50.00%
Epoch: 50, Loss: 1574487497637888.0000, Train: 50.01%, Valid: 50.00%, Test: 50.00%
Epoch: 75, Loss: 56883465991225344.0000, Train: 50.01%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 4496710231392256.0000, Train: 50.01%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 11882775707648.0000, Train: 50.01%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 870995007111168.0000, Train: 50.01%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 384144123101184.0000, Train: 49.99%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 52.07
Highest Valid: 52.08
  Final Train: 52.07
   Final Test: 52.06
All runs:
Highest Train: 52.07, nan
Highest Valid: 52.08, nan
  Final Train: 52.07, nan
   Final Test: 52.06, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 15.7961, Train: 15.95%, Valid: 15.89%, Test: 15.82%
Epoch: 25, Loss: 0.5162, Train: 86.88%, Valid: 86.96%, Test: 86.98%
Epoch: 50, Loss: 0.4661, Train: 86.49%, Valid: 86.53%, Test: 86.55%
Epoch: 75, Loss: 0.4151, Train: 85.93%, Valid: 85.96%, Test: 85.97%
Epoch: 100, Loss: 0.3805, Train: 85.68%, Valid: 85.67%, Test: 85.72%
Epoch: 125, Loss: 0.3608, Train: 86.46%, Valid: 86.47%, Test: 86.53%
Epoch: 150, Loss: 0.3507, Train: 85.96%, Valid: 86.00%, Test: 86.09%
Epoch: 175, Loss: 0.3454, Train: 85.46%, Valid: 85.48%, Test: 85.57%
Run 01:
Highest Train: 87.39
Highest Valid: 87.44
  Final Train: 87.39
   Final Test: 87.47
All runs:
Highest Train: 87.39, nan
Highest Valid: 87.44, nan
  Final Train: 87.39, nan
   Final Test: 87.47, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 5.9208, Train: 84.22%, Valid: 84.13%, Test: 84.28%
Epoch: 25, Loss: 0.4995, Train: 85.66%, Valid: 85.46%, Test: 85.79%
Epoch: 50, Loss: 0.3871, Train: 85.52%, Valid: 85.34%, Test: 85.57%
Epoch: 75, Loss: 0.3782, Train: 85.59%, Valid: 85.56%, Test: 85.68%
Epoch: 100, Loss: 0.3684, Train: 85.53%, Valid: 85.50%, Test: 85.61%
Epoch: 125, Loss: 0.3624, Train: 85.58%, Valid: 85.51%, Test: 85.64%
Epoch: 150, Loss: 0.3528, Train: 85.62%, Valid: 85.56%, Test: 85.67%
Epoch: 175, Loss: 0.3445, Train: 85.74%, Valid: 85.66%, Test: 85.76%
Run 01:
Highest Train: 86.40
Highest Valid: 86.39
  Final Train: 86.40
   Final Test: 86.50
All runs:
Highest Train: 86.40, nan
Highest Valid: 86.39, nan
  Final Train: 86.40, nan
   Final Test: 86.50, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 34.0347, Train: 53.68%, Valid: 53.70%, Test: 53.57%
Epoch: 25, Loss: 64.6574, Train: 66.75%, Valid: 66.97%, Test: 67.16%
Epoch: 50, Loss: 6.5323, Train: 83.71%, Valid: 83.62%, Test: 83.80%
Epoch: 75, Loss: 5.8555, Train: 84.28%, Valid: 84.17%, Test: 84.33%
Epoch: 100, Loss: 3.4277, Train: 84.53%, Valid: 84.41%, Test: 84.58%
Epoch: 125, Loss: 3.6094, Train: 84.44%, Valid: 84.31%, Test: 84.48%
Epoch: 150, Loss: 9.2762, Train: 85.09%, Valid: 84.98%, Test: 85.17%
Epoch: 175, Loss: 21.8475, Train: 84.82%, Valid: 84.71%, Test: 84.90%
Run 01:
Highest Train: 87.91
Highest Valid: 87.81
  Final Train: 87.91
   Final Test: 87.96
All runs:
Highest Train: 87.91, nan
Highest Valid: 87.81, nan
  Final Train: 87.91, nan
   Final Test: 87.96, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 7.1838, Train: 85.00%, Valid: 84.99%, Test: 85.11%
Epoch: 25, Loss: 0.5557, Train: 85.81%, Valid: 85.72%, Test: 85.84%
Epoch: 50, Loss: 0.4058, Train: 85.62%, Valid: 85.46%, Test: 85.62%
Epoch: 75, Loss: 0.3799, Train: 86.74%, Valid: 86.50%, Test: 86.84%
Epoch: 100, Loss: 0.3722, Train: 86.77%, Valid: 86.57%, Test: 86.79%
Epoch: 125, Loss: 0.3648, Train: 86.99%, Valid: 86.79%, Test: 87.00%
Epoch: 150, Loss: 0.3602, Train: 87.06%, Valid: 86.83%, Test: 87.07%
Epoch: 175, Loss: 0.3557, Train: 87.10%, Valid: 86.89%, Test: 87.08%
Run 01:
Highest Train: 87.27
Highest Valid: 87.02
  Final Train: 87.27
   Final Test: 87.26
All runs:
Highest Train: 87.27, nan
Highest Valid: 87.02, nan
  Final Train: 87.27, nan
   Final Test: 87.26, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 1.0687, Train: 83.58%, Valid: 83.57%, Test: 83.63%
Epoch: 25, Loss: 0.3848, Train: 85.19%, Valid: 85.03%, Test: 85.27%
Epoch: 50, Loss: 0.3686, Train: 85.54%, Valid: 85.35%, Test: 85.59%
Epoch: 75, Loss: 0.3600, Train: 85.51%, Valid: 85.30%, Test: 85.58%
Epoch: 100, Loss: 0.3524, Train: 85.51%, Valid: 85.30%, Test: 85.58%
Epoch: 125, Loss: 0.3486, Train: 85.39%, Valid: 85.20%, Test: 85.48%
Epoch: 150, Loss: 0.3421, Train: 85.40%, Valid: 85.22%, Test: 85.51%
Epoch: 175, Loss: 0.3375, Train: 85.47%, Valid: 85.27%, Test: 85.57%
Run 01:
Highest Train: 87.63
Highest Valid: 87.44
  Final Train: 87.63
   Final Test: 87.70
All runs:
Highest Train: 87.63, nan
Highest Valid: 87.44, nan
  Final Train: 87.63, nan
   Final Test: 87.70, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 251.5680, Train: 53.33%, Valid: 53.42%, Test: 53.17%
Epoch: 25, Loss: 7.0337, Train: 54.08%, Valid: 54.01%, Test: 54.06%
Epoch: 50, Loss: 6.0228, Train: 82.06%, Valid: 81.88%, Test: 82.03%
Epoch: 75, Loss: 17.9642, Train: 83.66%, Valid: 83.54%, Test: 83.68%
Epoch: 100, Loss: 7.6238, Train: 84.05%, Valid: 83.95%, Test: 84.10%
Epoch: 125, Loss: 1.5309, Train: 85.10%, Valid: 84.96%, Test: 85.18%
Epoch: 150, Loss: 0.6742, Train: 83.84%, Valid: 83.87%, Test: 84.02%
Epoch: 175, Loss: 0.8876, Train: 80.22%, Valid: 80.34%, Test: 80.60%
Run 01:
Highest Train: 85.85
Highest Valid: 85.69
  Final Train: 85.81
   Final Test: 85.93
All runs:
Highest Train: 85.85, nan
Highest Valid: 85.69, nan
  Final Train: 85.81, nan
   Final Test: 85.93, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 7.2648, Train: 83.64%, Valid: 83.45%, Test: 83.78%
Epoch: 25, Loss: 0.5463, Train: 87.45%, Valid: 87.46%, Test: 87.42%
Epoch: 50, Loss: 0.4644, Train: 86.80%, Valid: 86.82%, Test: 86.83%
Epoch: 75, Loss: 0.3963, Train: 85.97%, Valid: 85.99%, Test: 86.05%
Epoch: 100, Loss: 0.3821, Train: 85.61%, Valid: 85.66%, Test: 85.72%
Epoch: 125, Loss: 0.3708, Train: 85.73%, Valid: 85.73%, Test: 85.80%
Epoch: 150, Loss: 0.3649, Train: 85.51%, Valid: 85.56%, Test: 85.62%
Epoch: 175, Loss: 0.3607, Train: 85.46%, Valid: 85.50%, Test: 85.56%
Run 01:
Highest Train: 88.13
Highest Valid: 88.17
  Final Train: 88.13
   Final Test: 88.13
All runs:
Highest Train: 88.13, nan
Highest Valid: 88.17, nan
  Final Train: 88.13, nan
   Final Test: 88.13, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 6.4976, Train: 80.92%, Valid: 81.03%, Test: 81.20%
Epoch: 25, Loss: 0.4362, Train: 85.63%, Valid: 85.57%, Test: 85.79%
Epoch: 50, Loss: 0.3851, Train: 84.82%, Valid: 84.79%, Test: 84.88%
Epoch: 75, Loss: 0.3798, Train: 85.21%, Valid: 85.20%, Test: 85.29%
Epoch: 100, Loss: 0.3705, Train: 85.02%, Valid: 85.10%, Test: 85.13%
Epoch: 125, Loss: 0.3656, Train: 84.72%, Valid: 84.80%, Test: 84.87%
Epoch: 150, Loss: 0.3582, Train: 86.26%, Valid: 86.10%, Test: 86.28%
Epoch: 175, Loss: 0.3582, Train: 86.31%, Valid: 86.17%, Test: 86.35%
Run 01:
Highest Train: 86.34
Highest Valid: 86.20
  Final Train: 86.34
   Final Test: 86.38
All runs:
Highest Train: 86.34, nan
Highest Valid: 86.20, nan
  Final Train: 86.34, nan
   Final Test: 86.38, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 225.0335, Train: 54.26%, Valid: 54.34%, Test: 54.17%
Epoch: 25, Loss: 13.6811, Train: 66.32%, Valid: 66.07%, Test: 65.98%
Epoch: 50, Loss: 13.9291, Train: 83.81%, Valid: 83.68%, Test: 83.90%
Epoch: 75, Loss: 8.2710, Train: 78.42%, Valid: 78.41%, Test: 78.72%
Epoch: 100, Loss: 6.1796, Train: 74.69%, Valid: 74.73%, Test: 74.99%
Epoch: 125, Loss: 2.8619, Train: 76.31%, Valid: 76.43%, Test: 76.66%
Epoch: 150, Loss: 1.4970, Train: 82.67%, Valid: 82.74%, Test: 82.88%
Epoch: 175, Loss: 1.2514, Train: 82.60%, Valid: 82.69%, Test: 82.88%
Run 01:
Highest Train: 85.83
Highest Valid: 85.71
  Final Train: 85.83
   Final Test: 85.90
All runs:
Highest Train: 85.83, nan
Highest Valid: 85.71, nan
  Final Train: 85.83, nan
   Final Test: 85.90, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 13.4431, Train: 85.94%, Valid: 85.97%, Test: 86.02%
Epoch: 25, Loss: 0.6622, Train: 86.45%, Valid: 86.49%, Test: 86.54%
Epoch: 50, Loss: 0.5356, Train: 85.37%, Valid: 85.43%, Test: 85.47%
Epoch: 75, Loss: 0.3545, Train: 85.56%, Valid: 85.55%, Test: 85.65%
Epoch: 100, Loss: 0.3405, Train: 86.73%, Valid: 86.53%, Test: 86.81%
Epoch: 125, Loss: 0.3297, Train: 86.53%, Valid: 86.40%, Test: 86.63%
Epoch: 150, Loss: 0.3261, Train: 86.47%, Valid: 86.26%, Test: 86.59%
Epoch: 175, Loss: 0.3248, Train: 85.67%, Valid: 85.64%, Test: 85.76%
Run 01:
Highest Train: 87.80
Highest Valid: 87.68
  Final Train: 87.80
   Final Test: 87.83
All runs:
Highest Train: 87.80, nan
Highest Valid: 87.68, nan
  Final Train: 87.80, nan
   Final Test: 87.83, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 11.3316, Train: 84.25%, Valid: 84.19%, Test: 84.38%
Epoch: 25, Loss: 0.5734, Train: 86.03%, Valid: 85.91%, Test: 86.07%
Epoch: 50, Loss: 0.3732, Train: 85.84%, Valid: 85.69%, Test: 85.91%
Epoch: 75, Loss: 0.3555, Train: 85.71%, Valid: 85.54%, Test: 85.75%
Epoch: 100, Loss: 0.3424, Train: 85.62%, Valid: 85.46%, Test: 85.67%
Epoch: 125, Loss: 0.3732, Train: 85.50%, Valid: 85.33%, Test: 85.56%
Epoch: 150, Loss: 0.3444, Train: 85.73%, Valid: 85.56%, Test: 85.80%
Epoch: 175, Loss: 0.5872, Train: 85.45%, Valid: 85.33%, Test: 85.54%
Run 01:
Highest Train: 86.71
Highest Valid: 86.65
  Final Train: 86.71
   Final Test: 86.80
All runs:
Highest Train: 86.71, nan
Highest Valid: 86.65, nan
  Final Train: 86.71, nan
   Final Test: 86.80, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 13306.1855, Train: 60.96%, Valid: 60.80%, Test: 60.66%
Epoch: 25, Loss: 43.4269, Train: 37.46%, Valid: 37.31%, Test: 37.02%
Epoch: 50, Loss: 19.1323, Train: 78.98%, Valid: 78.90%, Test: 78.77%
Epoch: 75, Loss: 14.4774, Train: 82.04%, Valid: 81.99%, Test: 81.90%
Epoch: 100, Loss: 10.4007, Train: 83.35%, Valid: 83.28%, Test: 83.29%
Epoch: 125, Loss: 7.5143, Train: 83.50%, Valid: 83.49%, Test: 83.47%
Epoch: 150, Loss: 5.4841, Train: 83.81%, Valid: 83.79%, Test: 83.79%
Epoch: 175, Loss: 3.4887, Train: 84.00%, Valid: 83.93%, Test: 84.01%
Run 01:
Highest Train: 84.08
Highest Valid: 84.01
  Final Train: 84.08
   Final Test: 84.09
All runs:
Highest Train: 84.08, nan
Highest Valid: 84.01, nan
  Final Train: 84.08, nan
   Final Test: 84.09, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 14.4707, Train: 84.53%, Valid: 84.54%, Test: 84.69%
Epoch: 25, Loss: 0.6179, Train: 86.47%, Valid: 86.53%, Test: 86.55%
Epoch: 50, Loss: 0.3928, Train: 85.71%, Valid: 85.74%, Test: 85.81%
Epoch: 75, Loss: 0.3737, Train: 85.24%, Valid: 85.26%, Test: 85.31%
Epoch: 100, Loss: 0.3624, Train: 85.36%, Valid: 85.39%, Test: 85.45%
Epoch: 125, Loss: 0.3552, Train: 85.42%, Valid: 85.44%, Test: 85.50%
Epoch: 150, Loss: 0.3470, Train: 85.77%, Valid: 85.76%, Test: 85.87%
Epoch: 175, Loss: 0.3442, Train: 85.70%, Valid: 85.69%, Test: 85.79%
Run 01:
Highest Train: 88.06
Highest Valid: 88.14
  Final Train: 88.06
   Final Test: 88.10
All runs:
Highest Train: 88.06, nan
Highest Valid: 88.14, nan
  Final Train: 88.06, nan
   Final Test: 88.10, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 2.4875, Train: 85.22%, Valid: 85.12%, Test: 85.17%
Epoch: 25, Loss: 0.5765, Train: 86.41%, Valid: 86.41%, Test: 86.48%
Epoch: 50, Loss: 0.4244, Train: 85.13%, Valid: 85.04%, Test: 85.26%
Epoch: 75, Loss: 0.3587, Train: 85.40%, Valid: 85.18%, Test: 85.48%
Epoch: 100, Loss: 0.3495, Train: 85.30%, Valid: 85.11%, Test: 85.40%
Epoch: 125, Loss: 0.3409, Train: 85.31%, Valid: 85.10%, Test: 85.39%
Epoch: 150, Loss: 0.3363, Train: 85.79%, Valid: 85.57%, Test: 85.86%
Epoch: 175, Loss: 0.3321, Train: 85.65%, Valid: 85.42%, Test: 85.72%
Run 01:
Highest Train: 87.23
Highest Valid: 87.13
  Final Train: 87.18
   Final Test: 87.27
All runs:
Highest Train: 87.23, nan
Highest Valid: 87.13, nan
  Final Train: 87.18, nan
   Final Test: 87.27, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 783.9274, Train: 68.27%, Valid: 68.15%, Test: 67.97%
Epoch: 25, Loss: 4.9161, Train: 80.95%, Valid: 80.81%, Test: 80.86%
Epoch: 50, Loss: 2.5808, Train: 83.78%, Valid: 83.69%, Test: 83.83%
Epoch: 75, Loss: 2.6699, Train: 84.25%, Valid: 84.14%, Test: 84.30%
Epoch: 100, Loss: 2.7997, Train: 84.01%, Valid: 83.93%, Test: 84.08%
Epoch: 125, Loss: 1.9439, Train: 84.27%, Valid: 84.16%, Test: 84.34%
Epoch: 150, Loss: 1.9680, Train: 84.54%, Valid: 84.44%, Test: 84.61%
Epoch: 175, Loss: 1.8717, Train: 84.60%, Valid: 84.48%, Test: 84.66%
Run 01:
Highest Train: 84.68
Highest Valid: 84.56
  Final Train: 84.68
   Final Test: 84.75
All runs:
Highest Train: 84.68, nan
Highest Valid: 84.56, nan
  Final Train: 84.68, nan
   Final Test: 84.75, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 4.4090, Train: 86.21%, Valid: 86.14%, Test: 86.21%
Epoch: 25, Loss: 0.6903, Train: 86.42%, Valid: 86.48%, Test: 86.50%
Epoch: 50, Loss: 0.4002, Train: 85.48%, Valid: 85.52%, Test: 85.57%
Epoch: 75, Loss: 0.3652, Train: 85.96%, Valid: 86.00%, Test: 86.05%
Epoch: 100, Loss: 0.3605, Train: 85.40%, Valid: 85.39%, Test: 85.50%
Epoch: 125, Loss: 0.3529, Train: 84.96%, Valid: 84.97%, Test: 85.06%
Epoch: 150, Loss: 0.3442, Train: 85.03%, Valid: 85.01%, Test: 85.15%
Epoch: 175, Loss: 0.3419, Train: 85.20%, Valid: 85.19%, Test: 85.31%
Run 01:
Highest Train: 86.74
Highest Valid: 86.75
  Final Train: 86.74
   Final Test: 86.74
All runs:
Highest Train: 86.74, nan
Highest Valid: 86.75, nan
  Final Train: 86.74, nan
   Final Test: 86.74, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 8.2899, Train: 85.54%, Valid: 85.41%, Test: 85.50%
Epoch: 25, Loss: 0.5785, Train: 87.53%, Valid: 87.55%, Test: 87.54%
Epoch: 50, Loss: 0.4222, Train: 86.07%, Valid: 85.98%, Test: 86.14%
Epoch: 75, Loss: 0.3741, Train: 85.46%, Valid: 85.27%, Test: 85.53%
Epoch: 100, Loss: 0.3605, Train: 85.56%, Valid: 85.38%, Test: 85.63%
Epoch: 125, Loss: 0.3513, Train: 86.34%, Valid: 86.18%, Test: 86.36%
Epoch: 150, Loss: 0.3431, Train: 86.36%, Valid: 86.22%, Test: 86.40%
Epoch: 175, Loss: 0.3410, Train: 85.93%, Valid: 85.75%, Test: 85.99%
Run 01:
Highest Train: 87.80
Highest Valid: 87.77
  Final Train: 87.80
   Final Test: 87.84
All runs:
Highest Train: 87.80, nan
Highest Valid: 87.77, nan
  Final Train: 87.80, nan
   Final Test: 87.84, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 516.9471, Train: 57.71%, Valid: 57.55%, Test: 57.41%
Epoch: 25, Loss: 75.8306, Train: 83.21%, Valid: 83.11%, Test: 83.25%
Epoch: 50, Loss: 4.1697, Train: 84.98%, Valid: 84.84%, Test: 85.04%
Epoch: 75, Loss: 2.8918, Train: 80.35%, Valid: 80.43%, Test: 80.72%
Epoch: 100, Loss: 6.6000, Train: 78.28%, Valid: 78.44%, Test: 78.67%
Epoch: 125, Loss: 2.2991, Train: 77.52%, Valid: 77.70%, Test: 77.92%
Epoch: 150, Loss: 2.0606, Train: 75.84%, Valid: 75.92%, Test: 76.14%
Epoch: 175, Loss: 1.7848, Train: 73.73%, Valid: 73.83%, Test: 74.05%
Run 01:
Highest Train: 86.86
Highest Valid: 86.73
  Final Train: 86.86
   Final Test: 86.89
All runs:
Highest Train: 86.86, nan
Highest Valid: 86.73, nan
  Final Train: 86.86, nan
   Final Test: 86.89, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 4.7839, Train: 86.21%, Valid: 86.08%, Test: 86.25%
Epoch: 25, Loss: 0.5842, Train: 87.25%, Valid: 87.15%, Test: 87.25%
Epoch: 50, Loss: 0.3868, Train: 86.68%, Valid: 86.77%, Test: 86.79%
Epoch: 75, Loss: 0.3742, Train: 85.54%, Valid: 85.54%, Test: 85.65%
Epoch: 100, Loss: 0.3654, Train: 85.50%, Valid: 85.52%, Test: 85.62%
Epoch: 125, Loss: 0.3540, Train: 85.44%, Valid: 85.50%, Test: 85.56%
Epoch: 150, Loss: 0.3481, Train: 85.72%, Valid: 85.56%, Test: 85.79%
Epoch: 175, Loss: 0.3463, Train: 85.58%, Valid: 85.58%, Test: 85.66%
Run 01:
Highest Train: 87.47
Highest Valid: 87.32
  Final Train: 87.46
   Final Test: 87.48
All runs:
Highest Train: 87.47, nan
Highest Valid: 87.32, nan
  Final Train: 87.46, nan
   Final Test: 87.48, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 5.6081, Train: 84.94%, Valid: 85.02%, Test: 85.06%
Epoch: 25, Loss: 0.6302, Train: 86.32%, Valid: 86.27%, Test: 86.33%
Epoch: 50, Loss: 0.4251, Train: 86.35%, Valid: 86.29%, Test: 86.32%
Epoch: 75, Loss: 0.3761, Train: 85.61%, Valid: 85.39%, Test: 85.67%
Epoch: 100, Loss: 0.3667, Train: 85.25%, Valid: 85.05%, Test: 85.35%
Epoch: 125, Loss: 0.3567, Train: 86.41%, Valid: 86.36%, Test: 86.46%
Epoch: 150, Loss: 0.3509, Train: 86.95%, Valid: 86.85%, Test: 87.10%
Epoch: 175, Loss: 0.3492, Train: 85.74%, Valid: 85.59%, Test: 85.87%
Run 01:
Highest Train: 88.02
Highest Valid: 87.93
  Final Train: 88.02
   Final Test: 87.97
All runs:
Highest Train: 88.02, nan
Highest Valid: 87.93, nan
  Final Train: 88.02, nan
   Final Test: 87.97, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 141.2370, Train: 70.84%, Valid: 70.66%, Test: 70.57%
Epoch: 25, Loss: 3.4000, Train: 77.27%, Valid: 77.07%, Test: 77.13%
Epoch: 50, Loss: 1.7824, Train: 83.15%, Valid: 83.06%, Test: 83.18%
Epoch: 75, Loss: 1.6751, Train: 84.27%, Valid: 84.15%, Test: 84.30%
Epoch: 100, Loss: 1.5168, Train: 76.40%, Valid: 76.53%, Test: 76.75%
Epoch: 125, Loss: 1.5756, Train: 74.71%, Valid: 74.77%, Test: 75.05%
Epoch: 150, Loss: 1.3633, Train: 73.74%, Valid: 73.88%, Test: 74.11%
Epoch: 175, Loss: 1.2835, Train: 75.11%, Valid: 75.19%, Test: 75.44%
Run 01:
Highest Train: 85.59
Highest Valid: 85.47
  Final Train: 85.59
   Final Test: 85.70
All runs:
Highest Train: 85.59, nan
Highest Valid: 85.47, nan
  Final Train: 85.59, nan
   Final Test: 85.70, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 0.4275, Train: 88.20%, Valid: 88.24%, Test: 88.21%
Epoch: 25, Loss: 0.3598, Train: 86.49%, Valid: 86.49%, Test: 86.52%
Epoch: 50, Loss: 0.3475, Train: 86.06%, Valid: 86.03%, Test: 86.13%
Epoch: 75, Loss: 0.3398, Train: 87.51%, Valid: 87.56%, Test: 87.60%
Epoch: 100, Loss: 0.3312, Train: 85.44%, Valid: 85.42%, Test: 85.58%
Epoch: 125, Loss: 0.6909, Train: 85.61%, Valid: 85.66%, Test: 85.71%
Epoch: 150, Loss: 0.6085, Train: 85.33%, Valid: 85.33%, Test: 85.45%
Epoch: 175, Loss: 0.3607, Train: 87.33%, Valid: 87.17%, Test: 87.34%
Run 01:
Highest Train: 88.52
Highest Valid: 88.52
  Final Train: 88.49
   Final Test: 88.49
All runs:
Highest Train: 88.52, nan
Highest Valid: 88.52, nan
  Final Train: 88.49, nan
   Final Test: 88.49, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 13.2792, Train: 84.23%, Valid: 84.10%, Test: 84.34%
Epoch: 25, Loss: 0.7310, Train: 86.17%, Valid: 86.08%, Test: 86.17%
Epoch: 50, Loss: 0.6391, Train: 85.53%, Valid: 85.35%, Test: 85.58%
Epoch: 75, Loss: 0.5140, Train: 85.35%, Valid: 85.16%, Test: 85.43%
Epoch: 100, Loss: 0.4163, Train: 85.31%, Valid: 85.12%, Test: 85.40%
Epoch: 125, Loss: 0.3579, Train: 86.00%, Valid: 85.80%, Test: 86.04%
Epoch: 150, Loss: 0.3595, Train: 87.19%, Valid: 87.18%, Test: 87.21%
Epoch: 175, Loss: 0.3639, Train: 85.63%, Valid: 85.51%, Test: 85.68%
Run 01:
Highest Train: 87.50
Highest Valid: 87.47
  Final Train: 87.50
   Final Test: 87.48
All runs:
Highest Train: 87.50, nan
Highest Valid: 87.47, nan
  Final Train: 87.50, nan
   Final Test: 87.48, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 570.4149, Train: 62.47%, Valid: 62.36%, Test: 62.19%
Epoch: 25, Loss: 1875.4961, Train: 48.97%, Valid: 49.20%, Test: 49.29%
Epoch: 50, Loss: 90.7839, Train: 75.94%, Valid: 75.80%, Test: 75.79%
Epoch: 75, Loss: 46.2905, Train: 57.97%, Valid: 58.44%, Test: 58.44%
Epoch: 100, Loss: 910.2602, Train: 52.60%, Valid: 52.82%, Test: 52.90%
Epoch: 125, Loss: 704.4492, Train: 62.48%, Valid: 62.81%, Test: 62.92%
Epoch: 150, Loss: 1640.6550, Train: 78.53%, Valid: 78.38%, Test: 78.39%
Epoch: 175, Loss: 334.0224, Train: 81.06%, Valid: 80.95%, Test: 80.98%
Run 01:
Highest Train: 84.07
Highest Valid: 83.96
  Final Train: 84.07
   Final Test: 84.09
All runs:
Highest Train: 84.07, nan
Highest Valid: 83.96, nan
  Final Train: 84.07, nan
   Final Test: 84.09, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 9.4085, Train: 87.99%, Valid: 88.01%, Test: 87.94%
Epoch: 25, Loss: 0.5318, Train: 86.13%, Valid: 86.17%, Test: 86.19%
Epoch: 50, Loss: 0.4180, Train: 85.56%, Valid: 85.61%, Test: 85.64%
Epoch: 75, Loss: 0.3626, Train: 86.33%, Valid: 86.20%, Test: 86.42%
Epoch: 100, Loss: 0.3460, Train: 85.53%, Valid: 85.55%, Test: 85.64%
Epoch: 125, Loss: 0.3401, Train: 85.91%, Valid: 85.91%, Test: 86.01%
Epoch: 150, Loss: 0.3334, Train: 86.03%, Valid: 86.03%, Test: 86.11%
Epoch: 175, Loss: 0.3329, Train: 85.91%, Valid: 85.92%, Test: 85.97%
Run 01:
Highest Train: 87.99
Highest Valid: 88.01
  Final Train: 87.99
   Final Test: 87.94
All runs:
Highest Train: 87.99, nan
Highest Valid: 88.01, nan
  Final Train: 87.99, nan
   Final Test: 87.94, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 6.1230, Train: 85.41%, Valid: 85.30%, Test: 85.56%
Epoch: 25, Loss: 0.5234, Train: 86.26%, Valid: 86.14%, Test: 86.30%
Epoch: 50, Loss: 0.4260, Train: 85.68%, Valid: 85.69%, Test: 85.88%
Epoch: 75, Loss: 0.3937, Train: 86.16%, Valid: 86.05%, Test: 86.26%
Epoch: 100, Loss: 0.3773, Train: 86.01%, Valid: 85.88%, Test: 86.07%
Epoch: 125, Loss: 0.3550, Train: 86.66%, Valid: 86.49%, Test: 86.67%
Epoch: 150, Loss: 0.3460, Train: 86.75%, Valid: 86.58%, Test: 86.77%
Epoch: 175, Loss: 0.3402, Train: 86.64%, Valid: 86.49%, Test: 86.71%
Run 01:
Highest Train: 87.62
Highest Valid: 87.56
  Final Train: 87.62
   Final Test: 87.57
All runs:
Highest Train: 87.62, nan
Highest Valid: 87.56, nan
  Final Train: 87.62, nan
   Final Test: 87.57, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 116.2330, Train: 58.32%, Valid: 58.26%, Test: 58.07%
Epoch: 25, Loss: 349.6507, Train: 83.03%, Valid: 82.95%, Test: 83.06%
Epoch: 50, Loss: 8.0397, Train: 80.37%, Valid: 80.24%, Test: 80.29%
Epoch: 75, Loss: 9.6632, Train: 83.27%, Valid: 83.19%, Test: 83.31%
Epoch: 100, Loss: 5.7258, Train: 83.72%, Valid: 83.63%, Test: 83.78%
Epoch: 125, Loss: 6.6100, Train: 83.90%, Valid: 83.82%, Test: 83.95%
Epoch: 150, Loss: 4.9223, Train: 84.08%, Valid: 83.99%, Test: 84.13%
Epoch: 175, Loss: 4.3744, Train: 84.32%, Valid: 84.21%, Test: 84.36%
Run 01:
Highest Train: 84.37
Highest Valid: 84.25
  Final Train: 84.37
   Final Test: 84.41
All runs:
Highest Train: 84.37, nan
Highest Valid: 84.25, nan
  Final Train: 84.37, nan
   Final Test: 84.41, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 14.8007, Train: 84.23%, Valid: 84.00%, Test: 84.38%
Epoch: 25, Loss: 0.7858, Train: 85.18%, Valid: 85.04%, Test: 85.28%
Epoch: 50, Loss: 0.4328, Train: 86.46%, Valid: 86.34%, Test: 86.46%
Epoch: 75, Loss: 0.3944, Train: 85.79%, Valid: 85.86%, Test: 85.88%
Epoch: 100, Loss: 0.3780, Train: 85.74%, Valid: 85.81%, Test: 85.81%
Epoch: 125, Loss: 0.3639, Train: 86.83%, Valid: 86.91%, Test: 86.93%
Epoch: 150, Loss: 0.3502, Train: 86.86%, Valid: 86.91%, Test: 86.92%
Epoch: 175, Loss: 0.3461, Train: 85.75%, Valid: 85.79%, Test: 85.86%
Run 01:
Highest Train: 87.38
Highest Valid: 87.28
  Final Train: 87.38
   Final Test: 87.41
All runs:
Highest Train: 87.38, nan
Highest Valid: 87.28, nan
  Final Train: 87.38, nan
   Final Test: 87.41, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 5.8415, Train: 86.62%, Valid: 86.60%, Test: 86.65%
Epoch: 25, Loss: 0.6249, Train: 86.09%, Valid: 85.99%, Test: 86.13%
Epoch: 50, Loss: 0.4007, Train: 86.13%, Valid: 86.00%, Test: 86.20%
Epoch: 75, Loss: 0.3791, Train: 86.61%, Valid: 86.49%, Test: 86.60%
Epoch: 100, Loss: 0.3556, Train: 86.55%, Valid: 86.40%, Test: 86.59%
Epoch: 125, Loss: 0.3483, Train: 86.16%, Valid: 86.00%, Test: 86.25%
Epoch: 150, Loss: 0.3439, Train: 86.04%, Valid: 85.86%, Test: 86.10%
Epoch: 175, Loss: 0.3414, Train: 86.27%, Valid: 86.10%, Test: 86.33%
Run 01:
Highest Train: 86.65
Highest Valid: 86.60
  Final Train: 86.62
   Final Test: 86.65
All runs:
Highest Train: 86.65, nan
Highest Valid: 86.60, nan
  Final Train: 86.62, nan
   Final Test: 86.65, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 608.7584, Train: 60.53%, Valid: 60.33%, Test: 60.29%
Epoch: 25, Loss: 158.1725, Train: 32.77%, Valid: 32.55%, Test: 32.40%
Epoch: 50, Loss: 43.6567, Train: 31.44%, Valid: 31.23%, Test: 31.05%
Epoch: 75, Loss: 4.8753, Train: 86.13%, Valid: 86.11%, Test: 86.16%
Epoch: 100, Loss: 1.1205, Train: 85.50%, Valid: 85.35%, Test: 85.58%
Epoch: 125, Loss: 0.9394, Train: 85.88%, Valid: 85.73%, Test: 85.95%
Epoch: 150, Loss: 0.8383, Train: 87.97%, Valid: 87.76%, Test: 87.98%
Epoch: 175, Loss: 0.8538, Train: 88.13%, Valid: 87.88%, Test: 88.09%
Run 01:
Highest Train: 88.17
Highest Valid: 87.90
  Final Train: 88.17
   Final Test: 88.13
All runs:
Highest Train: 88.17, nan
Highest Valid: 87.90, nan
  Final Train: 88.17, nan
   Final Test: 88.13, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 4.7400, Train: 85.13%, Valid: 85.05%, Test: 85.30%
Epoch: 25, Loss: 0.6634, Train: 86.32%, Valid: 86.35%, Test: 86.35%
Epoch: 50, Loss: 0.3898, Train: 87.36%, Valid: 87.39%, Test: 87.37%
Epoch: 75, Loss: 0.3715, Train: 85.70%, Valid: 85.70%, Test: 85.79%
Epoch: 100, Loss: 0.3617, Train: 86.83%, Valid: 86.67%, Test: 86.92%
Epoch: 125, Loss: 0.3568, Train: 85.97%, Valid: 85.79%, Test: 86.04%
Epoch: 150, Loss: 0.3524, Train: 85.42%, Valid: 85.48%, Test: 85.59%
Epoch: 175, Loss: 0.3477, Train: 85.52%, Valid: 85.58%, Test: 85.68%
Run 01:
Highest Train: 87.50
Highest Valid: 87.51
  Final Train: 87.50
   Final Test: 87.48
All runs:
Highest Train: 87.50, nan
Highest Valid: 87.51, nan
  Final Train: 87.50, nan
   Final Test: 87.48, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 10.1974, Train: 85.34%, Valid: 85.35%, Test: 85.40%
Epoch: 25, Loss: 0.8439, Train: 85.71%, Valid: 85.56%, Test: 85.76%
Epoch: 50, Loss: 0.4611, Train: 86.04%, Valid: 85.86%, Test: 86.08%
Epoch: 75, Loss: 0.3896, Train: 85.56%, Valid: 85.32%, Test: 85.60%
Epoch: 100, Loss: 0.3782, Train: 85.59%, Valid: 85.40%, Test: 85.66%
Epoch: 125, Loss: 0.3613, Train: 86.25%, Valid: 86.09%, Test: 86.31%
Epoch: 150, Loss: 0.3582, Train: 86.05%, Valid: 85.92%, Test: 86.14%
Epoch: 175, Loss: 0.3544, Train: 86.07%, Valid: 85.94%, Test: 86.16%
Run 01:
Highest Train: 86.26
Highest Valid: 86.11
  Final Train: 86.26
   Final Test: 86.32
All runs:
Highest Train: 86.26, nan
Highest Valid: 86.11, nan
  Final Train: 86.26, nan
   Final Test: 86.32, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 559.7211, Train: 56.11%, Valid: 55.99%, Test: 56.03%
Epoch: 25, Loss: 95.0105, Train: 20.41%, Valid: 20.40%, Test: 20.27%
Epoch: 50, Loss: 21.1395, Train: 84.06%, Valid: 83.97%, Test: 84.11%
Epoch: 75, Loss: 2.2658, Train: 85.69%, Valid: 85.54%, Test: 85.76%
Epoch: 100, Loss: 2.2362, Train: 85.52%, Valid: 85.39%, Test: 85.61%
Epoch: 125, Loss: 1.5140, Train: 83.90%, Valid: 83.95%, Test: 84.07%
Epoch: 150, Loss: 1.4265, Train: 83.80%, Valid: 83.85%, Test: 83.98%
Epoch: 175, Loss: 1.3696, Train: 82.70%, Valid: 82.64%, Test: 82.81%
Run 01:
Highest Train: 85.78
Highest Valid: 85.61
  Final Train: 85.78
   Final Test: 85.85
All runs:
Highest Train: 85.78, nan
Highest Valid: 85.61, nan
  Final Train: 85.78, nan
   Final Test: 85.85, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.8983, Train: 86.90%, Valid: 86.95%, Test: 86.92%
Epoch: 25, Loss: 0.4920, Train: 85.75%, Valid: 85.83%, Test: 85.83%
Epoch: 50, Loss: 0.3552, Train: 86.05%, Valid: 86.06%, Test: 86.19%
Epoch: 75, Loss: 0.3538, Train: 85.92%, Valid: 85.94%, Test: 86.03%
Epoch: 100, Loss: 0.3770, Train: 85.92%, Valid: 85.92%, Test: 86.01%
Epoch: 125, Loss: 1.1316, Train: 86.13%, Valid: 86.21%, Test: 86.22%
Epoch: 150, Loss: 2.3615, Train: 85.60%, Valid: 85.65%, Test: 85.68%
Epoch: 175, Loss: 2.1804, Train: 85.33%, Valid: 85.34%, Test: 85.45%
Run 01:
Highest Train: 87.95
Highest Valid: 88.00
  Final Train: 87.95
   Final Test: 88.00
All runs:
Highest Train: 87.95, nan
Highest Valid: 88.00, nan
  Final Train: 87.95, nan
   Final Test: 88.00, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 7.8876, Train: 83.55%, Valid: 83.58%, Test: 83.70%
Epoch: 25, Loss: 0.7016, Train: 85.45%, Valid: 85.26%, Test: 85.48%
Epoch: 50, Loss: 0.5036, Train: 85.57%, Valid: 85.40%, Test: 85.61%
Epoch: 75, Loss: 0.3642, Train: 85.72%, Valid: 85.57%, Test: 85.78%
Epoch: 100, Loss: 0.3563, Train: 85.65%, Valid: 85.49%, Test: 85.71%
Epoch: 125, Loss: 0.3486, Train: 85.68%, Valid: 85.50%, Test: 85.73%
Epoch: 150, Loss: 0.3424, Train: 85.59%, Valid: 85.42%, Test: 85.65%
Epoch: 175, Loss: 0.3556, Train: 85.74%, Valid: 85.54%, Test: 85.79%
Run 01:
Highest Train: 86.94
Highest Valid: 86.86
  Final Train: 86.94
   Final Test: 87.00
All runs:
Highest Train: 86.94, nan
Highest Valid: 86.86, nan
  Final Train: 86.94, nan
   Final Test: 87.00, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 515818794254336.0000, Train: 49.99%, Valid: 49.97%, Test: 49.92%
Epoch: 25, Loss: 820.3139, Train: 15.31%, Valid: 15.36%, Test: 15.18%
Epoch: 50, Loss: 832.9556, Train: 15.26%, Valid: 15.30%, Test: 15.16%
Epoch: 75, Loss: 804.9760, Train: 15.26%, Valid: 15.31%, Test: 15.15%
Epoch: 100, Loss: 848.7069, Train: 15.25%, Valid: 15.31%, Test: 15.13%
Epoch: 125, Loss: 698.6401, Train: 15.25%, Valid: 15.30%, Test: 15.13%
Epoch: 150, Loss: 913.9753, Train: 15.25%, Valid: 15.29%, Test: 15.14%
Epoch: 175, Loss: 833.0593, Train: 15.25%, Valid: 15.30%, Test: 15.13%
Run 01:
Highest Train: 59.42
Highest Valid: 59.40
  Final Train: 59.42
   Final Test: 59.28
All runs:
Highest Train: 59.42, nan
Highest Valid: 59.40, nan
  Final Train: 59.42, nan
   Final Test: 59.28, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 11.8309, Train: 85.84%, Valid: 85.69%, Test: 85.82%
Epoch: 25, Loss: 0.5367, Train: 86.82%, Valid: 86.88%, Test: 86.83%
Epoch: 50, Loss: 0.3691, Train: 85.86%, Valid: 85.70%, Test: 85.86%
Epoch: 75, Loss: 0.3506, Train: 86.24%, Valid: 86.21%, Test: 86.26%
Epoch: 100, Loss: 0.3406, Train: 85.62%, Valid: 85.63%, Test: 85.75%
Epoch: 125, Loss: 0.3365, Train: 86.05%, Valid: 86.06%, Test: 86.14%
Epoch: 150, Loss: 0.3333, Train: 86.06%, Valid: 86.06%, Test: 86.14%
Epoch: 175, Loss: 0.3314, Train: 85.50%, Valid: 85.53%, Test: 85.63%
Run 01:
Highest Train: 87.32
Highest Valid: 87.18
  Final Train: 87.32
   Final Test: 87.29
All runs:
Highest Train: 87.32, nan
Highest Valid: 87.18, nan
  Final Train: 87.32, nan
   Final Test: 87.29, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6925, Train: 84.52%, Valid: 84.53%, Test: 84.55%
Epoch: 25, Loss: 0.5832, Train: 86.08%, Valid: 85.92%, Test: 86.11%
Epoch: 50, Loss: 0.3701, Train: 86.29%, Valid: 86.29%, Test: 86.37%
Epoch: 75, Loss: 0.3548, Train: 86.39%, Valid: 86.21%, Test: 86.40%
Epoch: 100, Loss: 0.3452, Train: 86.63%, Valid: 86.60%, Test: 86.73%
Epoch: 125, Loss: 0.3411, Train: 86.58%, Valid: 86.49%, Test: 86.63%
Epoch: 150, Loss: 0.3350, Train: 86.88%, Valid: 86.81%, Test: 86.90%
Epoch: 175, Loss: 0.3325, Train: 87.10%, Valid: 87.04%, Test: 87.16%
Run 01:
Highest Train: 87.50
Highest Valid: 87.47
  Final Train: 87.50
   Final Test: 87.56
All runs:
Highest Train: 87.50, nan
Highest Valid: 87.47, nan
  Final Train: 87.50, nan
   Final Test: 87.56, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 42.0072, Train: 45.70%, Valid: 45.88%, Test: 45.63%
Epoch: 25, Loss: 7.9501, Train: 77.97%, Valid: 77.89%, Test: 78.18%
Epoch: 50, Loss: 29.5411, Train: 82.66%, Valid: 82.55%, Test: 82.65%
Epoch: 75, Loss: 65.8432, Train: 79.07%, Valid: 79.09%, Test: 79.38%
Epoch: 100, Loss: 6.4874, Train: 84.06%, Valid: 83.95%, Test: 84.10%
Epoch: 125, Loss: 4.4793, Train: 84.69%, Valid: 84.56%, Test: 84.74%
Epoch: 150, Loss: 2.9846, Train: 85.07%, Valid: 84.95%, Test: 85.14%
Epoch: 175, Loss: 1.1049, Train: 85.64%, Valid: 85.49%, Test: 85.72%
Run 01:
Highest Train: 86.44
Highest Valid: 86.45
  Final Train: 86.44
   Final Test: 86.49
All runs:
Highest Train: 86.44, nan
Highest Valid: 86.45, nan
  Final Train: 86.44, nan
   Final Test: 86.49, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 7.0601, Train: 87.13%, Valid: 87.19%, Test: 87.20%
Epoch: 25, Loss: 0.5640, Train: 86.09%, Valid: 86.10%, Test: 86.18%
Epoch: 50, Loss: 0.3886, Train: 85.81%, Valid: 85.69%, Test: 85.85%
Epoch: 75, Loss: 0.3663, Train: 86.06%, Valid: 85.95%, Test: 86.08%
Epoch: 100, Loss: 0.3578, Train: 87.19%, Valid: 87.06%, Test: 87.24%
Epoch: 125, Loss: 0.3527, Train: 87.11%, Valid: 87.00%, Test: 87.14%
Epoch: 150, Loss: 0.3462, Train: 85.35%, Valid: 85.33%, Test: 85.43%
Epoch: 175, Loss: 0.3389, Train: 85.90%, Valid: 85.88%, Test: 86.00%
Run 01:
Highest Train: 87.42
Highest Valid: 87.28
  Final Train: 87.42
   Final Test: 87.42
All runs:
Highest Train: 87.42, nan
Highest Valid: 87.28, nan
  Final Train: 87.42, nan
   Final Test: 87.42, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 5.5390, Train: 86.08%, Valid: 86.04%, Test: 86.16%
Epoch: 25, Loss: 0.5810, Train: 85.98%, Valid: 85.86%, Test: 86.02%
Epoch: 50, Loss: 0.3996, Train: 86.33%, Valid: 86.32%, Test: 86.41%
Epoch: 75, Loss: 0.3749, Train: 85.66%, Valid: 85.49%, Test: 85.72%
Epoch: 100, Loss: 0.3684, Train: 85.94%, Valid: 85.78%, Test: 86.00%
Epoch: 125, Loss: 0.3554, Train: 85.07%, Valid: 84.92%, Test: 85.17%
Epoch: 150, Loss: 0.3523, Train: 85.96%, Valid: 85.81%, Test: 86.04%
Epoch: 175, Loss: 0.3446, Train: 86.06%, Valid: 85.90%, Test: 86.13%
Run 01:
Highest Train: 87.19
Highest Valid: 87.23
  Final Train: 87.19
   Final Test: 87.23
All runs:
Highest Train: 87.19, nan
Highest Valid: 87.23, nan
  Final Train: 87.19, nan
   Final Test: 87.23, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 7.6250, Train: 46.96%, Valid: 46.86%, Test: 47.04%
Epoch: 25, Loss: 7.8215, Train: 54.33%, Valid: 54.24%, Test: 54.28%
Epoch: 50, Loss: 6.7023, Train: 83.06%, Valid: 82.98%, Test: 83.07%
Epoch: 75, Loss: 5.6611, Train: 83.10%, Valid: 83.01%, Test: 83.13%
Epoch: 100, Loss: 1.4009, Train: 84.44%, Valid: 84.48%, Test: 84.55%
Epoch: 125, Loss: 0.6999, Train: 83.52%, Valid: 83.64%, Test: 83.72%
Epoch: 150, Loss: 0.7383, Train: 84.23%, Valid: 84.29%, Test: 84.37%
Epoch: 175, Loss: 1.6888, Train: 80.42%, Valid: 80.57%, Test: 80.81%
Run 01:
Highest Train: 87.84
Highest Valid: 87.70
  Final Train: 87.84
   Final Test: 87.76
All runs:
Highest Train: 87.84, nan
Highest Valid: 87.70, nan
  Final Train: 87.84, nan
   Final Test: 87.76, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 8.7328, Train: 87.67%, Valid: 87.72%, Test: 87.70%
Epoch: 25, Loss: 0.7053, Train: 85.44%, Valid: 85.44%, Test: 85.56%
Epoch: 50, Loss: 0.4168, Train: 87.14%, Valid: 86.99%, Test: 87.17%
Epoch: 75, Loss: 0.3884, Train: 85.17%, Valid: 85.15%, Test: 85.28%
Epoch: 100, Loss: 0.3820, Train: 85.49%, Valid: 85.48%, Test: 85.62%
Epoch: 125, Loss: 0.3617, Train: 85.49%, Valid: 85.57%, Test: 85.61%
Epoch: 150, Loss: 0.3549, Train: 85.75%, Valid: 85.77%, Test: 85.84%
Epoch: 175, Loss: 0.3486, Train: 85.37%, Valid: 85.36%, Test: 85.46%
Run 01:
Highest Train: 88.13
Highest Valid: 88.17
  Final Train: 88.13
   Final Test: 88.20
All runs:
Highest Train: 88.13, nan
Highest Valid: 88.17, nan
  Final Train: 88.13, nan
   Final Test: 88.20, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 5.2060, Train: 84.70%, Valid: 84.57%, Test: 84.82%
Epoch: 25, Loss: 0.6872, Train: 85.47%, Valid: 85.30%, Test: 85.53%
Epoch: 50, Loss: 0.4126, Train: 85.53%, Valid: 85.31%, Test: 85.59%
Epoch: 75, Loss: 0.3855, Train: 85.94%, Valid: 85.93%, Test: 86.06%
Epoch: 100, Loss: 0.3669, Train: 85.61%, Valid: 85.41%, Test: 85.67%
Epoch: 125, Loss: 0.3603, Train: 85.57%, Valid: 85.37%, Test: 85.65%
Epoch: 150, Loss: 0.3524, Train: 86.60%, Valid: 86.54%, Test: 86.70%
Epoch: 175, Loss: 0.3481, Train: 87.12%, Valid: 87.10%, Test: 87.25%
Run 01:
Highest Train: 87.17
Highest Valid: 87.10
  Final Train: 87.12
   Final Test: 87.25
All runs:
Highest Train: 87.17, nan
Highest Valid: 87.10, nan
  Final Train: 87.12, nan
   Final Test: 87.25, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 309.4239, Train: 55.49%, Valid: 55.33%, Test: 55.26%
Epoch: 25, Loss: 63.1033, Train: 81.26%, Valid: 81.23%, Test: 81.34%
Epoch: 50, Loss: 8.6510, Train: 83.52%, Valid: 83.44%, Test: 83.58%
Epoch: 75, Loss: 8.6631, Train: 83.69%, Valid: 83.61%, Test: 83.73%
Epoch: 100, Loss: 6.8649, Train: 83.95%, Valid: 83.85%, Test: 83.99%
Epoch: 125, Loss: 7.0805, Train: 83.96%, Valid: 83.85%, Test: 84.00%
Epoch: 150, Loss: 5.6900, Train: 84.20%, Valid: 84.09%, Test: 84.24%
Epoch: 175, Loss: 6.4716, Train: 84.51%, Valid: 84.38%, Test: 84.56%
Run 01:
Highest Train: 84.74
Highest Valid: 84.62
  Final Train: 84.74
   Final Test: 84.81
All runs:
Highest Train: 84.74, nan
Highest Valid: 84.62, nan
  Final Train: 84.74, nan
   Final Test: 84.81, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 12.1707, Train: 87.11%, Valid: 86.97%, Test: 87.09%
Epoch: 25, Loss: 2.1366, Train: 85.93%, Valid: 85.98%, Test: 86.01%
Epoch: 50, Loss: 2.1227, Train: 85.41%, Valid: 85.47%, Test: 85.53%
Epoch: 75, Loss: 0.6665, Train: 84.88%, Valid: 84.95%, Test: 85.00%
Epoch: 100, Loss: 0.3456, Train: 87.62%, Valid: 87.41%, Test: 87.65%
Epoch: 125, Loss: 0.3490, Train: 87.51%, Valid: 87.31%, Test: 87.56%
Epoch: 150, Loss: 0.3476, Train: 87.41%, Valid: 87.24%, Test: 87.41%
Epoch: 175, Loss: 0.3442, Train: 87.30%, Valid: 87.31%, Test: 87.39%
Run 01:
Highest Train: 88.23
Highest Valid: 88.25
  Final Train: 88.23
   Final Test: 88.35
All runs:
Highest Train: 88.23, nan
Highest Valid: 88.25, nan
  Final Train: 88.23, nan
   Final Test: 88.35, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 18.3956, Train: 87.80%, Valid: 87.67%, Test: 87.70%
Epoch: 25, Loss: 13.7171, Train: 88.03%, Valid: 87.89%, Test: 87.90%
Epoch: 50, Loss: 5.6609, Train: 85.44%, Valid: 85.31%, Test: 85.49%
Epoch: 75, Loss: 5.6286, Train: 84.20%, Valid: 84.17%, Test: 84.38%
Epoch: 100, Loss: 5.7755, Train: 85.46%, Valid: 85.37%, Test: 85.59%
Epoch: 125, Loss: 2.3885, Train: 84.91%, Valid: 84.90%, Test: 85.05%
Epoch: 150, Loss: 4.7140, Train: 87.32%, Valid: 87.20%, Test: 87.29%
Epoch: 175, Loss: 4.7150, Train: 87.74%, Valid: 87.58%, Test: 87.67%
Run 01:
Highest Train: 88.14
Highest Valid: 87.99
  Final Train: 88.14
   Final Test: 88.03
All runs:
Highest Train: 88.14, nan
Highest Valid: 87.99, nan
  Final Train: 88.14, nan
   Final Test: 88.03, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 32714.4668, Train: 45.29%, Valid: 45.57%, Test: 45.55%
Epoch: 25, Loss: 96.2429, Train: 80.42%, Valid: 80.28%, Test: 80.32%
Epoch: 50, Loss: 54.4244, Train: 82.90%, Valid: 82.82%, Test: 82.90%
Epoch: 75, Loss: 45.5703, Train: 83.12%, Valid: 83.03%, Test: 83.13%
Epoch: 100, Loss: 37.7882, Train: 83.25%, Valid: 83.17%, Test: 83.28%
Epoch: 125, Loss: 31.5321, Train: 83.39%, Valid: 83.31%, Test: 83.43%
Epoch: 150, Loss: 21.9220, Train: 83.63%, Valid: 83.56%, Test: 83.70%
Epoch: 175, Loss: 14.6695, Train: 83.86%, Valid: 83.78%, Test: 83.89%
Run 01:
Highest Train: 84.08
Highest Valid: 83.97
  Final Train: 84.08
   Final Test: 84.10
All runs:
Highest Train: 84.08, nan
Highest Valid: 83.97, nan
  Final Train: 84.08, nan
   Final Test: 84.10, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 17.0692, Train: 85.57%, Valid: 85.45%, Test: 85.63%
Epoch: 25, Loss: 0.7157, Train: 86.42%, Valid: 86.52%, Test: 86.56%
Epoch: 50, Loss: 0.4262, Train: 86.88%, Valid: 86.75%, Test: 86.89%
Epoch: 75, Loss: 0.3853, Train: 85.34%, Valid: 85.36%, Test: 85.44%
Epoch: 100, Loss: 0.3736, Train: 85.64%, Valid: 85.63%, Test: 85.75%
Epoch: 125, Loss: 0.3524, Train: 85.53%, Valid: 85.46%, Test: 85.63%
Epoch: 150, Loss: 0.3411, Train: 86.05%, Valid: 85.92%, Test: 86.09%
Epoch: 175, Loss: 0.3368, Train: 86.43%, Valid: 86.29%, Test: 86.50%
Run 01:
Highest Train: 87.24
Highest Valid: 87.29
  Final Train: 87.24
   Final Test: 87.31
All runs:
Highest Train: 87.24, nan
Highest Valid: 87.29, nan
  Final Train: 87.24, nan
   Final Test: 87.31, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 4.1146, Train: 86.46%, Valid: 86.38%, Test: 86.49%
Epoch: 25, Loss: 0.5853, Train: 86.29%, Valid: 86.17%, Test: 86.30%
Epoch: 50, Loss: 0.4644, Train: 86.61%, Valid: 86.50%, Test: 86.64%
Epoch: 75, Loss: 0.4363, Train: 86.61%, Valid: 86.50%, Test: 86.67%
Epoch: 100, Loss: 0.3812, Train: 85.85%, Valid: 85.67%, Test: 85.87%
Epoch: 125, Loss: 0.3609, Train: 85.78%, Valid: 85.61%, Test: 85.83%
Epoch: 150, Loss: 0.3461, Train: 85.79%, Valid: 85.63%, Test: 85.86%
Epoch: 175, Loss: 0.3391, Train: 85.99%, Valid: 85.81%, Test: 86.08%
Run 01:
Highest Train: 88.09
Highest Valid: 87.99
  Final Train: 88.09
   Final Test: 88.09
All runs:
Highest Train: 88.09, nan
Highest Valid: 87.99, nan
  Final Train: 88.09, nan
   Final Test: 88.09, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 20.1775, Train: 64.09%, Valid: 64.27%, Test: 64.63%
Epoch: 25, Loss: 2.4933, Train: 83.89%, Valid: 83.80%, Test: 83.96%
Epoch: 50, Loss: 1.7170, Train: 84.65%, Valid: 84.53%, Test: 84.71%
Epoch: 75, Loss: 1.8836, Train: 84.88%, Valid: 84.76%, Test: 84.95%
Epoch: 100, Loss: 1.0748, Train: 85.32%, Valid: 85.19%, Test: 85.39%
Epoch: 125, Loss: 0.9021, Train: 85.43%, Valid: 85.27%, Test: 85.51%
Epoch: 150, Loss: 0.6215, Train: 87.25%, Valid: 87.11%, Test: 87.22%
Epoch: 175, Loss: 0.6775, Train: 85.40%, Valid: 85.27%, Test: 85.45%
Run 01:
Highest Train: 87.75
Highest Valid: 87.52
  Final Train: 87.75
   Final Test: 87.73
All runs:
Highest Train: 87.75, nan
Highest Valid: 87.52, nan
  Final Train: 87.75, nan
   Final Test: 87.73, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 29.5032, Train: 84.86%, Valid: 84.66%, Test: 84.95%
Epoch: 25, Loss: 0.6529, Train: 86.60%, Valid: 86.67%, Test: 86.70%
Epoch: 50, Loss: 0.4502, Train: 86.27%, Valid: 86.31%, Test: 86.36%
Epoch: 75, Loss: 0.3839, Train: 85.60%, Valid: 85.64%, Test: 85.69%
Epoch: 100, Loss: 0.3587, Train: 86.02%, Valid: 85.83%, Test: 86.11%
Epoch: 125, Loss: 0.3484, Train: 85.78%, Valid: 85.77%, Test: 85.89%
Epoch: 150, Loss: 0.3420, Train: 85.74%, Valid: 85.76%, Test: 85.88%
Epoch: 175, Loss: 0.3369, Train: 87.35%, Valid: 87.34%, Test: 87.41%
Run 01:
Highest Train: 87.61
Highest Valid: 87.70
  Final Train: 87.60
   Final Test: 87.64
All runs:
Highest Train: 87.61, nan
Highest Valid: 87.70, nan
  Final Train: 87.60, nan
   Final Test: 87.64, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 3.0148, Train: 84.59%, Valid: 84.70%, Test: 84.75%
Epoch: 25, Loss: 0.8729, Train: 86.58%, Valid: 86.48%, Test: 86.63%
Epoch: 50, Loss: 0.4200, Train: 86.23%, Valid: 86.07%, Test: 86.27%
Epoch: 75, Loss: 0.3814, Train: 86.04%, Valid: 86.03%, Test: 86.18%
Epoch: 100, Loss: 0.3599, Train: 85.61%, Valid: 85.55%, Test: 85.81%
Epoch: 125, Loss: 0.3470, Train: 85.93%, Valid: 85.74%, Test: 86.00%
Epoch: 150, Loss: 0.3404, Train: 87.21%, Valid: 87.17%, Test: 87.24%
Epoch: 175, Loss: 0.3386, Train: 87.43%, Valid: 87.29%, Test: 87.51%
Run 01:
Highest Train: 88.31
Highest Valid: 88.15
  Final Train: 88.31
   Final Test: 88.37
All runs:
Highest Train: 88.31, nan
Highest Valid: 88.15, nan
  Final Train: 88.31, nan
   Final Test: 88.37, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 520.9479, Train: 66.58%, Valid: 66.38%, Test: 66.27%
Epoch: 25, Loss: 3.2784, Train: 84.54%, Valid: 84.42%, Test: 84.60%
Epoch: 50, Loss: 3.0994, Train: 84.73%, Valid: 84.62%, Test: 84.79%
Epoch: 75, Loss: 3.6716, Train: 84.89%, Valid: 84.76%, Test: 84.96%
Epoch: 100, Loss: 3.3014, Train: 85.06%, Valid: 84.93%, Test: 85.14%
Epoch: 125, Loss: 2.0569, Train: 84.63%, Valid: 84.52%, Test: 84.69%
Epoch: 150, Loss: 2.1397, Train: 84.02%, Valid: 83.93%, Test: 84.06%
Epoch: 175, Loss: 1.5887, Train: 73.86%, Valid: 73.91%, Test: 74.15%
Run 01:
Highest Train: 87.17
Highest Valid: 87.05
  Final Train: 87.17
   Final Test: 87.09
All runs:
Highest Train: 87.17, nan
Highest Valid: 87.05, nan
  Final Train: 87.17, nan
   Final Test: 87.09, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 9.6669, Train: 86.65%, Valid: 86.68%, Test: 86.71%
Epoch: 25, Loss: 1.1573, Train: 86.73%, Valid: 86.83%, Test: 86.83%
Epoch: 50, Loss: 0.4351, Train: 86.28%, Valid: 86.38%, Test: 86.39%
Epoch: 75, Loss: 0.3757, Train: 85.28%, Valid: 85.31%, Test: 85.38%
Epoch: 100, Loss: 0.3636, Train: 85.22%, Valid: 85.18%, Test: 85.33%
Epoch: 125, Loss: 0.3522, Train: 86.11%, Valid: 85.96%, Test: 86.23%
Epoch: 150, Loss: 0.3490, Train: 87.58%, Valid: 87.46%, Test: 87.58%
Epoch: 175, Loss: 0.3473, Train: 85.31%, Valid: 85.27%, Test: 85.44%
Run 01:
Highest Train: 87.65
Highest Valid: 87.52
  Final Train: 87.65
   Final Test: 87.69
All runs:
Highest Train: 87.65, nan
Highest Valid: 87.52, nan
  Final Train: 87.65, nan
   Final Test: 87.69, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 3.6094, Train: 85.62%, Valid: 85.49%, Test: 85.57%
Epoch: 25, Loss: 0.7556, Train: 85.66%, Valid: 85.64%, Test: 85.78%
Epoch: 50, Loss: 0.3944, Train: 85.94%, Valid: 85.81%, Test: 86.05%
Epoch: 75, Loss: 0.3651, Train: 85.51%, Valid: 85.33%, Test: 85.57%
Epoch: 100, Loss: 0.3509, Train: 85.70%, Valid: 85.53%, Test: 85.76%
Epoch: 125, Loss: 0.3438, Train: 87.07%, Valid: 87.07%, Test: 87.15%
Epoch: 150, Loss: 0.3418, Train: 86.45%, Valid: 86.41%, Test: 86.52%
Epoch: 175, Loss: 0.3377, Train: 86.35%, Valid: 86.30%, Test: 86.44%
Run 01:
Highest Train: 87.51
Highest Valid: 87.42
  Final Train: 87.51
   Final Test: 87.49
All runs:
Highest Train: 87.51, nan
Highest Valid: 87.42, nan
  Final Train: 87.51, nan
   Final Test: 87.49, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 16.8151, Train: 52.83%, Valid: 53.00%, Test: 53.11%
Epoch: 25, Loss: 2.3538, Train: 84.43%, Valid: 84.31%, Test: 84.47%
Epoch: 50, Loss: 2.0739, Train: 84.39%, Valid: 84.27%, Test: 84.43%
Epoch: 75, Loss: 1.9676, Train: 84.82%, Valid: 84.69%, Test: 84.88%
Epoch: 100, Loss: 1.9345, Train: 84.53%, Valid: 84.41%, Test: 84.57%
Epoch: 125, Loss: 1.8406, Train: 84.45%, Valid: 84.33%, Test: 84.49%
Epoch: 150, Loss: 1.6399, Train: 84.34%, Valid: 84.22%, Test: 84.38%
Epoch: 175, Loss: 1.6692, Train: 84.32%, Valid: 84.20%, Test: 84.35%
Run 01:
Highest Train: 84.82
Highest Valid: 84.69
  Final Train: 84.82
   Final Test: 84.88
All runs:
Highest Train: 84.82, nan
Highest Valid: 84.69, nan
  Final Train: 84.82, nan
   Final Test: 84.88, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.1895, Train: 85.82%, Valid: 85.67%, Test: 85.81%
Epoch: 25, Loss: 3.0784, Train: 86.74%, Valid: 86.77%, Test: 86.75%
Epoch: 50, Loss: 3.8339, Train: 86.30%, Valid: 86.33%, Test: 86.33%
Epoch: 75, Loss: 4.3121, Train: 87.84%, Valid: 87.83%, Test: 87.80%
Epoch: 100, Loss: 4.5355, Train: 87.84%, Valid: 87.83%, Test: 87.79%
Epoch: 125, Loss: 4.5665, Train: 87.71%, Valid: 87.70%, Test: 87.66%
Epoch: 150, Loss: 4.6251, Train: 87.81%, Valid: 87.80%, Test: 87.77%
Epoch: 175, Loss: 4.5396, Train: 87.84%, Valid: 87.83%, Test: 87.82%
Run 01:
Highest Train: 87.92
Highest Valid: 87.92
  Final Train: 87.92
   Final Test: 87.88
All runs:
Highest Train: 87.92, nan
Highest Valid: 87.92, nan
  Final Train: 87.92, nan
   Final Test: 87.88, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 22.8639, Train: 86.25%, Valid: 86.26%, Test: 86.35%
Epoch: 25, Loss: 1.2662, Train: 86.69%, Valid: 86.63%, Test: 86.74%
Epoch: 50, Loss: 145.0663, Train: 18.17%, Valid: 18.27%, Test: 18.11%
Epoch: 75, Loss: 186.7621, Train: 17.97%, Valid: 18.12%, Test: 17.89%
Epoch: 100, Loss: 179.4143, Train: 17.34%, Valid: 17.42%, Test: 17.24%
Epoch: 125, Loss: 35.6167, Train: 16.90%, Valid: 16.79%, Test: 16.84%
Epoch: 150, Loss: 1.2551, Train: 85.83%, Valid: 85.65%, Test: 85.89%
Epoch: 175, Loss: 1.1754, Train: 86.00%, Valid: 85.85%, Test: 86.04%
Run 01:
Highest Train: 86.82
Highest Valid: 86.84
  Final Train: 86.80
   Final Test: 86.87
All runs:
Highest Train: 86.82, nan
Highest Valid: 86.84, nan
  Final Train: 86.80, nan
   Final Test: 86.87, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 27452.8867, Train: 47.23%, Valid: 47.20%, Test: 47.32%
Epoch: 25, Loss: 12061.9023, Train: 61.73%, Valid: 62.02%, Test: 62.27%
Epoch: 50, Loss: 1299.9438, Train: 72.35%, Valid: 72.53%, Test: 72.74%
Epoch: 75, Loss: 7885.6646, Train: 22.32%, Valid: 22.59%, Test: 22.41%
Epoch: 100, Loss: 6041.8354, Train: 16.30%, Valid: 16.45%, Test: 16.24%
Epoch: 125, Loss: 4475.6953, Train: 16.37%, Valid: 16.53%, Test: 16.32%
Epoch: 150, Loss: 4159.9355, Train: 17.94%, Valid: 18.10%, Test: 17.96%
Epoch: 175, Loss: 5785.3682, Train: 16.32%, Valid: 16.48%, Test: 16.28%
Run 01:
Highest Train: 84.00
Highest Valid: 83.87
  Final Train: 84.00
   Final Test: 84.12
All runs:
Highest Train: 84.00, nan
Highest Valid: 83.87, nan
  Final Train: 84.00, nan
   Final Test: 84.12, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 3.5777, Train: 85.76%, Valid: 85.61%, Test: 85.78%
Epoch: 25, Loss: 0.7745, Train: 86.88%, Valid: 86.94%, Test: 86.94%
Epoch: 50, Loss: 0.4628, Train: 86.07%, Valid: 86.09%, Test: 86.17%
Epoch: 75, Loss: 0.4141, Train: 85.92%, Valid: 85.91%, Test: 86.02%
Epoch: 100, Loss: 0.3823, Train: 85.89%, Valid: 85.90%, Test: 86.00%
Epoch: 125, Loss: 0.3695, Train: 86.06%, Valid: 86.05%, Test: 86.17%
Epoch: 150, Loss: 0.3574, Train: 86.01%, Valid: 86.00%, Test: 86.12%
Epoch: 175, Loss: 0.3416, Train: 85.91%, Valid: 85.92%, Test: 86.01%
Run 01:
Highest Train: 86.90
Highest Valid: 86.97
  Final Train: 86.90
   Final Test: 86.94
All runs:
Highest Train: 86.90, nan
Highest Valid: 86.97, nan
  Final Train: 86.90, nan
   Final Test: 86.94, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.5127, Train: 85.84%, Valid: 85.65%, Test: 85.88%
Epoch: 25, Loss: 0.3790, Train: 85.97%, Valid: 85.82%, Test: 85.99%
Epoch: 50, Loss: 0.3601, Train: 85.79%, Valid: 85.62%, Test: 85.84%
Epoch: 75, Loss: 0.3481, Train: 85.34%, Valid: 85.16%, Test: 85.41%
Epoch: 100, Loss: 0.3385, Train: 85.38%, Valid: 85.21%, Test: 85.46%
Epoch: 125, Loss: 0.3335, Train: 85.98%, Valid: 85.80%, Test: 86.06%
Epoch: 150, Loss: 0.3299, Train: 86.11%, Valid: 85.91%, Test: 86.18%
Epoch: 175, Loss: 0.3278, Train: 86.78%, Valid: 86.66%, Test: 86.85%
Run 01:
Highest Train: 88.27
Highest Valid: 88.10
  Final Train: 88.27
   Final Test: 88.23
All runs:
Highest Train: 88.27, nan
Highest Valid: 88.10, nan
  Final Train: 88.27, nan
   Final Test: 88.23, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 179.2139, Train: 70.97%, Valid: 70.79%, Test: 70.68%
Epoch: 25, Loss: 317.2854, Train: 69.46%, Valid: 69.60%, Test: 69.79%
Epoch: 50, Loss: 2.7092, Train: 84.22%, Valid: 84.14%, Test: 84.29%
Epoch: 75, Loss: 1.3280, Train: 81.79%, Valid: 81.89%, Test: 82.05%
Epoch: 100, Loss: 0.9047, Train: 85.46%, Valid: 85.32%, Test: 85.53%
Epoch: 125, Loss: 0.8847, Train: 84.81%, Valid: 84.77%, Test: 84.90%
Epoch: 150, Loss: 0.8689, Train: 85.70%, Valid: 85.54%, Test: 85.75%
Epoch: 175, Loss: 0.8753, Train: 83.89%, Valid: 83.94%, Test: 84.03%
Run 01:
Highest Train: 86.05
Highest Valid: 85.91
  Final Train: 86.05
   Final Test: 86.03
All runs:
Highest Train: 86.05, nan
Highest Valid: 85.91, nan
  Final Train: 86.05, nan
   Final Test: 86.03, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 5.3565, Train: 88.34%, Valid: 88.40%, Test: 88.39%
Epoch: 25, Loss: 0.7749, Train: 85.75%, Valid: 85.62%, Test: 85.79%
Epoch: 50, Loss: 0.4471, Train: 86.01%, Valid: 86.06%, Test: 86.09%
Epoch: 75, Loss: 0.3857, Train: 85.85%, Valid: 85.89%, Test: 85.94%
Epoch: 100, Loss: 0.3508, Train: 86.06%, Valid: 86.08%, Test: 86.15%
Epoch: 125, Loss: 0.3417, Train: 85.85%, Valid: 85.89%, Test: 85.94%
Epoch: 150, Loss: 0.3368, Train: 85.98%, Valid: 86.00%, Test: 86.07%
Epoch: 175, Loss: 0.3359, Train: 87.99%, Valid: 88.05%, Test: 88.01%
Run 01:
Highest Train: 88.34
Highest Valid: 88.40
  Final Train: 88.34
   Final Test: 88.39
All runs:
Highest Train: 88.34, nan
Highest Valid: 88.40, nan
  Final Train: 88.34, nan
   Final Test: 88.39, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 4.5141, Train: 86.15%, Valid: 86.02%, Test: 86.23%
Epoch: 25, Loss: 0.8872, Train: 86.52%, Valid: 86.40%, Test: 86.57%
Epoch: 50, Loss: 0.4162, Train: 85.75%, Valid: 85.54%, Test: 85.79%
Epoch: 75, Loss: 0.3651, Train: 86.89%, Valid: 86.93%, Test: 87.05%
Epoch: 100, Loss: 0.3527, Train: 86.66%, Valid: 86.63%, Test: 86.72%
Epoch: 125, Loss: 0.3469, Train: 86.54%, Valid: 86.38%, Test: 86.61%
Epoch: 150, Loss: 0.3407, Train: 86.37%, Valid: 86.20%, Test: 86.45%
Epoch: 175, Loss: 0.3355, Train: 88.10%, Valid: 87.99%, Test: 88.11%
Run 01:
Highest Train: 88.35
Highest Valid: 88.25
  Final Train: 88.35
   Final Test: 88.32
All runs:
Highest Train: 88.35, nan
Highest Valid: 88.25, nan
  Final Train: 88.35, nan
   Final Test: 88.32, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 338.2768, Train: 61.72%, Valid: 61.58%, Test: 61.49%
Epoch: 25, Loss: 10.4459, Train: 81.80%, Valid: 81.68%, Test: 81.79%
Epoch: 50, Loss: 8.6802, Train: 67.59%, Valid: 67.80%, Test: 67.96%
Epoch: 75, Loss: 8.9502, Train: 64.13%, Valid: 64.42%, Test: 64.60%
Epoch: 100, Loss: 8.4769, Train: 69.08%, Valid: 69.16%, Test: 69.38%
Epoch: 125, Loss: 7.6749, Train: 71.76%, Valid: 71.78%, Test: 72.08%
Epoch: 150, Loss: 7.3039, Train: 71.90%, Valid: 72.05%, Test: 72.24%
Epoch: 175, Loss: 7.5286, Train: 77.69%, Valid: 77.76%, Test: 78.02%
Run 01:
Highest Train: 83.81
Highest Valid: 83.70
  Final Train: 83.81
   Final Test: 83.89
All runs:
Highest Train: 83.81, nan
Highest Valid: 83.70, nan
  Final Train: 83.81, nan
   Final Test: 83.89, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 5.8344, Train: 86.62%, Valid: 86.61%, Test: 86.75%
Epoch: 25, Loss: 1.0168, Train: 88.07%, Valid: 88.11%, Test: 88.07%
Epoch: 50, Loss: 0.4415, Train: 85.99%, Valid: 86.02%, Test: 86.05%
Epoch: 75, Loss: 0.3711, Train: 86.60%, Valid: 86.66%, Test: 86.63%
Epoch: 100, Loss: 0.3614, Train: 85.88%, Valid: 85.93%, Test: 86.00%
Epoch: 125, Loss: 0.3539, Train: 85.47%, Valid: 85.47%, Test: 85.54%
Epoch: 150, Loss: 0.3433, Train: 85.38%, Valid: 85.35%, Test: 85.45%
Epoch: 175, Loss: 0.3411, Train: 85.66%, Valid: 85.67%, Test: 85.74%
Run 01:
Highest Train: 88.10
Highest Valid: 88.13
  Final Train: 88.10
   Final Test: 88.10
All runs:
Highest Train: 88.10, nan
Highest Valid: 88.13, nan
  Final Train: 88.10, nan
   Final Test: 88.10, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 13.5676, Train: 85.51%, Valid: 85.33%, Test: 85.57%
Epoch: 25, Loss: 1.3223, Train: 86.72%, Valid: 86.76%, Test: 86.78%
Epoch: 50, Loss: 0.4929, Train: 86.08%, Valid: 86.10%, Test: 86.14%
Epoch: 75, Loss: 0.3847, Train: 84.83%, Valid: 84.90%, Test: 85.01%
Epoch: 100, Loss: 0.3666, Train: 86.04%, Valid: 86.02%, Test: 86.14%
Epoch: 125, Loss: 0.3613, Train: 87.00%, Valid: 86.95%, Test: 87.04%
Epoch: 150, Loss: 0.3470, Train: 86.21%, Valid: 86.15%, Test: 86.26%
Epoch: 175, Loss: 0.3421, Train: 86.48%, Valid: 86.34%, Test: 86.52%
Run 01:
Highest Train: 87.78
Highest Valid: 87.67
  Final Train: 87.78
   Final Test: 87.81
All runs:
Highest Train: 87.78, nan
Highest Valid: 87.67, nan
  Final Train: 87.78, nan
   Final Test: 87.81, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 288.9558, Train: 54.46%, Valid: 54.54%, Test: 54.39%
Epoch: 25, Loss: 12.6051, Train: 77.40%, Valid: 77.20%, Test: 77.28%
Epoch: 50, Loss: 8.9112, Train: 66.33%, Valid: 66.60%, Test: 66.85%
Epoch: 75, Loss: 9.0252, Train: 82.58%, Valid: 82.49%, Test: 82.62%
Epoch: 100, Loss: 7.5468, Train: 82.61%, Valid: 82.52%, Test: 82.65%
Epoch: 125, Loss: 6.8458, Train: 83.29%, Valid: 83.21%, Test: 83.38%
Epoch: 150, Loss: 5.8752, Train: 79.92%, Valid: 79.89%, Test: 80.13%
Epoch: 175, Loss: 4.4665, Train: 83.82%, Valid: 83.75%, Test: 83.87%
Run 01:
Highest Train: 86.02
Highest Valid: 85.88
  Final Train: 86.02
   Final Test: 85.94
All runs:
Highest Train: 86.02, nan
Highest Valid: 85.88, nan
  Final Train: 86.02, nan
   Final Test: 85.94, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 16.1272, Train: 85.81%, Valid: 85.66%, Test: 85.79%
Epoch: 25, Loss: 1.9992, Train: 86.39%, Valid: 86.37%, Test: 86.49%
Epoch: 50, Loss: 2.3465, Train: 86.95%, Valid: 86.78%, Test: 87.06%
Epoch: 75, Loss: 2.3865, Train: 86.65%, Valid: 86.52%, Test: 86.80%
Epoch: 100, Loss: 2.3991, Train: 86.64%, Valid: 86.52%, Test: 86.79%
Epoch: 125, Loss: 2.4023, Train: 86.62%, Valid: 86.49%, Test: 86.76%
Epoch: 150, Loss: 2.4110, Train: 86.67%, Valid: 86.55%, Test: 86.81%
Epoch: 175, Loss: 2.3931, Train: 86.61%, Valid: 86.48%, Test: 86.76%
Run 01:
Highest Train: 87.08
Highest Valid: 86.92
  Final Train: 87.08
   Final Test: 87.21
All runs:
Highest Train: 87.08, nan
Highest Valid: 86.92, nan
  Final Train: 87.08, nan
   Final Test: 87.21, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 11.4417, Train: 84.67%, Valid: 84.67%, Test: 84.75%
Epoch: 25, Loss: 0.6702, Train: 85.98%, Valid: 85.82%, Test: 86.04%
Epoch: 50, Loss: 0.3739, Train: 85.76%, Valid: 85.58%, Test: 85.81%
Epoch: 75, Loss: 0.3546, Train: 85.79%, Valid: 85.62%, Test: 85.85%
Epoch: 100, Loss: 0.3432, Train: 85.74%, Valid: 85.56%, Test: 85.80%
Epoch: 125, Loss: 0.3372, Train: 85.54%, Valid: 85.35%, Test: 85.63%
Epoch: 150, Loss: 0.3295, Train: 85.68%, Valid: 85.48%, Test: 85.75%
Epoch: 175, Loss: 1.1084, Train: 84.96%, Valid: 84.88%, Test: 85.12%
Run 01:
Highest Train: 86.76
Highest Valid: 86.66
  Final Train: 86.76
   Final Test: 86.72
All runs:
Highest Train: 86.76, nan
Highest Valid: 86.66, nan
  Final Train: 86.76, nan
   Final Test: 86.72, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 112.0699, Train: 49.93%, Valid: 49.92%, Test: 49.93%
Epoch: 25, Loss: 740.6986, Train: 75.91%, Valid: 76.00%, Test: 76.24%
Epoch: 50, Loss: 852.3295, Train: 75.49%, Valid: 75.58%, Test: 75.77%
Epoch: 75, Loss: 785.0735, Train: 76.82%, Valid: 76.96%, Test: 77.18%
Epoch: 100, Loss: 713.9488, Train: 76.12%, Valid: 76.23%, Test: 76.47%
Epoch: 125, Loss: 714.4894, Train: 77.03%, Valid: 77.18%, Test: 77.40%
Epoch: 150, Loss: 658.2466, Train: 75.70%, Valid: 75.79%, Test: 76.01%
Epoch: 175, Loss: 700.0098, Train: 76.69%, Valid: 76.84%, Test: 77.06%
Run 01:
Highest Train: 79.48
Highest Valid: 79.59
  Final Train: 79.48
   Final Test: 79.82
All runs:
Highest Train: 79.48, nan
Highest Valid: 79.59, nan
  Final Train: 79.48, nan
   Final Test: 79.82, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.8793, Train: 86.54%, Valid: 86.39%, Test: 86.56%
Epoch: 25, Loss: 0.4017, Train: 85.78%, Valid: 85.78%, Test: 85.84%
Epoch: 50, Loss: 0.3662, Train: 85.97%, Valid: 85.98%, Test: 86.07%
Epoch: 75, Loss: 0.3538, Train: 87.17%, Valid: 87.11%, Test: 87.24%
Epoch: 100, Loss: 0.3428, Train: 86.18%, Valid: 86.17%, Test: 86.28%
Epoch: 125, Loss: 0.3381, Train: 86.06%, Valid: 86.08%, Test: 86.15%
Epoch: 150, Loss: 0.3319, Train: 86.04%, Valid: 86.07%, Test: 86.14%
Epoch: 175, Loss: 0.3324, Train: 86.06%, Valid: 86.08%, Test: 86.15%
Run 01:
Highest Train: 87.53
Highest Valid: 87.61
  Final Train: 87.53
   Final Test: 87.55
All runs:
Highest Train: 87.53, nan
Highest Valid: 87.61, nan
  Final Train: 87.53, nan
   Final Test: 87.55, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 5.1009, Train: 85.77%, Valid: 85.74%, Test: 85.85%
Epoch: 25, Loss: 0.9574, Train: 86.04%, Valid: 86.06%, Test: 86.15%
Epoch: 50, Loss: 0.3976, Train: 85.30%, Valid: 85.27%, Test: 85.44%
Epoch: 75, Loss: 0.3709, Train: 85.83%, Valid: 85.79%, Test: 85.91%
Epoch: 100, Loss: 0.3581, Train: 86.24%, Valid: 86.20%, Test: 86.37%
Epoch: 125, Loss: 0.3473, Train: 85.35%, Valid: 85.34%, Test: 85.47%
Epoch: 150, Loss: 0.3395, Train: 85.36%, Valid: 85.35%, Test: 85.47%
Epoch: 175, Loss: 0.3366, Train: 85.75%, Valid: 85.55%, Test: 85.83%
Run 01:
Highest Train: 87.02
Highest Valid: 87.07
  Final Train: 87.02
   Final Test: 87.10
All runs:
Highest Train: 87.02, nan
Highest Valid: 87.07, nan
  Final Train: 87.02, nan
   Final Test: 87.10, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 314.1287, Train: 36.35%, Valid: 36.02%, Test: 35.84%
Epoch: 25, Loss: 43.1105, Train: 29.29%, Valid: 29.14%, Test: 28.89%
Epoch: 50, Loss: 12.8731, Train: 80.82%, Valid: 80.94%, Test: 81.15%
Epoch: 75, Loss: 1.2435, Train: 82.76%, Valid: 82.88%, Test: 83.04%
Epoch: 100, Loss: 1.1662, Train: 83.89%, Valid: 83.98%, Test: 84.04%
Epoch: 125, Loss: 1.1365, Train: 83.71%, Valid: 83.66%, Test: 83.75%
Epoch: 150, Loss: 1.0942, Train: 86.22%, Valid: 86.23%, Test: 86.28%
Epoch: 175, Loss: 1.0609, Train: 84.40%, Valid: 84.51%, Test: 84.63%
Run 01:
Highest Train: 86.23
Highest Valid: 86.24
  Final Train: 86.23
   Final Test: 86.32
All runs:
Highest Train: 86.23, nan
Highest Valid: 86.24, nan
  Final Train: 86.23, nan
   Final Test: 86.32, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 6.8883, Train: 86.31%, Valid: 86.29%, Test: 86.46%
Epoch: 25, Loss: 1.0419, Train: 86.99%, Valid: 87.02%, Test: 87.06%
Epoch: 50, Loss: 0.4192, Train: 85.67%, Valid: 85.52%, Test: 85.69%
Epoch: 75, Loss: 0.3765, Train: 85.73%, Valid: 85.71%, Test: 85.81%
Epoch: 100, Loss: 0.3654, Train: 86.00%, Valid: 85.99%, Test: 86.15%
Epoch: 125, Loss: 0.3530, Train: 85.61%, Valid: 85.58%, Test: 85.76%
Epoch: 150, Loss: 0.3441, Train: 87.30%, Valid: 87.14%, Test: 87.33%
Epoch: 175, Loss: 0.3423, Train: 86.05%, Valid: 86.04%, Test: 86.15%
Run 01:
Highest Train: 87.43
Highest Valid: 87.53
  Final Train: 87.43
   Final Test: 87.51
All runs:
Highest Train: 87.43, nan
Highest Valid: 87.53, nan
  Final Train: 87.43, nan
   Final Test: 87.51, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 6.0167, Train: 85.93%, Valid: 85.77%, Test: 86.01%
Epoch: 25, Loss: 0.8839, Train: 85.65%, Valid: 85.46%, Test: 85.69%
Epoch: 50, Loss: 0.4070, Train: 85.60%, Valid: 85.43%, Test: 85.64%
Epoch: 75, Loss: 0.3663, Train: 85.69%, Valid: 85.64%, Test: 85.75%
Epoch: 100, Loss: 0.3541, Train: 85.59%, Valid: 85.55%, Test: 85.65%
Epoch: 125, Loss: 0.3463, Train: 86.23%, Valid: 86.13%, Test: 86.33%
Epoch: 150, Loss: 0.3443, Train: 85.68%, Valid: 85.50%, Test: 85.76%
Epoch: 175, Loss: 0.3415, Train: 85.90%, Valid: 85.72%, Test: 85.96%
Run 01:
Highest Train: 86.79
Highest Valid: 86.78
  Final Train: 86.79
   Final Test: 86.83
All runs:
Highest Train: 86.79, nan
Highest Valid: 86.78, nan
  Final Train: 86.79, nan
   Final Test: 86.83, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 162.0619, Train: 59.04%, Valid: 58.86%, Test: 58.92%
Epoch: 25, Loss: 10.7867, Train: 84.64%, Valid: 84.51%, Test: 84.70%
Epoch: 50, Loss: 2.8788, Train: 84.34%, Valid: 84.42%, Test: 84.47%
Epoch: 75, Loss: 2.4772, Train: 85.73%, Valid: 85.59%, Test: 85.80%
Epoch: 100, Loss: 2.1226, Train: 85.61%, Valid: 85.48%, Test: 85.68%
Epoch: 125, Loss: 1.6059, Train: 88.11%, Valid: 87.83%, Test: 88.01%
Epoch: 150, Loss: 1.1826, Train: 85.72%, Valid: 85.56%, Test: 85.80%
Epoch: 175, Loss: 0.9543, Train: 85.64%, Valid: 85.48%, Test: 85.71%
Run 01:
Highest Train: 88.26
Highest Valid: 88.14
  Final Train: 88.26
   Final Test: 88.31
All runs:
Highest Train: 88.26, nan
Highest Valid: 88.14, nan
  Final Train: 88.26, nan
   Final Test: 88.31, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.1422, Train: 87.62%, Valid: 87.70%, Test: 87.63%
Epoch: 25, Loss: 0.5449, Train: 85.85%, Valid: 85.75%, Test: 85.90%
Epoch: 50, Loss: 0.3739, Train: 86.05%, Valid: 85.88%, Test: 86.10%
Epoch: 75, Loss: 0.3618, Train: 86.32%, Valid: 86.31%, Test: 86.37%
Epoch: 100, Loss: 0.3573, Train: 85.88%, Valid: 85.93%, Test: 85.96%
Epoch: 125, Loss: 0.3479, Train: 85.65%, Valid: 85.66%, Test: 85.68%
Epoch: 150, Loss: 0.3410, Train: 85.83%, Valid: 85.81%, Test: 85.93%
Epoch: 175, Loss: 0.3373, Train: 85.72%, Valid: 85.73%, Test: 85.84%
Run 01:
Highest Train: 87.62
Highest Valid: 87.70
  Final Train: 87.62
   Final Test: 87.63
All runs:
Highest Train: 87.62, nan
Highest Valid: 87.70, nan
  Final Train: 87.62, nan
   Final Test: 87.63, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 4.8839, Train: 86.46%, Valid: 86.48%, Test: 86.54%
Epoch: 25, Loss: 0.9170, Train: 86.48%, Valid: 86.49%, Test: 86.55%
Epoch: 50, Loss: 0.4181, Train: 85.59%, Valid: 85.41%, Test: 85.64%
Epoch: 75, Loss: 0.3693, Train: 84.34%, Valid: 84.22%, Test: 84.51%
Epoch: 100, Loss: 0.3585, Train: 85.77%, Valid: 85.58%, Test: 85.83%
Epoch: 125, Loss: 0.3543, Train: 86.87%, Valid: 86.89%, Test: 86.92%
Epoch: 150, Loss: 0.3446, Train: 86.92%, Valid: 86.93%, Test: 86.99%
Epoch: 175, Loss: 0.3402, Train: 87.00%, Valid: 86.97%, Test: 87.07%
Run 01:
Highest Train: 87.55
Highest Valid: 87.54
  Final Train: 87.55
   Final Test: 87.61
All runs:
Highest Train: 87.55, nan
Highest Valid: 87.54, nan
  Final Train: 87.55, nan
   Final Test: 87.61, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 9.1948, Train: 49.99%, Valid: 49.98%, Test: 50.00%
Epoch: 25, Loss: 43.5139, Train: 31.31%, Valid: 31.11%, Test: 30.92%
Epoch: 50, Loss: 39.7142, Train: 34.33%, Valid: 34.07%, Test: 33.95%
Epoch: 75, Loss: 27.6711, Train: 83.45%, Valid: 83.32%, Test: 83.49%
Epoch: 100, Loss: 17.3875, Train: 84.54%, Valid: 84.43%, Test: 84.60%
Epoch: 125, Loss: 2.8005, Train: 82.21%, Valid: 82.22%, Test: 82.47%
Epoch: 150, Loss: 1.4787, Train: 85.98%, Valid: 85.96%, Test: 86.03%
Epoch: 175, Loss: 1.4377, Train: 85.77%, Valid: 85.75%, Test: 85.85%
Run 01:
Highest Train: 86.02
Highest Valid: 86.04
  Final Train: 86.02
   Final Test: 86.13
All runs:
Highest Train: 86.02, nan
Highest Valid: 86.04, nan
  Final Train: 86.02, nan
   Final Test: 86.13, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.4525, Train: 86.14%, Valid: 86.19%, Test: 86.23%
Epoch: 25, Loss: 3.6953, Train: 84.08%, Valid: 83.88%, Test: 84.19%
Epoch: 50, Loss: 4.6289, Train: 16.26%, Valid: 16.38%, Test: 16.28%
Epoch: 75, Loss: 1353.3191, Train: 16.18%, Valid: 16.39%, Test: 16.07%
Epoch: 100, Loss: 346.2758, Train: 16.87%, Valid: 17.07%, Test: 16.77%
Epoch: 125, Loss: 193.8398, Train: 87.88%, Valid: 87.94%, Test: 87.86%
Epoch: 150, Loss: 204.6409, Train: 87.86%, Valid: 87.92%, Test: 87.83%
Epoch: 175, Loss: 245.2603, Train: 87.85%, Valid: 87.91%, Test: 87.83%
Run 01:
Highest Train: 88.34
Highest Valid: 88.41
  Final Train: 88.32
   Final Test: 88.34
All runs:
Highest Train: 88.34, nan
Highest Valid: 88.41, nan
  Final Train: 88.32, nan
   Final Test: 88.34, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 17.7827, Train: 86.84%, Valid: 86.90%, Test: 86.91%
Epoch: 25, Loss: 4.9758, Train: 85.47%, Valid: 85.24%, Test: 85.50%
Epoch: 50, Loss: 5.3252, Train: 86.46%, Valid: 86.41%, Test: 86.44%
Epoch: 75, Loss: 4.9819, Train: 85.51%, Valid: 85.33%, Test: 85.54%
Epoch: 100, Loss: 4.9765, Train: 85.57%, Valid: 85.39%, Test: 85.60%
Epoch: 125, Loss: 4.7748, Train: 85.54%, Valid: 85.37%, Test: 85.56%
Epoch: 150, Loss: 4.4863, Train: 85.46%, Valid: 85.30%, Test: 85.51%
Epoch: 175, Loss: 4.1019, Train: 85.66%, Valid: 85.51%, Test: 85.71%
Run 01:
Highest Train: 88.00
Highest Valid: 87.96
  Final Train: 88.00
   Final Test: 87.98
All runs:
Highest Train: 88.00, nan
Highest Valid: 87.96, nan
  Final Train: 88.00, nan
   Final Test: 87.98, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 30978.8105, Train: 58.06%, Valid: 58.41%, Test: 58.56%
Epoch: 25, Loss: 66.6622, Train: 84.34%, Valid: 84.18%, Test: 84.40%
Epoch: 50, Loss: 36.7469, Train: 84.48%, Valid: 84.32%, Test: 84.54%
Epoch: 75, Loss: 19.7843, Train: 85.17%, Valid: 85.09%, Test: 85.32%
Epoch: 100, Loss: 10.8758, Train: 85.67%, Valid: 85.67%, Test: 85.80%
Epoch: 125, Loss: 9.5367, Train: 85.76%, Valid: 85.73%, Test: 85.89%
Epoch: 150, Loss: 9.1333, Train: 85.77%, Valid: 85.75%, Test: 85.90%
Epoch: 175, Loss: 8.8604, Train: 85.81%, Valid: 85.79%, Test: 85.93%
Run 01:
Highest Train: 85.85
Highest Valid: 85.83
  Final Train: 85.85
   Final Test: 85.97
All runs:
Highest Train: 85.85, nan
Highest Valid: 85.83, nan
  Final Train: 85.85, nan
   Final Test: 85.97, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 18.5937, Train: 84.84%, Valid: 84.62%, Test: 84.93%
Epoch: 25, Loss: 1.4538, Train: 86.59%, Valid: 86.66%, Test: 86.65%
Epoch: 50, Loss: 0.7403, Train: 85.70%, Valid: 85.56%, Test: 85.75%
Epoch: 75, Loss: 0.5404, Train: 85.39%, Valid: 85.21%, Test: 85.47%
Epoch: 100, Loss: 0.4680, Train: 85.76%, Valid: 85.61%, Test: 85.81%
Epoch: 125, Loss: 0.3957, Train: 85.57%, Valid: 85.65%, Test: 85.67%
Epoch: 150, Loss: 0.3590, Train: 87.26%, Valid: 87.27%, Test: 87.31%
Epoch: 175, Loss: 0.3491, Train: 87.21%, Valid: 87.19%, Test: 87.28%
Run 01:
Highest Train: 88.32
Highest Valid: 88.33
  Final Train: 88.32
   Final Test: 88.39
All runs:
Highest Train: 88.32, nan
Highest Valid: 88.33, nan
  Final Train: 88.32, nan
   Final Test: 88.39, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.1875, Train: 85.61%, Valid: 85.66%, Test: 85.77%
Epoch: 25, Loss: 1.3991, Train: 86.19%, Valid: 86.03%, Test: 86.18%
Epoch: 50, Loss: 0.6131, Train: 86.00%, Valid: 85.85%, Test: 86.05%
Epoch: 75, Loss: 0.4449, Train: 85.56%, Valid: 85.42%, Test: 85.60%
Epoch: 100, Loss: 0.3847, Train: 86.51%, Valid: 86.33%, Test: 86.47%
Epoch: 125, Loss: 0.3750, Train: 85.71%, Valid: 85.52%, Test: 85.79%
Epoch: 150, Loss: 0.3595, Train: 85.76%, Valid: 85.55%, Test: 85.82%
Epoch: 175, Loss: 0.3527, Train: 85.76%, Valid: 85.56%, Test: 85.83%
Run 01:
Highest Train: 87.85
Highest Valid: 87.72
  Final Train: 87.85
   Final Test: 87.85
All runs:
Highest Train: 87.85, nan
Highest Valid: 87.72, nan
  Final Train: 87.85, nan
   Final Test: 87.85, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 630.3169, Train: 51.46%, Valid: 51.29%, Test: 51.14%
Epoch: 25, Loss: 5.5961, Train: 84.48%, Valid: 84.40%, Test: 84.56%
Epoch: 50, Loss: 5.8690, Train: 84.61%, Valid: 84.48%, Test: 84.68%
Epoch: 75, Loss: 4.8221, Train: 84.57%, Valid: 84.44%, Test: 84.64%
Epoch: 100, Loss: 4.0769, Train: 84.71%, Valid: 84.60%, Test: 84.79%
Epoch: 125, Loss: 3.4382, Train: 84.95%, Valid: 84.83%, Test: 85.02%
Epoch: 150, Loss: 3.0415, Train: 84.94%, Valid: 84.83%, Test: 85.00%
Epoch: 175, Loss: 2.5288, Train: 84.88%, Valid: 84.77%, Test: 84.95%
Run 01:
Highest Train: 85.53
Highest Valid: 85.38
  Final Train: 85.53
   Final Test: 85.53
All runs:
Highest Train: 85.53, nan
Highest Valid: 85.38, nan
  Final Train: 85.53, nan
   Final Test: 85.53, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 20.1621, Train: 85.41%, Valid: 85.22%, Test: 85.51%
Epoch: 25, Loss: 1.3257, Train: 87.10%, Valid: 86.95%, Test: 87.14%
Epoch: 50, Loss: 0.8367, Train: 85.59%, Valid: 85.47%, Test: 85.66%
Epoch: 75, Loss: 0.5441, Train: 85.56%, Valid: 85.43%, Test: 85.64%
Epoch: 100, Loss: 0.4162, Train: 87.77%, Valid: 87.81%, Test: 87.86%
Epoch: 125, Loss: 0.3648, Train: 87.07%, Valid: 87.07%, Test: 87.15%
Epoch: 150, Loss: 0.3513, Train: 85.39%, Valid: 85.43%, Test: 85.51%
Epoch: 175, Loss: 0.3438, Train: 85.69%, Valid: 85.69%, Test: 85.76%
Run 01:
Highest Train: 88.18
Highest Valid: 88.22
  Final Train: 88.18
   Final Test: 88.21
All runs:
Highest Train: 88.18, nan
Highest Valid: 88.22, nan
  Final Train: 88.18, nan
   Final Test: 88.21, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 4.1599, Train: 85.52%, Valid: 85.50%, Test: 85.70%
Epoch: 25, Loss: 0.7768, Train: 86.25%, Valid: 86.14%, Test: 86.26%
Epoch: 50, Loss: 0.6458, Train: 86.49%, Valid: 86.34%, Test: 86.51%
Epoch: 75, Loss: 0.4288, Train: 86.34%, Valid: 86.19%, Test: 86.44%
Epoch: 100, Loss: 0.3743, Train: 88.00%, Valid: 87.91%, Test: 88.00%
Epoch: 125, Loss: 0.3571, Train: 85.79%, Valid: 85.60%, Test: 85.87%
Epoch: 150, Loss: 0.3502, Train: 85.79%, Valid: 85.57%, Test: 85.87%
Epoch: 175, Loss: 0.3457, Train: 85.96%, Valid: 85.77%, Test: 86.05%
Run 01:
Highest Train: 88.04
Highest Valid: 87.93
  Final Train: 88.04
   Final Test: 88.04
All runs:
Highest Train: 88.04, nan
Highest Valid: 87.93, nan
  Final Train: 88.04, nan
   Final Test: 88.04, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 721.6683, Train: 64.13%, Valid: 64.15%, Test: 64.27%
Epoch: 25, Loss: 6.7478, Train: 84.67%, Valid: 84.56%, Test: 84.75%
Epoch: 50, Loss: 6.2722, Train: 84.94%, Valid: 84.82%, Test: 85.02%
Epoch: 75, Loss: 4.9792, Train: 85.21%, Valid: 85.07%, Test: 85.28%
Epoch: 100, Loss: 3.1116, Train: 84.95%, Valid: 84.82%, Test: 85.01%
Epoch: 125, Loss: 2.8543, Train: 85.15%, Valid: 85.02%, Test: 85.23%
Epoch: 150, Loss: 2.7913, Train: 85.11%, Valid: 84.97%, Test: 85.17%
Epoch: 175, Loss: 2.6878, Train: 85.20%, Valid: 85.08%, Test: 85.26%
Run 01:
Highest Train: 86.73
Highest Valid: 86.53
  Final Train: 86.73
   Final Test: 86.76
All runs:
Highest Train: 86.73, nan
Highest Valid: 86.53, nan
  Final Train: 86.73, nan
   Final Test: 86.76, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.0995, Train: 84.06%, Valid: 83.83%, Test: 84.19%
Epoch: 25, Loss: 0.9625, Train: 87.85%, Valid: 87.92%, Test: 87.90%
Epoch: 50, Loss: 0.3854, Train: 86.40%, Valid: 86.32%, Test: 86.48%
Epoch: 75, Loss: 0.3602, Train: 85.12%, Valid: 84.92%, Test: 85.21%
Epoch: 100, Loss: 0.3487, Train: 85.42%, Valid: 85.42%, Test: 85.52%
Epoch: 125, Loss: 0.3388, Train: 85.49%, Valid: 85.52%, Test: 85.61%
Epoch: 150, Loss: 0.3372, Train: 86.65%, Valid: 86.62%, Test: 86.68%
Epoch: 175, Loss: 0.3341, Train: 87.17%, Valid: 87.18%, Test: 87.24%
Run 01:
Highest Train: 88.27
Highest Valid: 88.32
  Final Train: 88.27
   Final Test: 88.33
All runs:
Highest Train: 88.27, nan
Highest Valid: 88.32, nan
  Final Train: 88.27, nan
   Final Test: 88.33, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 22.8495, Train: 87.11%, Valid: 86.97%, Test: 87.06%
Epoch: 25, Loss: 1.7451, Train: 86.97%, Valid: 86.83%, Test: 86.94%
Epoch: 50, Loss: 0.5866, Train: 86.51%, Valid: 86.38%, Test: 86.54%
Epoch: 75, Loss: 0.4096, Train: 86.04%, Valid: 85.84%, Test: 86.09%
Epoch: 100, Loss: 0.3723, Train: 85.81%, Valid: 85.72%, Test: 85.90%
Epoch: 125, Loss: 0.3610, Train: 85.95%, Valid: 85.78%, Test: 86.01%
Epoch: 150, Loss: 0.3498, Train: 85.92%, Valid: 85.76%, Test: 85.99%
Epoch: 175, Loss: 0.3428, Train: 85.90%, Valid: 85.73%, Test: 85.96%
Run 01:
Highest Train: 87.73
Highest Valid: 87.66
  Final Train: 87.73
   Final Test: 87.78
All runs:
Highest Train: 87.73, nan
Highest Valid: 87.66, nan
  Final Train: 87.73, nan
   Final Test: 87.78, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 12.9278, Train: 79.74%, Valid: 79.64%, Test: 79.63%
Epoch: 25, Loss: 2.6955, Train: 79.22%, Valid: 79.23%, Test: 79.37%
Epoch: 50, Loss: 2.0511, Train: 74.50%, Valid: 74.56%, Test: 74.79%
Epoch: 75, Loss: 1.5297, Train: 76.62%, Valid: 76.74%, Test: 76.96%
Epoch: 100, Loss: 1.0669, Train: 80.37%, Valid: 80.43%, Test: 80.72%
Epoch: 125, Loss: 0.6396, Train: 80.27%, Valid: 80.28%, Test: 80.60%
Epoch: 150, Loss: 0.7391, Train: 84.73%, Valid: 84.77%, Test: 84.89%
Epoch: 175, Loss: 0.5351, Train: 78.83%, Valid: 78.93%, Test: 79.14%
Run 01:
Highest Train: 87.92
Highest Valid: 87.71
  Final Train: 87.92
   Final Test: 87.90
All runs:
Highest Train: 87.92, nan
Highest Valid: 87.71, nan
  Final Train: 87.92, nan
   Final Test: 87.90, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.3955, Train: 86.69%, Valid: 86.74%, Test: 86.68%
Epoch: 25, Loss: 3.9130, Train: 14.84%, Valid: 15.00%, Test: 14.74%
Epoch: 50, Loss: 4.9938, Train: 86.78%, Valid: 86.83%, Test: 86.79%
Epoch: 75, Loss: 5269491810304.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 24898848161792.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 37735124716063031296.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 150, Loss: 49542252321168162816.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Epoch: 175, Loss: 175159405093904693052765491953664.0000, Train: 50.00%, Valid: 50.00%, Test: 50.00%
Run 01:
Highest Train: 86.81
Highest Valid: 86.87
  Final Train: 86.81
   Final Test: 86.85
All runs:
Highest Train: 86.81, nan
Highest Valid: 86.87, nan
  Final Train: 86.81, nan
   Final Test: 86.85, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 11.2244, Train: 86.34%, Valid: 86.34%, Test: 86.43%
Epoch: 25, Loss: 19.5558, Train: 86.72%, Valid: 86.78%, Test: 86.82%
Epoch: 50, Loss: 127.6408, Train: 85.66%, Valid: 85.71%, Test: 85.79%
Epoch: 75, Loss: 5.7434, Train: 85.46%, Valid: 85.45%, Test: 85.59%
Epoch: 100, Loss: 4.6945, Train: 85.64%, Valid: 85.61%, Test: 85.77%
Epoch: 125, Loss: 4.6064, Train: 85.91%, Valid: 85.89%, Test: 86.02%
Epoch: 150, Loss: 4.4509, Train: 85.89%, Valid: 85.87%, Test: 85.99%
Epoch: 175, Loss: 2.9210, Train: 86.24%, Valid: 86.16%, Test: 86.34%
Run 01:
Highest Train: 87.36
Highest Valid: 87.31
  Final Train: 87.36
   Final Test: 87.34
All runs:
Highest Train: 87.36, nan
Highest Valid: 87.31, nan
  Final Train: 87.36, nan
   Final Test: 87.34, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2596619.7500, Train: 84.43%, Valid: 84.34%, Test: 84.56%
Epoch: 25, Loss: 529.8475, Train: 83.82%, Valid: 83.69%, Test: 83.89%
Epoch: 50, Loss: 85.4081, Train: 83.88%, Valid: 83.75%, Test: 83.96%
Epoch: 75, Loss: 82.9164, Train: 83.92%, Valid: 83.77%, Test: 83.99%
Epoch: 100, Loss: 86.6708, Train: 83.91%, Valid: 83.78%, Test: 83.99%
Epoch: 125, Loss: 88.2982, Train: 83.92%, Valid: 83.78%, Test: 83.99%
Epoch: 150, Loss: 84.0981, Train: 83.93%, Valid: 83.79%, Test: 84.00%
Epoch: 175, Loss: 75.2016, Train: 83.95%, Valid: 83.81%, Test: 84.03%
Run 01:
Highest Train: 85.22
Highest Valid: 85.16
  Final Train: 85.22
   Final Test: 85.33
All runs:
Highest Train: 85.22, nan
Highest Valid: 85.16, nan
  Final Train: 85.22, nan
   Final Test: 85.33, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 6.5417, Train: 88.18%, Valid: 88.21%, Test: 88.24%
Epoch: 25, Loss: 1.5734, Train: 87.94%, Valid: 88.01%, Test: 87.95%
Epoch: 50, Loss: 0.5991, Train: 87.08%, Valid: 86.91%, Test: 87.05%
Epoch: 75, Loss: 0.4566, Train: 86.57%, Valid: 86.54%, Test: 86.61%
Epoch: 100, Loss: 0.4096, Train: 84.92%, Valid: 84.72%, Test: 84.99%
Epoch: 125, Loss: 0.3731, Train: 85.78%, Valid: 85.58%, Test: 85.84%
Epoch: 150, Loss: 0.3522, Train: 85.68%, Valid: 85.47%, Test: 85.74%
Epoch: 175, Loss: 0.3421, Train: 84.85%, Valid: 84.62%, Test: 84.90%
Run 01:
Highest Train: 88.27
Highest Valid: 88.31
  Final Train: 88.26
   Final Test: 88.31
All runs:
Highest Train: 88.27, nan
Highest Valid: 88.31, nan
  Final Train: 88.26, nan
   Final Test: 88.31, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 4.6687, Train: 85.93%, Valid: 85.79%, Test: 85.97%
Epoch: 25, Loss: 1.2597, Train: 85.78%, Valid: 85.76%, Test: 85.82%
Epoch: 50, Loss: 0.6004, Train: 86.25%, Valid: 86.12%, Test: 86.26%
Epoch: 75, Loss: 0.4504, Train: 85.34%, Valid: 85.18%, Test: 85.39%
Epoch: 100, Loss: 0.3830, Train: 86.61%, Valid: 86.62%, Test: 86.68%
Epoch: 125, Loss: 0.3633, Train: 86.07%, Valid: 85.89%, Test: 86.12%
Epoch: 150, Loss: 0.3486, Train: 85.91%, Valid: 85.74%, Test: 85.95%
Epoch: 175, Loss: 0.3419, Train: 85.63%, Valid: 85.42%, Test: 85.70%
Run 01:
Highest Train: 87.91
Highest Valid: 87.85
  Final Train: 87.91
   Final Test: 87.89
All runs:
Highest Train: 87.91, nan
Highest Valid: 87.85, nan
  Final Train: 87.91, nan
   Final Test: 87.89, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 234.0434, Train: 68.73%, Valid: 68.52%, Test: 68.41%
Epoch: 25, Loss: 28.8296, Train: 47.63%, Valid: 47.60%, Test: 47.23%
Epoch: 50, Loss: 6.0812, Train: 84.78%, Valid: 84.67%, Test: 84.83%
Epoch: 75, Loss: 4.0966, Train: 85.16%, Valid: 85.03%, Test: 85.22%
Epoch: 100, Loss: 3.4514, Train: 85.29%, Valid: 85.16%, Test: 85.35%
Epoch: 125, Loss: 2.2749, Train: 85.39%, Valid: 85.25%, Test: 85.45%
Epoch: 150, Loss: 1.9845, Train: 85.51%, Valid: 85.36%, Test: 85.57%
Epoch: 175, Loss: 1.5331, Train: 83.30%, Valid: 83.39%, Test: 83.50%
Run 01:
Highest Train: 86.12
Highest Valid: 85.99
  Final Train: 86.12
   Final Test: 86.16
All runs:
Highest Train: 86.12, nan
Highest Valid: 85.99, nan
  Final Train: 86.12, nan
   Final Test: 86.16, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 5.0760, Train: 86.68%, Valid: 86.74%, Test: 86.63%
Epoch: 25, Loss: 1.7320, Train: 87.08%, Valid: 87.10%, Test: 87.16%
Epoch: 50, Loss: 0.6141, Train: 86.74%, Valid: 86.71%, Test: 86.69%
Epoch: 75, Loss: 0.3832, Train: 86.29%, Valid: 86.32%, Test: 86.37%
Epoch: 100, Loss: 0.3595, Train: 86.02%, Valid: 86.03%, Test: 86.13%
Epoch: 125, Loss: 0.3471, Train: 86.92%, Valid: 86.91%, Test: 87.01%
Epoch: 150, Loss: 0.3407, Train: 86.01%, Valid: 86.03%, Test: 86.10%
Epoch: 175, Loss: 0.3366, Train: 86.05%, Valid: 86.06%, Test: 86.14%
Run 01:
Highest Train: 88.03
Highest Valid: 88.11
  Final Train: 88.02
   Final Test: 88.14
All runs:
Highest Train: 88.03, nan
Highest Valid: 88.11, nan
  Final Train: 88.02, nan
   Final Test: 88.14, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 4.0865, Train: 86.61%, Valid: 86.63%, Test: 86.70%
Epoch: 25, Loss: 0.9706, Train: 84.98%, Valid: 84.86%, Test: 85.10%
Epoch: 50, Loss: 0.5718, Train: 85.75%, Valid: 85.59%, Test: 85.79%
Epoch: 75, Loss: 0.4067, Train: 86.78%, Valid: 86.80%, Test: 86.84%
Epoch: 100, Loss: 0.3562, Train: 86.88%, Valid: 86.78%, Test: 86.96%
Epoch: 125, Loss: 0.3459, Train: 85.87%, Valid: 85.68%, Test: 85.93%
Epoch: 150, Loss: 0.3409, Train: 85.77%, Valid: 85.58%, Test: 85.85%
Epoch: 175, Loss: 0.3390, Train: 85.94%, Valid: 85.76%, Test: 86.00%
Run 01:
Highest Train: 87.81
Highest Valid: 87.73
  Final Train: 87.81
   Final Test: 87.74
All runs:
Highest Train: 87.81, nan
Highest Valid: 87.73, nan
  Final Train: 87.81, nan
   Final Test: 87.74, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 227.9665, Train: 83.02%, Valid: 83.05%, Test: 83.24%
Epoch: 25, Loss: 15.7365, Train: 84.22%, Valid: 84.10%, Test: 84.25%
Epoch: 50, Loss: 2.6524, Train: 80.85%, Valid: 80.97%, Test: 81.21%
Epoch: 75, Loss: 2.7517, Train: 78.09%, Valid: 78.24%, Test: 78.52%
Epoch: 100, Loss: 2.4070, Train: 79.82%, Valid: 79.91%, Test: 80.20%
Epoch: 125, Loss: 2.2628, Train: 79.45%, Valid: 79.55%, Test: 79.86%
Epoch: 150, Loss: 2.2211, Train: 80.18%, Valid: 80.27%, Test: 80.56%
Epoch: 175, Loss: 2.1136, Train: 80.27%, Valid: 80.37%, Test: 80.66%
Run 01:
Highest Train: 85.34
Highest Valid: 85.20
  Final Train: 85.34
   Final Test: 85.43
All runs:
Highest Train: 85.34, nan
Highest Valid: 85.20, nan
  Final Train: 85.34, nan
   Final Test: 85.43, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 17.4631, Train: 85.57%, Valid: 85.43%, Test: 85.60%
Epoch: 25, Loss: 1.7460, Train: 85.62%, Valid: 85.50%, Test: 85.66%
Epoch: 50, Loss: 0.6521, Train: 85.89%, Valid: 85.75%, Test: 85.93%
Epoch: 75, Loss: 0.4186, Train: 84.91%, Valid: 84.68%, Test: 85.01%
Epoch: 100, Loss: 0.3650, Train: 86.37%, Valid: 86.37%, Test: 86.46%
Epoch: 125, Loss: 0.3529, Train: 85.95%, Valid: 85.96%, Test: 86.06%
Epoch: 150, Loss: 0.3435, Train: 85.94%, Valid: 85.94%, Test: 86.03%
Epoch: 175, Loss: 0.3413, Train: 87.15%, Valid: 86.96%, Test: 87.18%
Run 01:
Highest Train: 87.61
Highest Valid: 87.66
  Final Train: 87.61
   Final Test: 87.68
All runs:
Highest Train: 87.61, nan
Highest Valid: 87.66, nan
  Final Train: 87.61, nan
   Final Test: 87.68, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 6.2583, Train: 86.05%, Valid: 85.90%, Test: 86.07%
Epoch: 25, Loss: 1.2711, Train: 85.90%, Valid: 85.97%, Test: 85.98%
Epoch: 50, Loss: 0.4821, Train: 85.97%, Valid: 85.79%, Test: 86.05%
Epoch: 75, Loss: 0.3796, Train: 85.54%, Valid: 85.38%, Test: 85.61%
Epoch: 100, Loss: 0.3625, Train: 86.14%, Valid: 86.05%, Test: 86.19%
Epoch: 125, Loss: 0.3498, Train: 85.90%, Valid: 85.68%, Test: 85.96%
Epoch: 150, Loss: 0.3453, Train: 86.84%, Valid: 86.80%, Test: 86.93%
Epoch: 175, Loss: 0.3425, Train: 87.31%, Valid: 87.30%, Test: 87.37%
Run 01:
Highest Train: 87.65
Highest Valid: 87.57
  Final Train: 87.65
   Final Test: 87.63
All runs:
Highest Train: 87.65, nan
Highest Valid: 87.57, nan
  Final Train: 87.65, nan
   Final Test: 87.63, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 72.6318, Train: 61.00%, Valid: 61.35%, Test: 61.54%
Epoch: 25, Loss: 3.4615, Train: 84.94%, Valid: 84.80%, Test: 85.00%
Epoch: 50, Loss: 3.4674, Train: 85.10%, Valid: 84.96%, Test: 85.16%
Epoch: 75, Loss: 3.4586, Train: 85.15%, Valid: 85.01%, Test: 85.21%
Epoch: 100, Loss: 3.4018, Train: 85.18%, Valid: 85.04%, Test: 85.24%
Epoch: 125, Loss: 3.1079, Train: 85.23%, Valid: 85.10%, Test: 85.30%
Epoch: 150, Loss: 2.6195, Train: 83.78%, Valid: 83.75%, Test: 83.92%
Epoch: 175, Loss: 4.3821, Train: 82.76%, Valid: 82.90%, Test: 83.07%
Run 01:
Highest Train: 85.44
Highest Valid: 85.29
  Final Train: 85.44
   Final Test: 85.52
All runs:
Highest Train: 85.44, nan
Highest Valid: 85.29, nan
  Final Train: 85.44, nan
   Final Test: 85.52, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 16.9327, Train: 86.69%, Valid: 86.75%, Test: 86.71%
Epoch: 25, Loss: 3.4616, Train: 84.21%, Valid: 84.05%, Test: 84.33%
Epoch: 50, Loss: 3.9683, Train: 84.24%, Valid: 84.07%, Test: 84.32%
Epoch: 75, Loss: 4.0298, Train: 84.25%, Valid: 84.07%, Test: 84.33%
Epoch: 100, Loss: 4.0355, Train: 84.25%, Valid: 84.07%, Test: 84.33%
Epoch: 125, Loss: 4.0351, Train: 84.25%, Valid: 84.07%, Test: 84.33%
Epoch: 150, Loss: 4.0313, Train: 84.25%, Valid: 84.07%, Test: 84.33%
Epoch: 175, Loss: 4.0296, Train: 84.25%, Valid: 84.07%, Test: 84.33%
Run 01:
Highest Train: 87.57
Highest Valid: 87.60
  Final Train: 87.57
   Final Test: 87.59
All runs:
Highest Train: 87.57, nan
Highest Valid: 87.60, nan
  Final Train: 87.57, nan
   Final Test: 87.59, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 15.0074, Train: 86.05%, Valid: 85.91%, Test: 86.11%
Epoch: 25, Loss: 131.4567, Train: 86.83%, Valid: 86.88%, Test: 86.88%
Epoch: 50, Loss: 4.5437, Train: 86.50%, Valid: 86.37%, Test: 86.48%
Epoch: 75, Loss: 153.2985, Train: 86.72%, Valid: 86.60%, Test: 86.74%
Epoch: 100, Loss: 217.3979, Train: 86.72%, Valid: 86.60%, Test: 86.73%
Epoch: 125, Loss: 131.1391, Train: 85.27%, Valid: 85.10%, Test: 85.32%
Epoch: 150, Loss: 277.9263, Train: 14.98%, Valid: 15.17%, Test: 14.87%
Epoch: 175, Loss: 291.7958, Train: 86.68%, Valid: 86.54%, Test: 86.67%
Run 01:
Highest Train: 87.63
Highest Valid: 87.64
  Final Train: 87.63
   Final Test: 87.61
All runs:
Highest Train: 87.63, nan
Highest Valid: 87.64, nan
  Final Train: 87.63, nan
   Final Test: 87.61, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 497445997561133609779200.0000, Train: 67.84%, Valid: 68.00%, Test: 68.32%
Epoch: 25, Loss: 1179.0933, Train: 78.43%, Valid: 78.53%, Test: 78.92%
Epoch: 50, Loss: 7261.2778, Train: 17.84%, Valid: 18.05%, Test: 17.84%
Epoch: 75, Loss: 4275.5747, Train: 74.54%, Valid: 74.60%, Test: 74.89%
Epoch: 100, Loss: 3508.1768, Train: 17.09%, Valid: 17.29%, Test: 17.10%
Epoch: 125, Loss: 9302.6680, Train: 76.96%, Valid: 77.03%, Test: 77.40%
Epoch: 150, Loss: 2110.1453, Train: 15.78%, Valid: 15.93%, Test: 15.74%
Epoch: 175, Loss: 290.7520, Train: 84.96%, Valid: 84.86%, Test: 85.03%
Run 01:
Highest Train: 85.03
Highest Valid: 84.93
  Final Train: 85.03
   Final Test: 85.08
All runs:
Highest Train: 85.03, nan
Highest Valid: 84.93, nan
  Final Train: 85.03, nan
   Final Test: 85.08, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.1379, Train: 85.99%, Valid: 86.06%, Test: 86.08%
Epoch: 25, Loss: 1.1976, Train: 86.79%, Valid: 86.85%, Test: 86.79%
Epoch: 50, Loss: 0.4253, Train: 86.56%, Valid: 86.63%, Test: 86.62%
Epoch: 75, Loss: 0.3535, Train: 87.32%, Valid: 87.32%, Test: 87.41%
Epoch: 100, Loss: 0.3435, Train: 86.77%, Valid: 86.70%, Test: 86.85%
Epoch: 125, Loss: 0.3373, Train: 87.11%, Valid: 87.08%, Test: 87.26%
Epoch: 150, Loss: 0.3370, Train: 85.80%, Valid: 85.80%, Test: 85.93%
Epoch: 175, Loss: 0.3470, Train: 86.70%, Valid: 86.64%, Test: 86.73%
Run 01:
Highest Train: 88.32
Highest Valid: 88.38
  Final Train: 88.32
   Final Test: 88.36
All runs:
Highest Train: 88.32, nan
Highest Valid: 88.38, nan
  Final Train: 88.32, nan
   Final Test: 88.36, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 16.7931, Train: 85.46%, Valid: 85.29%, Test: 85.53%
Epoch: 25, Loss: 2.1561, Train: 85.61%, Valid: 85.42%, Test: 85.64%
Epoch: 50, Loss: 0.7380, Train: 86.29%, Valid: 86.28%, Test: 86.35%
Epoch: 75, Loss: 0.4988, Train: 86.12%, Valid: 85.96%, Test: 86.20%
Epoch: 100, Loss: 0.4252, Train: 86.36%, Valid: 86.30%, Test: 86.40%
Epoch: 125, Loss: 0.3930, Train: 86.83%, Valid: 86.79%, Test: 86.93%
Epoch: 150, Loss: 0.3603, Train: 86.60%, Valid: 86.54%, Test: 86.64%
Epoch: 175, Loss: 0.3435, Train: 85.84%, Valid: 85.79%, Test: 85.92%
Run 01:
Highest Train: 87.06
Highest Valid: 86.97
  Final Train: 87.06
   Final Test: 87.11
All runs:
Highest Train: 87.06, nan
Highest Valid: 86.97, nan
  Final Train: 87.06, nan
   Final Test: 87.11, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 336.2576, Train: 52.01%, Valid: 51.98%, Test: 51.97%
Epoch: 25, Loss: 9.6164, Train: 85.16%, Valid: 85.02%, Test: 85.23%
Epoch: 50, Loss: 5.2964, Train: 85.33%, Valid: 85.19%, Test: 85.40%
Epoch: 75, Loss: 3.7642, Train: 85.71%, Valid: 85.57%, Test: 85.79%
Epoch: 100, Loss: 1.7379, Train: 86.46%, Valid: 86.37%, Test: 86.56%
Epoch: 125, Loss: 1.7164, Train: 85.69%, Valid: 85.54%, Test: 85.74%
Epoch: 150, Loss: 1.6179, Train: 86.59%, Valid: 86.60%, Test: 86.70%
Epoch: 175, Loss: 1.4080, Train: 86.59%, Valid: 86.61%, Test: 86.69%
Run 01:
Highest Train: 87.63
Highest Valid: 87.55
  Final Train: 87.61
   Final Test: 87.74
All runs:
Highest Train: 87.63, nan
Highest Valid: 87.55, nan
  Final Train: 87.61, nan
   Final Test: 87.74, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 14.3030, Train: 87.58%, Valid: 87.59%, Test: 87.68%
Epoch: 25, Loss: 1.2436, Train: 85.96%, Valid: 85.82%, Test: 86.00%
Epoch: 50, Loss: 0.6755, Train: 85.74%, Valid: 85.78%, Test: 85.85%
Epoch: 75, Loss: 0.4500, Train: 86.51%, Valid: 86.54%, Test: 86.64%
Epoch: 100, Loss: 0.3712, Train: 86.94%, Valid: 86.91%, Test: 87.09%
Epoch: 125, Loss: 0.3497, Train: 85.54%, Valid: 85.49%, Test: 85.67%
Epoch: 150, Loss: 0.3427, Train: 85.67%, Valid: 85.65%, Test: 85.76%
Epoch: 175, Loss: 0.3405, Train: 85.90%, Valid: 85.72%, Test: 85.96%
Run 01:
Highest Train: 87.61
Highest Valid: 87.64
  Final Train: 87.60
   Final Test: 87.62
All runs:
Highest Train: 87.61, nan
Highest Valid: 87.64, nan
  Final Train: 87.60, nan
   Final Test: 87.62, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.4603, Train: 16.56%, Valid: 16.42%, Test: 16.44%
Epoch: 25, Loss: 1.9409, Train: 86.35%, Valid: 86.16%, Test: 86.35%
Epoch: 50, Loss: 0.6438, Train: 86.16%, Valid: 86.00%, Test: 86.17%
Epoch: 75, Loss: 0.4426, Train: 86.03%, Valid: 85.90%, Test: 86.11%
Epoch: 100, Loss: 0.3857, Train: 86.43%, Valid: 86.26%, Test: 86.50%
Epoch: 125, Loss: 0.3582, Train: 86.24%, Valid: 86.10%, Test: 86.32%
Epoch: 150, Loss: 0.3556, Train: 86.24%, Valid: 86.09%, Test: 86.33%
Epoch: 175, Loss: 0.3437, Train: 86.43%, Valid: 86.28%, Test: 86.49%
Run 01:
Highest Train: 87.41
Highest Valid: 87.39
  Final Train: 87.41
   Final Test: 87.49
All runs:
Highest Train: 87.41, nan
Highest Valid: 87.39, nan
  Final Train: 87.41, nan
   Final Test: 87.49, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 400.6152, Train: 53.14%, Valid: 53.21%, Test: 53.12%
Epoch: 25, Loss: 4.8444, Train: 84.65%, Valid: 84.51%, Test: 84.70%
Epoch: 50, Loss: 2.8096, Train: 85.17%, Valid: 85.03%, Test: 85.24%
Epoch: 75, Loss: 2.4494, Train: 85.38%, Valid: 85.23%, Test: 85.44%
Epoch: 100, Loss: 2.2706, Train: 84.35%, Valid: 84.32%, Test: 84.46%
Epoch: 125, Loss: 1.9684, Train: 83.89%, Valid: 83.95%, Test: 84.08%
Epoch: 150, Loss: 1.8326, Train: 83.81%, Valid: 83.91%, Test: 84.03%
Epoch: 175, Loss: 1.7297, Train: 83.87%, Valid: 83.95%, Test: 84.10%
Run 01:
Highest Train: 85.96
Highest Valid: 85.85
  Final Train: 85.96
   Final Test: 86.06
All runs:
Highest Train: 85.96, nan
Highest Valid: 85.85, nan
  Final Train: 85.96, nan
   Final Test: 86.06, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 6.3377, Train: 84.62%, Valid: 84.49%, Test: 84.68%
Epoch: 25, Loss: 1.3486, Train: 85.65%, Valid: 85.53%, Test: 85.68%
Epoch: 50, Loss: 0.5285, Train: 85.64%, Valid: 85.53%, Test: 85.68%
Epoch: 75, Loss: 0.3910, Train: 85.84%, Valid: 85.91%, Test: 85.93%
Epoch: 100, Loss: 0.3541, Train: 86.51%, Valid: 86.56%, Test: 86.59%
Epoch: 125, Loss: 0.3483, Train: 85.78%, Valid: 85.84%, Test: 85.87%
Epoch: 150, Loss: 0.3410, Train: 85.61%, Valid: 85.63%, Test: 85.73%
Epoch: 175, Loss: 0.3379, Train: 85.91%, Valid: 85.92%, Test: 86.02%
Run 01:
Highest Train: 87.23
Highest Valid: 87.25
  Final Train: 87.23
   Final Test: 87.33
All runs:
Highest Train: 87.23, nan
Highest Valid: 87.25, nan
  Final Train: 87.23, nan
   Final Test: 87.33, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 3.8091, Train: 85.63%, Valid: 85.45%, Test: 85.68%
Epoch: 25, Loss: 0.9540, Train: 86.35%, Valid: 86.38%, Test: 86.43%
Epoch: 50, Loss: 0.4445, Train: 85.50%, Valid: 85.47%, Test: 85.58%
Epoch: 75, Loss: 0.3634, Train: 85.97%, Valid: 85.90%, Test: 86.03%
Epoch: 100, Loss: 0.3537, Train: 85.77%, Valid: 85.57%, Test: 85.83%
Epoch: 125, Loss: 0.3483, Train: 85.77%, Valid: 85.57%, Test: 85.83%
Epoch: 150, Loss: 0.3444, Train: 85.89%, Valid: 85.84%, Test: 85.98%
Epoch: 175, Loss: 0.3396, Train: 86.03%, Valid: 85.96%, Test: 86.10%
Run 01:
Highest Train: 86.55
Highest Valid: 86.44
  Final Train: 86.55
   Final Test: 86.64
All runs:
Highest Train: 86.55, nan
Highest Valid: 86.44, nan
  Final Train: 86.55, nan
   Final Test: 86.64, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 324.4451, Train: 50.03%, Valid: 50.06%, Test: 49.99%
Epoch: 25, Loss: 158.1231, Train: 26.14%, Valid: 25.91%, Test: 25.81%
Epoch: 50, Loss: 3.2511, Train: 85.53%, Valid: 85.38%, Test: 85.59%
Epoch: 75, Loss: 3.3237, Train: 86.40%, Valid: 86.42%, Test: 86.47%
Epoch: 100, Loss: 2.4168, Train: 86.29%, Valid: 86.21%, Test: 86.37%
Epoch: 125, Loss: 2.1493, Train: 86.03%, Valid: 85.86%, Test: 86.09%
Epoch: 150, Loss: 1.9760, Train: 85.96%, Valid: 85.98%, Test: 86.06%
Epoch: 175, Loss: 1.8975, Train: 87.00%, Valid: 87.03%, Test: 87.07%
Run 01:
Highest Train: 87.87
Highest Valid: 87.83
  Final Train: 87.87
   Final Test: 87.84
All runs:
Highest Train: 87.87, nan
Highest Valid: 87.83, nan
  Final Train: 87.87, nan
   Final Test: 87.84, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 6.4831, Train: 13.40%, Valid: 13.35%, Test: 13.35%
Epoch: 25, Loss: 0.3682, Train: 86.02%, Valid: 85.86%, Test: 86.07%
Epoch: 50, Loss: 0.3673, Train: 86.00%, Valid: 85.87%, Test: 86.05%
Epoch: 75, Loss: 0.3641, Train: 85.81%, Valid: 85.70%, Test: 85.86%
Epoch: 100, Loss: 0.3621, Train: 86.39%, Valid: 86.42%, Test: 86.46%
Epoch: 125, Loss: 0.3598, Train: 85.45%, Valid: 85.50%, Test: 85.55%
Epoch: 150, Loss: 0.3572, Train: 85.41%, Valid: 85.42%, Test: 85.49%
Epoch: 175, Loss: 0.3544, Train: 85.43%, Valid: 85.43%, Test: 85.51%
Run 01:
Highest Train: 86.56
Highest Valid: 86.60
  Final Train: 86.56
   Final Test: 86.60
All runs:
Highest Train: 86.56, nan
Highest Valid: 86.60, nan
  Final Train: 86.56, nan
   Final Test: 86.60, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 0.5772, Train: 85.56%, Valid: 85.46%, Test: 85.58%
Epoch: 25, Loss: 0.4472, Train: 85.84%, Valid: 85.74%, Test: 85.90%
Epoch: 50, Loss: 0.3997, Train: 85.78%, Valid: 85.65%, Test: 85.83%
Epoch: 75, Loss: 0.3697, Train: 85.49%, Valid: 85.36%, Test: 85.50%
Epoch: 100, Loss: 0.3643, Train: 85.31%, Valid: 85.17%, Test: 85.38%
Epoch: 125, Loss: 0.3621, Train: 85.56%, Valid: 85.42%, Test: 85.63%
Epoch: 150, Loss: 0.3598, Train: 85.58%, Valid: 85.43%, Test: 85.65%
Epoch: 175, Loss: 0.3574, Train: 85.58%, Valid: 85.43%, Test: 85.65%
Run 01:
Highest Train: 85.90
Highest Valid: 85.80
  Final Train: 85.90
   Final Test: 85.95
All runs:
Highest Train: 85.90, nan
Highest Valid: 85.80, nan
  Final Train: 85.90, nan
   Final Test: 85.95, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 114364.4453, Train: 37.25%, Valid: 37.43%, Test: 37.50%
Epoch: 25, Loss: 213.1346, Train: 69.53%, Valid: 69.28%, Test: 69.01%
Epoch: 50, Loss: 74.6158, Train: 68.74%, Valid: 68.63%, Test: 68.34%
Epoch: 75, Loss: 73.5091, Train: 68.79%, Valid: 68.68%, Test: 68.37%
Epoch: 100, Loss: 72.2936, Train: 68.98%, Valid: 68.88%, Test: 68.56%
Epoch: 125, Loss: 69.3074, Train: 69.36%, Valid: 69.24%, Test: 68.96%
Epoch: 150, Loss: 66.7218, Train: 69.77%, Valid: 69.64%, Test: 69.38%
Epoch: 175, Loss: 63.3572, Train: 70.27%, Valid: 70.07%, Test: 69.89%
Run 01:
Highest Train: 70.52
Highest Valid: 70.31
  Final Train: 70.52
   Final Test: 70.15
All runs:
Highest Train: 70.52, nan
Highest Valid: 70.31, nan
  Final Train: 70.52, nan
   Final Test: 70.15, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 1.9784, Train: 86.67%, Valid: 86.81%, Test: 86.72%
Epoch: 25, Loss: 0.3929, Train: 86.89%, Valid: 86.91%, Test: 86.96%
Epoch: 50, Loss: 0.3945, Train: 86.59%, Valid: 86.65%, Test: 86.69%
Epoch: 75, Loss: 0.3828, Train: 85.05%, Valid: 85.01%, Test: 85.18%
Epoch: 100, Loss: 0.3813, Train: 85.07%, Valid: 85.05%, Test: 85.21%
Epoch: 125, Loss: 0.3725, Train: 85.09%, Valid: 85.06%, Test: 85.22%
Epoch: 150, Loss: 0.3711, Train: 85.21%, Valid: 85.18%, Test: 85.32%
Epoch: 175, Loss: 0.3651, Train: 85.21%, Valid: 85.18%, Test: 85.32%
Run 01:
Highest Train: 87.23
Highest Valid: 87.28
  Final Train: 87.23
   Final Test: 87.30
All runs:
Highest Train: 87.23, nan
Highest Valid: 87.28, nan
  Final Train: 87.23, nan
   Final Test: 87.30, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 1.0864, Train: 79.05%, Valid: 79.09%, Test: 79.54%
Epoch: 25, Loss: 0.3862, Train: 85.89%, Valid: 85.73%, Test: 85.88%
Epoch: 50, Loss: 0.3810, Train: 85.48%, Valid: 85.28%, Test: 85.42%
Epoch: 75, Loss: 0.3729, Train: 86.95%, Valid: 86.72%, Test: 86.89%
Epoch: 100, Loss: 0.3693, Train: 86.28%, Valid: 86.14%, Test: 86.27%
Epoch: 125, Loss: 0.3683, Train: 85.72%, Valid: 85.55%, Test: 85.74%
Epoch: 150, Loss: 0.3625, Train: 85.45%, Valid: 85.26%, Test: 85.51%
Epoch: 175, Loss: 0.3613, Train: 85.43%, Valid: 85.24%, Test: 85.49%
Run 01:
Highest Train: 86.97
Highest Valid: 86.76
  Final Train: 86.97
   Final Test: 86.93
All runs:
Highest Train: 86.97, nan
Highest Valid: 86.76, nan
  Final Train: 86.97, nan
   Final Test: 86.93, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 145.9453, Train: 47.17%, Valid: 47.20%, Test: 47.09%
Epoch: 25, Loss: 15.7857, Train: 70.26%, Valid: 70.09%, Test: 70.01%
Epoch: 50, Loss: 14.0143, Train: 73.52%, Valid: 73.35%, Test: 73.33%
Epoch: 75, Loss: 10.7314, Train: 74.70%, Valid: 74.53%, Test: 74.51%
Epoch: 100, Loss: 9.2621, Train: 75.78%, Valid: 75.61%, Test: 75.63%
Epoch: 125, Loss: 11.7309, Train: 76.32%, Valid: 76.16%, Test: 76.18%
Epoch: 150, Loss: 5.8774, Train: 77.04%, Valid: 76.88%, Test: 76.91%
Epoch: 175, Loss: 7.5603, Train: 77.84%, Valid: 77.66%, Test: 77.71%
Run 01:
Highest Train: 78.85
Highest Valid: 78.68
  Final Train: 78.85
   Final Test: 78.75
All runs:
Highest Train: 78.85, nan
Highest Valid: 78.68, nan
  Final Train: 78.85, nan
   Final Test: 78.75, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 2.5633, Train: 82.15%, Valid: 82.02%, Test: 82.22%
Epoch: 25, Loss: 0.4057, Train: 85.16%, Valid: 84.92%, Test: 85.24%
Epoch: 50, Loss: 0.4032, Train: 84.74%, Valid: 84.47%, Test: 84.77%
Epoch: 75, Loss: 0.3935, Train: 84.03%, Valid: 83.86%, Test: 84.15%
Epoch: 100, Loss: 0.3864, Train: 84.12%, Valid: 83.92%, Test: 84.22%
Epoch: 125, Loss: 0.3806, Train: 88.51%, Valid: 88.55%, Test: 88.53%
Epoch: 150, Loss: 0.3734, Train: 88.37%, Valid: 88.44%, Test: 88.41%
Epoch: 175, Loss: 0.3709, Train: 88.28%, Valid: 88.35%, Test: 88.30%
Run 01:
Highest Train: 88.52
Highest Valid: 88.56
  Final Train: 88.52
   Final Test: 88.54
All runs:
Highest Train: 88.52, nan
Highest Valid: 88.56, nan
  Final Train: 88.52, nan
   Final Test: 88.54, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 2.7961, Train: 85.78%, Valid: 85.75%, Test: 85.80%
Epoch: 25, Loss: 0.4828, Train: 85.50%, Valid: 85.42%, Test: 85.57%
Epoch: 50, Loss: 0.4457, Train: 85.71%, Valid: 85.57%, Test: 85.77%
Epoch: 75, Loss: 0.4197, Train: 85.68%, Valid: 85.58%, Test: 85.74%
Epoch: 100, Loss: 0.4210, Train: 85.92%, Valid: 85.87%, Test: 85.97%
Epoch: 125, Loss: 0.3927, Train: 85.44%, Valid: 85.25%, Test: 85.52%
Epoch: 150, Loss: 0.3950, Train: 85.51%, Valid: 85.31%, Test: 85.56%
Epoch: 175, Loss: 0.3909, Train: 85.45%, Valid: 85.25%, Test: 85.51%
Run 01:
Highest Train: 86.78
Highest Valid: 86.59
  Final Train: 86.78
   Final Test: 86.84
All runs:
Highest Train: 86.78, nan
Highest Valid: 86.59, nan
  Final Train: 86.78, nan
   Final Test: 86.84, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 523.0568, Train: 32.86%, Valid: 33.09%, Test: 33.19%
Epoch: 25, Loss: 129.6274, Train: 49.98%, Valid: 49.64%, Test: 49.63%
Epoch: 50, Loss: 69.2660, Train: 60.19%, Valid: 60.03%, Test: 59.90%
Epoch: 75, Loss: 18.3615, Train: 74.81%, Valid: 74.62%, Test: 74.63%
Epoch: 100, Loss: 7.3879, Train: 80.45%, Valid: 80.32%, Test: 80.36%
Epoch: 125, Loss: 4.1448, Train: 82.98%, Valid: 82.89%, Test: 82.98%
Epoch: 150, Loss: 20.1942, Train: 83.33%, Valid: 83.24%, Test: 83.37%
Epoch: 175, Loss: 2.8030, Train: 83.76%, Valid: 83.68%, Test: 83.80%
Run 01:
Highest Train: 83.84
Highest Valid: 83.76
  Final Train: 83.84
   Final Test: 83.90
All runs:
Highest Train: 83.84, nan
Highest Valid: 83.76, nan
  Final Train: 83.84, nan
   Final Test: 83.90, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 3.8429, Train: 86.12%, Valid: 86.06%, Test: 86.21%
Epoch: 25, Loss: 0.5116, Train: 87.40%, Valid: 87.26%, Test: 87.40%
Epoch: 50, Loss: 0.4692, Train: 87.51%, Valid: 87.36%, Test: 87.50%
Epoch: 75, Loss: 0.4566, Train: 87.59%, Valid: 87.43%, Test: 87.61%
Epoch: 100, Loss: 0.4340, Train: 87.56%, Valid: 87.42%, Test: 87.58%
Epoch: 125, Loss: 0.4290, Train: 88.37%, Valid: 88.42%, Test: 88.36%
Epoch: 150, Loss: 0.4155, Train: 86.61%, Valid: 86.67%, Test: 86.69%
Epoch: 175, Loss: 0.3960, Train: 86.11%, Valid: 86.17%, Test: 86.19%
Run 01:
Highest Train: 88.45
Highest Valid: 88.49
  Final Train: 88.45
   Final Test: 88.47
All runs:
Highest Train: 88.45, nan
Highest Valid: 88.49, nan
  Final Train: 88.45, nan
   Final Test: 88.47, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 5.5353, Train: 72.26%, Valid: 72.43%, Test: 72.59%
Epoch: 25, Loss: 0.5228, Train: 85.44%, Valid: 85.47%, Test: 85.48%
Epoch: 50, Loss: 0.4366, Train: 86.96%, Valid: 87.00%, Test: 87.02%
Epoch: 75, Loss: 0.4203, Train: 86.95%, Valid: 86.99%, Test: 87.00%
Epoch: 100, Loss: 0.4110, Train: 87.06%, Valid: 87.10%, Test: 87.10%
Epoch: 125, Loss: 0.4053, Train: 86.03%, Valid: 86.10%, Test: 86.17%
Epoch: 150, Loss: 0.4038, Train: 85.76%, Valid: 85.84%, Test: 85.86%
Epoch: 175, Loss: 0.3927, Train: 86.53%, Valid: 86.47%, Test: 86.59%
Run 01:
Highest Train: 87.36
Highest Valid: 87.38
  Final Train: 87.36
   Final Test: 87.47
All runs:
Highest Train: 87.36, nan
Highest Valid: 87.38, nan
  Final Train: 87.36, nan
   Final Test: 87.47, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 380.4078, Train: 39.19%, Valid: 39.49%, Test: 39.50%
Epoch: 25, Loss: 163.6285, Train: 61.20%, Valid: 61.03%, Test: 60.99%
Epoch: 50, Loss: 102.1854, Train: 69.02%, Valid: 68.84%, Test: 68.77%
Epoch: 75, Loss: 11.4447, Train: 74.83%, Valid: 74.64%, Test: 74.64%
Epoch: 100, Loss: 27.0230, Train: 77.46%, Valid: 77.26%, Test: 77.34%
Epoch: 125, Loss: 6.8446, Train: 79.67%, Valid: 79.50%, Test: 79.58%
Epoch: 150, Loss: 14.4481, Train: 81.36%, Valid: 81.21%, Test: 81.29%
Epoch: 175, Loss: 4.4578, Train: 81.85%, Valid: 81.72%, Test: 81.80%
Run 01:
Highest Train: 82.65
Highest Valid: 82.57
  Final Train: 82.65
   Final Test: 82.66
All runs:
Highest Train: 82.65, nan
Highest Valid: 82.57, nan
  Final Train: 82.65, nan
   Final Test: 82.66, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 0.4362, Train: 84.40%, Valid: 84.22%, Test: 84.43%
Epoch: 25, Loss: 0.3680, Train: 86.29%, Valid: 86.31%, Test: 86.37%
Epoch: 50, Loss: 0.3663, Train: 86.07%, Valid: 86.06%, Test: 86.19%
Epoch: 75, Loss: 0.3643, Train: 85.05%, Valid: 84.87%, Test: 85.15%
Epoch: 100, Loss: 0.3617, Train: 84.92%, Valid: 84.77%, Test: 85.03%
Epoch: 125, Loss: 0.3579, Train: 85.02%, Valid: 84.89%, Test: 85.08%
Epoch: 150, Loss: 0.3531, Train: 85.95%, Valid: 85.75%, Test: 86.00%
Epoch: 175, Loss: 0.3474, Train: 84.93%, Valid: 84.73%, Test: 85.03%
Run 01:
Highest Train: 86.77
Highest Valid: 86.62
  Final Train: 86.77
   Final Test: 86.84
All runs:
Highest Train: 86.77, nan
Highest Valid: 86.62, nan
  Final Train: 86.77, nan
   Final Test: 86.84, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 1.6865, Train: 83.58%, Valid: 83.71%, Test: 83.79%
Epoch: 25, Loss: 0.4998, Train: 85.06%, Valid: 84.88%, Test: 85.15%
Epoch: 50, Loss: 0.4890, Train: 85.09%, Valid: 84.91%, Test: 85.18%
Epoch: 75, Loss: 0.4701, Train: 85.04%, Valid: 84.87%, Test: 85.13%
Epoch: 100, Loss: 0.4630, Train: 85.03%, Valid: 84.87%, Test: 85.12%
Epoch: 125, Loss: 0.4447, Train: 84.84%, Valid: 84.69%, Test: 84.95%
Epoch: 150, Loss: 0.4538, Train: 85.03%, Valid: 84.87%, Test: 85.12%
Epoch: 175, Loss: 0.4453, Train: 85.00%, Valid: 84.84%, Test: 85.09%
Run 01:
Highest Train: 86.60
Highest Valid: 86.53
  Final Train: 86.60
   Final Test: 86.64
All runs:
Highest Train: 86.60, nan
Highest Valid: 86.53, nan
  Final Train: 86.60, nan
   Final Test: 86.64, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 320.5420, Train: 59.11%, Valid: 58.83%, Test: 58.74%
Epoch: 25, Loss: 858.2172, Train: 54.95%, Valid: 55.25%, Test: 55.31%
Epoch: 50, Loss: 1385.7621, Train: 53.70%, Valid: 53.96%, Test: 54.10%
Epoch: 75, Loss: 1420.1290, Train: 53.65%, Valid: 53.91%, Test: 54.04%
Epoch: 100, Loss: 1420.0522, Train: 53.65%, Valid: 53.91%, Test: 54.04%
Epoch: 125, Loss: 1418.5212, Train: 53.65%, Valid: 53.91%, Test: 54.05%
Epoch: 150, Loss: 1414.6221, Train: 53.67%, Valid: 53.93%, Test: 54.07%
Epoch: 175, Loss: 1412.3751, Train: 53.68%, Valid: 53.95%, Test: 54.08%
Run 01:
Highest Train: 82.08
Highest Valid: 81.95
  Final Train: 82.08
   Final Test: 82.06
All runs:
Highest Train: 82.08, nan
Highest Valid: 81.95, nan
  Final Train: 82.08, nan
   Final Test: 82.06, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 6.4887, Train: 85.29%, Valid: 85.26%, Test: 85.44%
Epoch: 25, Loss: 0.4380, Train: 85.11%, Valid: 85.09%, Test: 85.22%
Epoch: 50, Loss: 0.4225, Train: 87.24%, Valid: 87.10%, Test: 87.24%
Epoch: 75, Loss: 0.4182, Train: 86.47%, Valid: 86.35%, Test: 86.50%
Epoch: 100, Loss: 0.3933, Train: 85.89%, Valid: 85.78%, Test: 85.92%
Epoch: 125, Loss: 0.3885, Train: 85.76%, Valid: 85.63%, Test: 85.74%
Epoch: 150, Loss: 0.3814, Train: 85.67%, Valid: 85.56%, Test: 85.73%
Epoch: 175, Loss: 0.3790, Train: 85.89%, Valid: 85.96%, Test: 85.93%
Run 01:
Highest Train: 87.24
Highest Valid: 87.12
  Final Train: 87.21
   Final Test: 87.23
All runs:
Highest Train: 87.24, nan
Highest Valid: 87.12, nan
  Final Train: 87.21, nan
   Final Test: 87.23, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 3.6949, Train: 62.22%, Valid: 62.36%, Test: 62.62%
Epoch: 25, Loss: 0.3822, Train: 85.95%, Valid: 85.83%, Test: 86.03%
Epoch: 50, Loss: 0.3779, Train: 86.03%, Valid: 85.92%, Test: 86.12%
Epoch: 75, Loss: 0.3776, Train: 86.00%, Valid: 85.88%, Test: 86.08%
Epoch: 100, Loss: 0.3739, Train: 85.38%, Valid: 85.27%, Test: 85.48%
Epoch: 125, Loss: 0.3717, Train: 86.10%, Valid: 86.04%, Test: 86.19%
Epoch: 150, Loss: 0.3703, Train: 85.63%, Valid: 85.60%, Test: 85.69%
Epoch: 175, Loss: 0.3709, Train: 85.77%, Valid: 85.72%, Test: 85.82%
Run 01:
Highest Train: 86.32
Highest Valid: 86.23
  Final Train: 86.32
   Final Test: 86.36
All runs:
Highest Train: 86.32, nan
Highest Valid: 86.23, nan
  Final Train: 86.32, nan
   Final Test: 86.36, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 326.1799, Train: 61.20%, Valid: 61.00%, Test: 60.90%
Epoch: 25, Loss: 55.9720, Train: 43.42%, Valid: 43.66%, Test: 43.55%
Epoch: 50, Loss: 38.8942, Train: 45.00%, Valid: 44.89%, Test: 44.99%
Epoch: 75, Loss: 51.5827, Train: 41.37%, Valid: 41.56%, Test: 41.60%
Epoch: 100, Loss: 28.8062, Train: 40.20%, Valid: 40.37%, Test: 40.42%
Epoch: 125, Loss: 40.9425, Train: 67.56%, Valid: 67.37%, Test: 67.31%
Epoch: 150, Loss: 19.3457, Train: 44.76%, Valid: 45.00%, Test: 45.14%
Epoch: 175, Loss: 8.6686, Train: 75.70%, Valid: 75.52%, Test: 75.53%
Run 01:
Highest Train: 82.80
Highest Valid: 82.74
  Final Train: 82.80
   Final Test: 82.81
All runs:
Highest Train: 82.80, nan
Highest Valid: 82.74, nan
  Final Train: 82.80, nan
   Final Test: 82.81, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 0.8123, Train: 84.31%, Valid: 84.16%, Test: 84.41%
Epoch: 25, Loss: 0.4425, Train: 85.19%, Valid: 84.98%, Test: 85.30%
Epoch: 50, Loss: 0.3917, Train: 86.35%, Valid: 86.33%, Test: 86.46%
Epoch: 75, Loss: 0.3914, Train: 86.43%, Valid: 86.41%, Test: 86.55%
Epoch: 100, Loss: 0.3786, Train: 86.49%, Valid: 86.49%, Test: 86.61%
Epoch: 125, Loss: 0.3763, Train: 86.05%, Valid: 86.01%, Test: 86.15%
Epoch: 150, Loss: 0.3703, Train: 86.64%, Valid: 86.62%, Test: 86.73%
Epoch: 175, Loss: 0.3665, Train: 86.49%, Valid: 86.48%, Test: 86.61%
Run 01:
Highest Train: 86.64
Highest Valid: 86.62
  Final Train: 86.64
   Final Test: 86.73
All runs:
Highest Train: 86.64, nan
Highest Valid: 86.62, nan
  Final Train: 86.64, nan
   Final Test: 86.73, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 3.7638, Train: 84.11%, Valid: 84.06%, Test: 84.24%
Epoch: 25, Loss: 0.4524, Train: 86.59%, Valid: 86.48%, Test: 86.61%
Epoch: 50, Loss: 0.4443, Train: 86.60%, Valid: 86.49%, Test: 86.63%
Epoch: 75, Loss: 0.4149, Train: 86.47%, Valid: 86.37%, Test: 86.52%
Epoch: 100, Loss: 0.4054, Train: 86.53%, Valid: 86.50%, Test: 86.63%
Epoch: 125, Loss: 0.3936, Train: 86.52%, Valid: 86.47%, Test: 86.60%
Epoch: 150, Loss: 0.3849, Train: 86.58%, Valid: 86.59%, Test: 86.65%
Epoch: 175, Loss: 0.3807, Train: 86.54%, Valid: 86.56%, Test: 86.64%
Run 01:
Highest Train: 87.90
Highest Valid: 87.83
  Final Train: 87.90
   Final Test: 87.88
All runs:
Highest Train: 87.90, nan
Highest Valid: 87.83, nan
  Final Train: 87.90, nan
   Final Test: 87.88, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 49.3493, Train: 46.55%, Valid: 46.57%, Test: 46.64%
Epoch: 25, Loss: 143.2218, Train: 75.63%, Valid: 75.45%, Test: 75.50%
Epoch: 50, Loss: 25.7557, Train: 81.27%, Valid: 81.16%, Test: 81.24%
Epoch: 75, Loss: 6.2746, Train: 79.37%, Valid: 79.21%, Test: 79.30%
Epoch: 100, Loss: 10.8918, Train: 68.18%, Valid: 68.39%, Test: 68.52%
Epoch: 125, Loss: 9.2458, Train: 80.97%, Valid: 80.85%, Test: 80.88%
Epoch: 150, Loss: 6.7734, Train: 78.62%, Valid: 78.43%, Test: 78.50%
Epoch: 175, Loss: 8.2973, Train: 77.88%, Valid: 77.69%, Test: 77.75%
Run 01:
Highest Train: 82.69
Highest Valid: 82.62
  Final Train: 82.69
   Final Test: 82.69
All runs:
Highest Train: 82.69, nan
Highest Valid: 82.62, nan
  Final Train: 82.69, nan
   Final Test: 82.69, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 3.8231, Train: 83.47%, Valid: 83.30%, Test: 83.61%
Epoch: 25, Loss: 0.5178, Train: 85.74%, Valid: 85.72%, Test: 85.87%
Epoch: 50, Loss: 0.4787, Train: 85.72%, Valid: 85.71%, Test: 85.86%
Epoch: 75, Loss: 0.4519, Train: 85.31%, Valid: 85.30%, Test: 85.47%
Epoch: 100, Loss: 0.4329, Train: 85.70%, Valid: 85.75%, Test: 85.82%
Epoch: 125, Loss: 0.4167, Train: 85.75%, Valid: 85.81%, Test: 85.87%
Epoch: 150, Loss: 0.4123, Train: 85.77%, Valid: 85.84%, Test: 85.88%
Epoch: 175, Loss: 0.3924, Train: 85.69%, Valid: 85.76%, Test: 85.80%
Run 01:
Highest Train: 87.09
Highest Valid: 87.06
  Final Train: 87.09
   Final Test: 87.10
All runs:
Highest Train: 87.09, nan
Highest Valid: 87.06, nan
  Final Train: 87.09, nan
   Final Test: 87.10, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 2.1417, Train: 85.38%, Valid: 85.25%, Test: 85.41%
Epoch: 25, Loss: 0.4624, Train: 86.39%, Valid: 86.27%, Test: 86.44%
Epoch: 50, Loss: 0.4310, Train: 86.09%, Valid: 85.95%, Test: 86.13%
Epoch: 75, Loss: 0.4054, Train: 87.19%, Valid: 86.99%, Test: 87.18%
Epoch: 100, Loss: 0.3880, Train: 86.69%, Valid: 86.63%, Test: 86.64%
Epoch: 125, Loss: 0.3852, Train: 86.58%, Valid: 86.57%, Test: 86.67%
Epoch: 150, Loss: 0.3807, Train: 86.12%, Valid: 86.04%, Test: 86.12%
Epoch: 175, Loss: 0.3758, Train: 85.68%, Valid: 85.54%, Test: 85.70%
Run 01:
Highest Train: 87.64
Highest Valid: 87.48
  Final Train: 87.64
   Final Test: 87.63
All runs:
Highest Train: 87.64, nan
Highest Valid: 87.48, nan
  Final Train: 87.64, nan
   Final Test: 87.63, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 152.4786, Train: 68.62%, Valid: 68.44%, Test: 68.28%
Epoch: 25, Loss: 118.3888, Train: 42.90%, Valid: 43.10%, Test: 43.21%
Epoch: 50, Loss: 51.5404, Train: 46.38%, Valid: 46.60%, Test: 46.72%
Epoch: 75, Loss: 7.5448, Train: 72.41%, Valid: 72.22%, Test: 72.16%
Epoch: 100, Loss: 7.4613, Train: 74.60%, Valid: 74.40%, Test: 74.39%
Epoch: 125, Loss: 7.8835, Train: 73.67%, Valid: 73.46%, Test: 73.47%
Epoch: 150, Loss: 7.7220, Train: 75.15%, Valid: 74.96%, Test: 74.97%
Epoch: 175, Loss: 7.3874, Train: 77.46%, Valid: 77.31%, Test: 77.36%
Run 01:
Highest Train: 80.23
Highest Valid: 80.15
  Final Train: 80.23
   Final Test: 80.27
All runs:
Highest Train: 80.23, nan
Highest Valid: 80.15, nan
  Final Train: 80.23, nan
   Final Test: 80.27, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 6.9570, Train: 16.61%, Valid: 16.81%, Test: 16.52%
Epoch: 25, Loss: 0.3746, Train: 88.03%, Valid: 88.12%, Test: 88.06%
Epoch: 50, Loss: 0.3667, Train: 86.22%, Valid: 86.12%, Test: 86.31%
Epoch: 75, Loss: 0.3649, Train: 86.35%, Valid: 86.32%, Test: 86.45%
Epoch: 100, Loss: 0.3631, Train: 86.14%, Valid: 86.09%, Test: 86.28%
Epoch: 125, Loss: 0.3610, Train: 86.40%, Valid: 86.37%, Test: 86.48%
Epoch: 150, Loss: 0.3585, Train: 86.55%, Valid: 86.51%, Test: 86.63%
Epoch: 175, Loss: 0.3560, Train: 86.31%, Valid: 86.22%, Test: 86.34%
Run 01:
Highest Train: 88.05
Highest Valid: 88.14
  Final Train: 88.05
   Final Test: 88.08
All runs:
Highest Train: 88.05, nan
Highest Valid: 88.14, nan
  Final Train: 88.05, nan
   Final Test: 88.08, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 1.3049, Train: 83.96%, Valid: 83.94%, Test: 84.03%
Epoch: 25, Loss: 0.6826, Train: 85.91%, Valid: 85.78%, Test: 85.93%
Epoch: 50, Loss: 0.4948, Train: 86.17%, Valid: 86.10%, Test: 86.21%
Epoch: 75, Loss: 0.4829, Train: 86.13%, Valid: 86.03%, Test: 86.20%
Epoch: 100, Loss: 0.4743, Train: 86.07%, Valid: 85.97%, Test: 86.13%
Epoch: 125, Loss: 0.4661, Train: 85.60%, Valid: 85.50%, Test: 85.69%
Epoch: 150, Loss: 0.4617, Train: 85.51%, Valid: 85.36%, Test: 85.61%
Epoch: 175, Loss: 0.4543, Train: 85.39%, Valid: 85.24%, Test: 85.45%
Run 01:
Highest Train: 86.20
Highest Valid: 86.19
  Final Train: 86.20
   Final Test: 86.27
All runs:
Highest Train: 86.20, nan
Highest Valid: 86.19, nan
  Final Train: 86.20, nan
   Final Test: 86.27, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 122.1152, Train: 44.40%, Valid: 44.43%, Test: 44.42%
Epoch: 25, Loss: 36.7876, Train: 80.32%, Valid: 80.16%, Test: 80.25%
Epoch: 50, Loss: 69.2640, Train: 64.62%, Valid: 64.92%, Test: 65.12%
Epoch: 75, Loss: 68.9045, Train: 80.91%, Valid: 80.74%, Test: 80.86%
Epoch: 100, Loss: 80.3623, Train: 80.89%, Valid: 80.72%, Test: 80.83%
Epoch: 125, Loss: 85.5963, Train: 80.94%, Valid: 80.76%, Test: 80.89%
Epoch: 150, Loss: 58.9111, Train: 80.89%, Valid: 80.71%, Test: 80.84%
Epoch: 175, Loss: 53.6253, Train: 80.88%, Valid: 80.70%, Test: 80.82%
Run 01:
Highest Train: 82.82
Highest Valid: 82.75
  Final Train: 82.82
   Final Test: 82.86
All runs:
Highest Train: 82.82, nan
Highest Valid: 82.75, nan
  Final Train: 82.82, nan
   Final Test: 82.86, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 4.6471, Train: 14.42%, Valid: 14.35%, Test: 14.42%
Epoch: 25, Loss: 0.3849, Train: 84.59%, Valid: 84.36%, Test: 84.65%
Epoch: 50, Loss: 0.3858, Train: 85.07%, Valid: 85.03%, Test: 85.13%
Epoch: 75, Loss: 0.3827, Train: 85.13%, Valid: 85.12%, Test: 85.19%
Epoch: 100, Loss: 0.3793, Train: 85.25%, Valid: 85.21%, Test: 85.31%
Epoch: 125, Loss: 0.3764, Train: 85.30%, Valid: 85.27%, Test: 85.35%
Epoch: 150, Loss: 0.3729, Train: 85.63%, Valid: 85.59%, Test: 85.65%
Epoch: 175, Loss: 0.3723, Train: 85.68%, Valid: 85.69%, Test: 85.71%
Run 01:
Highest Train: 86.28
Highest Valid: 86.37
  Final Train: 86.28
   Final Test: 86.33
All runs:
Highest Train: 86.28, nan
Highest Valid: 86.37, nan
  Final Train: 86.28, nan
   Final Test: 86.33, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 2.6214, Train: 80.54%, Valid: 80.71%, Test: 80.90%
Epoch: 25, Loss: 0.4088, Train: 85.43%, Valid: 85.26%, Test: 85.49%
Epoch: 50, Loss: 0.4128, Train: 85.40%, Valid: 85.24%, Test: 85.46%
Epoch: 75, Loss: 0.4046, Train: 85.42%, Valid: 85.26%, Test: 85.47%
Epoch: 100, Loss: 0.3989, Train: 85.47%, Valid: 85.32%, Test: 85.53%
Epoch: 125, Loss: 0.3897, Train: 85.41%, Valid: 85.27%, Test: 85.47%
Epoch: 150, Loss: 0.3835, Train: 85.69%, Valid: 85.55%, Test: 85.76%
Epoch: 175, Loss: 0.3833, Train: 85.96%, Valid: 85.94%, Test: 86.08%
Run 01:
Highest Train: 87.81
Highest Valid: 87.67
  Final Train: 87.81
   Final Test: 87.79
All runs:
Highest Train: 87.81, nan
Highest Valid: 87.67, nan
  Final Train: 87.81, nan
   Final Test: 87.79, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 13.6532, Train: 47.97%, Valid: 47.93%, Test: 48.11%
Epoch: 25, Loss: 34.4880, Train: 77.55%, Valid: 77.51%, Test: 77.61%
Epoch: 50, Loss: 13.4244, Train: 83.37%, Valid: 83.29%, Test: 83.41%
Epoch: 75, Loss: 9.9787, Train: 82.58%, Valid: 82.54%, Test: 82.62%
Epoch: 100, Loss: 6.3744, Train: 84.02%, Valid: 84.04%, Test: 84.22%
Epoch: 125, Loss: 7.3062, Train: 83.25%, Valid: 83.29%, Test: 83.43%
Epoch: 150, Loss: 7.0619, Train: 83.80%, Valid: 83.79%, Test: 83.88%
Epoch: 175, Loss: 5.9652, Train: 78.06%, Valid: 78.05%, Test: 78.17%
Run 01:
Highest Train: 84.56
Highest Valid: 84.52
  Final Train: 84.56
   Final Test: 84.70
All runs:
Highest Train: 84.56, nan
Highest Valid: 84.52, nan
  Final Train: 84.56, nan
   Final Test: 84.70, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 11.2069, Train: 14.90%, Valid: 14.91%, Test: 14.90%
Epoch: 25, Loss: 0.4659, Train: 84.31%, Valid: 84.11%, Test: 84.40%
Epoch: 50, Loss: 0.4495, Train: 87.81%, Valid: 87.87%, Test: 87.84%
Epoch: 75, Loss: 0.4361, Train: 87.87%, Valid: 87.93%, Test: 87.92%
Epoch: 100, Loss: 0.4203, Train: 87.83%, Valid: 87.92%, Test: 87.90%
Epoch: 125, Loss: 0.4214, Train: 87.80%, Valid: 87.88%, Test: 87.89%
Epoch: 150, Loss: 0.3884, Train: 85.83%, Valid: 85.69%, Test: 85.83%
Epoch: 175, Loss: 0.3831, Train: 85.82%, Valid: 85.68%, Test: 85.84%
Run 01:
Highest Train: 88.15
Highest Valid: 88.22
  Final Train: 88.15
   Final Test: 88.18
All runs:
Highest Train: 88.15, nan
Highest Valid: 88.22, nan
  Final Train: 88.15, nan
   Final Test: 88.18, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 2.1473, Train: 81.50%, Valid: 81.64%, Test: 81.70%
Epoch: 25, Loss: 0.3905, Train: 85.01%, Valid: 84.90%, Test: 85.10%
Epoch: 50, Loss: 0.3829, Train: 85.75%, Valid: 85.60%, Test: 85.82%
Epoch: 75, Loss: 0.3768, Train: 85.85%, Valid: 85.73%, Test: 85.91%
Epoch: 100, Loss: 0.3778, Train: 86.07%, Valid: 85.96%, Test: 86.11%
Epoch: 125, Loss: 0.3744, Train: 85.45%, Valid: 85.40%, Test: 85.55%
Epoch: 150, Loss: 0.3684, Train: 86.03%, Valid: 85.91%, Test: 86.11%
Epoch: 175, Loss: 0.3622, Train: 85.97%, Valid: 85.83%, Test: 86.06%
Run 01:
Highest Train: 86.94
Highest Valid: 86.87
  Final Train: 86.94
   Final Test: 86.96
All runs:
Highest Train: 86.94, nan
Highest Valid: 86.87, nan
  Final Train: 86.94, nan
   Final Test: 86.96, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 14.8285, Train: 55.54%, Valid: 55.44%, Test: 55.41%
Epoch: 25, Loss: 7.5735, Train: 48.64%, Valid: 48.61%, Test: 48.72%
Epoch: 50, Loss: 32.9867, Train: 51.48%, Valid: 51.47%, Test: 51.49%
Epoch: 75, Loss: 7.6129, Train: 49.99%, Valid: 50.00%, Test: 49.98%
Epoch: 100, Loss: 8.0791, Train: 49.99%, Valid: 50.00%, Test: 49.98%
Epoch: 125, Loss: 7.3010, Train: 47.92%, Valid: 47.92%, Test: 48.05%
Epoch: 150, Loss: 9.9533, Train: 51.82%, Valid: 51.82%, Test: 51.79%
Epoch: 175, Loss: 18.3551, Train: 42.88%, Valid: 43.10%, Test: 43.16%
Run 01:
Highest Train: 79.38
Highest Valid: 79.20
  Final Train: 79.38
   Final Test: 79.31
All runs:
Highest Train: 79.38, nan
Highest Valid: 79.20, nan
  Final Train: 79.38, nan
   Final Test: 79.31, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 4.0057, Train: 84.56%, Valid: 84.57%, Test: 84.70%
Epoch: 25, Loss: 0.5228, Train: 86.81%, Valid: 86.89%, Test: 86.86%
Epoch: 50, Loss: 0.4852, Train: 85.37%, Valid: 85.16%, Test: 85.44%
Epoch: 75, Loss: 0.4558, Train: 84.86%, Valid: 84.69%, Test: 84.96%
Epoch: 100, Loss: 0.4182, Train: 84.42%, Valid: 84.26%, Test: 84.48%
Epoch: 125, Loss: 0.4026, Train: 84.83%, Valid: 84.76%, Test: 84.89%
Epoch: 150, Loss: 0.3912, Train: 84.66%, Valid: 84.50%, Test: 84.74%
Epoch: 175, Loss: 0.3826, Train: 85.33%, Valid: 85.34%, Test: 85.43%
Run 01:
Highest Train: 87.15
Highest Valid: 87.19
  Final Train: 87.15
   Final Test: 87.19
All runs:
Highest Train: 87.15, nan
Highest Valid: 87.19, nan
  Final Train: 87.15, nan
   Final Test: 87.19, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 4.2316, Train: 84.28%, Valid: 84.12%, Test: 84.33%
Epoch: 25, Loss: 0.5186, Train: 85.19%, Valid: 85.11%, Test: 85.26%
Epoch: 50, Loss: 0.4955, Train: 85.71%, Valid: 85.58%, Test: 85.82%
Epoch: 75, Loss: 0.4729, Train: 85.64%, Valid: 85.50%, Test: 85.74%
Epoch: 100, Loss: 0.4649, Train: 85.77%, Valid: 85.61%, Test: 85.84%
Epoch: 125, Loss: 0.4327, Train: 86.16%, Valid: 85.99%, Test: 86.21%
Epoch: 150, Loss: 0.4238, Train: 86.24%, Valid: 86.08%, Test: 86.28%
Epoch: 175, Loss: 0.4089, Train: 85.48%, Valid: 85.31%, Test: 85.54%
Run 01:
Highest Train: 86.25
Highest Valid: 86.10
  Final Train: 86.23
   Final Test: 86.29
All runs:
Highest Train: 86.25, nan
Highest Valid: 86.10, nan
  Final Train: 86.23, nan
   Final Test: 86.29, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 8.9475, Train: 50.07%, Valid: 50.03%, Test: 50.08%
Epoch: 25, Loss: 10.0603, Train: 50.19%, Valid: 50.13%, Test: 50.19%
Epoch: 50, Loss: 7.1236, Train: 48.57%, Valid: 48.78%, Test: 48.49%
Epoch: 75, Loss: 6.6339, Train: 48.84%, Valid: 48.96%, Test: 48.81%
Epoch: 100, Loss: 7.9137, Train: 48.04%, Valid: 48.14%, Test: 48.01%
Epoch: 125, Loss: 7.5885, Train: 49.70%, Valid: 49.75%, Test: 49.64%
Epoch: 150, Loss: 8.3956, Train: 50.36%, Valid: 50.31%, Test: 50.42%
Epoch: 175, Loss: 8.4216, Train: 50.68%, Valid: 50.57%, Test: 50.70%
Run 01:
Highest Train: 75.87
Highest Valid: 75.64
  Final Train: 75.87
   Final Test: 75.71
All runs:
Highest Train: 75.87, nan
Highest Valid: 75.64, nan
  Final Train: 75.87, nan
   Final Test: 75.71, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 16.8707, Train: 11.78%, Valid: 11.78%, Test: 11.77%
Epoch: 25, Loss: 0.4395, Train: 85.00%, Valid: 84.89%, Test: 85.10%
Epoch: 50, Loss: 0.4430, Train: 85.54%, Valid: 85.48%, Test: 85.62%
Epoch: 75, Loss: 0.4284, Train: 86.48%, Valid: 86.58%, Test: 86.57%
Epoch: 100, Loss: 0.4135, Train: 86.33%, Valid: 86.41%, Test: 86.44%
Epoch: 125, Loss: 0.3986, Train: 85.93%, Valid: 85.94%, Test: 86.00%
Epoch: 150, Loss: 0.3830, Train: 85.60%, Valid: 85.63%, Test: 85.68%
Epoch: 175, Loss: 0.3629, Train: 85.31%, Valid: 85.35%, Test: 85.38%
Run 01:
Highest Train: 88.07
Highest Valid: 88.09
  Final Train: 88.07
   Final Test: 88.09
All runs:
Highest Train: 88.07, nan
Highest Valid: 88.09, nan
  Final Train: 88.07, nan
   Final Test: 88.09, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 18.2776, Train: 14.70%, Valid: 14.83%, Test: 14.70%
Epoch: 25, Loss: 0.4285, Train: 84.74%, Valid: 84.66%, Test: 84.86%
Epoch: 50, Loss: 0.4007, Train: 84.84%, Valid: 84.79%, Test: 84.96%
Epoch: 75, Loss: 0.3698, Train: 85.32%, Valid: 85.18%, Test: 85.36%
Epoch: 100, Loss: 0.3655, Train: 85.49%, Valid: 85.34%, Test: 85.55%
Epoch: 125, Loss: 0.3636, Train: 85.55%, Valid: 85.40%, Test: 85.61%
Epoch: 150, Loss: 0.3608, Train: 85.55%, Valid: 85.40%, Test: 85.61%
Epoch: 175, Loss: 0.3577, Train: 85.51%, Valid: 85.36%, Test: 85.57%
Run 01:
Highest Train: 86.02
Highest Valid: 85.91
  Final Train: 86.02
   Final Test: 86.11
All runs:
Highest Train: 86.02, nan
Highest Valid: 85.91, nan
  Final Train: 86.02, nan
   Final Test: 86.11, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 12812.6992, Train: 62.30%, Valid: 62.14%, Test: 62.03%
Epoch: 25, Loss: 1962.7051, Train: 69.86%, Valid: 69.77%, Test: 69.84%
Epoch: 50, Loss: 306.5223, Train: 78.50%, Valid: 78.27%, Test: 78.16%
Epoch: 75, Loss: 114.6794, Train: 77.37%, Valid: 77.19%, Test: 77.09%
Epoch: 100, Loss: 72.6256, Train: 77.63%, Valid: 77.37%, Test: 77.30%
Epoch: 125, Loss: 60.0609, Train: 77.72%, Valid: 77.48%, Test: 77.39%
Epoch: 150, Loss: 55.5565, Train: 77.75%, Valid: 77.54%, Test: 77.48%
Epoch: 175, Loss: 53.0961, Train: 77.75%, Valid: 77.53%, Test: 77.46%
Run 01:
Highest Train: 78.82
Highest Valid: 78.62
  Final Train: 78.82
   Final Test: 78.56
All runs:
Highest Train: 78.82, nan
Highest Valid: 78.62, nan
  Final Train: 78.82, nan
   Final Test: 78.56, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 3.0248, Train: 84.93%, Valid: 84.73%, Test: 84.96%
Epoch: 25, Loss: 0.4695, Train: 85.82%, Valid: 85.70%, Test: 85.86%
Epoch: 50, Loss: 0.4270, Train: 85.73%, Valid: 85.60%, Test: 85.76%
Epoch: 75, Loss: 0.3881, Train: 85.54%, Valid: 85.44%, Test: 85.61%
Epoch: 100, Loss: 0.3779, Train: 85.54%, Valid: 85.42%, Test: 85.60%
Epoch: 125, Loss: 0.3739, Train: 85.31%, Valid: 85.16%, Test: 85.39%
Epoch: 150, Loss: 0.3686, Train: 85.57%, Valid: 85.58%, Test: 85.64%
Epoch: 175, Loss: 0.3663, Train: 87.07%, Valid: 86.89%, Test: 87.12%
Run 01:
Highest Train: 87.74
Highest Valid: 87.80
  Final Train: 87.74
   Final Test: 87.81
All runs:
Highest Train: 87.74, nan
Highest Valid: 87.80, nan
  Final Train: 87.74, nan
   Final Test: 87.81, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 9.1496, Train: 76.39%, Valid: 76.45%, Test: 76.79%
Epoch: 25, Loss: 0.4818, Train: 85.98%, Valid: 85.88%, Test: 86.03%
Epoch: 50, Loss: 0.4626, Train: 85.98%, Valid: 85.89%, Test: 86.03%
Epoch: 75, Loss: 0.4195, Train: 85.26%, Valid: 85.09%, Test: 85.32%
Epoch: 100, Loss: 0.3850, Train: 85.32%, Valid: 85.13%, Test: 85.37%
Epoch: 125, Loss: 0.3804, Train: 85.39%, Valid: 85.20%, Test: 85.45%
Epoch: 150, Loss: 0.3724, Train: 85.42%, Valid: 85.23%, Test: 85.48%
Epoch: 175, Loss: 0.3728, Train: 85.42%, Valid: 85.23%, Test: 85.48%
Run 01:
Highest Train: 86.01
Highest Valid: 85.99
  Final Train: 86.01
   Final Test: 86.09
All runs:
Highest Train: 86.01, nan
Highest Valid: 85.99, nan
  Final Train: 86.01, nan
   Final Test: 86.09, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 271.4942, Train: 51.56%, Valid: 51.52%, Test: 51.78%
Epoch: 25, Loss: 22.0378, Train: 71.89%, Valid: 71.70%, Test: 71.65%
Epoch: 50, Loss: 44.0765, Train: 76.40%, Valid: 76.21%, Test: 76.30%
Epoch: 75, Loss: 4.9412, Train: 79.63%, Valid: 79.46%, Test: 79.53%
Epoch: 100, Loss: 98.7853, Train: 81.64%, Valid: 81.51%, Test: 81.58%
Epoch: 125, Loss: 5.8844, Train: 82.29%, Valid: 82.20%, Test: 82.29%
Epoch: 150, Loss: 5.6062, Train: 81.89%, Valid: 81.78%, Test: 81.86%
Epoch: 175, Loss: 7.6945, Train: 81.99%, Valid: 81.88%, Test: 81.95%
Run 01:
Highest Train: 82.58
Highest Valid: 82.50
  Final Train: 82.58
   Final Test: 82.59
All runs:
Highest Train: 82.58, nan
Highest Valid: 82.50, nan
  Final Train: 82.58, nan
   Final Test: 82.59, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 2.4414, Train: 87.57%, Valid: 87.69%, Test: 87.65%
Epoch: 25, Loss: 0.5046, Train: 86.51%, Valid: 86.50%, Test: 86.60%
Epoch: 50, Loss: 0.4400, Train: 86.02%, Valid: 86.01%, Test: 86.08%
Epoch: 75, Loss: 0.4023, Train: 85.29%, Valid: 85.30%, Test: 85.36%
Epoch: 100, Loss: 0.3921, Train: 85.33%, Valid: 85.36%, Test: 85.42%
Epoch: 125, Loss: 0.3861, Train: 87.20%, Valid: 87.03%, Test: 87.24%
Epoch: 150, Loss: 0.3759, Train: 85.09%, Valid: 85.07%, Test: 85.19%
Epoch: 175, Loss: 0.3683, Train: 85.37%, Valid: 85.38%, Test: 85.45%
Run 01:
Highest Train: 87.57
Highest Valid: 87.69
  Final Train: 87.57
   Final Test: 87.65
All runs:
Highest Train: 87.57, nan
Highest Valid: 87.69, nan
  Final Train: 87.57, nan
   Final Test: 87.65, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 11.7850, Train: 15.47%, Valid: 15.58%, Test: 15.46%
Epoch: 25, Loss: 0.5321, Train: 85.85%, Valid: 85.83%, Test: 85.93%
Epoch: 50, Loss: 0.4980, Train: 85.99%, Valid: 86.01%, Test: 86.01%
Epoch: 75, Loss: 0.4550, Train: 86.60%, Valid: 86.59%, Test: 86.66%
Epoch: 100, Loss: 0.4305, Train: 85.16%, Valid: 85.13%, Test: 85.31%
Epoch: 125, Loss: 0.4333, Train: 85.44%, Valid: 85.29%, Test: 85.56%
Epoch: 150, Loss: 0.4091, Train: 85.70%, Valid: 85.55%, Test: 85.77%
Epoch: 175, Loss: 0.3946, Train: 85.90%, Valid: 85.73%, Test: 85.97%
Run 01:
Highest Train: 86.68
Highest Valid: 86.61
  Final Train: 86.68
   Final Test: 86.71
All runs:
Highest Train: 86.68, nan
Highest Valid: 86.61, nan
  Final Train: 86.68, nan
   Final Test: 86.71, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 572.5054, Train: 58.03%, Valid: 57.86%, Test: 57.72%
Epoch: 25, Loss: 86.5717, Train: 53.60%, Valid: 53.40%, Test: 53.29%
Epoch: 50, Loss: 38.1612, Train: 54.95%, Valid: 54.75%, Test: 54.62%
Epoch: 75, Loss: 21.4563, Train: 75.93%, Valid: 75.74%, Test: 75.79%
Epoch: 100, Loss: 10.5857, Train: 80.31%, Valid: 80.17%, Test: 80.23%
Epoch: 125, Loss: 6.0816, Train: 82.09%, Valid: 81.97%, Test: 82.06%
Epoch: 150, Loss: 4.6320, Train: 83.25%, Valid: 83.16%, Test: 83.28%
Epoch: 175, Loss: 3.8024, Train: 83.35%, Valid: 83.25%, Test: 83.39%
Run 01:
Highest Train: 83.54
Highest Valid: 83.45
  Final Train: 83.54
   Final Test: 83.59
All runs:
Highest Train: 83.54, nan
Highest Valid: 83.45, nan
  Final Train: 83.54, nan
   Final Test: 83.59, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 6.5174, Train: 83.70%, Valid: 83.55%, Test: 83.75%
Epoch: 25, Loss: 0.6172, Train: 87.42%, Valid: 87.30%, Test: 87.43%
Epoch: 50, Loss: 0.5684, Train: 86.25%, Valid: 86.14%, Test: 86.31%
Epoch: 75, Loss: 0.5160, Train: 86.09%, Valid: 85.96%, Test: 86.16%
Epoch: 100, Loss: 0.4625, Train: 86.08%, Valid: 85.95%, Test: 86.13%
Epoch: 125, Loss: 0.4639, Train: 86.75%, Valid: 86.59%, Test: 86.76%
Epoch: 150, Loss: 0.4295, Train: 86.05%, Valid: 85.93%, Test: 86.11%
Epoch: 175, Loss: 0.4055, Train: 86.97%, Valid: 86.80%, Test: 86.99%
Run 01:
Highest Train: 87.43
Highest Valid: 87.32
  Final Train: 87.43
   Final Test: 87.46
All runs:
Highest Train: 87.43, nan
Highest Valid: 87.32, nan
  Final Train: 87.43, nan
   Final Test: 87.46, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 14.1513, Train: 16.24%, Valid: 16.30%, Test: 16.20%
Epoch: 25, Loss: 0.5327, Train: 86.33%, Valid: 86.25%, Test: 86.43%
Epoch: 50, Loss: 0.5275, Train: 86.45%, Valid: 86.40%, Test: 86.55%
Epoch: 75, Loss: 0.4885, Train: 84.90%, Valid: 84.79%, Test: 85.01%
Epoch: 100, Loss: 0.4564, Train: 85.60%, Valid: 85.47%, Test: 85.66%
Epoch: 125, Loss: 0.4382, Train: 85.36%, Valid: 85.20%, Test: 85.44%
Epoch: 150, Loss: 0.4212, Train: 85.99%, Valid: 85.87%, Test: 86.04%
Epoch: 175, Loss: 0.4171, Train: 86.11%, Valid: 85.95%, Test: 86.10%
Run 01:
Highest Train: 86.53
Highest Valid: 86.48
  Final Train: 86.52
   Final Test: 86.61
All runs:
Highest Train: 86.53, nan
Highest Valid: 86.48, nan
  Final Train: 86.52, nan
   Final Test: 86.61, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 160.5331, Train: 44.46%, Valid: 44.55%, Test: 44.72%
Epoch: 25, Loss: 9.2198, Train: 72.44%, Valid: 72.27%, Test: 72.22%
Epoch: 50, Loss: 6.8751, Train: 77.22%, Valid: 77.05%, Test: 77.14%
Epoch: 75, Loss: 6.1075, Train: 76.34%, Valid: 76.20%, Test: 76.25%
Epoch: 100, Loss: 17.0449, Train: 70.42%, Valid: 70.48%, Test: 70.56%
Epoch: 125, Loss: 5.0589, Train: 77.69%, Valid: 77.50%, Test: 77.60%
Epoch: 150, Loss: 4.2893, Train: 78.40%, Valid: 78.23%, Test: 78.32%
Epoch: 175, Loss: 3.7162, Train: 79.41%, Valid: 79.24%, Test: 79.30%
Run 01:
Highest Train: 79.89
Highest Valid: 79.74
  Final Train: 79.89
   Final Test: 79.78
All runs:
Highest Train: 79.89, nan
Highest Valid: 79.74, nan
  Final Train: 79.89, nan
   Final Test: 79.78, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 3.7444, Train: 85.59%, Valid: 85.68%, Test: 85.57%
Epoch: 25, Loss: 0.4222, Train: 87.92%, Valid: 87.99%, Test: 87.97%
Epoch: 50, Loss: 0.4067, Train: 87.64%, Valid: 87.67%, Test: 87.68%
Epoch: 75, Loss: 0.3798, Train: 86.93%, Valid: 86.98%, Test: 86.94%
Epoch: 100, Loss: 0.3592, Train: 86.44%, Valid: 86.37%, Test: 86.51%
Epoch: 125, Loss: 0.3528, Train: 86.66%, Valid: 86.64%, Test: 86.72%
Epoch: 150, Loss: 0.3454, Train: 86.16%, Valid: 86.13%, Test: 86.18%
Epoch: 175, Loss: 0.3374, Train: 85.46%, Valid: 85.47%, Test: 85.52%
Run 01:
Highest Train: 88.06
Highest Valid: 88.13
  Final Train: 88.06
   Final Test: 88.10
All runs:
Highest Train: 88.06, nan
Highest Valid: 88.13, nan
  Final Train: 88.06, nan
   Final Test: 88.10, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 18.3689, Train: 14.88%, Valid: 15.06%, Test: 14.75%
Epoch: 25, Loss: 0.5042, Train: 85.89%, Valid: 85.77%, Test: 85.97%
Epoch: 50, Loss: 0.4917, Train: 85.47%, Valid: 85.31%, Test: 85.52%
Epoch: 75, Loss: 0.4843, Train: 85.36%, Valid: 85.20%, Test: 85.42%
Epoch: 100, Loss: 0.4728, Train: 85.06%, Valid: 84.92%, Test: 85.14%
Epoch: 125, Loss: 0.4843, Train: 85.22%, Valid: 85.05%, Test: 85.29%
Epoch: 150, Loss: 0.4754, Train: 85.21%, Valid: 85.04%, Test: 85.28%
Epoch: 175, Loss: 0.4662, Train: 85.20%, Valid: 85.02%, Test: 85.26%
Run 01:
Highest Train: 86.01
Highest Valid: 85.91
  Final Train: 86.01
   Final Test: 86.06
All runs:
Highest Train: 86.01, nan
Highest Valid: 85.91, nan
  Final Train: 86.01, nan
   Final Test: 86.06, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 175.0148, Train: 42.67%, Valid: 42.80%, Test: 42.96%
Epoch: 25, Loss: 6268413.0000, Train: 57.45%, Valid: 57.45%, Test: 57.46%
Epoch: 50, Loss: 2213134592.0000, Train: 54.66%, Valid: 54.63%, Test: 54.82%
Epoch: 75, Loss: 2236347904.0000, Train: 51.72%, Valid: 51.82%, Test: 51.63%
Epoch: 100, Loss: 26842.4570, Train: 52.66%, Valid: 52.65%, Test: 52.61%
Epoch: 125, Loss: 291433.1250, Train: 37.76%, Valid: 37.64%, Test: 37.77%
Epoch: 150, Loss: 90265.1172, Train: 37.67%, Valid: 37.57%, Test: 37.68%
Epoch: 175, Loss: 1415412.3750, Train: 58.85%, Valid: 58.97%, Test: 58.95%
Run 01:
Highest Train: 63.80
Highest Valid: 63.65
  Final Train: 63.80
   Final Test: 63.55
All runs:
Highest Train: 63.80, nan
Highest Valid: 63.65, nan
  Final Train: 63.80, nan
   Final Test: 63.55, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 10.9737, Train: 13.01%, Valid: 13.11%, Test: 12.99%
Epoch: 25, Loss: 0.4934, Train: 86.33%, Valid: 86.36%, Test: 86.42%
Epoch: 50, Loss: 0.4747, Train: 86.13%, Valid: 86.21%, Test: 86.25%
Epoch: 75, Loss: 0.4472, Train: 86.33%, Valid: 86.37%, Test: 86.38%
Epoch: 100, Loss: 0.4234, Train: 86.03%, Valid: 86.10%, Test: 86.10%
Epoch: 125, Loss: 0.3904, Train: 85.69%, Valid: 85.79%, Test: 85.78%
Epoch: 150, Loss: 0.3752, Train: 85.85%, Valid: 85.86%, Test: 85.95%
Epoch: 175, Loss: 0.3710, Train: 85.82%, Valid: 85.82%, Test: 85.92%
Run 01:
Highest Train: 87.25
Highest Valid: 87.29
  Final Train: 87.25
   Final Test: 87.25
All runs:
Highest Train: 87.25, nan
Highest Valid: 87.29, nan
  Final Train: 87.25, nan
   Final Test: 87.25, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 9.2866, Train: 14.38%, Valid: 14.47%, Test: 14.36%
Epoch: 25, Loss: 0.4639, Train: 84.56%, Valid: 84.53%, Test: 84.70%
Epoch: 50, Loss: 0.4433, Train: 84.44%, Valid: 84.47%, Test: 84.60%
Epoch: 75, Loss: 0.4029, Train: 85.46%, Valid: 85.41%, Test: 85.57%
Epoch: 100, Loss: 0.3802, Train: 85.38%, Valid: 85.32%, Test: 85.47%
Epoch: 125, Loss: 0.3761, Train: 85.48%, Valid: 85.47%, Test: 85.58%
Epoch: 150, Loss: 0.3738, Train: 86.10%, Valid: 86.08%, Test: 86.15%
Epoch: 175, Loss: 0.3684, Train: 85.58%, Valid: 85.54%, Test: 85.64%
Run 01:
Highest Train: 86.53
Highest Valid: 86.42
  Final Train: 86.53
   Final Test: 86.59
All runs:
Highest Train: 86.53, nan
Highest Valid: 86.42, nan
  Final Train: 86.53, nan
   Final Test: 86.59, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 56.2476, Train: 45.87%, Valid: 46.08%, Test: 46.17%
Epoch: 25, Loss: 8.2297, Train: 79.96%, Valid: 79.95%, Test: 80.14%
Epoch: 50, Loss: 84.6865, Train: 73.51%, Valid: 73.29%, Test: 73.36%
Epoch: 75, Loss: 8.3359, Train: 53.74%, Valid: 54.04%, Test: 54.17%
Epoch: 100, Loss: 9.0674, Train: 81.80%, Valid: 81.69%, Test: 81.77%
Epoch: 125, Loss: 10.1835, Train: 80.84%, Valid: 80.71%, Test: 80.75%
Epoch: 150, Loss: 7.8836, Train: 81.35%, Valid: 81.21%, Test: 81.28%
Epoch: 175, Loss: 7.4698, Train: 81.21%, Valid: 81.07%, Test: 81.13%
Run 01:
Highest Train: 83.33
Highest Valid: 83.36
  Final Train: 83.33
   Final Test: 83.55
All runs:
Highest Train: 83.33, nan
Highest Valid: 83.36, nan
  Final Train: 83.33, nan
   Final Test: 83.55, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 7.6160, Train: 76.67%, Valid: 76.96%, Test: 77.00%
Epoch: 25, Loss: 0.5625, Train: 86.06%, Valid: 85.99%, Test: 86.15%
Epoch: 50, Loss: 0.5121, Train: 86.22%, Valid: 86.15%, Test: 86.31%
Epoch: 75, Loss: 0.4521, Train: 87.05%, Valid: 86.90%, Test: 87.05%
Epoch: 100, Loss: 0.4238, Train: 85.93%, Valid: 85.82%, Test: 85.94%
Epoch: 125, Loss: 0.4020, Train: 86.06%, Valid: 85.93%, Test: 86.05%
Epoch: 150, Loss: 0.3912, Train: 85.87%, Valid: 85.75%, Test: 85.90%
Epoch: 175, Loss: 0.3882, Train: 85.84%, Valid: 85.71%, Test: 85.87%
Run 01:
Highest Train: 87.10
Highest Valid: 86.93
  Final Train: 87.09
   Final Test: 87.12
All runs:
Highest Train: 87.10, nan
Highest Valid: 86.93, nan
  Final Train: 87.09, nan
   Final Test: 87.12, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 4.3504, Train: 82.24%, Valid: 82.38%, Test: 82.45%
Epoch: 25, Loss: 0.5183, Train: 85.51%, Valid: 85.30%, Test: 85.58%
Epoch: 50, Loss: 0.4767, Train: 85.13%, Valid: 84.97%, Test: 85.22%
Epoch: 75, Loss: 0.4307, Train: 85.29%, Valid: 85.11%, Test: 85.37%
Epoch: 100, Loss: 0.4102, Train: 85.74%, Valid: 85.62%, Test: 85.79%
Epoch: 125, Loss: 0.3923, Train: 85.76%, Valid: 85.64%, Test: 85.82%
Epoch: 150, Loss: 0.3818, Train: 87.12%, Valid: 87.09%, Test: 87.23%
Epoch: 175, Loss: 0.3801, Train: 86.73%, Valid: 86.71%, Test: 86.76%
Run 01:
Highest Train: 87.52
Highest Valid: 87.41
  Final Train: 87.51
   Final Test: 87.48
All runs:
Highest Train: 87.52, nan
Highest Valid: 87.41, nan
  Final Train: 87.51, nan
   Final Test: 87.48, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 73.4395, Train: 47.86%, Valid: 47.82%, Test: 47.86%
Epoch: 25, Loss: 36.8050, Train: 45.57%, Valid: 45.55%, Test: 45.51%
Epoch: 50, Loss: 19.1464, Train: 70.90%, Valid: 70.93%, Test: 70.98%
Epoch: 75, Loss: 81.6451, Train: 45.61%, Valid: 45.71%, Test: 45.90%
Epoch: 100, Loss: 87.0113, Train: 81.39%, Valid: 81.26%, Test: 81.37%
Epoch: 125, Loss: 13.3903, Train: 79.86%, Valid: 79.70%, Test: 79.80%
Epoch: 150, Loss: 6.4491, Train: 80.32%, Valid: 80.18%, Test: 80.27%
Epoch: 175, Loss: 148.4806, Train: 80.39%, Valid: 80.24%, Test: 80.31%
Run 01:
Highest Train: 82.87
Highest Valid: 82.78
  Final Train: 82.87
   Final Test: 82.91
All runs:
Highest Train: 82.87, nan
Highest Valid: 82.78, nan
  Final Train: 82.87, nan
   Final Test: 82.91, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 19.0337, Train: 13.24%, Valid: 13.31%, Test: 13.25%
Epoch: 25, Loss: 0.5429, Train: 85.50%, Valid: 85.52%, Test: 85.68%
Epoch: 50, Loss: 0.5563, Train: 85.34%, Valid: 85.31%, Test: 85.50%
Epoch: 75, Loss: 0.5064, Train: 86.13%, Valid: 86.13%, Test: 86.25%
Epoch: 100, Loss: 0.4795, Train: 86.65%, Valid: 86.69%, Test: 86.71%
Epoch: 125, Loss: 0.4579, Train: 86.60%, Valid: 86.66%, Test: 86.67%
Epoch: 150, Loss: 0.4393, Train: 86.34%, Valid: 86.42%, Test: 86.44%
Epoch: 175, Loss: 0.4159, Train: 86.98%, Valid: 87.00%, Test: 87.06%
Run 01:
Highest Train: 87.07
Highest Valid: 87.06
  Final Train: 87.07
   Final Test: 87.13
All runs:
Highest Train: 87.07, nan
Highest Valid: 87.06, nan
  Final Train: 87.07, nan
   Final Test: 87.13, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 6.0721, Train: 76.13%, Valid: 76.25%, Test: 76.56%
Epoch: 25, Loss: 0.5607, Train: 85.11%, Valid: 84.98%, Test: 85.15%
Epoch: 50, Loss: 0.5208, Train: 86.02%, Valid: 85.91%, Test: 86.06%
Epoch: 75, Loss: 0.4572, Train: 86.20%, Valid: 86.04%, Test: 86.21%
Epoch: 100, Loss: 0.4257, Train: 86.14%, Valid: 86.01%, Test: 86.20%
Epoch: 125, Loss: 0.4180, Train: 86.02%, Valid: 85.87%, Test: 86.09%
Epoch: 150, Loss: 0.3954, Train: 86.01%, Valid: 85.85%, Test: 86.07%
Epoch: 175, Loss: 0.3869, Train: 85.94%, Valid: 85.77%, Test: 85.99%
Run 01:
Highest Train: 86.24
Highest Valid: 86.10
  Final Train: 86.24
   Final Test: 86.25
All runs:
Highest Train: 86.24, nan
Highest Valid: 86.10, nan
  Final Train: 86.24, nan
   Final Test: 86.25, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 8.5835, Train: 73.22%, Valid: 73.23%, Test: 73.57%
Epoch: 25, Loss: 109.9220, Train: 44.37%, Valid: 44.53%, Test: 44.68%
Epoch: 50, Loss: 11.4539, Train: 46.57%, Valid: 46.83%, Test: 46.92%
Epoch: 75, Loss: 43.5779, Train: 56.09%, Valid: 56.34%, Test: 56.63%
Epoch: 100, Loss: 6.4496, Train: 80.65%, Valid: 80.48%, Test: 80.59%
Epoch: 125, Loss: 7.0608, Train: 68.08%, Valid: 68.14%, Test: 68.33%
Epoch: 150, Loss: 6.7711, Train: 67.43%, Valid: 67.46%, Test: 67.79%
Epoch: 175, Loss: 6.5643, Train: 73.05%, Valid: 72.99%, Test: 73.31%
Run 01:
Highest Train: 83.06
Highest Valid: 82.98
  Final Train: 83.06
   Final Test: 83.10
All runs:
Highest Train: 83.06, nan
Highest Valid: 82.98, nan
  Final Train: 83.06, nan
   Final Test: 83.10, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 7.5467, Train: 84.20%, Valid: 84.04%, Test: 84.26%
Epoch: 25, Loss: 0.4416, Train: 85.80%, Valid: 85.85%, Test: 85.91%
Epoch: 50, Loss: 0.4134, Train: 86.09%, Valid: 86.10%, Test: 86.15%
Epoch: 75, Loss: 0.3629, Train: 85.86%, Valid: 85.87%, Test: 85.96%
Epoch: 100, Loss: 0.3588, Train: 85.90%, Valid: 85.85%, Test: 85.97%
Epoch: 125, Loss: 0.3518, Train: 85.81%, Valid: 85.81%, Test: 85.92%
Epoch: 150, Loss: 0.3452, Train: 85.85%, Valid: 85.81%, Test: 85.93%
Epoch: 175, Loss: 0.3370, Train: 85.73%, Valid: 85.73%, Test: 85.84%
Run 01:
Highest Train: 86.94
Highest Valid: 86.94
  Final Train: 86.94
   Final Test: 87.06
All runs:
Highest Train: 86.94, nan
Highest Valid: 86.94, nan
  Final Train: 86.94, nan
   Final Test: 87.06, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 14.1500, Train: 14.96%, Valid: 15.05%, Test: 14.95%
Epoch: 25, Loss: 0.7606, Train: 86.24%, Valid: 86.21%, Test: 86.38%
Epoch: 50, Loss: 0.6305, Train: 85.29%, Valid: 85.09%, Test: 85.37%
Epoch: 75, Loss: 0.5352, Train: 85.27%, Valid: 85.07%, Test: 85.36%
Epoch: 100, Loss: 0.4957, Train: 85.28%, Valid: 85.11%, Test: 85.39%
Epoch: 125, Loss: 0.4687, Train: 85.26%, Valid: 85.08%, Test: 85.36%
Epoch: 150, Loss: 0.4405, Train: 84.80%, Valid: 84.76%, Test: 84.89%
Epoch: 175, Loss: 0.4286, Train: 85.12%, Valid: 84.94%, Test: 85.23%
Run 01:
Highest Train: 86.64
Highest Valid: 86.57
  Final Train: 86.64
   Final Test: 86.70
All runs:
Highest Train: 86.64, nan
Highest Valid: 86.57, nan
  Final Train: 86.64, nan
   Final Test: 86.70, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 2334788782391296.0000, Train: 51.69%, Valid: 51.45%, Test: 51.38%
Epoch: 25, Loss: 139331960832.0000, Train: 53.71%, Valid: 53.57%, Test: 53.66%
Epoch: 50, Loss: 98227840.0000, Train: 44.44%, Valid: 44.48%, Test: 44.50%
Epoch: 75, Loss: 151835680.0000, Train: 44.48%, Valid: 44.49%, Test: 44.45%
Epoch: 100, Loss: 7288957435904.0000, Train: 55.68%, Valid: 55.67%, Test: 55.73%
Epoch: 125, Loss: 7928378368.0000, Train: 44.29%, Valid: 44.31%, Test: 44.24%
Epoch: 150, Loss: 14795188.0000, Train: 44.30%, Valid: 44.31%, Test: 44.25%
Epoch: 175, Loss: 6181420544.0000, Train: 55.69%, Valid: 55.67%, Test: 55.73%
Run 01:
Highest Train: 61.81
Highest Valid: 61.69
  Final Train: 61.81
   Final Test: 61.59
All runs:
Highest Train: 61.81, nan
Highest Valid: 61.69, nan
  Final Train: 61.81, nan
   Final Test: 61.59, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 4.3874, Train: 87.91%, Valid: 87.92%, Test: 87.92%
Epoch: 25, Loss: 0.4360, Train: 85.72%, Valid: 85.61%, Test: 85.75%
Epoch: 50, Loss: 0.4214, Train: 85.82%, Valid: 85.67%, Test: 85.85%
Epoch: 75, Loss: 0.3927, Train: 85.98%, Valid: 85.85%, Test: 86.03%
Epoch: 100, Loss: 0.3746, Train: 85.98%, Valid: 86.08%, Test: 86.08%
Epoch: 125, Loss: 0.3703, Train: 85.85%, Valid: 85.88%, Test: 85.92%
Epoch: 150, Loss: 0.3657, Train: 85.89%, Valid: 85.93%, Test: 85.97%
Epoch: 175, Loss: 0.3626, Train: 85.90%, Valid: 85.89%, Test: 85.94%
Run 01:
Highest Train: 88.01
Highest Valid: 88.04
  Final Train: 88.01
   Final Test: 88.01
All runs:
Highest Train: 88.01, nan
Highest Valid: 88.04, nan
  Final Train: 88.01, nan
   Final Test: 88.01, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 20.9259, Train: 14.62%, Valid: 14.70%, Test: 14.54%
Epoch: 25, Loss: 0.4773, Train: 85.43%, Valid: 85.27%, Test: 85.49%
Epoch: 50, Loss: 0.4808, Train: 85.45%, Valid: 85.27%, Test: 85.52%
Epoch: 75, Loss: 0.4636, Train: 85.52%, Valid: 85.35%, Test: 85.59%
Epoch: 100, Loss: 0.4512, Train: 85.54%, Valid: 85.38%, Test: 85.60%
Epoch: 125, Loss: 0.4299, Train: 85.51%, Valid: 85.34%, Test: 85.56%
Epoch: 150, Loss: 0.4148, Train: 85.49%, Valid: 85.32%, Test: 85.55%
Epoch: 175, Loss: 0.4034, Train: 85.50%, Valid: 85.31%, Test: 85.55%
Run 01:
Highest Train: 85.83
Highest Valid: 85.72
  Final Train: 85.83
   Final Test: 85.85
All runs:
Highest Train: 85.83, nan
Highest Valid: 85.72, nan
  Final Train: 85.83, nan
   Final Test: 85.85, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 44.9505, Train: 50.01%, Valid: 50.00%, Test: 50.00%
Epoch: 25, Loss: 9.3673, Train: 49.98%, Valid: 49.98%, Test: 49.97%
Epoch: 50, Loss: 7.4863, Train: 49.97%, Valid: 49.98%, Test: 49.98%
Epoch: 75, Loss: 7.4267, Train: 49.99%, Valid: 50.00%, Test: 49.99%
Epoch: 100, Loss: 10.4378, Train: 73.20%, Valid: 73.02%, Test: 72.99%
Epoch: 125, Loss: 7.2908, Train: 44.19%, Valid: 44.27%, Test: 44.46%
Epoch: 150, Loss: 8.1037, Train: 75.11%, Valid: 75.04%, Test: 75.24%
Epoch: 175, Loss: 6.8360, Train: 43.38%, Valid: 43.59%, Test: 43.69%
Run 01:
Highest Train: 82.88
Highest Valid: 82.76
  Final Train: 82.88
   Final Test: 82.87
All runs:
Highest Train: 82.88, nan
Highest Valid: 82.76, nan
  Final Train: 82.88, nan
   Final Test: 82.87, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 3.7002, Train: 84.88%, Valid: 84.93%, Test: 84.91%
Epoch: 25, Loss: 0.4531, Train: 85.46%, Valid: 85.27%, Test: 85.60%
Epoch: 50, Loss: 0.4326, Train: 85.55%, Valid: 85.37%, Test: 85.69%
Epoch: 75, Loss: 0.4071, Train: 86.06%, Valid: 85.97%, Test: 86.16%
Epoch: 100, Loss: 0.3928, Train: 85.62%, Valid: 85.42%, Test: 85.72%
Epoch: 125, Loss: 0.3851, Train: 85.32%, Valid: 85.14%, Test: 85.42%
Epoch: 150, Loss: 0.3742, Train: 84.56%, Valid: 84.40%, Test: 84.72%
Epoch: 175, Loss: 0.3736, Train: 87.04%, Valid: 87.12%, Test: 87.07%
Run 01:
Highest Train: 87.59
Highest Valid: 87.60
  Final Train: 87.59
   Final Test: 87.56
All runs:
Highest Train: 87.59, nan
Highest Valid: 87.60, nan
  Final Train: 87.59, nan
   Final Test: 87.56, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 7.1727, Train: 77.17%, Valid: 77.42%, Test: 77.58%
Epoch: 25, Loss: 0.4877, Train: 86.10%, Valid: 85.96%, Test: 86.17%
Epoch: 50, Loss: 0.4705, Train: 86.04%, Valid: 85.90%, Test: 86.11%
Epoch: 75, Loss: 0.4368, Train: 85.96%, Valid: 85.82%, Test: 85.98%
Epoch: 100, Loss: 0.4056, Train: 85.86%, Valid: 85.69%, Test: 85.93%
Epoch: 125, Loss: 0.3914, Train: 85.91%, Valid: 85.75%, Test: 85.99%
Epoch: 150, Loss: 0.3831, Train: 85.79%, Valid: 85.66%, Test: 85.85%
Epoch: 175, Loss: 0.3768, Train: 86.19%, Valid: 86.06%, Test: 86.25%
Run 01:
Highest Train: 86.95
Highest Valid: 86.88
  Final Train: 86.95
   Final Test: 87.03
All runs:
Highest Train: 86.95, nan
Highest Valid: 86.88, nan
  Final Train: 86.95, nan
   Final Test: 87.03, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 37.8872, Train: 52.37%, Valid: 52.56%, Test: 52.31%
Epoch: 25, Loss: 61.6580, Train: 52.45%, Valid: 52.59%, Test: 52.47%
Epoch: 50, Loss: 7.5575, Train: 83.08%, Valid: 83.07%, Test: 83.13%
Epoch: 75, Loss: 8.5398, Train: 49.42%, Valid: 49.44%, Test: 49.40%
Epoch: 100, Loss: 60.9682, Train: 47.27%, Valid: 47.14%, Test: 47.28%
Epoch: 125, Loss: 7.6959, Train: 53.69%, Valid: 53.76%, Test: 53.54%
Epoch: 150, Loss: 17.7485, Train: 70.68%, Valid: 70.78%, Test: 71.12%
Epoch: 175, Loss: 7.5476, Train: 54.20%, Valid: 54.23%, Test: 54.04%
Run 01:
Highest Train: 83.16
Highest Valid: 83.08
  Final Train: 83.16
   Final Test: 83.18
All runs:
Highest Train: 83.16, nan
Highest Valid: 83.08, nan
  Final Train: 83.16, nan
   Final Test: 83.18, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 4.2746, Train: 84.72%, Valid: 84.57%, Test: 84.69%
Epoch: 25, Loss: 0.5533, Train: 84.66%, Valid: 84.45%, Test: 84.77%
Epoch: 50, Loss: 0.4802, Train: 85.58%, Valid: 85.65%, Test: 85.63%
Epoch: 75, Loss: 0.4380, Train: 85.93%, Valid: 86.00%, Test: 85.97%
Epoch: 100, Loss: 0.4122, Train: 85.68%, Valid: 85.74%, Test: 85.75%
Epoch: 125, Loss: 0.3929, Train: 85.79%, Valid: 85.88%, Test: 85.87%
Epoch: 150, Loss: 0.3884, Train: 85.78%, Valid: 85.84%, Test: 85.84%
Epoch: 175, Loss: 0.3863, Train: 85.78%, Valid: 85.82%, Test: 85.85%
Run 01:
Highest Train: 86.58
Highest Valid: 86.69
  Final Train: 86.58
   Final Test: 86.65
All runs:
Highest Train: 86.58, nan
Highest Valid: 86.69, nan
  Final Train: 86.58, nan
   Final Test: 86.65, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 12.9374, Train: 21.25%, Valid: 21.07%, Test: 20.87%
Epoch: 25, Loss: 0.5478, Train: 85.88%, Valid: 85.71%, Test: 85.95%
Epoch: 50, Loss: 0.5263, Train: 84.92%, Valid: 84.85%, Test: 85.01%
Epoch: 75, Loss: 0.4763, Train: 84.88%, Valid: 84.84%, Test: 84.93%
Epoch: 100, Loss: 0.4565, Train: 85.09%, Valid: 85.06%, Test: 85.16%
Epoch: 125, Loss: 0.4273, Train: 85.25%, Valid: 85.22%, Test: 85.31%
Epoch: 150, Loss: 0.4149, Train: 85.37%, Valid: 85.34%, Test: 85.43%
Epoch: 175, Loss: 0.4093, Train: 85.45%, Valid: 85.40%, Test: 85.51%
Run 01:
Highest Train: 85.89
Highest Valid: 85.71
  Final Train: 85.88
   Final Test: 85.95
All runs:
Highest Train: 85.89, nan
Highest Valid: 85.71, nan
  Final Train: 85.88, nan
   Final Test: 85.95, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 12.9672, Train: 49.99%, Valid: 49.99%, Test: 50.01%
Epoch: 25, Loss: 8.6012, Train: 49.99%, Valid: 49.98%, Test: 49.99%
Epoch: 50, Loss: 8.5855, Train: 49.98%, Valid: 49.97%, Test: 49.96%
Epoch: 75, Loss: 18.7963, Train: 50.01%, Valid: 50.01%, Test: 50.01%
Epoch: 100, Loss: 26.4067, Train: 49.96%, Valid: 49.94%, Test: 49.95%
Epoch: 125, Loss: 7.5223, Train: 50.00%, Valid: 49.99%, Test: 50.01%
Epoch: 150, Loss: 8.4270, Train: 49.99%, Valid: 49.98%, Test: 49.98%
Epoch: 175, Loss: 6.9028, Train: 49.70%, Valid: 49.72%, Test: 49.69%
Run 01:
Highest Train: 73.89
Highest Valid: 73.67
  Final Train: 73.89
   Final Test: 73.69
All runs:
Highest Train: 73.89, nan
Highest Valid: 73.67, nan
  Final Train: 73.89, nan
   Final Test: 73.69, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 7.6355, Train: 84.81%, Valid: 84.81%, Test: 84.95%
Epoch: 25, Loss: 0.4886, Train: 87.26%, Valid: 87.13%, Test: 87.25%
Epoch: 50, Loss: 0.3632, Train: 87.28%, Valid: 87.38%, Test: 87.36%
Epoch: 75, Loss: 0.3582, Train: 85.45%, Valid: 85.46%, Test: 85.57%
Epoch: 100, Loss: 0.3519, Train: 85.51%, Valid: 85.49%, Test: 85.59%
Epoch: 125, Loss: 0.3442, Train: 85.71%, Valid: 85.66%, Test: 85.77%
Epoch: 150, Loss: 0.3354, Train: 85.19%, Valid: 85.19%, Test: 85.28%
Epoch: 175, Loss: 0.3398, Train: 85.49%, Valid: 85.49%, Test: 85.61%
Run 01:
Highest Train: 88.04
Highest Valid: 88.11
  Final Train: 88.04
   Final Test: 88.06
All runs:
Highest Train: 88.04, nan
Highest Valid: 88.11, nan
  Final Train: 88.04, nan
   Final Test: 88.06, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 7.2991, Train: 84.46%, Valid: 84.30%, Test: 84.52%
Epoch: 25, Loss: 0.6389, Train: 85.50%, Valid: 85.33%, Test: 85.56%
Epoch: 50, Loss: 0.5590, Train: 86.48%, Valid: 86.49%, Test: 86.66%
Epoch: 75, Loss: 0.4501, Train: 86.12%, Valid: 86.04%, Test: 86.28%
Epoch: 100, Loss: 0.3672, Train: 85.57%, Valid: 85.43%, Test: 85.62%
Epoch: 125, Loss: 0.3618, Train: 85.56%, Valid: 85.41%, Test: 85.62%
Epoch: 150, Loss: 0.3579, Train: 85.57%, Valid: 85.42%, Test: 85.63%
Epoch: 175, Loss: 0.3530, Train: 85.59%, Valid: 85.43%, Test: 85.65%
Run 01:
Highest Train: 86.87
Highest Valid: 86.83
  Final Train: 86.87
   Final Test: 87.01
All runs:
Highest Train: 86.87, nan
Highest Valid: 86.83, nan
  Final Train: 86.87, nan
   Final Test: 87.01, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1451.0859, Train: 60.65%, Valid: 60.51%, Test: 60.34%
Epoch: 25, Loss: 60.6529, Train: 77.11%, Valid: 76.95%, Test: 76.95%
Epoch: 50, Loss: 17.1584, Train: 79.30%, Valid: 79.25%, Test: 79.15%
Epoch: 75, Loss: 12.4799, Train: 81.09%, Valid: 81.08%, Test: 80.98%
Epoch: 100, Loss: 9.4491, Train: 81.76%, Valid: 81.65%, Test: 81.66%
Epoch: 125, Loss: 13.3995, Train: 81.75%, Valid: 81.67%, Test: 81.66%
Epoch: 150, Loss: 8.8746, Train: 82.31%, Valid: 82.25%, Test: 82.25%
Epoch: 175, Loss: 7.1717, Train: 82.95%, Valid: 82.89%, Test: 82.95%
Run 01:
Highest Train: 83.20
Highest Valid: 83.15
  Final Train: 83.20
   Final Test: 83.20
All runs:
Highest Train: 83.20, nan
Highest Valid: 83.15, nan
  Final Train: 83.20, nan
   Final Test: 83.20, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 6.5263, Train: 84.67%, Valid: 84.68%, Test: 84.73%
Epoch: 25, Loss: 0.6907, Train: 85.76%, Valid: 85.63%, Test: 85.81%
Epoch: 50, Loss: 0.5852, Train: 85.83%, Valid: 85.71%, Test: 85.88%
Epoch: 75, Loss: 0.4664, Train: 85.80%, Valid: 85.67%, Test: 85.84%
Epoch: 100, Loss: 0.3982, Train: 86.35%, Valid: 86.42%, Test: 86.45%
Epoch: 125, Loss: 0.3892, Train: 86.11%, Valid: 86.15%, Test: 86.25%
Epoch: 150, Loss: 0.3760, Train: 85.77%, Valid: 85.65%, Test: 85.89%
Epoch: 175, Loss: 0.3731, Train: 86.11%, Valid: 85.92%, Test: 86.19%
Run 01:
Highest Train: 87.47
Highest Valid: 87.34
  Final Train: 87.47
   Final Test: 87.49
All runs:
Highest Train: 87.47, nan
Highest Valid: 87.34, nan
  Final Train: 87.47, nan
   Final Test: 87.49, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 4.0057, Train: 84.81%, Valid: 84.64%, Test: 84.91%
Epoch: 25, Loss: 0.5619, Train: 86.33%, Valid: 86.31%, Test: 86.35%
Epoch: 50, Loss: 0.4044, Train: 86.75%, Valid: 86.77%, Test: 86.86%
Epoch: 75, Loss: 0.3958, Train: 86.40%, Valid: 86.38%, Test: 86.50%
Epoch: 100, Loss: 0.3975, Train: 86.31%, Valid: 86.24%, Test: 86.40%
Epoch: 125, Loss: 0.3746, Train: 86.38%, Valid: 86.30%, Test: 86.44%
Epoch: 150, Loss: 0.3724, Train: 85.52%, Valid: 85.38%, Test: 85.60%
Epoch: 175, Loss: 0.3668, Train: 86.87%, Valid: 86.67%, Test: 86.88%
Run 01:
Highest Train: 87.01
Highest Valid: 86.89
  Final Train: 86.90
   Final Test: 86.98
All runs:
Highest Train: 87.01, nan
Highest Valid: 86.89, nan
  Final Train: 86.90, nan
   Final Test: 86.98, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 18.8496, Train: 74.98%, Valid: 74.74%, Test: 74.83%
Epoch: 25, Loss: 10.0223, Train: 53.10%, Valid: 53.41%, Test: 53.53%
Epoch: 50, Loss: 6.8637, Train: 51.35%, Valid: 51.53%, Test: 51.69%
Epoch: 75, Loss: 8.5390, Train: 79.77%, Valid: 79.64%, Test: 79.66%
Epoch: 100, Loss: 5.9347, Train: 78.99%, Valid: 78.84%, Test: 78.89%
Epoch: 125, Loss: 5.1621, Train: 70.09%, Valid: 70.17%, Test: 70.02%
Epoch: 150, Loss: 4.1172, Train: 74.05%, Valid: 74.13%, Test: 74.35%
Epoch: 175, Loss: 3.7626, Train: 64.50%, Valid: 64.62%, Test: 64.78%
Run 01:
Highest Train: 87.35
Highest Valid: 87.17
  Final Train: 87.35
   Final Test: 87.35
All runs:
Highest Train: 87.35, nan
Highest Valid: 87.17, nan
  Final Train: 87.35, nan
   Final Test: 87.35, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 9.5290, Train: 83.42%, Valid: 83.43%, Test: 83.50%
Epoch: 25, Loss: 0.6552, Train: 87.23%, Valid: 87.11%, Test: 87.21%
Epoch: 50, Loss: 0.5496, Train: 86.37%, Valid: 86.28%, Test: 86.36%
Epoch: 75, Loss: 0.4534, Train: 85.72%, Valid: 85.60%, Test: 85.78%
Epoch: 100, Loss: 0.4104, Train: 86.77%, Valid: 86.76%, Test: 86.86%
Epoch: 125, Loss: 0.3892, Train: 85.03%, Valid: 84.99%, Test: 85.13%
Epoch: 150, Loss: 0.3837, Train: 86.34%, Valid: 86.38%, Test: 86.44%
Epoch: 175, Loss: 0.3735, Train: 86.56%, Valid: 86.48%, Test: 86.58%
Run 01:
Highest Train: 87.36
Highest Valid: 87.23
  Final Train: 87.35
   Final Test: 87.32
All runs:
Highest Train: 87.36, nan
Highest Valid: 87.23, nan
  Final Train: 87.35, nan
   Final Test: 87.32, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 8.2194, Train: 84.84%, Valid: 84.77%, Test: 84.90%
Epoch: 25, Loss: 0.6763, Train: 85.63%, Valid: 85.59%, Test: 85.72%
Epoch: 50, Loss: 0.5623, Train: 86.49%, Valid: 86.46%, Test: 86.50%
Epoch: 75, Loss: 0.4752, Train: 86.27%, Valid: 86.26%, Test: 86.35%
Epoch: 100, Loss: 0.3978, Train: 85.35%, Valid: 85.21%, Test: 85.44%
Epoch: 125, Loss: 0.3848, Train: 85.66%, Valid: 85.58%, Test: 85.68%
Epoch: 150, Loss: 0.3735, Train: 85.96%, Valid: 85.93%, Test: 86.05%
Epoch: 175, Loss: 0.3718, Train: 86.02%, Valid: 85.93%, Test: 86.08%
Run 01:
Highest Train: 87.30
Highest Valid: 87.16
  Final Train: 87.30
   Final Test: 87.29
All runs:
Highest Train: 87.30, nan
Highest Valid: 87.16, nan
  Final Train: 87.30, nan
   Final Test: 87.29, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 377.5171, Train: 42.93%, Valid: 43.05%, Test: 43.10%
Epoch: 25, Loss: 60.3299, Train: 78.15%, Valid: 77.97%, Test: 78.01%
Epoch: 50, Loss: 4.9697, Train: 83.02%, Valid: 82.95%, Test: 83.04%
Epoch: 75, Loss: 4.5603, Train: 71.37%, Valid: 71.51%, Test: 71.78%
Epoch: 100, Loss: 4.5332, Train: 66.32%, Valid: 66.46%, Test: 66.69%
Epoch: 125, Loss: 3.9910, Train: 67.99%, Valid: 68.17%, Test: 68.36%
Epoch: 150, Loss: 4.2495, Train: 72.24%, Valid: 72.39%, Test: 72.66%
Epoch: 175, Loss: 3.9257, Train: 83.85%, Valid: 83.78%, Test: 83.90%
Run 01:
Highest Train: 84.99
Highest Valid: 84.89
  Final Train: 84.99
   Final Test: 85.09
All runs:
Highest Train: 84.99, nan
Highest Valid: 84.89, nan
  Final Train: 84.99, nan
   Final Test: 85.09, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 9.4766, Train: 87.05%, Valid: 87.10%, Test: 87.02%
Epoch: 25, Loss: 0.6896, Train: 85.80%, Valid: 85.68%, Test: 85.86%
Epoch: 50, Loss: 0.5910, Train: 85.83%, Valid: 85.69%, Test: 85.90%
Epoch: 75, Loss: 0.4914, Train: 86.62%, Valid: 86.73%, Test: 86.69%
Epoch: 100, Loss: 0.4147, Train: 86.51%, Valid: 86.58%, Test: 86.61%
Epoch: 125, Loss: 0.3950, Train: 86.08%, Valid: 86.12%, Test: 86.16%
Epoch: 150, Loss: 0.3880, Train: 85.90%, Valid: 85.93%, Test: 86.01%
Epoch: 175, Loss: 0.3787, Train: 85.62%, Valid: 85.66%, Test: 85.74%
Run 01:
Highest Train: 87.11
Highest Valid: 87.11
  Final Train: 87.11
   Final Test: 87.13
All runs:
Highest Train: 87.11, nan
Highest Valid: 87.11, nan
  Final Train: 87.11, nan
   Final Test: 87.13, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.8446, Train: 86.11%, Valid: 85.97%, Test: 86.15%
Epoch: 25, Loss: 0.5626, Train: 85.90%, Valid: 85.78%, Test: 85.92%
Epoch: 50, Loss: 0.4473, Train: 85.45%, Valid: 85.29%, Test: 85.47%
Epoch: 75, Loss: 0.3911, Train: 86.88%, Valid: 86.72%, Test: 86.87%
Epoch: 100, Loss: 0.3814, Train: 86.44%, Valid: 86.28%, Test: 86.42%
Epoch: 125, Loss: 0.3895, Train: 87.06%, Valid: 86.86%, Test: 87.05%
Epoch: 150, Loss: 0.3681, Train: 87.05%, Valid: 86.87%, Test: 87.07%
Epoch: 175, Loss: 0.3639, Train: 86.52%, Valid: 86.35%, Test: 86.56%
Run 01:
Highest Train: 87.08
Highest Valid: 86.90
  Final Train: 87.06
   Final Test: 87.01
All runs:
Highest Train: 87.08, nan
Highest Valid: 86.90, nan
  Final Train: 87.06, nan
   Final Test: 87.01, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 441.0640, Train: 41.73%, Valid: 41.93%, Test: 42.05%
Epoch: 25, Loss: 17.3310, Train: 84.19%, Valid: 84.08%, Test: 84.23%
Epoch: 50, Loss: 63.7683, Train: 76.20%, Valid: 76.32%, Test: 76.54%
Epoch: 75, Loss: 3.3757, Train: 83.96%, Valid: 83.86%, Test: 84.00%
Epoch: 100, Loss: 4.5376, Train: 81.62%, Valid: 81.50%, Test: 81.56%
Epoch: 125, Loss: 43.8366, Train: 81.55%, Valid: 81.42%, Test: 81.48%
Epoch: 150, Loss: 3.8301, Train: 81.29%, Valid: 81.14%, Test: 81.20%
Epoch: 175, Loss: 3.2557, Train: 81.86%, Valid: 81.74%, Test: 81.81%
Run 01:
Highest Train: 85.00
Highest Valid: 84.90
  Final Train: 85.00
   Final Test: 85.08
All runs:
Highest Train: 85.00, nan
Highest Valid: 84.90, nan
  Final Train: 85.00, nan
   Final Test: 85.08, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.4885, Train: 86.66%, Valid: 86.49%, Test: 86.73%
Epoch: 25, Loss: 0.3655, Train: 86.25%, Valid: 86.33%, Test: 86.37%
Epoch: 50, Loss: 0.3614, Train: 86.68%, Valid: 86.70%, Test: 86.75%
Epoch: 75, Loss: 0.3563, Train: 86.49%, Valid: 86.50%, Test: 86.59%
Epoch: 100, Loss: 0.3469, Train: 86.18%, Valid: 86.16%, Test: 86.26%
Epoch: 125, Loss: 0.3389, Train: 85.80%, Valid: 85.79%, Test: 85.88%
Epoch: 150, Loss: 0.3332, Train: 85.47%, Valid: 85.48%, Test: 85.64%
Epoch: 175, Loss: 0.8684, Train: 85.70%, Valid: 85.74%, Test: 85.78%
Run 01:
Highest Train: 87.28
Highest Valid: 87.30
  Final Train: 87.28
   Final Test: 87.37
All runs:
Highest Train: 87.28, nan
Highest Valid: 87.30, nan
  Final Train: 87.28, nan
   Final Test: 87.37, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 11.5413, Train: 77.48%, Valid: 77.49%, Test: 77.39%
Epoch: 25, Loss: 0.5146, Train: 85.74%, Valid: 85.57%, Test: 85.81%
Epoch: 50, Loss: 0.4953, Train: 85.35%, Valid: 85.17%, Test: 85.39%
Epoch: 75, Loss: 0.4643, Train: 85.28%, Valid: 85.14%, Test: 85.35%
Epoch: 100, Loss: 0.5155, Train: 85.99%, Valid: 85.85%, Test: 86.02%
Epoch: 125, Loss: 0.4793, Train: 85.93%, Valid: 85.78%, Test: 85.99%
Epoch: 150, Loss: 0.4577, Train: 85.66%, Valid: 85.55%, Test: 85.70%
Epoch: 175, Loss: 0.4425, Train: 85.15%, Valid: 85.00%, Test: 85.23%
Run 01:
Highest Train: 86.08
Highest Valid: 85.94
  Final Train: 86.08
   Final Test: 86.11
All runs:
Highest Train: 86.08, nan
Highest Valid: 85.94, nan
  Final Train: 86.08, nan
   Final Test: 86.11, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2229.5024, Train: 51.47%, Valid: 51.57%, Test: 51.40%
Epoch: 25, Loss: 406.1845, Train: 83.94%, Valid: 83.90%, Test: 83.99%
Epoch: 50, Loss: 150.3014, Train: 84.29%, Valid: 84.19%, Test: 84.32%
Epoch: 75, Loss: 129.9188, Train: 84.07%, Valid: 83.99%, Test: 84.11%
Epoch: 100, Loss: 125.5515, Train: 84.15%, Valid: 84.06%, Test: 84.19%
Epoch: 125, Loss: 125.4716, Train: 84.17%, Valid: 84.07%, Test: 84.20%
Epoch: 150, Loss: 122.9815, Train: 84.14%, Valid: 84.05%, Test: 84.18%
Epoch: 175, Loss: 143.1658, Train: 84.04%, Valid: 83.97%, Test: 84.10%
Run 01:
Highest Train: 85.85
Highest Valid: 85.78
  Final Train: 85.85
   Final Test: 85.79
All runs:
Highest Train: 85.85, nan
Highest Valid: 85.78, nan
  Final Train: 85.85, nan
   Final Test: 85.79, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.6159, Train: 86.11%, Valid: 86.12%, Test: 86.29%
Epoch: 25, Loss: 0.3921, Train: 85.80%, Valid: 85.65%, Test: 85.78%
Epoch: 50, Loss: 0.3691, Train: 85.41%, Valid: 85.38%, Test: 85.50%
Epoch: 75, Loss: 0.3643, Train: 85.61%, Valid: 85.66%, Test: 85.68%
Epoch: 100, Loss: 0.3602, Train: 86.24%, Valid: 86.23%, Test: 86.26%
Epoch: 125, Loss: 0.3525, Train: 87.10%, Valid: 87.09%, Test: 87.14%
Epoch: 150, Loss: 0.3467, Train: 87.04%, Valid: 87.03%, Test: 87.06%
Epoch: 175, Loss: 0.3449, Train: 86.48%, Valid: 86.41%, Test: 86.50%
Run 01:
Highest Train: 88.13
Highest Valid: 88.19
  Final Train: 88.13
   Final Test: 88.19
All runs:
Highest Train: 88.13, nan
Highest Valid: 88.19, nan
  Final Train: 88.13, nan
   Final Test: 88.19, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.7451, Train: 86.18%, Valid: 86.07%, Test: 86.25%
Epoch: 25, Loss: 0.4415, Train: 85.39%, Valid: 85.22%, Test: 85.45%
Epoch: 50, Loss: 0.3801, Train: 85.32%, Valid: 85.15%, Test: 85.40%
Epoch: 75, Loss: 0.3677, Train: 84.67%, Valid: 84.65%, Test: 84.77%
Epoch: 100, Loss: 0.3636, Train: 85.78%, Valid: 85.73%, Test: 85.84%
Epoch: 125, Loss: 0.3557, Train: 85.28%, Valid: 85.21%, Test: 85.39%
Epoch: 150, Loss: 0.3514, Train: 85.08%, Valid: 84.98%, Test: 85.17%
Epoch: 175, Loss: 0.3504, Train: 84.43%, Valid: 84.34%, Test: 84.60%
Run 01:
Highest Train: 87.37
Highest Valid: 87.29
  Final Train: 87.37
   Final Test: 87.36
All runs:
Highest Train: 87.37, nan
Highest Valid: 87.29, nan
  Final Train: 87.37, nan
   Final Test: 87.36, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 110.2008, Train: 67.37%, Valid: 67.21%, Test: 67.03%
Epoch: 25, Loss: 44.4768, Train: 61.93%, Valid: 61.69%, Test: 61.66%
Epoch: 50, Loss: 49.2942, Train: 45.31%, Valid: 45.62%, Test: 45.52%
Epoch: 75, Loss: 36.2118, Train: 77.26%, Valid: 77.06%, Test: 77.18%
Epoch: 100, Loss: 83.9647, Train: 48.72%, Valid: 48.96%, Test: 49.02%
Epoch: 125, Loss: 15.7356, Train: 80.27%, Valid: 80.12%, Test: 80.21%
Epoch: 150, Loss: 9.2684, Train: 75.98%, Valid: 75.79%, Test: 75.82%
Epoch: 175, Loss: 8.6125, Train: 81.39%, Valid: 81.27%, Test: 81.33%
Run 01:
Highest Train: 82.47
Highest Valid: 82.39
  Final Train: 82.47
   Final Test: 82.47
All runs:
Highest Train: 82.47, nan
Highest Valid: 82.39, nan
  Final Train: 82.47, nan
   Final Test: 82.47, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 7.0856, Train: 84.00%, Valid: 83.81%, Test: 84.10%
Epoch: 25, Loss: 0.7573, Train: 88.20%, Valid: 88.26%, Test: 88.21%
Epoch: 50, Loss: 0.6265, Train: 85.92%, Valid: 85.81%, Test: 85.94%
Epoch: 75, Loss: 0.5059, Train: 85.79%, Valid: 85.65%, Test: 85.80%
Epoch: 100, Loss: 0.4313, Train: 85.76%, Valid: 85.63%, Test: 85.77%
Epoch: 125, Loss: 0.4072, Train: 86.64%, Valid: 86.70%, Test: 86.71%
Epoch: 150, Loss: 0.3964, Train: 86.20%, Valid: 86.24%, Test: 86.29%
Epoch: 175, Loss: 0.3798, Train: 87.32%, Valid: 87.38%, Test: 87.38%
Run 01:
Highest Train: 88.22
Highest Valid: 88.29
  Final Train: 88.22
   Final Test: 88.23
All runs:
Highest Train: 88.22, nan
Highest Valid: 88.29, nan
  Final Train: 88.22, nan
   Final Test: 88.23, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 8.7441, Train: 85.05%, Valid: 84.89%, Test: 85.12%
Epoch: 25, Loss: 0.7240, Train: 86.20%, Valid: 86.09%, Test: 86.30%
Epoch: 50, Loss: 0.6265, Train: 86.11%, Valid: 85.98%, Test: 86.17%
Epoch: 75, Loss: 0.4887, Train: 85.59%, Valid: 85.56%, Test: 85.64%
Epoch: 100, Loss: 0.4402, Train: 87.82%, Valid: 87.76%, Test: 87.75%
Epoch: 125, Loss: 0.4100, Train: 86.76%, Valid: 86.77%, Test: 86.81%
Epoch: 150, Loss: 0.4048, Train: 85.79%, Valid: 85.74%, Test: 85.81%
Epoch: 175, Loss: 0.4028, Train: 85.38%, Valid: 85.29%, Test: 85.41%
Run 01:
Highest Train: 87.82
Highest Valid: 87.77
  Final Train: 87.82
   Final Test: 87.75
All runs:
Highest Train: 87.82, nan
Highest Valid: 87.77, nan
  Final Train: 87.82, nan
   Final Test: 87.75, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 433.9735, Train: 60.04%, Valid: 59.88%, Test: 59.77%
Epoch: 25, Loss: 84.6689, Train: 50.95%, Valid: 50.68%, Test: 50.59%
Epoch: 50, Loss: 78.0166, Train: 41.70%, Valid: 41.44%, Test: 41.20%
Epoch: 75, Loss: 55.2618, Train: 38.82%, Valid: 38.57%, Test: 38.32%
Epoch: 100, Loss: 30.7048, Train: 38.42%, Valid: 38.16%, Test: 37.97%
Epoch: 125, Loss: 15.3180, Train: 82.63%, Valid: 82.49%, Test: 82.62%
Epoch: 150, Loss: 53.7762, Train: 82.93%, Valid: 82.78%, Test: 82.93%
Epoch: 175, Loss: 22.2673, Train: 46.86%, Valid: 46.60%, Test: 46.46%
Run 01:
Highest Train: 84.01
Highest Valid: 83.89
  Final Train: 84.01
   Final Test: 84.08
All runs:
Highest Train: 84.01, nan
Highest Valid: 83.89, nan
  Final Train: 84.01, nan
   Final Test: 84.08, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 4.9462, Train: 86.22%, Valid: 86.25%, Test: 86.21%
Epoch: 25, Loss: 0.5969, Train: 87.39%, Valid: 87.45%, Test: 87.41%
Epoch: 50, Loss: 0.4836, Train: 86.98%, Valid: 86.98%, Test: 86.97%
Epoch: 75, Loss: 0.4068, Train: 87.03%, Valid: 87.12%, Test: 87.07%
Epoch: 100, Loss: 0.3863, Train: 87.05%, Valid: 87.11%, Test: 87.11%
Epoch: 125, Loss: 0.3825, Train: 87.09%, Valid: 87.14%, Test: 87.14%
Epoch: 150, Loss: 0.3755, Train: 86.65%, Valid: 86.68%, Test: 86.68%
Epoch: 175, Loss: 0.3726, Train: 86.73%, Valid: 86.74%, Test: 86.77%
Run 01:
Highest Train: 87.63
Highest Valid: 87.63
  Final Train: 87.63
   Final Test: 87.60
All runs:
Highest Train: 87.63, nan
Highest Valid: 87.63, nan
  Final Train: 87.63, nan
   Final Test: 87.60, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 9.2928, Train: 85.61%, Valid: 85.46%, Test: 85.73%
Epoch: 25, Loss: 0.8233, Train: 85.98%, Valid: 85.96%, Test: 86.08%
Epoch: 50, Loss: 0.6437, Train: 86.04%, Valid: 85.90%, Test: 86.08%
Epoch: 75, Loss: 0.4981, Train: 85.57%, Valid: 85.44%, Test: 85.62%
Epoch: 100, Loss: 0.4261, Train: 85.83%, Valid: 85.66%, Test: 85.88%
Epoch: 125, Loss: 0.4081, Train: 85.92%, Valid: 85.74%, Test: 85.97%
Epoch: 150, Loss: 0.3881, Train: 85.96%, Valid: 85.91%, Test: 86.06%
Epoch: 175, Loss: 0.3826, Train: 86.38%, Valid: 86.29%, Test: 86.42%
Run 01:
Highest Train: 86.42
Highest Valid: 86.36
  Final Train: 86.42
   Final Test: 86.47
All runs:
Highest Train: 86.42, nan
Highest Valid: 86.36, nan
  Final Train: 86.42, nan
   Final Test: 86.47, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 415.2556, Train: 53.62%, Valid: 53.75%, Test: 53.56%
Epoch: 25, Loss: 137.1756, Train: 59.21%, Valid: 59.02%, Test: 58.89%
Epoch: 50, Loss: 9.6831, Train: 79.44%, Valid: 79.28%, Test: 79.36%
Epoch: 75, Loss: 11.3959, Train: 79.72%, Valid: 79.56%, Test: 79.64%
Epoch: 100, Loss: 10.4233, Train: 82.67%, Valid: 82.58%, Test: 82.67%
Epoch: 125, Loss: 51.9776, Train: 81.13%, Valid: 81.00%, Test: 81.08%
Epoch: 150, Loss: 7.6634, Train: 81.57%, Valid: 81.44%, Test: 81.54%
Epoch: 175, Loss: 7.3276, Train: 81.22%, Valid: 81.21%, Test: 81.42%
Run 01:
Highest Train: 82.86
Highest Valid: 82.77
  Final Train: 82.86
   Final Test: 82.91
All runs:
Highest Train: 82.86, nan
Highest Valid: 82.77, nan
  Final Train: 82.86, nan
   Final Test: 82.91, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 4.2117, Train: 87.55%, Valid: 87.61%, Test: 87.60%
Epoch: 25, Loss: 0.5183, Train: 88.10%, Valid: 88.20%, Test: 88.12%
Epoch: 50, Loss: 0.4692, Train: 87.74%, Valid: 87.81%, Test: 87.77%
Epoch: 75, Loss: 0.3736, Train: 85.52%, Valid: 85.57%, Test: 85.62%
Epoch: 100, Loss: 0.3525, Train: 85.40%, Valid: 85.44%, Test: 85.48%
Epoch: 125, Loss: 0.3423, Train: 85.70%, Valid: 85.68%, Test: 85.78%
Epoch: 150, Loss: 0.3335, Train: 85.58%, Valid: 85.59%, Test: 85.70%
Epoch: 175, Loss: 0.4003, Train: 85.72%, Valid: 85.75%, Test: 85.79%
Run 01:
Highest Train: 88.13
Highest Valid: 88.21
  Final Train: 88.13
   Final Test: 88.14
All runs:
Highest Train: 88.13, nan
Highest Valid: 88.21, nan
  Final Train: 88.13, nan
   Final Test: 88.14, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 17.1770, Train: 22.25%, Valid: 22.11%, Test: 21.89%
Epoch: 25, Loss: 0.7457, Train: 85.35%, Valid: 85.17%, Test: 85.41%
Epoch: 50, Loss: 0.7581, Train: 85.87%, Valid: 85.77%, Test: 85.88%
Epoch: 75, Loss: 0.8021, Train: 85.46%, Valid: 85.27%, Test: 85.51%
Epoch: 100, Loss: 0.7850, Train: 85.50%, Valid: 85.33%, Test: 85.55%
Epoch: 125, Loss: 0.7393, Train: 85.47%, Valid: 85.31%, Test: 85.54%
Epoch: 150, Loss: 0.6716, Train: 85.44%, Valid: 85.28%, Test: 85.50%
Epoch: 175, Loss: 0.6405, Train: 85.35%, Valid: 85.20%, Test: 85.44%
Run 01:
Highest Train: 86.34
Highest Valid: 86.23
  Final Train: 86.34
   Final Test: 86.33
All runs:
Highest Train: 86.34, nan
Highest Valid: 86.23, nan
  Final Train: 86.34, nan
   Final Test: 86.33, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1726.0908, Train: 42.98%, Valid: 42.88%, Test: 43.00%
Epoch: 25, Loss: 44800.8203, Train: 62.25%, Valid: 62.13%, Test: 61.99%
Epoch: 50, Loss: 160815.5312, Train: 59.77%, Valid: 59.67%, Test: 59.53%
Epoch: 75, Loss: 32.2741, Train: 29.64%, Valid: 29.82%, Test: 29.97%
Epoch: 100, Loss: 1054.7305, Train: 51.26%, Valid: 51.08%, Test: 50.96%
Epoch: 125, Loss: 29392.9102, Train: 25.55%, Valid: 25.74%, Test: 25.72%
Epoch: 150, Loss: 1632.8254, Train: 60.23%, Valid: 60.12%, Test: 59.99%
Epoch: 175, Loss: 97769.3125, Train: 22.41%, Valid: 22.56%, Test: 22.48%
Run 01:
Highest Train: 63.39
Highest Valid: 63.26
  Final Train: 63.39
   Final Test: 63.12
All runs:
Highest Train: 63.39, nan
Highest Valid: 63.26, nan
  Final Train: 63.39, nan
   Final Test: 63.12, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.0335, Train: 85.75%, Valid: 85.82%, Test: 85.75%
Epoch: 25, Loss: 0.5298, Train: 86.76%, Valid: 86.79%, Test: 86.79%
Epoch: 50, Loss: 0.4103, Train: 85.71%, Valid: 85.78%, Test: 85.78%
Epoch: 75, Loss: 0.3817, Train: 85.81%, Valid: 85.88%, Test: 85.88%
Epoch: 100, Loss: 0.3826, Train: 85.81%, Valid: 85.87%, Test: 85.90%
Epoch: 125, Loss: 0.3712, Train: 85.73%, Valid: 85.74%, Test: 85.82%
Epoch: 150, Loss: 0.3611, Train: 85.96%, Valid: 85.98%, Test: 86.00%
Epoch: 175, Loss: 0.3578, Train: 86.21%, Valid: 86.19%, Test: 86.25%
Run 01:
Highest Train: 86.77
Highest Valid: 86.81
  Final Train: 86.77
   Final Test: 86.79
All runs:
Highest Train: 86.77, nan
Highest Valid: 86.81, nan
  Final Train: 86.77, nan
   Final Test: 86.79, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 11.7255, Train: 83.29%, Valid: 83.24%, Test: 83.36%
Epoch: 25, Loss: 0.6703, Train: 86.41%, Valid: 86.37%, Test: 86.42%
Epoch: 50, Loss: 0.5701, Train: 86.53%, Valid: 86.57%, Test: 86.59%
Epoch: 75, Loss: 0.4745, Train: 86.26%, Valid: 86.28%, Test: 86.30%
Epoch: 100, Loss: 0.4101, Train: 86.15%, Valid: 86.12%, Test: 86.20%
Epoch: 125, Loss: 0.3761, Train: 85.60%, Valid: 85.43%, Test: 85.66%
Epoch: 150, Loss: 0.3687, Train: 85.68%, Valid: 85.50%, Test: 85.74%
Epoch: 175, Loss: 0.3658, Train: 85.65%, Valid: 85.48%, Test: 85.71%
Run 01:
Highest Train: 86.63
Highest Valid: 86.58
  Final Train: 86.54
   Final Test: 86.60
All runs:
Highest Train: 86.63, nan
Highest Valid: 86.58, nan
  Final Train: 86.54, nan
   Final Test: 86.60, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 9.0983, Train: 82.90%, Valid: 82.93%, Test: 83.04%
Epoch: 25, Loss: 7.4611, Train: 48.59%, Valid: 48.69%, Test: 48.75%
Epoch: 50, Loss: 8.9707, Train: 48.86%, Valid: 48.85%, Test: 48.96%
Epoch: 75, Loss: 29.2826, Train: 83.36%, Valid: 83.28%, Test: 83.40%
Epoch: 100, Loss: 13.4506, Train: 83.53%, Valid: 83.44%, Test: 83.58%
Epoch: 125, Loss: 9.5906, Train: 83.63%, Valid: 83.54%, Test: 83.69%
Epoch: 150, Loss: 8.9914, Train: 83.60%, Valid: 83.49%, Test: 83.65%
Epoch: 175, Loss: 9.2423, Train: 83.72%, Valid: 83.62%, Test: 83.78%
Run 01:
Highest Train: 83.90
Highest Valid: 83.82
  Final Train: 83.90
   Final Test: 83.96
All runs:
Highest Train: 83.90, nan
Highest Valid: 83.82, nan
  Final Train: 83.90, nan
   Final Test: 83.96, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 9.0706, Train: 87.27%, Valid: 87.34%, Test: 87.31%
Epoch: 25, Loss: 0.7029, Train: 87.59%, Valid: 87.62%, Test: 87.64%
Epoch: 50, Loss: 0.6272, Train: 86.63%, Valid: 86.66%, Test: 86.64%
Epoch: 75, Loss: 0.5052, Train: 86.45%, Valid: 86.49%, Test: 86.50%
Epoch: 100, Loss: 0.4317, Train: 86.03%, Valid: 86.08%, Test: 86.13%
Epoch: 125, Loss: 0.4014, Train: 85.87%, Valid: 85.94%, Test: 85.96%
Epoch: 150, Loss: 0.3887, Train: 85.83%, Valid: 85.84%, Test: 85.91%
Epoch: 175, Loss: 0.3827, Train: 85.80%, Valid: 85.84%, Test: 85.87%
Run 01:
Highest Train: 87.61
Highest Valid: 87.65
  Final Train: 87.61
   Final Test: 87.68
All runs:
Highest Train: 87.61, nan
Highest Valid: 87.65, nan
  Final Train: 87.61, nan
   Final Test: 87.68, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 18.6943, Train: 14.82%, Valid: 14.93%, Test: 14.79%
Epoch: 25, Loss: 0.6514, Train: 85.51%, Valid: 85.34%, Test: 85.56%
Epoch: 50, Loss: 0.5965, Train: 85.48%, Valid: 85.30%, Test: 85.52%
Epoch: 75, Loss: 0.5184, Train: 85.43%, Valid: 85.25%, Test: 85.47%
Epoch: 100, Loss: 0.4689, Train: 85.52%, Valid: 85.35%, Test: 85.57%
Epoch: 125, Loss: 0.4036, Train: 85.68%, Valid: 85.53%, Test: 85.74%
Epoch: 150, Loss: 0.4023, Train: 85.97%, Valid: 85.83%, Test: 86.02%
Epoch: 175, Loss: 0.3816, Train: 86.11%, Valid: 85.97%, Test: 86.17%
Run 01:
Highest Train: 86.71
Highest Valid: 86.69
  Final Train: 86.71
   Final Test: 86.80
All runs:
Highest Train: 86.71, nan
Highest Valid: 86.69, nan
  Final Train: 86.71, nan
   Final Test: 86.80, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 170.4102, Train: 55.13%, Valid: 55.22%, Test: 55.19%
Epoch: 25, Loss: 7.1033, Train: 51.85%, Valid: 51.85%, Test: 51.85%
Epoch: 50, Loss: 7.9303, Train: 50.04%, Valid: 50.06%, Test: 50.03%
Epoch: 75, Loss: 9.0860, Train: 49.99%, Valid: 50.00%, Test: 50.00%
Epoch: 100, Loss: 8.6625, Train: 50.01%, Valid: 50.00%, Test: 50.00%
Epoch: 125, Loss: 9.0083, Train: 67.75%, Valid: 67.57%, Test: 67.43%
Epoch: 150, Loss: 8.7368, Train: 81.92%, Valid: 81.79%, Test: 81.86%
Epoch: 175, Loss: 8.5393, Train: 60.85%, Valid: 61.22%, Test: 61.44%
Run 01:
Highest Train: 84.11
Highest Valid: 84.06
  Final Train: 84.11
   Final Test: 84.22
All runs:
Highest Train: 84.11, nan
Highest Valid: 84.06, nan
  Final Train: 84.11, nan
   Final Test: 84.22, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 3.1161, Train: 83.72%, Valid: 83.54%, Test: 83.80%
Epoch: 25, Loss: 0.5830, Train: 84.29%, Valid: 84.14%, Test: 84.42%
Epoch: 50, Loss: 0.4353, Train: 85.35%, Valid: 85.44%, Test: 85.44%
Epoch: 75, Loss: 0.3910, Train: 86.68%, Valid: 86.60%, Test: 86.69%
Epoch: 100, Loss: 0.3821, Train: 87.00%, Valid: 86.86%, Test: 86.96%
Epoch: 125, Loss: 0.3873, Train: 87.18%, Valid: 87.04%, Test: 87.17%
Epoch: 150, Loss: 0.3703, Train: 86.53%, Valid: 86.34%, Test: 86.49%
Epoch: 175, Loss: 0.3675, Train: 86.85%, Valid: 86.92%, Test: 86.89%
Run 01:
Highest Train: 87.41
Highest Valid: 87.39
  Final Train: 87.36
   Final Test: 87.53
All runs:
Highest Train: 87.41, nan
Highest Valid: 87.39, nan
  Final Train: 87.36, nan
   Final Test: 87.53, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 15.0099, Train: 22.91%, Valid: 22.68%, Test: 22.52%
Epoch: 25, Loss: 0.7107, Train: 85.83%, Valid: 85.68%, Test: 85.88%
Epoch: 50, Loss: 0.6033, Train: 85.82%, Valid: 85.69%, Test: 85.89%
Epoch: 75, Loss: 0.5196, Train: 85.68%, Valid: 85.53%, Test: 85.72%
Epoch: 100, Loss: 0.4290, Train: 85.48%, Valid: 85.31%, Test: 85.54%
Epoch: 125, Loss: 0.4158, Train: 85.50%, Valid: 85.32%, Test: 85.55%
Epoch: 150, Loss: 0.3962, Train: 85.51%, Valid: 85.33%, Test: 85.57%
Epoch: 175, Loss: 0.3901, Train: 85.54%, Valid: 85.36%, Test: 85.60%
Run 01:
Highest Train: 86.28
Highest Valid: 86.20
  Final Train: 86.28
   Final Test: 86.35
All runs:
Highest Train: 86.28, nan
Highest Valid: 86.20, nan
  Final Train: 86.28, nan
   Final Test: 86.35, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 63.1664, Train: 56.16%, Valid: 56.51%, Test: 56.81%
Epoch: 25, Loss: 13.3398, Train: 82.81%, Valid: 82.70%, Test: 82.85%
Epoch: 50, Loss: 8.3564, Train: 46.99%, Valid: 46.90%, Test: 47.08%
Epoch: 75, Loss: 7.8459, Train: 52.34%, Valid: 52.39%, Test: 52.34%
Epoch: 100, Loss: 8.5199, Train: 83.76%, Valid: 83.75%, Test: 83.96%
Epoch: 125, Loss: 8.5181, Train: 83.31%, Valid: 83.17%, Test: 83.37%
Epoch: 150, Loss: 10.2013, Train: 82.33%, Valid: 82.16%, Test: 82.33%
Epoch: 175, Loss: 8.1899, Train: 83.43%, Valid: 83.32%, Test: 83.48%
Run 01:
Highest Train: 83.97
Highest Valid: 83.98
  Final Train: 83.97
   Final Test: 84.13
All runs:
Highest Train: 83.97, nan
Highest Valid: 83.98, nan
  Final Train: 83.97, nan
   Final Test: 84.13, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 3.2158, Train: 88.28%, Valid: 88.35%, Test: 88.31%
Epoch: 25, Loss: 0.9576, Train: 87.56%, Valid: 87.65%, Test: 87.63%
Epoch: 50, Loss: 0.9814, Train: 86.63%, Valid: 86.73%, Test: 86.74%
Epoch: 75, Loss: 0.8898, Train: 85.73%, Valid: 85.77%, Test: 85.87%
Epoch: 100, Loss: 0.7062, Train: 85.42%, Valid: 85.45%, Test: 85.52%
Epoch: 125, Loss: 0.3701, Train: 85.50%, Valid: 85.50%, Test: 85.60%
Epoch: 150, Loss: 0.3429, Train: 85.90%, Valid: 85.87%, Test: 86.00%
Epoch: 175, Loss: 0.3364, Train: 85.65%, Valid: 85.65%, Test: 85.76%
Run 01:
Highest Train: 88.39
Highest Valid: 88.46
  Final Train: 88.39
   Final Test: 88.46
All runs:
Highest Train: 88.39, nan
Highest Valid: 88.46, nan
  Final Train: 88.39, nan
   Final Test: 88.46, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.9395, Train: 85.70%, Valid: 85.58%, Test: 85.84%
Epoch: 25, Loss: 0.3832, Train: 85.40%, Valid: 85.28%, Test: 85.46%
Epoch: 50, Loss: 0.3637, Train: 85.57%, Valid: 85.43%, Test: 85.63%
Epoch: 75, Loss: 0.3582, Train: 85.52%, Valid: 85.36%, Test: 85.59%
Epoch: 100, Loss: 0.3499, Train: 85.46%, Valid: 85.30%, Test: 85.54%
Epoch: 125, Loss: 0.3503, Train: 86.12%, Valid: 85.96%, Test: 86.19%
Epoch: 150, Loss: 0.3433, Train: 85.54%, Valid: 85.37%, Test: 85.61%
Epoch: 175, Loss: 0.3359, Train: 85.53%, Valid: 85.36%, Test: 85.59%
Run 01:
Highest Train: 87.80
Highest Valid: 87.70
  Final Train: 87.80
   Final Test: 87.83
All runs:
Highest Train: 87.80, nan
Highest Valid: 87.70, nan
  Final Train: 87.80, nan
   Final Test: 87.83, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 3535.6772, Train: 68.44%, Valid: 68.33%, Test: 68.12%
Epoch: 25, Loss: 178.1158, Train: 43.63%, Valid: 43.36%, Test: 43.14%
Epoch: 50, Loss: 134.8484, Train: 40.91%, Valid: 40.61%, Test: 40.37%
Epoch: 75, Loss: 105.0751, Train: 40.79%, Valid: 40.48%, Test: 40.28%
Epoch: 100, Loss: 85.0808, Train: 47.79%, Valid: 47.53%, Test: 47.26%
Epoch: 125, Loss: 68.1417, Train: 81.51%, Valid: 81.36%, Test: 81.43%
Epoch: 150, Loss: 50.8692, Train: 83.33%, Valid: 83.25%, Test: 83.36%
Epoch: 175, Loss: 33.5159, Train: 83.71%, Valid: 83.64%, Test: 83.76%
Run 01:
Highest Train: 83.78
Highest Valid: 83.72
  Final Train: 83.78
   Final Test: 83.83
All runs:
Highest Train: 83.78, nan
Highest Valid: 83.72, nan
  Final Train: 83.78, nan
   Final Test: 83.83, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 10.7636, Train: 84.04%, Valid: 83.88%, Test: 84.12%
Epoch: 25, Loss: 0.9042, Train: 84.85%, Valid: 84.88%, Test: 84.96%
Epoch: 50, Loss: 0.5047, Train: 85.20%, Valid: 85.26%, Test: 85.29%
Epoch: 75, Loss: 0.4338, Train: 87.55%, Valid: 87.37%, Test: 87.59%
Epoch: 100, Loss: 0.4117, Train: 87.60%, Valid: 87.47%, Test: 87.64%
Epoch: 125, Loss: 0.4143, Train: 87.28%, Valid: 87.13%, Test: 87.31%
Epoch: 150, Loss: 0.3901, Train: 87.60%, Valid: 87.45%, Test: 87.63%
Epoch: 175, Loss: 0.3773, Train: 87.62%, Valid: 87.47%, Test: 87.65%
Run 01:
Highest Train: 88.15
Highest Valid: 88.21
  Final Train: 88.15
   Final Test: 88.24
All runs:
Highest Train: 88.15, nan
Highest Valid: 88.21, nan
  Final Train: 88.15, nan
   Final Test: 88.24, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 12.6771, Train: 85.59%, Valid: 85.60%, Test: 85.66%
Epoch: 25, Loss: 0.9129, Train: 86.03%, Valid: 85.93%, Test: 86.09%
Epoch: 50, Loss: 0.5784, Train: 86.40%, Valid: 86.32%, Test: 86.47%
Epoch: 75, Loss: 0.4109, Train: 86.68%, Valid: 86.63%, Test: 86.77%
Epoch: 100, Loss: 0.3860, Train: 86.71%, Valid: 86.67%, Test: 86.82%
Epoch: 125, Loss: 0.3827, Train: 86.82%, Valid: 86.73%, Test: 86.86%
Epoch: 150, Loss: 0.3784, Train: 87.09%, Valid: 86.90%, Test: 87.02%
Epoch: 175, Loss: 0.3617, Train: 86.63%, Valid: 86.48%, Test: 86.63%
Run 01:
Highest Train: 87.17
Highest Valid: 87.08
  Final Train: 87.14
   Final Test: 87.24
All runs:
Highest Train: 87.17, nan
Highest Valid: 87.08, nan
  Final Train: 87.14, nan
   Final Test: 87.24, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 493.1770, Train: 80.93%, Valid: 80.77%, Test: 80.93%
Epoch: 25, Loss: 8.6664, Train: 82.95%, Valid: 82.86%, Test: 82.97%
Epoch: 50, Loss: 5.0392, Train: 83.07%, Valid: 82.98%, Test: 83.10%
Epoch: 75, Loss: 8.7072, Train: 83.30%, Valid: 83.22%, Test: 83.36%
Epoch: 100, Loss: 6.8207, Train: 83.65%, Valid: 83.56%, Test: 83.73%
Epoch: 125, Loss: 5.8097, Train: 83.89%, Valid: 83.81%, Test: 83.95%
Epoch: 150, Loss: 6.4871, Train: 84.17%, Valid: 84.09%, Test: 84.25%
Epoch: 175, Loss: 4.5020, Train: 84.27%, Valid: 84.18%, Test: 84.34%
Run 01:
Highest Train: 84.53
Highest Valid: 84.41
  Final Train: 84.53
   Final Test: 84.59
All runs:
Highest Train: 84.53, nan
Highest Valid: 84.41, nan
  Final Train: 84.53, nan
   Final Test: 84.59, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 3.2685, Train: 86.25%, Valid: 86.11%, Test: 86.38%
Epoch: 25, Loss: 0.8204, Train: 86.50%, Valid: 86.57%, Test: 86.58%
Epoch: 50, Loss: 0.6008, Train: 85.99%, Valid: 86.06%, Test: 86.09%
Epoch: 75, Loss: 0.4051, Train: 85.49%, Valid: 85.54%, Test: 85.59%
Epoch: 100, Loss: 0.3783, Train: 85.45%, Valid: 85.49%, Test: 85.55%
Epoch: 125, Loss: 0.3689, Train: 86.32%, Valid: 86.31%, Test: 86.33%
Epoch: 150, Loss: 0.3632, Train: 86.36%, Valid: 86.32%, Test: 86.37%
Epoch: 175, Loss: 0.3548, Train: 86.25%, Valid: 86.22%, Test: 86.29%
Run 01:
Highest Train: 86.71
Highest Valid: 86.68
  Final Train: 86.71
   Final Test: 86.73
All runs:
Highest Train: 86.71, nan
Highest Valid: 86.68, nan
  Final Train: 86.71, nan
   Final Test: 86.73, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 3.0491, Train: 86.15%, Valid: 85.99%, Test: 86.22%
Epoch: 25, Loss: 0.8946, Train: 86.14%, Valid: 86.12%, Test: 86.27%
Epoch: 50, Loss: 0.5408, Train: 86.52%, Valid: 86.42%, Test: 86.55%
Epoch: 75, Loss: 0.4202, Train: 86.33%, Valid: 86.16%, Test: 86.32%
Epoch: 100, Loss: 0.3976, Train: 86.29%, Valid: 86.10%, Test: 86.29%
Epoch: 125, Loss: 0.3922, Train: 85.88%, Valid: 85.68%, Test: 85.88%
Epoch: 150, Loss: 0.3722, Train: 85.55%, Valid: 85.37%, Test: 85.61%
Epoch: 175, Loss: 0.3673, Train: 85.64%, Valid: 85.47%, Test: 85.71%
Run 01:
Highest Train: 86.74
Highest Valid: 86.74
  Final Train: 86.74
   Final Test: 86.79
All runs:
Highest Train: 86.74, nan
Highest Valid: 86.74, nan
  Final Train: 86.74, nan
   Final Test: 86.79, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 11.6592, Train: 78.96%, Valid: 78.77%, Test: 78.87%
Epoch: 25, Loss: 4.8426, Train: 59.66%, Valid: 59.94%, Test: 60.23%
Epoch: 50, Loss: 3.9671, Train: 83.20%, Valid: 83.13%, Test: 83.24%
Epoch: 75, Loss: 6.0128, Train: 83.09%, Valid: 83.01%, Test: 83.12%
Epoch: 100, Loss: 3.8282, Train: 83.80%, Valid: 83.73%, Test: 83.87%
Epoch: 125, Loss: 3.7722, Train: 81.52%, Valid: 81.35%, Test: 81.44%
Epoch: 150, Loss: 3.3063, Train: 83.38%, Valid: 83.29%, Test: 83.40%
Epoch: 175, Loss: 2.8854, Train: 83.92%, Valid: 83.84%, Test: 83.98%
Run 01:
Highest Train: 84.74
Highest Valid: 84.62
  Final Train: 84.74
   Final Test: 84.79
All runs:
Highest Train: 84.74, nan
Highest Valid: 84.62, nan
  Final Train: 84.74, nan
   Final Test: 84.79, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 3.4172, Train: 85.04%, Valid: 85.00%, Test: 85.18%
Epoch: 25, Loss: 0.8082, Train: 86.08%, Valid: 86.11%, Test: 86.16%
Epoch: 50, Loss: 0.5175, Train: 86.13%, Valid: 86.17%, Test: 86.24%
Epoch: 75, Loss: 0.3905, Train: 85.30%, Valid: 85.32%, Test: 85.41%
Epoch: 100, Loss: 0.3762, Train: 85.68%, Valid: 85.70%, Test: 85.76%
Epoch: 125, Loss: 0.3703, Train: 85.37%, Valid: 85.41%, Test: 85.46%
Epoch: 150, Loss: 0.3651, Train: 85.57%, Valid: 85.54%, Test: 85.66%
Epoch: 175, Loss: 0.3605, Train: 85.52%, Valid: 85.54%, Test: 85.60%
Run 01:
Highest Train: 86.42
Highest Valid: 86.47
  Final Train: 86.42
   Final Test: 86.52
All runs:
Highest Train: 86.42, nan
Highest Valid: 86.47, nan
  Final Train: 86.42, nan
   Final Test: 86.52, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 16.0696, Train: 85.42%, Valid: 85.27%, Test: 85.48%
Epoch: 25, Loss: 1.1692, Train: 85.59%, Valid: 85.42%, Test: 85.61%
Epoch: 50, Loss: 0.8260, Train: 85.91%, Valid: 85.76%, Test: 85.93%
Epoch: 75, Loss: 0.5481, Train: 86.07%, Valid: 85.94%, Test: 86.10%
Epoch: 100, Loss: 0.4221, Train: 86.89%, Valid: 86.87%, Test: 86.98%
Epoch: 125, Loss: 0.4002, Train: 85.84%, Valid: 85.81%, Test: 85.96%
Epoch: 150, Loss: 0.3919, Train: 85.50%, Valid: 85.36%, Test: 85.60%
Epoch: 175, Loss: 0.3810, Train: 85.59%, Valid: 85.38%, Test: 85.60%
Run 01:
Highest Train: 87.41
Highest Valid: 87.17
  Final Train: 87.41
   Final Test: 87.35
All runs:
Highest Train: 87.41, nan
Highest Valid: 87.17, nan
  Final Train: 87.41, nan
   Final Test: 87.35, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 662.4202, Train: 68.18%, Valid: 68.01%, Test: 67.90%
Epoch: 25, Loss: 2.7781, Train: 82.22%, Valid: 82.12%, Test: 82.22%
Epoch: 50, Loss: 35.8927, Train: 83.53%, Valid: 83.43%, Test: 83.60%
Epoch: 75, Loss: 3.3375, Train: 83.97%, Valid: 83.88%, Test: 84.03%
Epoch: 100, Loss: 9.7519, Train: 84.79%, Valid: 84.67%, Test: 84.85%
Epoch: 125, Loss: 3.3708, Train: 84.24%, Valid: 84.13%, Test: 84.30%
Epoch: 150, Loss: 3.0926, Train: 84.32%, Valid: 84.21%, Test: 84.39%
Epoch: 175, Loss: 2.9646, Train: 84.37%, Valid: 84.27%, Test: 84.45%
Run 01:
Highest Train: 84.80
Highest Valid: 84.68
  Final Train: 84.80
   Final Test: 84.86
All runs:
Highest Train: 84.80, nan
Highest Valid: 84.68, nan
  Final Train: 84.80, nan
   Final Test: 84.86, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.3959, Train: 86.83%, Valid: 86.81%, Test: 86.96%
Epoch: 25, Loss: 0.3724, Train: 87.90%, Valid: 87.95%, Test: 87.91%
Epoch: 50, Loss: 0.3580, Train: 85.65%, Valid: 85.68%, Test: 85.73%
Epoch: 75, Loss: 0.3497, Train: 85.75%, Valid: 85.77%, Test: 85.80%
Epoch: 100, Loss: 0.3970, Train: 85.71%, Valid: 85.73%, Test: 85.78%
Epoch: 125, Loss: 1.3661, Train: 85.77%, Valid: 85.77%, Test: 85.86%
Epoch: 150, Loss: 2.5774, Train: 86.70%, Valid: 86.72%, Test: 86.74%
Epoch: 175, Loss: 3.0443, Train: 86.51%, Valid: 86.53%, Test: 86.53%
Run 01:
Highest Train: 88.52
Highest Valid: 88.60
  Final Train: 88.52
   Final Test: 88.58
All runs:
Highest Train: 88.52, nan
Highest Valid: 88.60, nan
  Final Train: 88.52, nan
   Final Test: 88.58, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 8.2837, Train: 84.27%, Valid: 84.31%, Test: 84.36%
Epoch: 25, Loss: 1.1955, Train: 85.92%, Valid: 85.87%, Test: 85.97%
Epoch: 50, Loss: 0.7532, Train: 85.93%, Valid: 85.90%, Test: 86.04%
Epoch: 75, Loss: 0.4177, Train: 85.75%, Valid: 85.62%, Test: 85.84%
Epoch: 100, Loss: 0.3880, Train: 85.49%, Valid: 85.34%, Test: 85.57%
Epoch: 125, Loss: 0.3690, Train: 85.60%, Valid: 85.44%, Test: 85.67%
Epoch: 150, Loss: 0.3570, Train: 85.59%, Valid: 85.41%, Test: 85.66%
Epoch: 175, Loss: 0.3598, Train: 85.60%, Valid: 85.43%, Test: 85.67%
Run 01:
Highest Train: 86.18
Highest Valid: 86.15
  Final Train: 86.16
   Final Test: 86.24
All runs:
Highest Train: 86.18, nan
Highest Valid: 86.15, nan
  Final Train: 86.16, nan
   Final Test: 86.24, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 28005152.0000, Train: 51.82%, Valid: 52.04%, Test: 52.09%
Epoch: 25, Loss: 238.0238, Train: 83.00%, Valid: 82.93%, Test: 83.04%
Epoch: 50, Loss: 963.9531, Train: 83.52%, Valid: 83.45%, Test: 83.57%
Epoch: 75, Loss: 641.6328, Train: 83.15%, Valid: 83.08%, Test: 83.21%
Epoch: 100, Loss: 872.9184, Train: 83.20%, Valid: 83.13%, Test: 83.26%
Epoch: 125, Loss: 1090.0442, Train: 80.00%, Valid: 79.98%, Test: 80.14%
Epoch: 150, Loss: 1268.3184, Train: 83.07%, Valid: 82.99%, Test: 83.11%
Epoch: 175, Loss: 664.8419, Train: 83.46%, Valid: 83.39%, Test: 83.52%
Run 01:
Highest Train: 84.21
Highest Valid: 84.16
  Final Train: 84.21
   Final Test: 84.28
All runs:
Highest Train: 84.21, nan
Highest Valid: 84.16, nan
  Final Train: 84.21, nan
   Final Test: 84.28, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 18.6348, Train: 85.42%, Valid: 85.28%, Test: 85.41%
Epoch: 25, Loss: 0.9740, Train: 87.04%, Valid: 87.13%, Test: 87.10%
Epoch: 50, Loss: 0.6586, Train: 87.11%, Valid: 87.12%, Test: 87.12%
Epoch: 75, Loss: 0.4158, Train: 86.66%, Valid: 86.64%, Test: 86.68%
Epoch: 100, Loss: 0.3982, Train: 85.57%, Valid: 85.56%, Test: 85.69%
Epoch: 125, Loss: 0.3912, Train: 86.80%, Valid: 86.69%, Test: 86.83%
Epoch: 150, Loss: 0.3726, Train: 85.81%, Valid: 85.69%, Test: 85.83%
Epoch: 175, Loss: 0.3688, Train: 86.33%, Valid: 86.21%, Test: 86.43%
Run 01:
Highest Train: 87.52
Highest Valid: 87.56
  Final Train: 87.52
   Final Test: 87.55
All runs:
Highest Train: 87.52, nan
Highest Valid: 87.56, nan
  Final Train: 87.52, nan
   Final Test: 87.55, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.0897, Train: 84.46%, Valid: 84.42%, Test: 84.57%
Epoch: 25, Loss: 0.6579, Train: 85.81%, Valid: 85.69%, Test: 85.86%
Epoch: 50, Loss: 0.4041, Train: 85.38%, Valid: 85.18%, Test: 85.45%
Epoch: 75, Loss: 0.3813, Train: 85.44%, Valid: 85.24%, Test: 85.50%
Epoch: 100, Loss: 0.3686, Train: 85.41%, Valid: 85.22%, Test: 85.49%
Epoch: 125, Loss: 0.3620, Train: 85.15%, Valid: 84.99%, Test: 85.26%
Epoch: 150, Loss: 0.3588, Train: 85.41%, Valid: 85.25%, Test: 85.51%
Epoch: 175, Loss: 0.3525, Train: 85.59%, Valid: 85.42%, Test: 85.68%
Run 01:
Highest Train: 87.56
Highest Valid: 87.55
  Final Train: 87.54
   Final Test: 87.54
All runs:
Highest Train: 87.56, nan
Highest Valid: 87.55, nan
  Final Train: 87.54, nan
   Final Test: 87.54, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 25.3834, Train: 55.30%, Valid: 55.66%, Test: 55.74%
Epoch: 25, Loss: 12.7984, Train: 78.47%, Valid: 78.25%, Test: 78.38%
Epoch: 50, Loss: 9.8368, Train: 82.66%, Valid: 82.58%, Test: 82.69%
Epoch: 75, Loss: 47.1778, Train: 81.08%, Valid: 80.95%, Test: 81.03%
Epoch: 100, Loss: 34.2733, Train: 78.63%, Valid: 78.46%, Test: 78.53%
Epoch: 125, Loss: 9.1291, Train: 81.63%, Valid: 81.54%, Test: 81.58%
Epoch: 150, Loss: 8.5456, Train: 82.75%, Valid: 82.69%, Test: 82.75%
Epoch: 175, Loss: 8.5947, Train: 82.86%, Valid: 82.79%, Test: 82.88%
Run 01:
Highest Train: 83.62
Highest Valid: 83.63
  Final Train: 83.62
   Final Test: 83.71
All runs:
Highest Train: 83.62, nan
Highest Valid: 83.63, nan
  Final Train: 83.62, nan
   Final Test: 83.71, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 6.6751, Train: 86.07%, Valid: 85.87%, Test: 86.14%
Epoch: 25, Loss: 0.9207, Train: 86.33%, Valid: 86.36%, Test: 86.37%
Epoch: 50, Loss: 0.5177, Train: 86.29%, Valid: 86.32%, Test: 86.34%
Epoch: 75, Loss: 0.4671, Train: 86.32%, Valid: 86.35%, Test: 86.34%
Epoch: 100, Loss: 0.4318, Train: 86.32%, Valid: 86.35%, Test: 86.36%
Epoch: 125, Loss: 0.4124, Train: 86.03%, Valid: 86.09%, Test: 86.11%
Epoch: 150, Loss: 0.3910, Train: 85.91%, Valid: 85.96%, Test: 85.98%
Epoch: 175, Loss: 0.3829, Train: 85.87%, Valid: 85.92%, Test: 85.96%
Run 01:
Highest Train: 86.77
Highest Valid: 86.80
  Final Train: 86.77
   Final Test: 86.81
All runs:
Highest Train: 86.77, nan
Highest Valid: 86.80, nan
  Final Train: 86.77, nan
   Final Test: 86.81, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 5.9477, Train: 85.23%, Valid: 85.05%, Test: 85.36%
Epoch: 25, Loss: 0.8157, Train: 86.05%, Valid: 85.89%, Test: 86.13%
Epoch: 50, Loss: 0.5256, Train: 85.67%, Valid: 85.52%, Test: 85.68%
Epoch: 75, Loss: 0.4169, Train: 87.18%, Valid: 87.04%, Test: 87.27%
Epoch: 100, Loss: 0.3977, Train: 87.27%, Valid: 87.18%, Test: 87.25%
Epoch: 125, Loss: 0.3745, Train: 85.97%, Valid: 85.93%, Test: 86.03%
Epoch: 150, Loss: 0.3666, Train: 85.56%, Valid: 85.38%, Test: 85.63%
Epoch: 175, Loss: 0.3577, Train: 85.63%, Valid: 85.45%, Test: 85.71%
Run 01:
Highest Train: 87.85
Highest Valid: 87.77
  Final Train: 87.81
   Final Test: 87.80
All runs:
Highest Train: 87.85, nan
Highest Valid: 87.77, nan
  Final Train: 87.81, nan
   Final Test: 87.80, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 162.5434, Train: 60.01%, Valid: 59.84%, Test: 59.67%
Epoch: 25, Loss: 803.9251, Train: 59.35%, Valid: 59.19%, Test: 59.06%
Epoch: 50, Loss: 388.3653, Train: 78.95%, Valid: 78.77%, Test: 78.83%
Epoch: 75, Loss: 9.0760, Train: 63.08%, Valid: 63.27%, Test: 63.49%
Epoch: 100, Loss: 12.2522, Train: 80.25%, Valid: 80.09%, Test: 80.15%
Epoch: 125, Loss: 14.3385, Train: 71.61%, Valid: 71.42%, Test: 71.36%
Epoch: 150, Loss: 6.6393, Train: 82.82%, Valid: 82.74%, Test: 82.83%
Epoch: 175, Loss: 94.9759, Train: 82.01%, Valid: 81.90%, Test: 81.97%
Run 01:
Highest Train: 83.51
Highest Valid: 83.48
  Final Train: 83.51
   Final Test: 83.60
All runs:
Highest Train: 83.51, nan
Highest Valid: 83.48, nan
  Final Train: 83.51, nan
   Final Test: 83.60, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 6.1851, Train: 84.19%, Valid: 83.99%, Test: 84.30%
Epoch: 25, Loss: 0.8276, Train: 86.54%, Valid: 86.60%, Test: 86.52%
Epoch: 50, Loss: 0.6011, Train: 86.52%, Valid: 86.56%, Test: 86.57%
Epoch: 75, Loss: 0.4187, Train: 86.13%, Valid: 86.18%, Test: 86.21%
Epoch: 100, Loss: 0.3816, Train: 85.68%, Valid: 85.70%, Test: 85.76%
Epoch: 125, Loss: 0.3728, Train: 85.82%, Valid: 85.86%, Test: 85.90%
Epoch: 150, Loss: 0.3649, Train: 85.90%, Valid: 85.91%, Test: 86.00%
Epoch: 175, Loss: 0.3620, Train: 85.97%, Valid: 85.98%, Test: 86.06%
Run 01:
Highest Train: 87.27
Highest Valid: 87.12
  Final Train: 87.27
   Final Test: 87.33
All runs:
Highest Train: 87.27, nan
Highest Valid: 87.12, nan
  Final Train: 87.27, nan
   Final Test: 87.33, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 6.3616, Train: 86.20%, Valid: 86.05%, Test: 86.24%
Epoch: 25, Loss: 0.9015, Train: 86.57%, Valid: 86.44%, Test: 86.61%
Epoch: 50, Loss: 0.5771, Train: 86.17%, Valid: 86.02%, Test: 86.20%
Epoch: 75, Loss: 0.4196, Train: 85.77%, Valid: 85.78%, Test: 85.88%
Epoch: 100, Loss: 0.3935, Train: 85.83%, Valid: 85.81%, Test: 85.94%
Epoch: 125, Loss: 0.3832, Train: 85.77%, Valid: 85.71%, Test: 85.80%
Epoch: 150, Loss: 0.3715, Train: 85.81%, Valid: 85.80%, Test: 85.93%
Epoch: 175, Loss: 0.3659, Train: 85.96%, Valid: 85.98%, Test: 86.10%
Run 01:
Highest Train: 86.57
Highest Valid: 86.49
  Final Train: 86.56
   Final Test: 86.63
All runs:
Highest Train: 86.57, nan
Highest Valid: 86.49, nan
  Final Train: 86.56, nan
   Final Test: 86.63, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 418.0506, Train: 58.86%, Valid: 58.64%, Test: 58.57%
Epoch: 25, Loss: 212.5269, Train: 77.43%, Valid: 77.23%, Test: 77.34%
Epoch: 50, Loss: 26.0377, Train: 78.78%, Valid: 78.61%, Test: 78.68%
Epoch: 75, Loss: 18.3334, Train: 43.44%, Valid: 43.23%, Test: 42.95%
Epoch: 100, Loss: 12.9279, Train: 80.71%, Valid: 80.53%, Test: 80.68%
Epoch: 125, Loss: 44.4958, Train: 79.90%, Valid: 79.90%, Test: 80.15%
Epoch: 150, Loss: 12.9750, Train: 83.74%, Valid: 83.64%, Test: 83.83%
Epoch: 175, Loss: 7.5565, Train: 84.16%, Valid: 84.04%, Test: 84.21%
Run 01:
Highest Train: 84.16
Highest Valid: 84.05
  Final Train: 84.16
   Final Test: 84.22
All runs:
Highest Train: 84.16, nan
Highest Valid: 84.05, nan
  Final Train: 84.16, nan
   Final Test: 84.22, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 13.0336, Train: 86.63%, Valid: 86.44%, Test: 86.65%
Epoch: 25, Loss: 0.9446, Train: 86.30%, Valid: 86.32%, Test: 86.30%
Epoch: 50, Loss: 0.8693, Train: 86.31%, Valid: 86.36%, Test: 86.38%
Epoch: 75, Loss: 0.4284, Train: 85.85%, Valid: 85.93%, Test: 85.94%
Epoch: 100, Loss: 0.3575, Train: 85.75%, Valid: 85.77%, Test: 85.81%
Epoch: 125, Loss: 0.3495, Train: 85.65%, Valid: 85.62%, Test: 85.70%
Epoch: 150, Loss: 0.3360, Train: 85.45%, Valid: 85.41%, Test: 85.61%
Epoch: 175, Loss: 1.0934, Train: 85.75%, Valid: 85.76%, Test: 85.89%
Run 01:
Highest Train: 87.39
Highest Valid: 87.45
  Final Train: 87.37
   Final Test: 87.49
All runs:
Highest Train: 87.39, nan
Highest Valid: 87.45, nan
  Final Train: 87.37, nan
   Final Test: 87.49, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 16.6202, Train: 85.50%, Valid: 85.42%, Test: 85.58%
Epoch: 25, Loss: 0.8969, Train: 85.74%, Valid: 85.59%, Test: 85.77%
Epoch: 50, Loss: 0.6274, Train: 85.98%, Valid: 85.83%, Test: 86.01%
Epoch: 75, Loss: 0.3783, Train: 85.60%, Valid: 85.45%, Test: 85.67%
Epoch: 100, Loss: 0.3653, Train: 85.64%, Valid: 85.46%, Test: 85.70%
Epoch: 125, Loss: 0.3587, Train: 85.64%, Valid: 85.47%, Test: 85.70%
Epoch: 150, Loss: 0.3535, Train: 85.74%, Valid: 85.57%, Test: 85.80%
Epoch: 175, Loss: 0.3504, Train: 85.68%, Valid: 85.51%, Test: 85.75%
Run 01:
Highest Train: 86.35
Highest Valid: 86.24
  Final Train: 86.35
   Final Test: 86.37
All runs:
Highest Train: 86.35, nan
Highest Valid: 86.24, nan
  Final Train: 86.35, nan
   Final Test: 86.37, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 101184430151827456.0000, Train: 48.30%, Valid: 48.35%, Test: 48.28%
Epoch: 25, Loss: 1733014784.0000, Train: 53.02%, Valid: 53.04%, Test: 53.06%
Epoch: 50, Loss: 107177.1797, Train: 52.31%, Valid: 52.07%, Test: 51.98%
Epoch: 75, Loss: 1654.4510, Train: 35.10%, Valid: 34.86%, Test: 34.63%
Epoch: 100, Loss: 11024.5234, Train: 41.72%, Valid: 41.48%, Test: 41.24%
Epoch: 125, Loss: 8162.9468, Train: 41.65%, Valid: 41.40%, Test: 41.16%
Epoch: 150, Loss: 20775.8828, Train: 17.81%, Valid: 17.93%, Test: 17.84%
Epoch: 175, Loss: 37330.4414, Train: 45.66%, Valid: 45.39%, Test: 45.25%
Run 01:
Highest Train: 59.58
Highest Valid: 59.38
  Final Train: 59.58
   Final Test: 59.33
All runs:
Highest Train: 59.58, nan
Highest Valid: 59.38, nan
  Final Train: 59.58, nan
   Final Test: 59.33, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 13.3625, Train: 86.82%, Valid: 86.77%, Test: 86.96%
Epoch: 25, Loss: 0.9006, Train: 86.21%, Valid: 86.24%, Test: 86.29%
Epoch: 50, Loss: 0.5139, Train: 86.60%, Valid: 86.68%, Test: 86.66%
Epoch: 75, Loss: 0.4233, Train: 84.77%, Valid: 84.56%, Test: 84.84%
Epoch: 100, Loss: 0.4136, Train: 85.86%, Valid: 85.74%, Test: 85.91%
Epoch: 125, Loss: 0.3986, Train: 85.82%, Valid: 85.68%, Test: 85.88%
Epoch: 150, Loss: 0.3909, Train: 86.59%, Valid: 86.61%, Test: 86.67%
Epoch: 175, Loss: 0.3760, Train: 85.15%, Valid: 85.14%, Test: 85.31%
Run 01:
Highest Train: 88.05
Highest Valid: 88.10
  Final Train: 88.05
   Final Test: 88.21
All runs:
Highest Train: 88.05, nan
Highest Valid: 88.10, nan
  Final Train: 88.05, nan
   Final Test: 88.21, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 19.0725, Train: 85.82%, Valid: 85.70%, Test: 85.88%
Epoch: 25, Loss: 0.8892, Train: 85.60%, Valid: 85.57%, Test: 85.65%
Epoch: 50, Loss: 0.7130, Train: 86.70%, Valid: 86.57%, Test: 86.69%
Epoch: 75, Loss: 0.4469, Train: 85.61%, Valid: 85.43%, Test: 85.65%
Epoch: 100, Loss: 0.3895, Train: 84.97%, Valid: 84.93%, Test: 85.03%
Epoch: 125, Loss: 0.3821, Train: 85.45%, Valid: 85.23%, Test: 85.52%
Epoch: 150, Loss: 0.3885, Train: 85.49%, Valid: 85.27%, Test: 85.55%
Epoch: 175, Loss: 0.3698, Train: 85.54%, Valid: 85.34%, Test: 85.61%
Run 01:
Highest Train: 86.72
Highest Valid: 86.58
  Final Train: 86.72
   Final Test: 86.72
All runs:
Highest Train: 86.72, nan
Highest Valid: 86.58, nan
  Final Train: 86.72, nan
   Final Test: 86.72, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 393.0320, Train: 48.28%, Valid: 48.27%, Test: 48.39%
Epoch: 25, Loss: 72.1621, Train: 64.53%, Valid: 64.37%, Test: 64.40%
Epoch: 50, Loss: 9.1422, Train: 83.48%, Valid: 83.39%, Test: 83.55%
Epoch: 75, Loss: 7.3069, Train: 83.64%, Valid: 83.56%, Test: 83.68%
Epoch: 100, Loss: 9.0863, Train: 83.45%, Valid: 83.35%, Test: 83.48%
Epoch: 125, Loss: 9.3482, Train: 83.72%, Valid: 83.64%, Test: 83.76%
Epoch: 150, Loss: 7.4595, Train: 84.02%, Valid: 83.93%, Test: 84.08%
Epoch: 175, Loss: 7.9142, Train: 84.17%, Valid: 84.07%, Test: 84.23%
Run 01:
Highest Train: 84.35
Highest Valid: 84.24
  Final Train: 84.35
   Final Test: 84.40
All runs:
Highest Train: 84.35, nan
Highest Valid: 84.24, nan
  Final Train: 84.35, nan
   Final Test: 84.40, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.1448, Train: 87.72%, Valid: 87.75%, Test: 87.73%
Epoch: 25, Loss: 0.5603, Train: 87.80%, Valid: 87.86%, Test: 87.76%
Epoch: 50, Loss: 0.3900, Train: 87.99%, Valid: 88.07%, Test: 88.00%
Epoch: 75, Loss: 0.3747, Train: 87.31%, Valid: 87.22%, Test: 87.38%
Epoch: 100, Loss: 0.3632, Train: 85.76%, Valid: 85.84%, Test: 85.85%
Epoch: 125, Loss: 0.3566, Train: 85.96%, Valid: 85.99%, Test: 86.00%
Epoch: 150, Loss: 0.3520, Train: 85.70%, Valid: 85.71%, Test: 85.80%
Epoch: 175, Loss: 0.3475, Train: 85.67%, Valid: 85.70%, Test: 85.77%
Run 01:
Highest Train: 88.30
Highest Valid: 88.37
  Final Train: 88.30
   Final Test: 88.29
All runs:
Highest Train: 88.30, nan
Highest Valid: 88.37, nan
  Final Train: 88.30, nan
   Final Test: 88.29, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 8.3659, Train: 83.99%, Valid: 83.89%, Test: 84.04%
Epoch: 25, Loss: 0.8915, Train: 86.19%, Valid: 86.08%, Test: 86.24%
Epoch: 50, Loss: 0.6668, Train: 86.07%, Valid: 85.93%, Test: 86.12%
Epoch: 75, Loss: 0.4690, Train: 85.92%, Valid: 85.74%, Test: 85.99%
Epoch: 100, Loss: 0.3999, Train: 85.66%, Valid: 85.50%, Test: 85.71%
Epoch: 125, Loss: 0.3752, Train: 85.88%, Valid: 85.75%, Test: 85.95%
Epoch: 150, Loss: 0.3680, Train: 85.85%, Valid: 85.75%, Test: 85.94%
Epoch: 175, Loss: 0.3626, Train: 86.03%, Valid: 85.94%, Test: 86.08%
Run 01:
Highest Train: 87.94
Highest Valid: 87.78
  Final Train: 87.94
   Final Test: 87.91
All runs:
Highest Train: 87.94, nan
Highest Valid: 87.78, nan
  Final Train: 87.94, nan
   Final Test: 87.91, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 118.4753, Train: 45.94%, Valid: 45.97%, Test: 46.04%
Epoch: 25, Loss: 11.2682, Train: 81.78%, Valid: 81.65%, Test: 81.75%
Epoch: 50, Loss: 8.5207, Train: 84.31%, Valid: 84.28%, Test: 84.45%
Epoch: 75, Loss: 17.6809, Train: 82.99%, Valid: 82.91%, Test: 83.03%
Epoch: 100, Loss: 9.0257, Train: 83.07%, Valid: 82.99%, Test: 83.08%
Epoch: 125, Loss: 9.3590, Train: 83.52%, Valid: 83.42%, Test: 83.56%
Epoch: 150, Loss: 10.4355, Train: 83.44%, Valid: 83.36%, Test: 83.49%
Epoch: 175, Loss: 7.7646, Train: 83.62%, Valid: 83.51%, Test: 83.66%
Run 01:
Highest Train: 84.37
Highest Valid: 84.34
  Final Train: 84.37
   Final Test: 84.50
All runs:
Highest Train: 84.37, nan
Highest Valid: 84.34, nan
  Final Train: 84.37, nan
   Final Test: 84.50, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.5416, Train: 86.79%, Valid: 86.79%, Test: 86.80%
Epoch: 25, Loss: 0.4839, Train: 85.97%, Valid: 85.87%, Test: 85.97%
Epoch: 50, Loss: 0.4156, Train: 87.24%, Valid: 87.29%, Test: 87.26%
Epoch: 75, Loss: 0.3741, Train: 86.39%, Valid: 86.50%, Test: 86.42%
Epoch: 100, Loss: 0.3716, Train: 85.53%, Valid: 85.54%, Test: 85.68%
Epoch: 125, Loss: 0.3618, Train: 86.30%, Valid: 86.31%, Test: 86.36%
Epoch: 150, Loss: 0.3565, Train: 85.85%, Valid: 85.85%, Test: 85.91%
Epoch: 175, Loss: 0.3513, Train: 85.83%, Valid: 85.83%, Test: 85.92%
Run 01:
Highest Train: 88.14
Highest Valid: 88.22
  Final Train: 88.14
   Final Test: 88.18
All runs:
Highest Train: 88.14, nan
Highest Valid: 88.22, nan
  Final Train: 88.14, nan
   Final Test: 88.18, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.8290, Train: 85.34%, Valid: 85.15%, Test: 85.41%
Epoch: 25, Loss: 0.7905, Train: 85.45%, Valid: 85.29%, Test: 85.51%
Epoch: 50, Loss: 0.4418, Train: 85.40%, Valid: 85.22%, Test: 85.46%
Epoch: 75, Loss: 0.3852, Train: 85.50%, Valid: 85.31%, Test: 85.56%
Epoch: 100, Loss: 0.3730, Train: 85.55%, Valid: 85.36%, Test: 85.61%
Epoch: 125, Loss: 0.3675, Train: 85.59%, Valid: 85.40%, Test: 85.64%
Epoch: 150, Loss: 0.3639, Train: 85.65%, Valid: 85.47%, Test: 85.70%
Epoch: 175, Loss: 0.3603, Train: 85.72%, Valid: 85.55%, Test: 85.78%
Run 01:
Highest Train: 86.50
Highest Valid: 86.54
  Final Train: 86.50
   Final Test: 86.59
All runs:
Highest Train: 86.50, nan
Highest Valid: 86.54, nan
  Final Train: 86.50, nan
   Final Test: 86.59, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.001, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 294.3564, Train: 49.99%, Valid: 49.99%, Test: 50.00%
Epoch: 25, Loss: 322.4952, Train: 30.03%, Valid: 29.91%, Test: 29.63%
Epoch: 50, Loss: 45.7079, Train: 30.06%, Valid: 29.89%, Test: 29.66%
Epoch: 75, Loss: 22.6130, Train: 83.36%, Valid: 83.29%, Test: 83.40%
Epoch: 100, Loss: 180.9943, Train: 84.39%, Valid: 84.28%, Test: 84.43%
Epoch: 125, Loss: 29.7282, Train: 84.55%, Valid: 84.42%, Test: 84.60%
Epoch: 150, Loss: 22.7736, Train: 84.67%, Valid: 84.54%, Test: 84.73%
Epoch: 175, Loss: 19.8974, Train: 84.75%, Valid: 84.62%, Test: 84.81%
Run 01:
Highest Train: 84.86
Highest Valid: 84.73
  Final Train: 84.86
   Final Test: 84.95
All runs:
Highest Train: 84.86, nan
Highest Valid: 84.73, nan
  Final Train: 84.86, nan
   Final Test: 84.95, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 8.8950, Train: 14.48%, Valid: 14.55%, Test: 14.46%
Epoch: 25, Loss: 0.3956, Train: 86.31%, Valid: 86.29%, Test: 86.42%
Epoch: 50, Loss: 0.3692, Train: 86.49%, Valid: 86.44%, Test: 86.58%
Epoch: 75, Loss: 0.3674, Train: 86.43%, Valid: 86.36%, Test: 86.51%
Epoch: 100, Loss: 0.3668, Train: 86.53%, Valid: 86.47%, Test: 86.61%
Epoch: 125, Loss: 0.3663, Train: 86.57%, Valid: 86.52%, Test: 86.66%
Epoch: 150, Loss: 0.3654, Train: 86.52%, Valid: 86.50%, Test: 86.61%
Epoch: 175, Loss: 0.3647, Train: 86.49%, Valid: 86.47%, Test: 86.58%
Run 01:
Highest Train: 86.61
Highest Valid: 86.56
  Final Train: 86.60
   Final Test: 86.69
All runs:
Highest Train: 86.61, nan
Highest Valid: 86.56, nan
  Final Train: 86.60, nan
   Final Test: 86.69, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 12.1562, Train: 16.61%, Valid: 16.51%, Test: 16.46%
Epoch: 25, Loss: 0.4826, Train: 85.72%, Valid: 85.59%, Test: 85.74%
Epoch: 50, Loss: 0.4155, Train: 85.87%, Valid: 85.75%, Test: 85.90%
Epoch: 75, Loss: 0.3890, Train: 85.72%, Valid: 85.61%, Test: 85.75%
Epoch: 100, Loss: 0.3844, Train: 85.79%, Valid: 85.66%, Test: 85.81%
Epoch: 125, Loss: 0.3813, Train: 85.99%, Valid: 85.86%, Test: 86.00%
Epoch: 150, Loss: 0.3788, Train: 86.12%, Valid: 85.98%, Test: 86.11%
Epoch: 175, Loss: 0.3751, Train: 86.13%, Valid: 86.00%, Test: 86.12%
Run 01:
Highest Train: 86.26
Highest Valid: 86.13
  Final Train: 86.26
   Final Test: 86.25
All runs:
Highest Train: 86.26, nan
Highest Valid: 86.13, nan
  Final Train: 86.26, nan
   Final Test: 86.25, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 69341.9062, Train: 34.92%, Valid: 35.11%, Test: 35.16%
Epoch: 25, Loss: 1527.8873, Train: 56.28%, Valid: 56.05%, Test: 55.96%
Epoch: 50, Loss: 226.5066, Train: 55.59%, Valid: 55.44%, Test: 55.44%
Epoch: 75, Loss: 112.0910, Train: 52.36%, Valid: 52.23%, Test: 52.27%
Epoch: 100, Loss: 89.8396, Train: 52.10%, Valid: 52.07%, Test: 51.96%
Epoch: 125, Loss: 80.1300, Train: 52.40%, Valid: 52.30%, Test: 52.15%
Epoch: 150, Loss: 74.2383, Train: 52.24%, Valid: 52.11%, Test: 51.97%
Epoch: 175, Loss: 70.8053, Train: 52.12%, Valid: 51.95%, Test: 51.81%
Run 01:
Highest Train: 57.73
Highest Valid: 57.52
  Final Train: 57.73
   Final Test: 57.41
All runs:
Highest Train: 57.73, nan
Highest Valid: 57.52, nan
  Final Train: 57.73, nan
   Final Test: 57.41, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 15.2567, Train: 82.27%, Valid: 82.20%, Test: 82.37%
Epoch: 25, Loss: 0.5214, Train: 85.78%, Valid: 85.87%, Test: 85.84%
Epoch: 50, Loss: 0.4342, Train: 86.65%, Valid: 86.71%, Test: 86.69%
Epoch: 75, Loss: 0.4101, Train: 84.94%, Valid: 84.79%, Test: 85.04%
Epoch: 100, Loss: 0.4098, Train: 85.34%, Valid: 85.16%, Test: 85.44%
Epoch: 125, Loss: 0.4030, Train: 85.40%, Valid: 85.24%, Test: 85.54%
Epoch: 150, Loss: 0.3978, Train: 85.45%, Valid: 85.28%, Test: 85.56%
Epoch: 175, Loss: 0.3929, Train: 85.46%, Valid: 85.27%, Test: 85.56%
Run 01:
Highest Train: 86.87
Highest Valid: 86.90
  Final Train: 86.86
   Final Test: 86.91
All runs:
Highest Train: 86.87, nan
Highest Valid: 86.90, nan
  Final Train: 86.86, nan
   Final Test: 86.91, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 1.6693, Train: 83.40%, Valid: 83.44%, Test: 83.51%
Epoch: 25, Loss: 0.4395, Train: 86.58%, Valid: 86.56%, Test: 86.66%
Epoch: 50, Loss: 0.4065, Train: 86.66%, Valid: 86.62%, Test: 86.72%
Epoch: 75, Loss: 0.3992, Train: 86.61%, Valid: 86.57%, Test: 86.66%
Epoch: 100, Loss: 0.3957, Train: 86.55%, Valid: 86.53%, Test: 86.62%
Epoch: 125, Loss: 0.3985, Train: 86.80%, Valid: 86.77%, Test: 86.86%
Epoch: 150, Loss: 0.3878, Train: 87.88%, Valid: 87.88%, Test: 87.89%
Epoch: 175, Loss: 0.3878, Train: 87.49%, Valid: 87.51%, Test: 87.49%
Run 01:
Highest Train: 88.08
Highest Valid: 88.06
  Final Train: 88.07
   Final Test: 88.10
All runs:
Highest Train: 88.08, nan
Highest Valid: 88.06, nan
  Final Train: 88.07, nan
   Final Test: 88.10, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 1609.6461, Train: 24.93%, Valid: 25.15%, Test: 25.09%
Epoch: 25, Loss: 513.5671, Train: 16.88%, Valid: 16.84%, Test: 16.65%
Epoch: 50, Loss: 357.8515, Train: 20.35%, Valid: 20.34%, Test: 20.04%
Epoch: 75, Loss: 317.5314, Train: 30.22%, Valid: 30.05%, Test: 29.72%
Epoch: 100, Loss: 246.5273, Train: 31.61%, Valid: 31.48%, Test: 31.20%
Epoch: 125, Loss: 208.5532, Train: 32.85%, Valid: 32.73%, Test: 32.44%
Epoch: 150, Loss: 219.9817, Train: 33.74%, Valid: 33.59%, Test: 33.29%
Epoch: 175, Loss: 181.7976, Train: 34.23%, Valid: 34.10%, Test: 33.76%
Run 01:
Highest Train: 35.71
Highest Valid: 35.54
  Final Train: 35.71
   Final Test: 35.25
All runs:
Highest Train: 35.71, nan
Highest Valid: 35.54, nan
  Final Train: 35.71, nan
   Final Test: 35.25, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 24.2738, Train: 16.75%, Valid: 16.92%, Test: 16.67%
Epoch: 25, Loss: 4.9805, Train: 15.67%, Valid: 15.65%, Test: 15.59%
Epoch: 50, Loss: 0.8869, Train: 87.40%, Valid: 87.28%, Test: 87.42%
Epoch: 75, Loss: 0.5666, Train: 87.50%, Valid: 87.38%, Test: 87.53%
Epoch: 100, Loss: 0.4709, Train: 87.50%, Valid: 87.37%, Test: 87.51%
Epoch: 125, Loss: 0.4463, Train: 87.50%, Valid: 87.37%, Test: 87.51%
Epoch: 150, Loss: 0.4464, Train: 87.45%, Valid: 87.31%, Test: 87.47%
Epoch: 175, Loss: 0.4473, Train: 85.32%, Valid: 85.35%, Test: 85.41%
Run 01:
Highest Train: 87.51
Highest Valid: 87.38
  Final Train: 87.50
   Final Test: 87.53
All runs:
Highest Train: 87.51, nan
Highest Valid: 87.38, nan
  Final Train: 87.50, nan
   Final Test: 87.53, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 4.6442, Train: 73.54%, Valid: 73.71%, Test: 73.90%
Epoch: 25, Loss: 0.6332, Train: 84.43%, Valid: 84.30%, Test: 84.52%
Epoch: 50, Loss: 0.4504, Train: 86.00%, Valid: 86.01%, Test: 86.12%
Epoch: 75, Loss: 0.4639, Train: 84.78%, Valid: 84.64%, Test: 84.87%
Epoch: 100, Loss: 0.4279, Train: 85.20%, Valid: 85.15%, Test: 85.30%
Epoch: 125, Loss: 0.4318, Train: 86.49%, Valid: 86.51%, Test: 86.60%
Epoch: 150, Loss: 0.4231, Train: 86.46%, Valid: 86.48%, Test: 86.58%
Epoch: 175, Loss: 0.4175, Train: 86.42%, Valid: 86.43%, Test: 86.54%
Run 01:
Highest Train: 86.51
Highest Valid: 86.53
  Final Train: 86.51
   Final Test: 86.63
All runs:
Highest Train: 86.51, nan
Highest Valid: 86.53, nan
  Final Train: 86.51, nan
   Final Test: 86.63, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 591.1024, Train: 66.38%, Valid: 66.17%, Test: 66.06%
Epoch: 25, Loss: 21.4615, Train: 80.36%, Valid: 80.27%, Test: 80.27%
Epoch: 50, Loss: 16.6999, Train: 73.73%, Valid: 73.56%, Test: 73.53%
Epoch: 75, Loss: 18.5947, Train: 75.72%, Valid: 75.57%, Test: 75.50%
Epoch: 100, Loss: 14.7077, Train: 76.93%, Valid: 76.80%, Test: 76.74%
Epoch: 125, Loss: 16.9010, Train: 78.47%, Valid: 78.33%, Test: 78.35%
Epoch: 150, Loss: 12.7280, Train: 80.07%, Valid: 79.95%, Test: 79.97%
Epoch: 175, Loss: 11.9731, Train: 79.32%, Valid: 79.17%, Test: 79.21%
Run 01:
Highest Train: 81.48
Highest Valid: 81.40
  Final Train: 81.48
   Final Test: 81.55
All runs:
Highest Train: 81.48, nan
Highest Valid: 81.40, nan
  Final Train: 81.48, nan
   Final Test: 81.55, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 25.3454, Train: 12.86%, Valid: 12.97%, Test: 12.86%
Epoch: 25, Loss: 7.0486, Train: 17.75%, Valid: 17.73%, Test: 17.57%
Epoch: 50, Loss: 1.4055, Train: 85.01%, Valid: 85.01%, Test: 85.04%
Epoch: 75, Loss: 0.6339, Train: 85.79%, Valid: 85.78%, Test: 85.90%
Epoch: 100, Loss: 0.5179, Train: 86.44%, Valid: 86.42%, Test: 86.57%
Epoch: 125, Loss: 0.4975, Train: 86.53%, Valid: 86.50%, Test: 86.67%
Epoch: 150, Loss: 0.5019, Train: 86.59%, Valid: 86.55%, Test: 86.74%
Epoch: 175, Loss: 0.4829, Train: 86.61%, Valid: 86.58%, Test: 86.77%
Run 01:
Highest Train: 86.62
Highest Valid: 86.59
  Final Train: 86.61
   Final Test: 86.76
All runs:
Highest Train: 86.62, nan
Highest Valid: 86.59, nan
  Final Train: 86.61, nan
   Final Test: 86.76, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 6.0692, Train: 83.20%, Valid: 83.21%, Test: 83.27%
Epoch: 25, Loss: 0.7799, Train: 85.55%, Valid: 85.41%, Test: 85.55%
Epoch: 50, Loss: 0.5734, Train: 86.38%, Valid: 86.24%, Test: 86.35%
Epoch: 75, Loss: 0.5476, Train: 86.80%, Valid: 86.66%, Test: 86.73%
Epoch: 100, Loss: 0.5684, Train: 86.78%, Valid: 86.66%, Test: 86.70%
Epoch: 125, Loss: 0.5504, Train: 86.62%, Valid: 86.52%, Test: 86.53%
Epoch: 150, Loss: 0.5154, Train: 86.62%, Valid: 86.52%, Test: 86.56%
Epoch: 175, Loss: 0.5044, Train: 86.64%, Valid: 86.56%, Test: 86.59%
Run 01:
Highest Train: 86.98
Highest Valid: 86.84
  Final Train: 86.97
   Final Test: 86.93
All runs:
Highest Train: 86.98, nan
Highest Valid: 86.84, nan
  Final Train: 86.97, nan
   Final Test: 86.93, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 115.3692, Train: 42.95%, Valid: 43.14%, Test: 43.29%
Epoch: 25, Loss: 11.2262, Train: 44.43%, Valid: 44.62%, Test: 44.74%
Epoch: 50, Loss: 9.7449, Train: 45.14%, Valid: 45.32%, Test: 45.43%
Epoch: 75, Loss: 9.3185, Train: 58.91%, Valid: 59.29%, Test: 59.30%
Epoch: 100, Loss: 7.2666, Train: 81.55%, Valid: 81.44%, Test: 81.48%
Epoch: 125, Loss: 10.1378, Train: 82.01%, Valid: 81.93%, Test: 81.96%
Epoch: 150, Loss: 11.3889, Train: 82.20%, Valid: 82.16%, Test: 82.24%
Epoch: 175, Loss: 7.0588, Train: 82.57%, Valid: 82.62%, Test: 82.70%
Run 01:
Highest Train: 82.72
Highest Valid: 82.76
  Final Train: 82.72
   Final Test: 82.82
All runs:
Highest Train: 82.72, nan
Highest Valid: 82.76, nan
  Final Train: 82.72, nan
   Final Test: 82.82, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 15.6041, Train: 16.84%, Valid: 16.99%, Test: 16.75%
Epoch: 25, Loss: 0.4896, Train: 83.90%, Valid: 83.73%, Test: 84.07%
Epoch: 50, Loss: 0.3902, Train: 88.08%, Valid: 88.14%, Test: 88.09%
Epoch: 75, Loss: 0.3709, Train: 84.08%, Valid: 83.87%, Test: 84.22%
Epoch: 100, Loss: 0.3682, Train: 84.11%, Valid: 83.92%, Test: 84.25%
Epoch: 125, Loss: 0.3669, Train: 84.10%, Valid: 83.90%, Test: 84.23%
Epoch: 150, Loss: 0.3659, Train: 84.93%, Valid: 84.94%, Test: 85.07%
Epoch: 175, Loss: 0.3653, Train: 84.96%, Valid: 84.97%, Test: 85.10%
Run 01:
Highest Train: 88.09
Highest Valid: 88.15
  Final Train: 88.08
   Final Test: 88.10
All runs:
Highest Train: 88.09, nan
Highest Valid: 88.15, nan
  Final Train: 88.08, nan
   Final Test: 88.10, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 15.2273, Train: 15.63%, Valid: 15.58%, Test: 15.58%
Epoch: 25, Loss: 2.7386, Train: 75.17%, Valid: 75.28%, Test: 75.53%
Epoch: 50, Loss: 0.6754, Train: 84.70%, Valid: 84.56%, Test: 84.80%
Epoch: 75, Loss: 0.5384, Train: 84.67%, Valid: 84.52%, Test: 84.81%
Epoch: 100, Loss: 0.4905, Train: 84.67%, Valid: 84.51%, Test: 84.80%
Epoch: 125, Loss: 0.4571, Train: 84.50%, Valid: 84.35%, Test: 84.63%
Epoch: 150, Loss: 0.4635, Train: 84.66%, Valid: 84.50%, Test: 84.76%
Epoch: 175, Loss: 0.4591, Train: 84.75%, Valid: 84.59%, Test: 84.85%
Run 01:
Highest Train: 85.24
Highest Valid: 85.07
  Final Train: 85.24
   Final Test: 85.39
All runs:
Highest Train: 85.24, nan
Highest Valid: 85.07, nan
  Final Train: 85.24, nan
   Final Test: 85.39, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 12774370.0000, Train: 57.49%, Valid: 57.51%, Test: 57.24%
Epoch: 25, Loss: 3244079.2500, Train: 44.46%, Valid: 44.45%, Test: 44.43%
Epoch: 50, Loss: 30274694.0000, Train: 45.46%, Valid: 45.31%, Test: 45.50%
Epoch: 75, Loss: 34193396.0000, Train: 56.61%, Valid: 56.55%, Test: 56.56%
Epoch: 100, Loss: 37229540.0000, Train: 44.68%, Valid: 44.65%, Test: 44.72%
Epoch: 125, Loss: 4699480.5000, Train: 56.11%, Valid: 56.14%, Test: 56.11%
Epoch: 150, Loss: 1643182.2500, Train: 56.19%, Valid: 56.22%, Test: 56.20%
Epoch: 175, Loss: 21338698.0000, Train: 56.02%, Valid: 56.07%, Test: 56.03%
Run 01:
Highest Train: 60.93
Highest Valid: 60.80
  Final Train: 60.93
   Final Test: 60.73
All runs:
Highest Train: 60.93, nan
Highest Valid: 60.80, nan
  Final Train: 60.93, nan
   Final Test: 60.73, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 7.8483, Train: 14.47%, Valid: 14.43%, Test: 14.37%
Epoch: 25, Loss: 0.6845, Train: 85.48%, Valid: 85.44%, Test: 85.61%
Epoch: 50, Loss: 0.4217, Train: 85.46%, Valid: 85.43%, Test: 85.60%
Epoch: 75, Loss: 0.4077, Train: 85.43%, Valid: 85.40%, Test: 85.57%
Epoch: 100, Loss: 0.3978, Train: 85.49%, Valid: 85.46%, Test: 85.62%
Epoch: 125, Loss: 0.3858, Train: 85.73%, Valid: 85.73%, Test: 85.86%
Epoch: 150, Loss: 0.3874, Train: 85.73%, Valid: 85.73%, Test: 85.84%
Epoch: 175, Loss: 0.3833, Train: 85.73%, Valid: 85.73%, Test: 85.82%
Run 01:
Highest Train: 87.20
Highest Valid: 87.14
  Final Train: 87.20
   Final Test: 87.16
All runs:
Highest Train: 87.20, nan
Highest Valid: 87.14, nan
  Final Train: 87.20, nan
   Final Test: 87.16, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 13.2611, Train: 15.87%, Valid: 15.98%, Test: 15.80%
Epoch: 25, Loss: 0.6460, Train: 85.77%, Valid: 85.73%, Test: 85.84%
Epoch: 50, Loss: 0.4279, Train: 85.22%, Valid: 85.06%, Test: 85.31%
Epoch: 75, Loss: 0.4158, Train: 86.55%, Valid: 86.48%, Test: 86.61%
Epoch: 100, Loss: 0.4180, Train: 86.14%, Valid: 86.04%, Test: 86.20%
Epoch: 125, Loss: 0.4126, Train: 85.93%, Valid: 85.83%, Test: 86.01%
Epoch: 150, Loss: 0.4041, Train: 86.08%, Valid: 85.99%, Test: 86.14%
Epoch: 175, Loss: 0.3982, Train: 85.94%, Valid: 85.84%, Test: 86.01%
Run 01:
Highest Train: 86.56
Highest Valid: 86.49
  Final Train: 86.56
   Final Test: 86.62
All runs:
Highest Train: 86.56, nan
Highest Valid: 86.49, nan
  Final Train: 86.56, nan
   Final Test: 86.62, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 178.5861, Train: 63.79%, Valid: 63.56%, Test: 63.54%
Epoch: 25, Loss: 16.0441, Train: 57.49%, Valid: 57.43%, Test: 57.33%
Epoch: 50, Loss: 113.0506, Train: 44.90%, Valid: 45.01%, Test: 44.95%
Epoch: 75, Loss: 9.7538, Train: 44.91%, Valid: 45.14%, Test: 45.28%
Epoch: 100, Loss: 15.0517, Train: 43.25%, Valid: 43.40%, Test: 43.53%
Epoch: 125, Loss: 46.8336, Train: 71.05%, Valid: 70.87%, Test: 70.77%
Epoch: 150, Loss: 19.2223, Train: 42.56%, Valid: 42.73%, Test: 42.86%
Epoch: 175, Loss: 204.7601, Train: 67.06%, Valid: 66.89%, Test: 66.75%
Run 01:
Highest Train: 79.58
Highest Valid: 79.44
  Final Train: 79.58
   Final Test: 79.52
All runs:
Highest Train: 79.58, nan
Highest Valid: 79.44, nan
  Final Train: 79.58, nan
   Final Test: 79.52, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 2.2109, Train: 85.99%, Valid: 86.00%, Test: 85.97%
Epoch: 25, Loss: 0.4799, Train: 86.30%, Valid: 86.32%, Test: 86.44%
Epoch: 50, Loss: 0.4392, Train: 84.61%, Valid: 84.45%, Test: 84.71%
Epoch: 75, Loss: 0.4264, Train: 85.58%, Valid: 85.58%, Test: 85.66%
Epoch: 100, Loss: 0.4170, Train: 85.77%, Valid: 85.76%, Test: 85.86%
Epoch: 125, Loss: 0.4144, Train: 85.89%, Valid: 85.88%, Test: 85.96%
Epoch: 150, Loss: 0.4129, Train: 86.25%, Valid: 86.24%, Test: 86.28%
Epoch: 175, Loss: 0.4064, Train: 85.97%, Valid: 86.00%, Test: 86.02%
Run 01:
Highest Train: 86.36
Highest Valid: 86.39
  Final Train: 86.36
   Final Test: 86.48
All runs:
Highest Train: 86.36, nan
Highest Valid: 86.39, nan
  Final Train: 86.36, nan
   Final Test: 86.48, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 6.9060, Train: 14.32%, Valid: 14.46%, Test: 14.21%
Epoch: 25, Loss: 0.6987, Train: 85.51%, Valid: 85.49%, Test: 85.65%
Epoch: 50, Loss: 0.4861, Train: 86.17%, Valid: 86.09%, Test: 86.26%
Epoch: 75, Loss: 0.4713, Train: 86.16%, Valid: 86.04%, Test: 86.19%
Epoch: 100, Loss: 0.4545, Train: 84.38%, Valid: 84.30%, Test: 84.51%
Epoch: 125, Loss: 0.4601, Train: 84.25%, Valid: 84.17%, Test: 84.41%
Epoch: 150, Loss: 0.4417, Train: 84.16%, Valid: 84.08%, Test: 84.34%
Epoch: 175, Loss: 0.4476, Train: 83.90%, Valid: 83.81%, Test: 84.08%
Run 01:
Highest Train: 87.35
Highest Valid: 87.28
  Final Train: 87.35
   Final Test: 87.37
All runs:
Highest Train: 87.35, nan
Highest Valid: 87.28, nan
  Final Train: 87.35, nan
   Final Test: 87.37, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 250.8380, Train: 47.00%, Valid: 47.14%, Test: 47.09%
Epoch: 25, Loss: 41.3118, Train: 48.94%, Valid: 48.97%, Test: 48.99%
Epoch: 50, Loss: 12.6420, Train: 55.80%, Valid: 55.64%, Test: 55.70%
Epoch: 75, Loss: 28.5563, Train: 61.32%, Valid: 61.04%, Test: 60.99%
Epoch: 100, Loss: 23.0088, Train: 55.65%, Valid: 55.52%, Test: 55.55%
Epoch: 125, Loss: 133.6746, Train: 49.04%, Valid: 49.01%, Test: 49.09%
Epoch: 150, Loss: 178.5465, Train: 55.02%, Valid: 54.91%, Test: 54.99%
Epoch: 175, Loss: 11.6928, Train: 55.97%, Valid: 55.80%, Test: 55.90%
Run 01:
Highest Train: 66.54
Highest Valid: 66.28
  Final Train: 66.54
   Final Test: 66.19
All runs:
Highest Train: 66.54, nan
Highest Valid: 66.28, nan
  Final Train: 66.54, nan
   Final Test: 66.19, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 5.2212, Train: 83.89%, Valid: 83.75%, Test: 83.99%
Epoch: 25, Loss: 0.7011, Train: 86.33%, Valid: 86.32%, Test: 86.45%
Epoch: 50, Loss: 0.5439, Train: 86.30%, Valid: 86.27%, Test: 86.42%
Epoch: 75, Loss: 0.5502, Train: 85.79%, Valid: 85.65%, Test: 85.82%
Epoch: 100, Loss: 0.5104, Train: 85.85%, Valid: 85.72%, Test: 85.89%
Epoch: 125, Loss: 0.5150, Train: 85.95%, Valid: 85.82%, Test: 85.98%
Epoch: 150, Loss: 0.4852, Train: 85.95%, Valid: 85.82%, Test: 85.98%
Epoch: 175, Loss: 0.4721, Train: 85.96%, Valid: 85.82%, Test: 85.99%
Run 01:
Highest Train: 86.44
Highest Valid: 86.45
  Final Train: 86.44
   Final Test: 86.47
All runs:
Highest Train: 86.44, nan
Highest Valid: 86.45, nan
  Final Train: 86.44, nan
   Final Test: 86.47, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 13.1607, Train: 20.40%, Valid: 20.22%, Test: 20.06%
Epoch: 25, Loss: 1.2470, Train: 86.14%, Valid: 86.02%, Test: 86.16%
Epoch: 50, Loss: 0.5502, Train: 85.42%, Valid: 85.36%, Test: 85.49%
Epoch: 75, Loss: 0.5008, Train: 85.44%, Valid: 85.39%, Test: 85.50%
Epoch: 100, Loss: 0.4900, Train: 85.50%, Valid: 85.45%, Test: 85.56%
Epoch: 125, Loss: 0.4973, Train: 85.52%, Valid: 85.47%, Test: 85.58%
Epoch: 150, Loss: 0.4912, Train: 85.51%, Valid: 85.45%, Test: 85.57%
Epoch: 175, Loss: 0.4958, Train: 85.49%, Valid: 85.45%, Test: 85.56%
Run 01:
Highest Train: 86.24
Highest Valid: 86.11
  Final Train: 86.24
   Final Test: 86.24
All runs:
Highest Train: 86.24, nan
Highest Valid: 86.11, nan
  Final Train: 86.24, nan
   Final Test: 86.24, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 310.4109, Train: 62.85%, Valid: 62.68%, Test: 62.57%
Epoch: 25, Loss: 83.7478, Train: 74.54%, Valid: 74.63%, Test: 74.68%
Epoch: 50, Loss: 86.4659, Train: 70.38%, Valid: 70.40%, Test: 70.52%
Epoch: 75, Loss: 209.6819, Train: 79.32%, Valid: 79.24%, Test: 79.47%
Epoch: 100, Loss: 122.2522, Train: 56.97%, Valid: 56.98%, Test: 57.19%
Epoch: 125, Loss: 261.8395, Train: 73.97%, Valid: 73.79%, Test: 73.83%
Epoch: 150, Loss: 13.4981, Train: 46.47%, Valid: 46.64%, Test: 46.78%
Epoch: 175, Loss: 29.0288, Train: 80.88%, Valid: 80.68%, Test: 80.85%
Run 01:
Highest Train: 83.27
Highest Valid: 83.25
  Final Train: 83.27
   Final Test: 83.47
All runs:
Highest Train: 83.27, nan
Highest Valid: 83.25, nan
  Final Train: 83.27, nan
   Final Test: 83.47, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 7.0546, Train: 14.01%, Valid: 13.89%, Test: 14.00%
Epoch: 25, Loss: 0.4077, Train: 84.40%, Valid: 84.22%, Test: 84.54%
Epoch: 50, Loss: 0.3739, Train: 85.36%, Valid: 85.15%, Test: 85.49%
Epoch: 75, Loss: 0.3667, Train: 85.97%, Valid: 85.88%, Test: 86.07%
Epoch: 100, Loss: 0.3662, Train: 86.10%, Valid: 86.10%, Test: 86.25%
Epoch: 125, Loss: 0.3657, Train: 86.37%, Valid: 86.36%, Test: 86.53%
Epoch: 150, Loss: 0.3651, Train: 86.21%, Valid: 86.20%, Test: 86.32%
Epoch: 175, Loss: 0.3644, Train: 86.53%, Valid: 86.49%, Test: 86.66%
Run 01:
Highest Train: 86.66
Highest Valid: 86.67
  Final Train: 86.66
   Final Test: 86.82
All runs:
Highest Train: 86.66, nan
Highest Valid: 86.67, nan
  Final Train: 86.66, nan
   Final Test: 86.82, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 6.8172, Train: 22.57%, Valid: 22.48%, Test: 22.16%
Epoch: 25, Loss: 0.6268, Train: 85.94%, Valid: 85.81%, Test: 85.99%
Epoch: 50, Loss: 0.5075, Train: 86.05%, Valid: 85.93%, Test: 86.08%
Epoch: 75, Loss: 0.4720, Train: 86.02%, Valid: 85.92%, Test: 86.07%
Epoch: 100, Loss: 0.4641, Train: 85.99%, Valid: 85.90%, Test: 86.01%
Epoch: 125, Loss: 0.4573, Train: 85.80%, Valid: 85.71%, Test: 85.81%
Epoch: 150, Loss: 0.4630, Train: 85.27%, Valid: 85.11%, Test: 85.32%
Epoch: 175, Loss: 0.4734, Train: 85.24%, Valid: 85.07%, Test: 85.30%
Run 01:
Highest Train: 86.10
Highest Valid: 86.01
  Final Train: 86.10
   Final Test: 86.14
All runs:
Highest Train: 86.10, nan
Highest Valid: 86.01, nan
  Final Train: 86.10, nan
   Final Test: 86.14, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 34666686830102970368.0000, Train: 51.85%, Valid: 51.93%, Test: 51.81%
Epoch: 25, Loss: 707777280.0000, Train: 47.57%, Valid: 47.55%, Test: 47.70%
Epoch: 50, Loss: 311054.4375, Train: 53.89%, Valid: 53.87%, Test: 53.67%
Epoch: 75, Loss: 4655474.0000, Train: 53.39%, Valid: 53.37%, Test: 53.14%
Epoch: 100, Loss: 87613.2344, Train: 58.87%, Valid: 58.69%, Test: 58.53%
Epoch: 125, Loss: 63864028.0000, Train: 47.57%, Valid: 47.59%, Test: 47.72%
Epoch: 150, Loss: 167752.5000, Train: 45.11%, Valid: 45.02%, Test: 45.32%
Epoch: 175, Loss: 7679351.5000, Train: 47.32%, Valid: 47.32%, Test: 47.52%
Run 01:
Highest Train: 61.40
Highest Valid: 61.23
  Final Train: 61.40
   Final Test: 61.11
All runs:
Highest Train: 61.40, nan
Highest Valid: 61.23, nan
  Final Train: 61.40, nan
   Final Test: 61.11, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 9.7693, Train: 13.06%, Valid: 12.98%, Test: 12.98%
Epoch: 25, Loss: 1.9199, Train: 29.91%, Valid: 29.59%, Test: 29.36%
Epoch: 50, Loss: 0.4652, Train: 86.06%, Valid: 86.05%, Test: 86.16%
Epoch: 75, Loss: 0.3845, Train: 86.33%, Valid: 86.31%, Test: 86.44%
Epoch: 100, Loss: 0.3788, Train: 86.38%, Valid: 86.38%, Test: 86.49%
Epoch: 125, Loss: 0.3746, Train: 86.34%, Valid: 86.34%, Test: 86.47%
Epoch: 150, Loss: 0.3775, Train: 86.30%, Valid: 86.29%, Test: 86.43%
Epoch: 175, Loss: 0.3764, Train: 86.39%, Valid: 86.38%, Test: 86.50%
Run 01:
Highest Train: 86.40
Highest Valid: 86.39
  Final Train: 86.39
   Final Test: 86.50
All runs:
Highest Train: 86.40, nan
Highest Valid: 86.39, nan
  Final Train: 86.39, nan
   Final Test: 86.50, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 0.7195, Train: 83.10%, Valid: 83.17%, Test: 83.28%
Epoch: 25, Loss: 0.4198, Train: 85.57%, Valid: 85.44%, Test: 85.63%
Epoch: 50, Loss: 0.3992, Train: 85.50%, Valid: 85.36%, Test: 85.56%
Epoch: 75, Loss: 0.3912, Train: 85.26%, Valid: 85.15%, Test: 85.31%
Epoch: 100, Loss: 0.3846, Train: 85.29%, Valid: 85.15%, Test: 85.36%
Epoch: 125, Loss: 0.3766, Train: 86.03%, Valid: 85.95%, Test: 86.11%
Epoch: 150, Loss: 0.3793, Train: 86.08%, Valid: 86.00%, Test: 86.12%
Epoch: 175, Loss: 0.3757, Train: 85.68%, Valid: 85.61%, Test: 85.71%
Run 01:
Highest Train: 86.09
Highest Valid: 86.01
  Final Train: 86.09
   Final Test: 86.14
All runs:
Highest Train: 86.09, nan
Highest Valid: 86.01, nan
  Final Train: 86.09, nan
   Final Test: 86.14, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 8.6972, Train: 50.02%, Valid: 50.02%, Test: 50.02%
Epoch: 25, Loss: 21.2029, Train: 49.98%, Valid: 49.98%, Test: 49.98%
Epoch: 50, Loss: 11.3903, Train: 49.97%, Valid: 49.98%, Test: 49.97%
Epoch: 75, Loss: 8.4910, Train: 49.96%, Valid: 49.97%, Test: 49.94%
Epoch: 100, Loss: 12.5368, Train: 50.03%, Valid: 50.05%, Test: 50.07%
Epoch: 125, Loss: 7.1027, Train: 50.13%, Valid: 50.07%, Test: 50.22%
Epoch: 150, Loss: 8.0934, Train: 51.90%, Valid: 51.79%, Test: 51.92%
Epoch: 175, Loss: 7.5216, Train: 45.28%, Valid: 45.55%, Test: 45.62%
Run 01:
Highest Train: 75.88
Highest Valid: 75.65
  Final Train: 75.88
   Final Test: 75.77
All runs:
Highest Train: 75.88, nan
Highest Valid: 75.65, nan
  Final Train: 75.88, nan
   Final Test: 75.77, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 2.6305, Train: 85.07%, Valid: 85.05%, Test: 85.19%
Epoch: 25, Loss: 0.5178, Train: 85.26%, Valid: 85.24%, Test: 85.37%
Epoch: 50, Loss: 0.4437, Train: 87.76%, Valid: 87.74%, Test: 87.76%
Epoch: 75, Loss: 0.4218, Train: 87.85%, Valid: 87.91%, Test: 87.89%
Epoch: 100, Loss: 0.4280, Train: 87.30%, Valid: 87.33%, Test: 87.39%
Epoch: 125, Loss: 0.4098, Train: 86.69%, Valid: 86.76%, Test: 86.76%
Epoch: 150, Loss: 0.4098, Train: 86.61%, Valid: 86.67%, Test: 86.72%
Epoch: 175, Loss: 0.4035, Train: 86.51%, Valid: 86.51%, Test: 86.60%
Run 01:
Highest Train: 88.10
Highest Valid: 88.17
  Final Train: 88.10
   Final Test: 88.11
All runs:
Highest Train: 88.10, nan
Highest Valid: 88.17, nan
  Final Train: 88.10, nan
   Final Test: 88.11, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 1.5330, Train: 85.76%, Valid: 85.62%, Test: 85.75%
Epoch: 25, Loss: 0.4276, Train: 84.92%, Valid: 84.87%, Test: 85.00%
Epoch: 50, Loss: 0.4123, Train: 85.09%, Valid: 85.05%, Test: 85.16%
Epoch: 75, Loss: 0.4161, Train: 85.09%, Valid: 85.05%, Test: 85.16%
Epoch: 100, Loss: 0.4113, Train: 85.04%, Valid: 84.99%, Test: 85.11%
Epoch: 125, Loss: 0.4023, Train: 85.09%, Valid: 85.04%, Test: 85.15%
Epoch: 150, Loss: 0.3944, Train: 85.06%, Valid: 85.02%, Test: 85.12%
Epoch: 175, Loss: 0.3914, Train: 85.08%, Valid: 85.04%, Test: 85.15%
Run 01:
Highest Train: 85.89
Highest Valid: 85.76
  Final Train: 85.86
   Final Test: 85.97
All runs:
Highest Train: 85.89, nan
Highest Valid: 85.76, nan
  Final Train: 85.86, nan
   Final Test: 85.97, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 86.5723, Train: 48.79%, Valid: 48.83%, Test: 48.85%
Epoch: 25, Loss: 8.8599, Train: 49.64%, Valid: 49.63%, Test: 49.58%
Epoch: 50, Loss: 8.4398, Train: 49.60%, Valid: 49.62%, Test: 49.57%
Epoch: 75, Loss: 8.5743, Train: 49.62%, Valid: 49.64%, Test: 49.59%
Epoch: 100, Loss: 7.4078, Train: 50.38%, Valid: 50.26%, Test: 50.45%
Epoch: 125, Loss: 8.2677, Train: 49.73%, Valid: 49.79%, Test: 49.66%
Epoch: 150, Loss: 8.0011, Train: 50.19%, Valid: 50.17%, Test: 50.22%
Epoch: 175, Loss: 8.2089, Train: 49.30%, Valid: 49.24%, Test: 49.33%
Run 01:
Highest Train: 66.35
Highest Valid: 66.11
  Final Train: 66.35
   Final Test: 66.05
All runs:
Highest Train: 66.35, nan
Highest Valid: 66.11, nan
  Final Train: 66.35, nan
   Final Test: 66.05, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 3.7705, Train: 88.07%, Valid: 88.09%, Test: 88.10%
Epoch: 25, Loss: 0.7574, Train: 83.95%, Valid: 83.77%, Test: 84.07%
Epoch: 50, Loss: 0.6307, Train: 83.98%, Valid: 83.81%, Test: 84.10%
Epoch: 75, Loss: 0.5745, Train: 84.03%, Valid: 83.84%, Test: 84.15%
Epoch: 100, Loss: 0.5716, Train: 84.04%, Valid: 83.86%, Test: 84.16%
Epoch: 125, Loss: 0.5986, Train: 84.02%, Valid: 83.83%, Test: 84.11%
Epoch: 150, Loss: 0.5384, Train: 84.82%, Valid: 84.77%, Test: 84.91%
Epoch: 175, Loss: 0.4997, Train: 85.02%, Valid: 85.03%, Test: 85.09%
Run 01:
Highest Train: 88.14
Highest Valid: 88.17
  Final Train: 88.14
   Final Test: 88.18
All runs:
Highest Train: 88.14, nan
Highest Valid: 88.17, nan
  Final Train: 88.14, nan
   Final Test: 88.18, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 9.7504, Train: 15.91%, Valid: 16.02%, Test: 15.86%
Epoch: 25, Loss: 1.9927, Train: 85.79%, Valid: 85.67%, Test: 85.78%
Epoch: 50, Loss: 0.7009, Train: 85.89%, Valid: 85.75%, Test: 85.95%
Epoch: 75, Loss: 0.5041, Train: 85.88%, Valid: 85.73%, Test: 85.96%
Epoch: 100, Loss: 0.4999, Train: 85.88%, Valid: 85.72%, Test: 85.96%
Epoch: 125, Loss: 0.4766, Train: 85.89%, Valid: 85.72%, Test: 85.96%
Epoch: 150, Loss: 0.4602, Train: 85.90%, Valid: 85.72%, Test: 85.97%
Epoch: 175, Loss: 0.4435, Train: 85.89%, Valid: 85.70%, Test: 85.95%
Run 01:
Highest Train: 85.99
Highest Valid: 85.86
  Final Train: 85.99
   Final Test: 86.01
All runs:
Highest Train: 85.99, nan
Highest Valid: 85.86, nan
  Final Train: 85.99, nan
   Final Test: 86.01, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=32, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=2, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
)
Epoch: 00, Loss: 9.8781, Train: 46.32%, Valid: 46.19%, Test: 46.27%
Epoch: 25, Loss: 8.7942, Train: 46.98%, Valid: 46.95%, Test: 47.14%
Epoch: 50, Loss: 9.0184, Train: 46.97%, Valid: 46.90%, Test: 47.13%
Epoch: 75, Loss: 8.7117, Train: 53.03%, Valid: 53.10%, Test: 52.87%
Epoch: 100, Loss: 13.6009, Train: 53.15%, Valid: 53.19%, Test: 52.99%
Epoch: 125, Loss: 8.8947, Train: 53.07%, Valid: 53.15%, Test: 52.94%
Epoch: 150, Loss: 7.8622, Train: 53.09%, Valid: 53.18%, Test: 52.91%
Epoch: 175, Loss: 8.5114, Train: 53.09%, Valid: 53.18%, Test: 52.91%
Run 01:
Highest Train: 74.61
Highest Valid: 74.42
  Final Train: 74.61
   Final Test: 74.45
All runs:
Highest Train: 74.61, nan
Highest Valid: 74.42, nan
  Final Train: 74.61, nan
   Final Test: 74.45, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 0.6841, Train: 85.70%, Valid: 85.86%, Test: 85.80%
Epoch: 25, Loss: 0.3679, Train: 86.24%, Valid: 86.21%, Test: 86.35%
Epoch: 50, Loss: 0.3645, Train: 86.19%, Valid: 86.16%, Test: 86.27%
Epoch: 75, Loss: 0.3630, Train: 86.47%, Valid: 86.46%, Test: 86.55%
Epoch: 100, Loss: 0.3615, Train: 86.64%, Valid: 86.60%, Test: 86.73%
Epoch: 125, Loss: 0.3597, Train: 86.65%, Valid: 86.62%, Test: 86.74%
Epoch: 150, Loss: 0.3574, Train: 86.57%, Valid: 86.50%, Test: 86.64%
Epoch: 175, Loss: 0.3546, Train: 86.53%, Valid: 86.46%, Test: 86.61%
Run 01:
Highest Train: 86.95
Highest Valid: 87.02
  Final Train: 86.95
   Final Test: 86.99
All runs:
Highest Train: 86.95, nan
Highest Valid: 87.02, nan
  Final Train: 86.95, nan
   Final Test: 86.99, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 2.4764, Train: 83.64%, Valid: 83.68%, Test: 83.63%
Epoch: 25, Loss: 0.4724, Train: 85.65%, Valid: 85.57%, Test: 85.63%
Epoch: 50, Loss: 0.4357, Train: 85.91%, Valid: 85.81%, Test: 85.95%
Epoch: 75, Loss: 0.4128, Train: 85.83%, Valid: 85.71%, Test: 85.89%
Epoch: 100, Loss: 0.3957, Train: 85.71%, Valid: 85.59%, Test: 85.79%
Epoch: 125, Loss: 0.3847, Train: 85.35%, Valid: 85.21%, Test: 85.44%
Epoch: 150, Loss: 0.3775, Train: 85.31%, Valid: 85.17%, Test: 85.39%
Epoch: 175, Loss: 0.3724, Train: 85.28%, Valid: 85.15%, Test: 85.37%
Run 01:
Highest Train: 85.93
Highest Valid: 85.82
  Final Train: 85.93
   Final Test: 85.94
All runs:
Highest Train: 85.93, nan
Highest Valid: 85.82, nan
  Final Train: 85.93, nan
   Final Test: 85.94, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 920.5866, Train: 54.25%, Valid: 54.18%, Test: 54.05%
Epoch: 25, Loss: 177.2675, Train: 48.31%, Valid: 48.35%, Test: 48.23%
Epoch: 50, Loss: 124.7401, Train: 45.83%, Valid: 45.75%, Test: 45.70%
Epoch: 75, Loss: 87.2392, Train: 43.66%, Valid: 43.56%, Test: 43.50%
Epoch: 100, Loss: 60.4791, Train: 43.11%, Valid: 43.03%, Test: 42.92%
Epoch: 125, Loss: 44.0122, Train: 43.34%, Valid: 43.20%, Test: 43.09%
Epoch: 150, Loss: 25.2292, Train: 52.96%, Valid: 52.69%, Test: 52.79%
Epoch: 175, Loss: 17.9160, Train: 76.78%, Valid: 76.57%, Test: 76.78%
Run 01:
Highest Train: 78.24
Highest Valid: 78.06
  Final Train: 78.24
   Final Test: 78.18
All runs:
Highest Train: 78.24, nan
Highest Valid: 78.06, nan
  Final Train: 78.24, nan
   Final Test: 78.18, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 2.3918, Train: 85.14%, Valid: 85.15%, Test: 85.29%
Epoch: 25, Loss: 0.4157, Train: 87.23%, Valid: 87.11%, Test: 87.31%
Epoch: 50, Loss: 0.4090, Train: 87.33%, Valid: 87.21%, Test: 87.40%
Epoch: 75, Loss: 0.3971, Train: 87.33%, Valid: 87.20%, Test: 87.35%
Epoch: 100, Loss: 0.3888, Train: 87.12%, Valid: 87.00%, Test: 87.11%
Epoch: 125, Loss: 0.3879, Train: 86.57%, Valid: 86.46%, Test: 86.62%
Epoch: 150, Loss: 0.3771, Train: 86.10%, Valid: 85.99%, Test: 86.15%
Epoch: 175, Loss: 0.3775, Train: 86.09%, Valid: 85.96%, Test: 86.14%
Run 01:
Highest Train: 87.43
Highest Valid: 87.31
  Final Train: 87.43
   Final Test: 87.45
All runs:
Highest Train: 87.43, nan
Highest Valid: 87.31, nan
  Final Train: 87.43, nan
   Final Test: 87.45, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 19.9371, Train: 16.08%, Valid: 16.19%, Test: 16.01%
Epoch: 25, Loss: 0.4264, Train: 85.89%, Valid: 85.78%, Test: 85.92%
Epoch: 50, Loss: 0.4439, Train: 86.06%, Valid: 85.93%, Test: 86.07%
Epoch: 75, Loss: 0.4350, Train: 86.01%, Valid: 85.88%, Test: 86.03%
Epoch: 100, Loss: 0.4243, Train: 85.98%, Valid: 85.85%, Test: 86.00%
Epoch: 125, Loss: 0.4168, Train: 85.95%, Valid: 85.83%, Test: 85.97%
Epoch: 150, Loss: 0.4122, Train: 85.86%, Valid: 85.71%, Test: 85.86%
Epoch: 175, Loss: 0.4010, Train: 85.53%, Valid: 85.37%, Test: 85.56%
Run 01:
Highest Train: 86.15
Highest Valid: 86.12
  Final Train: 86.15
   Final Test: 86.29
All runs:
Highest Train: 86.15, nan
Highest Valid: 86.12, nan
  Final Train: 86.15, nan
   Final Test: 86.29, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 29.7582, Train: 44.33%, Valid: 44.68%, Test: 44.70%
Epoch: 25, Loss: 19.3085, Train: 80.03%, Valid: 79.99%, Test: 79.90%
Epoch: 50, Loss: 22.1907, Train: 73.10%, Valid: 72.88%, Test: 72.89%
Epoch: 75, Loss: 21.1088, Train: 76.05%, Valid: 75.85%, Test: 75.93%
Epoch: 100, Loss: 19.5765, Train: 74.22%, Valid: 73.99%, Test: 74.04%
Epoch: 125, Loss: 19.8486, Train: 68.35%, Valid: 68.38%, Test: 68.43%
Epoch: 150, Loss: 16.3399, Train: 79.39%, Valid: 79.23%, Test: 79.29%
Epoch: 175, Loss: 11.5224, Train: 78.86%, Valid: 78.69%, Test: 78.75%
Run 01:
Highest Train: 81.29
Highest Valid: 81.23
  Final Train: 81.29
   Final Test: 81.14
All runs:
Highest Train: 81.29, nan
Highest Valid: 81.23, nan
  Final Train: 81.29, nan
   Final Test: 81.14, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 10.5448, Train: 15.71%, Valid: 15.65%, Test: 15.54%
Epoch: 25, Loss: 0.4535, Train: 87.97%, Valid: 88.02%, Test: 87.98%
Epoch: 50, Loss: 0.4485, Train: 88.00%, Valid: 88.06%, Test: 88.01%
Epoch: 75, Loss: 0.4444, Train: 88.04%, Valid: 88.09%, Test: 88.01%
Epoch: 100, Loss: 0.4290, Train: 87.99%, Valid: 88.05%, Test: 88.00%
Epoch: 125, Loss: 0.4368, Train: 88.17%, Valid: 88.26%, Test: 88.18%
Epoch: 150, Loss: 0.4339, Train: 88.11%, Valid: 88.22%, Test: 88.12%
Epoch: 175, Loss: 0.4204, Train: 87.93%, Valid: 88.01%, Test: 87.97%
Run 01:
Highest Train: 88.18
Highest Valid: 88.26
  Final Train: 88.18
   Final Test: 88.18
All runs:
Highest Train: 88.18, nan
Highest Valid: 88.26, nan
  Final Train: 88.18, nan
   Final Test: 88.18, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 0.6697, Train: 85.72%, Valid: 85.64%, Test: 85.80%
Epoch: 25, Loss: 0.4533, Train: 86.08%, Valid: 85.98%, Test: 86.16%
Epoch: 50, Loss: 0.4145, Train: 86.00%, Valid: 85.87%, Test: 86.03%
Epoch: 75, Loss: 0.3929, Train: 85.96%, Valid: 85.83%, Test: 86.02%
Epoch: 100, Loss: 0.3874, Train: 85.94%, Valid: 85.82%, Test: 86.01%
Epoch: 125, Loss: 0.3816, Train: 86.02%, Valid: 85.93%, Test: 86.08%
Epoch: 150, Loss: 0.3732, Train: 86.18%, Valid: 86.05%, Test: 86.24%
Epoch: 175, Loss: 0.3731, Train: 86.16%, Valid: 86.02%, Test: 86.21%
Run 01:
Highest Train: 86.18
Highest Valid: 86.05
  Final Train: 86.17
   Final Test: 86.24
All runs:
Highest Train: 86.18, nan
Highest Valid: 86.05, nan
  Final Train: 86.17, nan
   Final Test: 86.24, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 564.7263, Train: 36.61%, Valid: 36.82%, Test: 36.88%
Epoch: 25, Loss: 13.4746, Train: 53.02%, Valid: 53.30%, Test: 53.51%
Epoch: 50, Loss: 10.5788, Train: 58.62%, Valid: 58.86%, Test: 59.18%
Epoch: 75, Loss: 10.4168, Train: 76.51%, Valid: 76.48%, Test: 76.74%
Epoch: 100, Loss: 10.5335, Train: 82.34%, Valid: 82.30%, Test: 82.38%
Epoch: 125, Loss: 10.1886, Train: 79.90%, Valid: 80.12%, Test: 80.13%
Epoch: 150, Loss: 22.3858, Train: 69.48%, Valid: 69.80%, Test: 69.84%
Epoch: 175, Loss: 7.5426, Train: 82.90%, Valid: 82.86%, Test: 82.92%
Run 01:
Highest Train: 84.86
Highest Valid: 84.78
  Final Train: 84.86
   Final Test: 84.94
All runs:
Highest Train: 84.86, nan
Highest Valid: 84.78, nan
  Final Train: 84.86, nan
   Final Test: 84.94, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 23.4732, Train: 16.87%, Valid: 17.03%, Test: 16.78%
Epoch: 25, Loss: 0.6388, Train: 88.35%, Valid: 88.39%, Test: 88.36%
Epoch: 50, Loss: 0.4865, Train: 88.29%, Valid: 88.35%, Test: 88.32%
Epoch: 75, Loss: 0.4767, Train: 87.43%, Valid: 87.30%, Test: 87.44%
Epoch: 100, Loss: 0.4732, Train: 87.46%, Valid: 87.32%, Test: 87.46%
Epoch: 125, Loss: 0.4930, Train: 87.44%, Valid: 87.31%, Test: 87.45%
Epoch: 150, Loss: 0.4790, Train: 87.51%, Valid: 87.39%, Test: 87.51%
Epoch: 175, Loss: 0.4675, Train: 87.47%, Valid: 87.35%, Test: 87.46%
Run 01:
Highest Train: 88.46
Highest Valid: 88.50
  Final Train: 88.46
   Final Test: 88.46
All runs:
Highest Train: 88.46, nan
Highest Valid: 88.50, nan
  Final Train: 88.46, nan
   Final Test: 88.46, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 4.6165, Train: 82.33%, Valid: 82.34%, Test: 82.54%
Epoch: 25, Loss: 0.5291, Train: 87.32%, Valid: 87.23%, Test: 87.26%
Epoch: 50, Loss: 0.5286, Train: 87.42%, Valid: 87.34%, Test: 87.38%
Epoch: 75, Loss: 0.4822, Train: 87.47%, Valid: 87.39%, Test: 87.45%
Epoch: 100, Loss: 0.4801, Train: 87.97%, Valid: 87.90%, Test: 87.96%
Epoch: 125, Loss: 0.4518, Train: 86.24%, Valid: 86.13%, Test: 86.27%
Epoch: 150, Loss: 0.4446, Train: 86.34%, Valid: 86.26%, Test: 86.40%
Epoch: 175, Loss: 0.4431, Train: 87.13%, Valid: 87.01%, Test: 87.12%
Run 01:
Highest Train: 88.05
Highest Valid: 87.97
  Final Train: 88.05
   Final Test: 88.04
All runs:
Highest Train: 88.05, nan
Highest Valid: 87.97, nan
  Final Train: 88.05, nan
   Final Test: 88.04, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 11.9351, Train: 73.13%, Valid: 73.09%, Test: 73.17%
Epoch: 25, Loss: 10.4247, Train: 70.17%, Valid: 69.99%, Test: 69.85%
Epoch: 50, Loss: 9.7922, Train: 76.23%, Valid: 76.05%, Test: 76.08%
Epoch: 75, Loss: 10.0837, Train: 81.11%, Valid: 80.98%, Test: 81.01%
Epoch: 100, Loss: 9.6594, Train: 50.81%, Valid: 51.14%, Test: 51.19%
Epoch: 125, Loss: 8.6811, Train: 53.57%, Valid: 53.91%, Test: 54.04%
Epoch: 150, Loss: 6.5588, Train: 60.59%, Valid: 60.85%, Test: 61.12%
Epoch: 175, Loss: 5.0553, Train: 81.55%, Valid: 81.41%, Test: 81.50%
Run 01:
Highest Train: 84.57
Highest Valid: 84.48
  Final Train: 84.57
   Final Test: 84.61
All runs:
Highest Train: 84.57, nan
Highest Valid: 84.48, nan
  Final Train: 84.57, nan
   Final Test: 84.61, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 0.3749, Train: 84.34%, Valid: 84.16%, Test: 84.48%
Epoch: 25, Loss: 0.3651, Train: 84.23%, Valid: 84.04%, Test: 84.34%
Epoch: 50, Loss: 0.3625, Train: 85.35%, Valid: 85.37%, Test: 85.46%
Epoch: 75, Loss: 0.3594, Train: 85.37%, Valid: 85.37%, Test: 85.48%
Epoch: 100, Loss: 0.3549, Train: 85.49%, Valid: 85.45%, Test: 85.64%
Epoch: 125, Loss: 0.3496, Train: 86.18%, Valid: 86.07%, Test: 86.22%
Epoch: 150, Loss: 0.3446, Train: 85.39%, Valid: 85.38%, Test: 85.47%
Epoch: 175, Loss: 0.3401, Train: 85.17%, Valid: 85.21%, Test: 85.29%
Run 01:
Highest Train: 88.08
Highest Valid: 88.13
  Final Train: 88.08
   Final Test: 88.11
All runs:
Highest Train: 88.08, nan
Highest Valid: 88.13, nan
  Final Train: 88.08, nan
   Final Test: 88.11, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.0652, Train: 84.47%, Valid: 84.47%, Test: 84.47%
Epoch: 25, Loss: 0.4851, Train: 85.23%, Valid: 85.06%, Test: 85.30%
Epoch: 50, Loss: 0.4683, Train: 85.12%, Valid: 84.94%, Test: 85.18%
Epoch: 75, Loss: 0.4593, Train: 85.13%, Valid: 84.95%, Test: 85.19%
Epoch: 100, Loss: 0.4510, Train: 85.16%, Valid: 84.98%, Test: 85.22%
Epoch: 125, Loss: 0.4363, Train: 84.91%, Valid: 84.76%, Test: 85.00%
Epoch: 150, Loss: 0.4634, Train: 85.23%, Valid: 85.05%, Test: 85.30%
Epoch: 175, Loss: 0.4409, Train: 85.18%, Valid: 85.01%, Test: 85.25%
Run 01:
Highest Train: 85.67
Highest Valid: 85.54
  Final Train: 85.67
   Final Test: 85.73
All runs:
Highest Train: 85.67, nan
Highest Valid: 85.54, nan
  Final Train: 85.67, nan
   Final Test: 85.73, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 14405020.0000, Train: 47.46%, Valid: 47.36%, Test: 47.48%
Epoch: 25, Loss: 17243392.0000, Train: 55.73%, Valid: 55.80%, Test: 55.83%
Epoch: 50, Loss: 9486990.0000, Train: 55.53%, Valid: 55.60%, Test: 55.63%
Epoch: 75, Loss: 1233270528.0000, Train: 54.31%, Valid: 54.37%, Test: 54.27%
Epoch: 100, Loss: 128379928.0000, Train: 45.64%, Valid: 45.55%, Test: 45.75%
Epoch: 125, Loss: 372863872.0000, Train: 45.78%, Valid: 45.62%, Test: 45.81%
Epoch: 150, Loss: 23703110.0000, Train: 55.11%, Valid: 55.23%, Test: 55.10%
Epoch: 175, Loss: 23393150.0000, Train: 55.23%, Valid: 55.31%, Test: 55.27%
Run 01:
Highest Train: 62.15
Highest Valid: 62.01
  Final Train: 62.15
   Final Test: 61.89
All runs:
Highest Train: 62.15, nan
Highest Valid: 62.01, nan
  Final Train: 62.15, nan
   Final Test: 61.89, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 5.9781, Train: 16.33%, Valid: 16.32%, Test: 16.24%
Epoch: 25, Loss: 0.3956, Train: 84.07%, Valid: 83.88%, Test: 84.21%
Epoch: 50, Loss: 0.3947, Train: 84.94%, Valid: 84.96%, Test: 85.09%
Epoch: 75, Loss: 0.3914, Train: 85.05%, Valid: 85.06%, Test: 85.21%
Epoch: 100, Loss: 0.3867, Train: 85.07%, Valid: 85.09%, Test: 85.24%
Epoch: 125, Loss: 0.3818, Train: 85.14%, Valid: 85.16%, Test: 85.32%
Epoch: 150, Loss: 0.3838, Train: 85.29%, Valid: 85.30%, Test: 85.46%
Epoch: 175, Loss: 0.3759, Train: 85.39%, Valid: 85.40%, Test: 85.57%
Run 01:
Highest Train: 86.07
Highest Valid: 86.12
  Final Train: 86.07
   Final Test: 86.26
All runs:
Highest Train: 86.07, nan
Highest Valid: 86.12, nan
  Final Train: 86.07, nan
   Final Test: 86.26, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 7.7994, Train: 14.47%, Valid: 14.47%, Test: 14.41%
Epoch: 25, Loss: 0.3937, Train: 85.90%, Valid: 85.76%, Test: 86.00%
Epoch: 50, Loss: 0.3969, Train: 85.96%, Valid: 85.80%, Test: 86.05%
Epoch: 75, Loss: 0.3933, Train: 85.93%, Valid: 85.77%, Test: 86.02%
Epoch: 100, Loss: 0.3901, Train: 86.07%, Valid: 85.92%, Test: 86.15%
Epoch: 125, Loss: 0.3881, Train: 86.15%, Valid: 86.05%, Test: 86.20%
Epoch: 150, Loss: 0.3795, Train: 86.23%, Valid: 86.14%, Test: 86.28%
Epoch: 175, Loss: 0.3803, Train: 86.28%, Valid: 86.21%, Test: 86.33%
Run 01:
Highest Train: 86.51
Highest Valid: 86.46
  Final Train: 86.51
   Final Test: 86.59
All runs:
Highest Train: 86.51, nan
Highest Valid: 86.46, nan
  Final Train: 86.51, nan
   Final Test: 86.59, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 58.6943, Train: 43.33%, Valid: 43.50%, Test: 43.65%
Epoch: 25, Loss: 13.8329, Train: 76.70%, Valid: 76.71%, Test: 76.86%
Epoch: 50, Loss: 192.4758, Train: 74.62%, Valid: 74.39%, Test: 74.44%
Epoch: 75, Loss: 91.0065, Train: 74.85%, Valid: 74.64%, Test: 74.66%
Epoch: 100, Loss: 27.7822, Train: 73.02%, Valid: 72.78%, Test: 72.81%
Epoch: 125, Loss: 19.4717, Train: 74.32%, Valid: 74.10%, Test: 74.11%
Epoch: 150, Loss: 45.6578, Train: 64.93%, Valid: 64.67%, Test: 64.58%
Epoch: 175, Loss: 193.2438, Train: 67.73%, Valid: 67.51%, Test: 67.42%
Run 01:
Highest Train: 81.70
Highest Valid: 81.53
  Final Train: 81.70
   Final Test: 81.64
All runs:
Highest Train: 81.70, nan
Highest Valid: 81.53, nan
  Final Train: 81.70, nan
   Final Test: 81.64, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 6.5456, Train: 86.14%, Valid: 86.17%, Test: 86.27%
Epoch: 25, Loss: 0.4382, Train: 86.95%, Valid: 86.83%, Test: 87.04%
Epoch: 50, Loss: 0.4322, Train: 87.15%, Valid: 87.05%, Test: 87.24%
Epoch: 75, Loss: 0.4219, Train: 87.09%, Valid: 86.97%, Test: 87.17%
Epoch: 100, Loss: 0.4251, Train: 86.90%, Valid: 86.73%, Test: 86.93%
Epoch: 125, Loss: 0.4077, Train: 86.85%, Valid: 86.69%, Test: 86.87%
Epoch: 150, Loss: 0.4046, Train: 87.26%, Valid: 87.17%, Test: 87.39%
Epoch: 175, Loss: 0.3975, Train: 87.84%, Valid: 87.93%, Test: 87.99%
Run 01:
Highest Train: 88.18
Highest Valid: 88.24
  Final Train: 88.18
   Final Test: 88.32
All runs:
Highest Train: 88.18, nan
Highest Valid: 88.24, nan
  Final Train: 88.18, nan
   Final Test: 88.32, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.4971, Train: 82.19%, Valid: 82.22%, Test: 82.47%
Epoch: 25, Loss: 0.4482, Train: 85.49%, Valid: 85.33%, Test: 85.57%
Epoch: 50, Loss: 0.4245, Train: 85.49%, Valid: 85.31%, Test: 85.56%
Epoch: 75, Loss: 0.4048, Train: 85.89%, Valid: 85.83%, Test: 85.96%
Epoch: 100, Loss: 0.4052, Train: 85.77%, Valid: 85.70%, Test: 85.86%
Epoch: 125, Loss: 0.3897, Train: 85.71%, Valid: 85.57%, Test: 85.76%
Epoch: 150, Loss: 0.3818, Train: 85.43%, Valid: 85.28%, Test: 85.53%
Epoch: 175, Loss: 0.3775, Train: 84.42%, Valid: 84.37%, Test: 84.57%
Run 01:
Highest Train: 86.55
Highest Valid: 86.55
  Final Train: 86.55
   Final Test: 86.76
All runs:
Highest Train: 86.55, nan
Highest Valid: 86.55, nan
  Final Train: 86.55, nan
   Final Test: 86.76, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 389.4479, Train: 57.03%, Valid: 57.00%, Test: 56.94%
Epoch: 25, Loss: 125.8953, Train: 40.61%, Valid: 40.63%, Test: 40.71%
Epoch: 50, Loss: 266.5119, Train: 37.25%, Valid: 37.39%, Test: 37.56%
Epoch: 75, Loss: 98.3043, Train: 33.05%, Valid: 32.79%, Test: 32.63%
Epoch: 100, Loss: 100.2602, Train: 41.44%, Valid: 41.13%, Test: 40.95%
Epoch: 125, Loss: 178.4810, Train: 39.06%, Valid: 38.75%, Test: 38.51%
Epoch: 150, Loss: 63.0927, Train: 36.26%, Valid: 36.01%, Test: 35.79%
Epoch: 175, Loss: 40.7720, Train: 37.32%, Valid: 37.08%, Test: 36.84%
Run 01:
Highest Train: 60.79
Highest Valid: 60.64
  Final Train: 60.79
   Final Test: 60.53
All runs:
Highest Train: 60.79, nan
Highest Valid: 60.64, nan
  Final Train: 60.79, nan
   Final Test: 60.53, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 5.7394, Train: 84.81%, Valid: 84.65%, Test: 84.90%
Epoch: 25, Loss: 0.5350, Train: 84.49%, Valid: 84.28%, Test: 84.58%
Epoch: 50, Loss: 0.5288, Train: 87.21%, Valid: 87.25%, Test: 87.31%
Epoch: 75, Loss: 0.4988, Train: 86.49%, Valid: 86.56%, Test: 86.58%
Epoch: 100, Loss: 0.5059, Train: 86.18%, Valid: 86.23%, Test: 86.29%
Epoch: 125, Loss: 0.4684, Train: 85.86%, Valid: 85.82%, Test: 85.93%
Epoch: 150, Loss: 0.4587, Train: 85.35%, Valid: 85.15%, Test: 85.36%
Epoch: 175, Loss: 0.4343, Train: 85.86%, Valid: 85.66%, Test: 85.90%
Run 01:
Highest Train: 87.53
Highest Valid: 87.54
  Final Train: 87.53
   Final Test: 87.62
All runs:
Highest Train: 87.53, nan
Highest Valid: 87.54, nan
  Final Train: 87.53, nan
   Final Test: 87.62, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 7.6461, Train: 84.70%, Valid: 84.60%, Test: 84.81%
Epoch: 25, Loss: 0.5596, Train: 86.51%, Valid: 86.44%, Test: 86.54%
Epoch: 50, Loss: 0.5454, Train: 86.60%, Valid: 86.52%, Test: 86.61%
Epoch: 75, Loss: 0.5085, Train: 86.54%, Valid: 86.42%, Test: 86.52%
Epoch: 100, Loss: 0.4815, Train: 86.49%, Valid: 86.36%, Test: 86.48%
Epoch: 125, Loss: 0.4828, Train: 86.47%, Valid: 86.33%, Test: 86.47%
Epoch: 150, Loss: 0.4529, Train: 86.38%, Valid: 86.23%, Test: 86.39%
Epoch: 175, Loss: 0.4472, Train: 86.29%, Valid: 86.09%, Test: 86.28%
Run 01:
Highest Train: 86.69
Highest Valid: 86.62
  Final Train: 86.69
   Final Test: 86.72
All runs:
Highest Train: 86.69, nan
Highest Valid: 86.62, nan
  Final Train: 86.69, nan
   Final Test: 86.72, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 32.6604, Train: 76.12%, Valid: 75.96%, Test: 75.95%
Epoch: 25, Loss: 10.4152, Train: 45.57%, Valid: 45.47%, Test: 45.71%
Epoch: 50, Loss: 11.6686, Train: 68.75%, Valid: 68.64%, Test: 68.36%
Epoch: 75, Loss: 12.7390, Train: 69.28%, Valid: 69.05%, Test: 69.02%
Epoch: 100, Loss: 13.4331, Train: 45.20%, Valid: 45.35%, Test: 45.38%
Epoch: 125, Loss: 12.2916, Train: 46.40%, Valid: 46.47%, Test: 46.38%
Epoch: 150, Loss: 9.9227, Train: 44.21%, Valid: 44.44%, Test: 44.49%
Epoch: 175, Loss: 12.1484, Train: 44.23%, Valid: 44.43%, Test: 44.53%
Run 01:
Highest Train: 81.82
Highest Valid: 81.64
  Final Train: 81.82
   Final Test: 81.70
All runs:
Highest Train: 81.82, nan
Highest Valid: 81.64, nan
  Final Train: 81.82, nan
   Final Test: 81.70, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.2899, Train: 84.39%, Valid: 84.50%, Test: 84.53%
Epoch: 25, Loss: 0.3671, Train: 87.60%, Valid: 87.59%, Test: 87.62%
Epoch: 50, Loss: 0.3659, Train: 87.97%, Valid: 87.96%, Test: 87.97%
Epoch: 75, Loss: 0.3644, Train: 84.90%, Valid: 84.87%, Test: 85.06%
Epoch: 100, Loss: 0.3630, Train: 84.92%, Valid: 84.85%, Test: 85.04%
Epoch: 125, Loss: 0.3613, Train: 86.28%, Valid: 86.23%, Test: 86.24%
Epoch: 150, Loss: 0.3592, Train: 86.81%, Valid: 86.81%, Test: 86.79%
Epoch: 175, Loss: 0.3564, Train: 85.60%, Valid: 85.60%, Test: 85.64%
Run 01:
Highest Train: 87.97
Highest Valid: 87.96
  Final Train: 87.97
   Final Test: 87.97
All runs:
Highest Train: 87.97, nan
Highest Valid: 87.96, nan
  Final Train: 87.97, nan
   Final Test: 87.97, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.6667, Train: 81.78%, Valid: 81.98%, Test: 82.07%
Epoch: 25, Loss: 0.4830, Train: 85.81%, Valid: 85.72%, Test: 85.88%
Epoch: 50, Loss: 0.4886, Train: 85.73%, Valid: 85.60%, Test: 85.81%
Epoch: 75, Loss: 0.4811, Train: 85.72%, Valid: 85.59%, Test: 85.80%
Epoch: 100, Loss: 0.4782, Train: 85.71%, Valid: 85.58%, Test: 85.78%
Epoch: 125, Loss: 0.4748, Train: 85.47%, Valid: 85.34%, Test: 85.52%
Epoch: 150, Loss: 0.4711, Train: 85.37%, Valid: 85.24%, Test: 85.42%
Epoch: 175, Loss: 0.4666, Train: 85.27%, Valid: 85.14%, Test: 85.33%
Run 01:
Highest Train: 85.85
Highest Valid: 85.76
  Final Train: 85.85
   Final Test: 85.90
All runs:
Highest Train: 85.85, nan
Highest Valid: 85.76, nan
  Final Train: 85.85, nan
   Final Test: 85.90, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 2457562.2500, Train: 52.70%, Valid: 52.83%, Test: 52.58%
Epoch: 25, Loss: 50175.9453, Train: 35.93%, Valid: 35.68%, Test: 35.46%
Epoch: 50, Loss: 154509507148382208.0000, Train: 52.41%, Valid: 52.57%, Test: 52.42%
Epoch: 75, Loss: 142200656.0000, Train: 47.36%, Valid: 47.21%, Test: 47.28%
Epoch: 100, Loss: 791178182656.0000, Train: 52.67%, Valid: 52.83%, Test: 52.74%
Epoch: 125, Loss: 3708592848896.0000, Train: 47.40%, Valid: 47.24%, Test: 47.30%
Epoch: 150, Loss: 247254.7500, Train: 55.18%, Valid: 55.32%, Test: 55.23%
Epoch: 175, Loss: 6553.4316, Train: 56.71%, Valid: 56.75%, Test: 56.65%
Run 01:
Highest Train: 61.16
Highest Valid: 61.00
  Final Train: 61.16
   Final Test: 60.89
All runs:
Highest Train: 61.16, nan
Highest Valid: 61.00, nan
  Final Train: 61.16, nan
   Final Test: 60.89, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 20.6854, Train: 14.79%, Valid: 14.87%, Test: 14.62%
Epoch: 25, Loss: 0.4157, Train: 87.39%, Valid: 87.30%, Test: 87.41%
Epoch: 50, Loss: 0.4326, Train: 87.19%, Valid: 87.03%, Test: 87.22%
Epoch: 75, Loss: 0.4263, Train: 87.16%, Valid: 87.00%, Test: 87.18%
Epoch: 100, Loss: 0.4085, Train: 87.11%, Valid: 86.98%, Test: 87.12%
Epoch: 125, Loss: 0.4039, Train: 87.06%, Valid: 86.94%, Test: 87.04%
Epoch: 150, Loss: 0.4038, Train: 87.07%, Valid: 86.94%, Test: 87.08%
Epoch: 175, Loss: 0.4019, Train: 87.09%, Valid: 86.96%, Test: 87.09%
Run 01:
Highest Train: 87.89
Highest Valid: 87.89
  Final Train: 87.89
   Final Test: 87.89
All runs:
Highest Train: 87.89, nan
Highest Valid: 87.89, nan
  Final Train: 87.89, nan
   Final Test: 87.89, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 14.6584, Train: 16.99%, Valid: 16.85%, Test: 16.82%
Epoch: 25, Loss: 0.4273, Train: 86.45%, Valid: 86.36%, Test: 86.45%
Epoch: 50, Loss: 0.4560, Train: 85.93%, Valid: 85.87%, Test: 86.04%
Epoch: 75, Loss: 0.4413, Train: 85.32%, Valid: 85.19%, Test: 85.39%
Epoch: 100, Loss: 0.4278, Train: 85.07%, Valid: 84.96%, Test: 85.15%
Epoch: 125, Loss: 0.4201, Train: 86.30%, Valid: 86.17%, Test: 86.36%
Epoch: 150, Loss: 0.4076, Train: 86.22%, Valid: 86.07%, Test: 86.23%
Epoch: 175, Loss: 0.4037, Train: 86.09%, Valid: 85.95%, Test: 86.16%
Run 01:
Highest Train: 86.57
Highest Valid: 86.52
  Final Train: 86.57
   Final Test: 86.63
All runs:
Highest Train: 86.57, nan
Highest Valid: 86.52, nan
  Final Train: 86.57, nan
   Final Test: 86.63, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 58.6655, Train: 44.45%, Valid: 44.57%, Test: 44.31%
Epoch: 25, Loss: 6.7350, Train: 58.31%, Valid: 58.29%, Test: 58.41%
Epoch: 50, Loss: 7.9323, Train: 77.74%, Valid: 77.64%, Test: 77.63%
Epoch: 75, Loss: 6.8185, Train: 51.96%, Valid: 51.96%, Test: 51.87%
Epoch: 100, Loss: 7.4814, Train: 51.62%, Valid: 51.58%, Test: 51.51%
Epoch: 125, Loss: 7.9887, Train: 48.37%, Valid: 48.34%, Test: 48.50%
Epoch: 150, Loss: 8.4866, Train: 48.53%, Valid: 48.60%, Test: 48.67%
Epoch: 175, Loss: 6.8922, Train: 49.78%, Valid: 49.77%, Test: 49.84%
Run 01:
Highest Train: 80.33
Highest Valid: 80.20
  Final Train: 80.33
   Final Test: 80.22
All runs:
Highest Train: 80.33, nan
Highest Valid: 80.20, nan
  Final Train: 80.33, nan
   Final Test: 80.22, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 1.8073, Train: 85.33%, Valid: 85.27%, Test: 85.49%
Epoch: 25, Loss: 0.4548, Train: 87.09%, Valid: 86.99%, Test: 87.05%
Epoch: 50, Loss: 0.4412, Train: 87.14%, Valid: 87.02%, Test: 87.10%
Epoch: 75, Loss: 0.4297, Train: 87.12%, Valid: 86.99%, Test: 87.07%
Epoch: 100, Loss: 0.4173, Train: 87.05%, Valid: 86.92%, Test: 86.99%
Epoch: 125, Loss: 0.4061, Train: 86.32%, Valid: 86.21%, Test: 86.32%
Epoch: 150, Loss: 0.4013, Train: 86.33%, Valid: 86.26%, Test: 86.35%
Epoch: 175, Loss: 0.3870, Train: 86.23%, Valid: 86.14%, Test: 86.24%
Run 01:
Highest Train: 87.15
Highest Valid: 87.02
  Final Train: 87.14
   Final Test: 87.10
All runs:
Highest Train: 87.15, nan
Highest Valid: 87.02, nan
  Final Train: 87.14, nan
   Final Test: 87.10, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 15.0793, Train: 18.33%, Valid: 18.16%, Test: 18.07%
Epoch: 25, Loss: 0.4313, Train: 85.28%, Valid: 85.10%, Test: 85.37%
Epoch: 50, Loss: 0.4304, Train: 85.34%, Valid: 85.15%, Test: 85.41%
Epoch: 75, Loss: 0.4282, Train: 85.35%, Valid: 85.16%, Test: 85.42%
Epoch: 100, Loss: 0.4204, Train: 85.33%, Valid: 85.15%, Test: 85.42%
Epoch: 125, Loss: 0.4126, Train: 85.40%, Valid: 85.33%, Test: 85.45%
Epoch: 150, Loss: 0.4059, Train: 86.20%, Valid: 86.16%, Test: 86.23%
Epoch: 175, Loss: 0.4012, Train: 85.58%, Valid: 85.46%, Test: 85.63%
Run 01:
Highest Train: 86.35
Highest Valid: 86.30
  Final Train: 86.33
   Final Test: 86.36
All runs:
Highest Train: 86.35, nan
Highest Valid: 86.30, nan
  Final Train: 86.33, nan
   Final Test: 86.36, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 377.7354, Train: 50.00%, Valid: 49.99%, Test: 50.00%
Epoch: 25, Loss: 14.4115, Train: 54.11%, Valid: 54.05%, Test: 54.04%
Epoch: 50, Loss: 8.4668, Train: 47.85%, Valid: 47.82%, Test: 47.88%
Epoch: 75, Loss: 52.4182, Train: 48.10%, Valid: 48.08%, Test: 48.19%
Epoch: 100, Loss: 8.2542, Train: 51.70%, Valid: 51.67%, Test: 51.59%
Epoch: 125, Loss: 7.6860, Train: 78.02%, Valid: 77.81%, Test: 77.72%
Epoch: 150, Loss: 7.9945, Train: 79.73%, Valid: 79.61%, Test: 79.52%
Epoch: 175, Loss: 8.1179, Train: 51.73%, Valid: 51.75%, Test: 51.64%
Run 01:
Highest Train: 83.76
Highest Valid: 83.78
  Final Train: 83.76
   Final Test: 83.90
All runs:
Highest Train: 83.76, nan
Highest Valid: 83.78, nan
  Final Train: 83.76, nan
   Final Test: 83.90, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 13.5606, Train: 14.45%, Valid: 14.41%, Test: 14.47%
Epoch: 25, Loss: 0.5945, Train: 84.69%, Valid: 84.50%, Test: 84.74%
Epoch: 50, Loss: 0.5715, Train: 84.52%, Valid: 84.32%, Test: 84.60%
Epoch: 75, Loss: 0.5539, Train: 84.45%, Valid: 84.25%, Test: 84.55%
Epoch: 100, Loss: 0.5424, Train: 84.54%, Valid: 84.37%, Test: 84.63%
Epoch: 125, Loss: 0.5239, Train: 84.73%, Valid: 84.63%, Test: 84.79%
Epoch: 150, Loss: 0.5269, Train: 84.59%, Valid: 84.42%, Test: 84.70%
Epoch: 175, Loss: 0.5319, Train: 85.49%, Valid: 85.56%, Test: 85.56%
Run 01:
Highest Train: 86.86
Highest Valid: 86.85
  Final Train: 86.86
   Final Test: 86.92
All runs:
Highest Train: 86.86, nan
Highest Valid: 86.85, nan
  Final Train: 86.86, nan
   Final Test: 86.92, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 5.9726, Train: 84.88%, Valid: 84.82%, Test: 84.95%
Epoch: 25, Loss: 0.5638, Train: 85.49%, Valid: 85.30%, Test: 85.55%
Epoch: 50, Loss: 0.5658, Train: 85.52%, Valid: 85.33%, Test: 85.58%
Epoch: 75, Loss: 0.5096, Train: 85.54%, Valid: 85.36%, Test: 85.60%
Epoch: 100, Loss: 0.5148, Train: 85.61%, Valid: 85.43%, Test: 85.67%
Epoch: 125, Loss: 0.5015, Train: 85.65%, Valid: 85.48%, Test: 85.70%
Epoch: 150, Loss: 0.4914, Train: 85.71%, Valid: 85.56%, Test: 85.75%
Epoch: 175, Loss: 0.4662, Train: 85.71%, Valid: 85.59%, Test: 85.78%
Run 01:
Highest Train: 86.97
Highest Valid: 86.98
  Final Train: 86.97
   Final Test: 86.97
All runs:
Highest Train: 86.97, nan
Highest Valid: 86.98, nan
  Final Train: 86.97, nan
   Final Test: 86.97, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=64, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=2, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
)
Epoch: 00, Loss: 9.5080, Train: 47.50%, Valid: 47.39%, Test: 47.51%
Epoch: 25, Loss: 7.5227, Train: 48.82%, Valid: 48.80%, Test: 48.88%
Epoch: 50, Loss: 8.3535, Train: 47.67%, Valid: 47.56%, Test: 47.68%
Epoch: 75, Loss: 7.5413, Train: 53.37%, Valid: 53.23%, Test: 53.31%
Epoch: 100, Loss: 7.8673, Train: 47.40%, Valid: 47.50%, Test: 47.41%
Epoch: 125, Loss: 7.6789, Train: 47.20%, Valid: 47.25%, Test: 47.15%
Epoch: 150, Loss: 7.2418, Train: 47.63%, Valid: 47.73%, Test: 47.67%
Epoch: 175, Loss: 7.5553, Train: 41.73%, Valid: 41.82%, Test: 41.68%
Run 01:
Highest Train: 76.03
Highest Valid: 75.81
  Final Train: 76.03
   Final Test: 75.84
All runs:
Highest Train: 76.03, nan
Highest Valid: 75.81, nan
  Final Train: 76.03, nan
   Final Test: 75.84, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 5.4482, Train: 84.52%, Valid: 84.51%, Test: 84.66%
Epoch: 25, Loss: 0.4357, Train: 84.93%, Valid: 84.91%, Test: 85.05%
Epoch: 50, Loss: 0.4282, Train: 85.21%, Valid: 85.25%, Test: 85.31%
Epoch: 75, Loss: 0.3998, Train: 85.30%, Valid: 85.35%, Test: 85.41%
Epoch: 100, Loss: 0.3633, Train: 85.25%, Valid: 85.29%, Test: 85.32%
Epoch: 125, Loss: 0.3565, Train: 85.54%, Valid: 85.51%, Test: 85.64%
Epoch: 150, Loss: 0.3514, Train: 85.49%, Valid: 85.47%, Test: 85.59%
Epoch: 175, Loss: 0.3456, Train: 85.46%, Valid: 85.42%, Test: 85.58%
Run 01:
Highest Train: 86.92
Highest Valid: 86.91
  Final Train: 86.92
   Final Test: 87.01
All runs:
Highest Train: 86.92, nan
Highest Valid: 86.91, nan
  Final Train: 86.92, nan
   Final Test: 87.01, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 1.3471, Train: 84.54%, Valid: 84.54%, Test: 84.66%
Epoch: 25, Loss: 0.4430, Train: 85.98%, Valid: 85.89%, Test: 86.05%
Epoch: 50, Loss: 0.3992, Train: 85.85%, Valid: 85.74%, Test: 85.97%
Epoch: 75, Loss: 0.3724, Train: 85.65%, Valid: 85.56%, Test: 85.77%
Epoch: 100, Loss: 0.3648, Train: 85.44%, Valid: 85.28%, Test: 85.49%
Epoch: 125, Loss: 0.3625, Train: 85.50%, Valid: 85.34%, Test: 85.56%
Epoch: 150, Loss: 0.3601, Train: 85.54%, Valid: 85.39%, Test: 85.60%
Epoch: 175, Loss: 0.3572, Train: 85.57%, Valid: 85.40%, Test: 85.61%
Run 01:
Highest Train: 86.01
Highest Valid: 85.92
  Final Train: 86.00
   Final Test: 86.09
All runs:
Highest Train: 86.01, nan
Highest Valid: 85.92, nan
  Final Train: 86.00, nan
   Final Test: 86.09, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 3160.8586, Train: 58.32%, Valid: 58.18%, Test: 58.01%
Epoch: 25, Loss: 388.1104, Train: 55.84%, Valid: 55.64%, Test: 55.50%
Epoch: 50, Loss: 247.6758, Train: 51.70%, Valid: 51.50%, Test: 51.40%
Epoch: 75, Loss: 130.8199, Train: 45.21%, Valid: 44.86%, Test: 44.72%
Epoch: 100, Loss: 56.9120, Train: 40.01%, Valid: 39.74%, Test: 39.57%
Epoch: 125, Loss: 42.0869, Train: 40.98%, Valid: 40.66%, Test: 40.56%
Epoch: 150, Loss: 39.9211, Train: 40.87%, Valid: 40.55%, Test: 40.44%
Epoch: 175, Loss: 38.3928, Train: 40.61%, Valid: 40.31%, Test: 40.19%
Run 01:
Highest Train: 58.32
Highest Valid: 58.18
  Final Train: 58.32
   Final Test: 58.01
All runs:
Highest Train: 58.32, nan
Highest Valid: 58.18, nan
  Final Train: 58.32, nan
   Final Test: 58.01, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 17.6570, Train: 13.41%, Valid: 13.55%, Test: 13.42%
Epoch: 25, Loss: 0.5233, Train: 85.83%, Valid: 85.66%, Test: 85.89%
Epoch: 50, Loss: 0.5218, Train: 86.71%, Valid: 86.74%, Test: 86.80%
Epoch: 75, Loss: 0.4924, Train: 86.90%, Valid: 86.95%, Test: 86.97%
Epoch: 100, Loss: 0.4437, Train: 86.55%, Valid: 86.60%, Test: 86.64%
Epoch: 125, Loss: 0.4081, Train: 86.31%, Valid: 86.38%, Test: 86.42%
Epoch: 150, Loss: 0.3897, Train: 85.93%, Valid: 86.00%, Test: 86.03%
Epoch: 175, Loss: 0.3850, Train: 85.80%, Valid: 85.85%, Test: 85.92%
Run 01:
Highest Train: 86.93
Highest Valid: 86.97
  Final Train: 86.93
   Final Test: 87.01
All runs:
Highest Train: 86.93, nan
Highest Valid: 86.97, nan
  Final Train: 86.93, nan
   Final Test: 87.01, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.8905, Train: 84.89%, Valid: 84.76%, Test: 85.07%
Epoch: 25, Loss: 0.4175, Train: 87.50%, Valid: 87.32%, Test: 87.41%
Epoch: 50, Loss: 0.3873, Train: 87.41%, Valid: 87.20%, Test: 87.35%
Epoch: 75, Loss: 0.3773, Train: 86.71%, Valid: 86.56%, Test: 86.71%
Epoch: 100, Loss: 0.3730, Train: 86.01%, Valid: 85.85%, Test: 86.10%
Epoch: 125, Loss: 0.3673, Train: 86.58%, Valid: 86.45%, Test: 86.62%
Epoch: 150, Loss: 0.3627, Train: 85.50%, Valid: 85.35%, Test: 85.56%
Epoch: 175, Loss: 0.3602, Train: 86.78%, Valid: 86.63%, Test: 86.77%
Run 01:
Highest Train: 87.87
Highest Valid: 87.69
  Final Train: 87.87
   Final Test: 87.80
All runs:
Highest Train: 87.87, nan
Highest Valid: 87.69, nan
  Final Train: 87.87, nan
   Final Test: 87.80, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 588.5001, Train: 58.92%, Valid: 58.77%, Test: 58.61%
Epoch: 25, Loss: 21.3111, Train: 67.88%, Valid: 67.69%, Test: 67.57%
Epoch: 50, Loss: 54.8914, Train: 74.59%, Valid: 74.39%, Test: 74.41%
Epoch: 75, Loss: 14.4669, Train: 78.02%, Valid: 77.83%, Test: 77.90%
Epoch: 100, Loss: 36.3636, Train: 81.16%, Valid: 81.01%, Test: 81.07%
Epoch: 125, Loss: 8.5527, Train: 81.90%, Valid: 81.79%, Test: 81.86%
Epoch: 150, Loss: 15.2462, Train: 82.51%, Valid: 82.44%, Test: 82.52%
Epoch: 175, Loss: 8.7169, Train: 82.99%, Valid: 82.91%, Test: 83.01%
Run 01:
Highest Train: 83.10
Highest Valid: 83.01
  Final Train: 83.10
   Final Test: 83.12
All runs:
Highest Train: 83.10, nan
Highest Valid: 83.01, nan
  Final Train: 83.10, nan
   Final Test: 83.12, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.5518, Train: 83.86%, Valid: 83.66%, Test: 83.99%
Epoch: 25, Loss: 0.5426, Train: 87.26%, Valid: 87.15%, Test: 87.29%
Epoch: 50, Loss: 0.4569, Train: 87.30%, Valid: 87.20%, Test: 87.32%
Epoch: 75, Loss: 0.4264, Train: 87.22%, Valid: 87.10%, Test: 87.23%
Epoch: 100, Loss: 0.4079, Train: 85.24%, Valid: 85.27%, Test: 85.32%
Epoch: 125, Loss: 0.3960, Train: 85.37%, Valid: 85.39%, Test: 85.46%
Epoch: 150, Loss: 0.3863, Train: 87.36%, Valid: 87.23%, Test: 87.36%
Epoch: 175, Loss: 0.3797, Train: 85.43%, Valid: 85.43%, Test: 85.51%
Run 01:
Highest Train: 88.16
Highest Valid: 88.24
  Final Train: 88.16
   Final Test: 88.20
All runs:
Highest Train: 88.16, nan
Highest Valid: 88.24, nan
  Final Train: 88.16, nan
   Final Test: 88.20, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 9.2870, Train: 16.97%, Valid: 17.00%, Test: 16.93%
Epoch: 25, Loss: 0.5094, Train: 86.55%, Valid: 86.55%, Test: 86.64%
Epoch: 50, Loss: 0.4906, Train: 86.47%, Valid: 86.44%, Test: 86.57%
Epoch: 75, Loss: 0.4521, Train: 86.34%, Valid: 86.31%, Test: 86.41%
Epoch: 100, Loss: 0.4203, Train: 86.64%, Valid: 86.65%, Test: 86.74%
Epoch: 125, Loss: 0.4136, Train: 86.65%, Valid: 86.63%, Test: 86.75%
Epoch: 150, Loss: 0.4001, Train: 86.74%, Valid: 86.73%, Test: 86.80%
Epoch: 175, Loss: 0.3958, Train: 86.77%, Valid: 86.74%, Test: 86.84%
Run 01:
Highest Train: 86.86
Highest Valid: 86.86
  Final Train: 86.86
   Final Test: 86.91
All runs:
Highest Train: 86.86, nan
Highest Valid: 86.86, nan
  Final Train: 86.86, nan
   Final Test: 86.91, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 655.5278, Train: 57.27%, Valid: 57.05%, Test: 56.95%
Epoch: 25, Loss: 234.7076, Train: 72.02%, Valid: 71.82%, Test: 71.79%
Epoch: 50, Loss: 90.7684, Train: 74.74%, Valid: 74.56%, Test: 74.57%
Epoch: 75, Loss: 9.5514, Train: 76.34%, Valid: 76.15%, Test: 76.22%
Epoch: 100, Loss: 9.4268, Train: 77.83%, Valid: 77.62%, Test: 77.71%
Epoch: 125, Loss: 144.0156, Train: 79.89%, Valid: 79.73%, Test: 79.80%
Epoch: 150, Loss: 5.4993, Train: 81.18%, Valid: 81.03%, Test: 81.10%
Epoch: 175, Loss: 4.7992, Train: 81.77%, Valid: 81.65%, Test: 81.72%
Run 01:
Highest Train: 83.00
Highest Valid: 82.91
  Final Train: 83.00
   Final Test: 83.03
All runs:
Highest Train: 83.00, nan
Highest Valid: 82.91, nan
  Final Train: 83.00, nan
   Final Test: 83.03, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 9.7555, Train: 15.70%, Valid: 15.70%, Test: 15.62%
Epoch: 25, Loss: 0.5518, Train: 86.05%, Valid: 86.07%, Test: 86.14%
Epoch: 50, Loss: 0.5148, Train: 86.06%, Valid: 85.89%, Test: 86.10%
Epoch: 75, Loss: 0.4763, Train: 85.98%, Valid: 85.83%, Test: 86.05%
Epoch: 100, Loss: 0.4436, Train: 87.49%, Valid: 87.58%, Test: 87.56%
Epoch: 125, Loss: 0.4193, Train: 87.97%, Valid: 88.07%, Test: 88.05%
Epoch: 150, Loss: 0.4133, Train: 85.82%, Valid: 85.93%, Test: 85.97%
Epoch: 175, Loss: 0.4011, Train: 85.45%, Valid: 85.45%, Test: 85.52%
Run 01:
Highest Train: 87.98
Highest Valid: 88.08
  Final Train: 87.98
   Final Test: 88.06
All runs:
Highest Train: 87.98, nan
Highest Valid: 88.08, nan
  Final Train: 87.98, nan
   Final Test: 88.06, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.5472, Train: 86.41%, Valid: 86.30%, Test: 86.41%
Epoch: 25, Loss: 0.5695, Train: 85.82%, Valid: 85.73%, Test: 85.86%
Epoch: 50, Loss: 0.4837, Train: 85.96%, Valid: 85.90%, Test: 86.00%
Epoch: 75, Loss: 0.4481, Train: 86.73%, Valid: 86.55%, Test: 86.70%
Epoch: 100, Loss: 0.4323, Train: 85.52%, Valid: 85.35%, Test: 85.55%
Epoch: 125, Loss: 0.4048, Train: 85.50%, Valid: 85.33%, Test: 85.55%
Epoch: 150, Loss: 0.3939, Train: 85.55%, Valid: 85.38%, Test: 85.61%
Epoch: 175, Loss: 0.4016, Train: 85.82%, Valid: 85.65%, Test: 85.81%
Run 01:
Highest Train: 87.14
Highest Valid: 86.93
  Final Train: 87.14
   Final Test: 87.06
All runs:
Highest Train: 87.14, nan
Highest Valid: 86.93, nan
  Final Train: 87.14, nan
   Final Test: 87.06, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 283.2700, Train: 64.92%, Valid: 64.68%, Test: 64.64%
Epoch: 25, Loss: 6.0136, Train: 81.51%, Valid: 81.39%, Test: 81.44%
Epoch: 50, Loss: 8.1514, Train: 79.64%, Valid: 79.49%, Test: 79.54%
Epoch: 75, Loss: 4.9629, Train: 79.81%, Valid: 79.67%, Test: 79.71%
Epoch: 100, Loss: 4.6761, Train: 79.38%, Valid: 79.22%, Test: 79.28%
Epoch: 125, Loss: 13.4986, Train: 78.27%, Valid: 78.09%, Test: 78.14%
Epoch: 150, Loss: 51.1635, Train: 79.12%, Valid: 78.95%, Test: 79.02%
Epoch: 175, Loss: 5.1211, Train: 64.79%, Valid: 64.97%, Test: 65.25%
Run 01:
Highest Train: 84.11
Highest Valid: 84.06
  Final Train: 84.11
   Final Test: 84.20
All runs:
Highest Train: 84.11, nan
Highest Valid: 84.06, nan
  Final Train: 84.11, nan
   Final Test: 84.20, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 16.0755, Train: 16.40%, Valid: 16.48%, Test: 16.37%
Epoch: 25, Loss: 0.4454, Train: 87.71%, Valid: 87.73%, Test: 87.80%
Epoch: 50, Loss: 0.4486, Train: 88.00%, Valid: 88.01%, Test: 88.04%
Epoch: 75, Loss: 0.4339, Train: 87.90%, Valid: 87.93%, Test: 87.93%
Epoch: 100, Loss: 0.4156, Train: 87.60%, Valid: 87.62%, Test: 87.65%
Epoch: 125, Loss: 0.3732, Train: 86.85%, Valid: 86.89%, Test: 86.87%
Epoch: 150, Loss: 0.3563, Train: 85.39%, Valid: 85.37%, Test: 85.46%
Epoch: 175, Loss: 0.3524, Train: 87.01%, Valid: 87.06%, Test: 87.11%
Run 01:
Highest Train: 88.03
Highest Valid: 88.04
  Final Train: 88.03
   Final Test: 88.06
All runs:
Highest Train: 88.03, nan
Highest Valid: 88.04, nan
  Final Train: 88.03, nan
   Final Test: 88.06, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 12.8334, Train: 14.40%, Valid: 14.45%, Test: 14.33%
Epoch: 25, Loss: 0.7669, Train: 85.65%, Valid: 85.58%, Test: 85.60%
Epoch: 50, Loss: 0.6957, Train: 86.37%, Valid: 86.39%, Test: 86.45%
Epoch: 75, Loss: 0.6325, Train: 85.22%, Valid: 85.02%, Test: 85.28%
Epoch: 100, Loss: 0.5714, Train: 85.18%, Valid: 84.98%, Test: 85.25%
Epoch: 125, Loss: 0.5143, Train: 85.14%, Valid: 84.97%, Test: 85.25%
Epoch: 150, Loss: 0.4809, Train: 85.25%, Valid: 85.13%, Test: 85.34%
Epoch: 175, Loss: 0.4711, Train: 85.44%, Valid: 85.32%, Test: 85.54%
Run 01:
Highest Train: 86.53
Highest Valid: 86.61
  Final Train: 86.53
   Final Test: 86.64
All runs:
Highest Train: 86.53, nan
Highest Valid: 86.61, nan
  Final Train: 86.53, nan
   Final Test: 86.64, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 17812358.0000, Train: 57.19%, Valid: 57.14%, Test: 56.94%
Epoch: 25, Loss: 124.4233, Train: 32.64%, Valid: 32.64%, Test: 32.55%
Epoch: 50, Loss: 78.4692, Train: 36.80%, Valid: 36.69%, Test: 36.67%
Epoch: 75, Loss: 105.7880, Train: 37.49%, Valid: 37.39%, Test: 37.40%
Epoch: 100, Loss: 85.3628, Train: 36.87%, Valid: 36.78%, Test: 36.81%
Epoch: 125, Loss: 129.8368, Train: 16.90%, Valid: 16.98%, Test: 16.84%
Epoch: 150, Loss: 93.4564, Train: 36.74%, Valid: 36.64%, Test: 36.68%
Epoch: 175, Loss: 134.6902, Train: 39.80%, Valid: 39.75%, Test: 39.75%
Run 01:
Highest Train: 57.19
Highest Valid: 57.14
  Final Train: 57.19
   Final Test: 56.94
All runs:
Highest Train: 57.19, nan
Highest Valid: 57.14, nan
  Final Train: 57.19, nan
   Final Test: 56.94, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 4.6873, Train: 84.54%, Valid: 84.37%, Test: 84.59%
Epoch: 25, Loss: 0.4850, Train: 85.76%, Valid: 85.76%, Test: 85.87%
Epoch: 50, Loss: 0.4562, Train: 86.07%, Valid: 86.13%, Test: 86.15%
Epoch: 75, Loss: 0.4093, Train: 85.94%, Valid: 85.95%, Test: 85.99%
Epoch: 100, Loss: 0.3858, Train: 85.71%, Valid: 85.75%, Test: 85.83%
Epoch: 125, Loss: 0.3803, Train: 85.69%, Valid: 85.73%, Test: 85.81%
Epoch: 150, Loss: 0.3790, Train: 85.70%, Valid: 85.69%, Test: 85.82%
Epoch: 175, Loss: 0.3748, Train: 85.71%, Valid: 85.73%, Test: 85.83%
Run 01:
Highest Train: 86.49
Highest Valid: 86.54
  Final Train: 86.49
   Final Test: 86.57
All runs:
Highest Train: 86.49, nan
Highest Valid: 86.54, nan
  Final Train: 86.49, nan
   Final Test: 86.57, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 3.4776, Train: 83.94%, Valid: 83.94%, Test: 84.09%
Epoch: 25, Loss: 0.4677, Train: 85.26%, Valid: 85.11%, Test: 85.34%
Epoch: 50, Loss: 0.4440, Train: 84.32%, Valid: 84.18%, Test: 84.39%
Epoch: 75, Loss: 0.4110, Train: 85.22%, Valid: 85.08%, Test: 85.30%
Epoch: 100, Loss: 0.3822, Train: 85.29%, Valid: 85.11%, Test: 85.37%
Epoch: 125, Loss: 0.3770, Train: 85.23%, Valid: 85.06%, Test: 85.31%
Epoch: 150, Loss: 0.3759, Train: 84.80%, Valid: 84.70%, Test: 84.95%
Epoch: 175, Loss: 0.3801, Train: 85.19%, Valid: 85.02%, Test: 85.28%
Run 01:
Highest Train: 85.35
Highest Valid: 85.18
  Final Train: 85.35
   Final Test: 85.44
All runs:
Highest Train: 85.35, nan
Highest Valid: 85.18, nan
  Final Train: 85.35, nan
   Final Test: 85.44, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 260.6585, Train: 61.70%, Valid: 61.67%, Test: 61.65%
Epoch: 25, Loss: 70.6184, Train: 23.55%, Valid: 23.78%, Test: 23.70%
Epoch: 50, Loss: 97.5905, Train: 39.62%, Valid: 39.32%, Test: 39.08%
Epoch: 75, Loss: 53.7228, Train: 40.56%, Valid: 40.25%, Test: 40.05%
Epoch: 100, Loss: 63.1612, Train: 48.71%, Valid: 48.41%, Test: 48.31%
Epoch: 125, Loss: 45.5446, Train: 39.15%, Valid: 38.84%, Test: 38.63%
Epoch: 150, Loss: 23.9155, Train: 39.13%, Valid: 38.85%, Test: 38.61%
Epoch: 175, Loss: 31.2425, Train: 38.48%, Valid: 38.23%, Test: 37.99%
Run 01:
Highest Train: 61.89
Highest Valid: 61.83
  Final Train: 61.89
   Final Test: 61.68
All runs:
Highest Train: 61.89, nan
Highest Valid: 61.83, nan
  Final Train: 61.89, nan
   Final Test: 61.68, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.9830, Train: 86.50%, Valid: 86.48%, Test: 86.62%
Epoch: 25, Loss: 0.4556, Train: 83.86%, Valid: 83.66%, Test: 83.88%
Epoch: 50, Loss: 0.4055, Train: 84.86%, Valid: 84.86%, Test: 84.91%
Epoch: 75, Loss: 0.3902, Train: 85.10%, Valid: 85.17%, Test: 85.11%
Epoch: 100, Loss: 0.3803, Train: 85.66%, Valid: 85.71%, Test: 85.67%
Epoch: 125, Loss: 0.3735, Train: 85.57%, Valid: 85.63%, Test: 85.62%
Epoch: 150, Loss: 0.3725, Train: 85.67%, Valid: 85.72%, Test: 85.69%
Epoch: 175, Loss: 0.3705, Train: 85.69%, Valid: 85.73%, Test: 85.77%
Run 01:
Highest Train: 88.32
Highest Valid: 88.36
  Final Train: 88.32
   Final Test: 88.41
All runs:
Highest Train: 88.32, nan
Highest Valid: 88.36, nan
  Final Train: 88.32, nan
   Final Test: 88.41, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.9938, Train: 84.75%, Valid: 84.78%, Test: 84.78%
Epoch: 25, Loss: 0.4775, Train: 85.59%, Valid: 85.43%, Test: 85.69%
Epoch: 50, Loss: 0.4198, Train: 85.44%, Valid: 85.29%, Test: 85.51%
Epoch: 75, Loss: 0.3972, Train: 85.43%, Valid: 85.25%, Test: 85.53%
Epoch: 100, Loss: 0.3861, Train: 85.47%, Valid: 85.41%, Test: 85.53%
Epoch: 125, Loss: 0.3776, Train: 85.52%, Valid: 85.48%, Test: 85.63%
Epoch: 150, Loss: 0.3753, Train: 85.61%, Valid: 85.59%, Test: 85.72%
Epoch: 175, Loss: 0.3679, Train: 85.52%, Valid: 85.44%, Test: 85.57%
Run 01:
Highest Train: 86.93
Highest Valid: 86.88
  Final Train: 86.93
   Final Test: 86.95
All runs:
Highest Train: 86.93, nan
Highest Valid: 86.88, nan
  Final Train: 86.93, nan
   Final Test: 86.95, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 342.1417, Train: 70.99%, Valid: 70.81%, Test: 70.74%
Epoch: 25, Loss: 39.7374, Train: 54.53%, Valid: 54.53%, Test: 54.56%
Epoch: 50, Loss: 11.5990, Train: 47.92%, Valid: 47.93%, Test: 48.03%
Epoch: 75, Loss: 12.7905, Train: 47.31%, Valid: 47.35%, Test: 47.38%
Epoch: 100, Loss: 24.6514, Train: 76.14%, Valid: 75.92%, Test: 76.02%
Epoch: 125, Loss: 53.1806, Train: 64.16%, Valid: 64.08%, Test: 64.29%
Epoch: 150, Loss: 23.8718, Train: 78.25%, Valid: 78.01%, Test: 78.20%
Epoch: 175, Loss: 9.3166, Train: 76.99%, Valid: 76.77%, Test: 76.89%
Run 01:
Highest Train: 82.68
Highest Valid: 82.58
  Final Train: 82.68
   Final Test: 82.72
All runs:
Highest Train: 82.68, nan
Highest Valid: 82.58, nan
  Final Train: 82.68, nan
   Final Test: 82.72, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 2.9816, Train: 84.73%, Valid: 84.55%, Test: 84.88%
Epoch: 25, Loss: 0.5876, Train: 85.64%, Valid: 85.54%, Test: 85.66%
Epoch: 50, Loss: 0.5104, Train: 85.59%, Valid: 85.47%, Test: 85.60%
Epoch: 75, Loss: 0.4494, Train: 86.62%, Valid: 86.66%, Test: 86.76%
Epoch: 100, Loss: 0.4277, Train: 86.73%, Valid: 86.73%, Test: 86.84%
Epoch: 125, Loss: 0.4133, Train: 85.76%, Valid: 85.61%, Test: 85.76%
Epoch: 150, Loss: 0.4050, Train: 85.87%, Valid: 85.73%, Test: 85.86%
Epoch: 175, Loss: 0.3891, Train: 85.94%, Valid: 85.83%, Test: 85.97%
Run 01:
Highest Train: 86.77
Highest Valid: 86.78
  Final Train: 86.77
   Final Test: 86.90
All runs:
Highest Train: 86.77, nan
Highest Valid: 86.78, nan
  Final Train: 86.77, nan
   Final Test: 86.90, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 8.9794, Train: 83.25%, Valid: 83.17%, Test: 83.28%
Epoch: 25, Loss: 0.6516, Train: 85.41%, Valid: 85.22%, Test: 85.45%
Epoch: 50, Loss: 0.5937, Train: 85.71%, Valid: 85.68%, Test: 85.76%
Epoch: 75, Loss: 0.5331, Train: 85.38%, Valid: 85.18%, Test: 85.44%
Epoch: 100, Loss: 0.4721, Train: 85.38%, Valid: 85.17%, Test: 85.42%
Epoch: 125, Loss: 0.4624, Train: 85.41%, Valid: 85.19%, Test: 85.45%
Epoch: 150, Loss: 0.4410, Train: 85.52%, Valid: 85.28%, Test: 85.54%
Epoch: 175, Loss: 0.4316, Train: 85.54%, Valid: 85.32%, Test: 85.58%
Run 01:
Highest Train: 85.71
Highest Valid: 85.68
  Final Train: 85.71
   Final Test: 85.76
All runs:
Highest Train: 85.71, nan
Highest Valid: 85.68, nan
  Final Train: 85.71, nan
   Final Test: 85.76, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 191.2765, Train: 55.86%, Valid: 55.75%, Test: 55.85%
Epoch: 25, Loss: 38.7960, Train: 47.13%, Valid: 47.08%, Test: 47.25%
Epoch: 50, Loss: 189.1574, Train: 47.51%, Valid: 47.47%, Test: 47.63%
Epoch: 75, Loss: 16.6290, Train: 55.71%, Valid: 55.77%, Test: 55.79%
Epoch: 100, Loss: 24.1903, Train: 45.90%, Valid: 45.89%, Test: 45.94%
Epoch: 125, Loss: 52.2974, Train: 76.55%, Valid: 76.38%, Test: 76.45%
Epoch: 150, Loss: 7.3433, Train: 75.34%, Valid: 75.17%, Test: 75.23%
Epoch: 175, Loss: 7.8079, Train: 79.17%, Valid: 79.01%, Test: 79.12%
Run 01:
Highest Train: 81.02
Highest Valid: 80.87
  Final Train: 81.02
   Final Test: 80.99
All runs:
Highest Train: 81.02, nan
Highest Valid: 80.87, nan
  Final Train: 81.02, nan
   Final Test: 80.99, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.4875, Train: 84.85%, Valid: 85.02%, Test: 84.84%
Epoch: 25, Loss: 0.3709, Train: 86.34%, Valid: 86.40%, Test: 86.33%
Epoch: 50, Loss: 0.3626, Train: 86.64%, Valid: 86.66%, Test: 86.77%
Epoch: 75, Loss: 0.3592, Train: 86.65%, Valid: 86.69%, Test: 86.75%
Epoch: 100, Loss: 0.3549, Train: 86.67%, Valid: 86.68%, Test: 86.72%
Epoch: 125, Loss: 0.3491, Train: 85.87%, Valid: 85.84%, Test: 85.91%
Epoch: 150, Loss: 0.3430, Train: 85.58%, Valid: 85.56%, Test: 85.67%
Epoch: 175, Loss: 0.3376, Train: 85.37%, Valid: 85.38%, Test: 85.47%
Run 01:
Highest Train: 86.85
Highest Valid: 86.91
  Final Train: 86.85
   Final Test: 86.92
All runs:
Highest Train: 86.85, nan
Highest Valid: 86.91, nan
  Final Train: 86.85, nan
   Final Test: 86.92, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.8269, Train: 84.85%, Valid: 84.78%, Test: 84.94%
Epoch: 25, Loss: 0.4974, Train: 85.38%, Valid: 85.23%, Test: 85.44%
Epoch: 50, Loss: 0.4741, Train: 85.34%, Valid: 85.19%, Test: 85.41%
Epoch: 75, Loss: 0.4650, Train: 85.29%, Valid: 85.14%, Test: 85.35%
Epoch: 100, Loss: 0.4544, Train: 85.28%, Valid: 85.13%, Test: 85.35%
Epoch: 125, Loss: 0.3880, Train: 85.38%, Valid: 85.23%, Test: 85.45%
Epoch: 150, Loss: 0.3656, Train: 85.34%, Valid: 85.19%, Test: 85.41%
Epoch: 175, Loss: 0.3607, Train: 85.34%, Valid: 85.20%, Test: 85.41%
Run 01:
Highest Train: 85.98
Highest Valid: 85.80
  Final Train: 85.98
   Final Test: 86.00
All runs:
Highest Train: 85.98, nan
Highest Valid: 85.80, nan
  Final Train: 85.98, nan
   Final Test: 86.00, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 37230424883200.0000, Train: 47.80%, Valid: 47.76%, Test: 47.87%
Epoch: 25, Loss: 113115531472535552.0000, Train: 52.51%, Valid: 52.57%, Test: 52.52%
Epoch: 50, Loss: 113442891388893079273472.0000, Train: 47.79%, Valid: 47.73%, Test: 47.85%
Epoch: 75, Loss: 33508876896799031296.0000, Train: 52.01%, Valid: 52.05%, Test: 51.96%
Epoch: 100, Loss: 1454882719405178880.0000, Train: 52.04%, Valid: 52.05%, Test: 51.97%
Epoch: 125, Loss: 773422867256705024.0000, Train: 52.03%, Valid: 52.04%, Test: 51.96%
Epoch: 150, Loss: 8401509635080585216.0000, Train: 52.21%, Valid: 52.21%, Test: 52.12%
Epoch: 175, Loss: 32777877586188435456.0000, Train: 52.21%, Valid: 52.22%, Test: 52.12%
Run 01:
Highest Train: 56.80
Highest Valid: 56.77
  Final Train: 56.80
   Final Test: 56.73
All runs:
Highest Train: 56.80, nan
Highest Valid: 56.77, nan
  Final Train: 56.80, nan
   Final Test: 56.73, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 8.2420, Train: 16.08%, Valid: 16.29%, Test: 15.97%
Epoch: 25, Loss: 0.5131, Train: 86.05%, Valid: 85.87%, Test: 86.17%
Epoch: 50, Loss: 0.4930, Train: 86.81%, Valid: 86.79%, Test: 86.88%
Epoch: 75, Loss: 0.4379, Train: 86.69%, Valid: 86.67%, Test: 86.73%
Epoch: 100, Loss: 0.4120, Train: 85.66%, Valid: 85.66%, Test: 85.76%
Epoch: 125, Loss: 0.3900, Train: 87.30%, Valid: 87.26%, Test: 87.37%
Epoch: 150, Loss: 0.3864, Train: 87.38%, Valid: 87.28%, Test: 87.39%
Epoch: 175, Loss: 0.3901, Train: 86.83%, Valid: 86.61%, Test: 86.91%
Run 01:
Highest Train: 88.30
Highest Valid: 88.32
  Final Train: 88.30
   Final Test: 88.36
All runs:
Highest Train: 88.30, nan
Highest Valid: 88.32, nan
  Final Train: 88.30, nan
   Final Test: 88.36, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.5186, Train: 85.02%, Valid: 84.88%, Test: 85.10%
Epoch: 25, Loss: 0.3818, Train: 85.41%, Valid: 85.33%, Test: 85.47%
Epoch: 50, Loss: 0.3756, Train: 85.41%, Valid: 85.27%, Test: 85.48%
Epoch: 75, Loss: 0.3693, Train: 85.41%, Valid: 85.24%, Test: 85.48%
Epoch: 100, Loss: 0.3731, Train: 85.51%, Valid: 85.35%, Test: 85.57%
Epoch: 125, Loss: 0.3660, Train: 85.51%, Valid: 85.48%, Test: 85.56%
Epoch: 150, Loss: 0.3626, Train: 86.07%, Valid: 86.04%, Test: 86.11%
Epoch: 175, Loss: 0.3599, Train: 85.93%, Valid: 85.88%, Test: 85.97%
Run 01:
Highest Train: 86.40
Highest Valid: 86.33
  Final Train: 86.40
   Final Test: 86.46
All runs:
Highest Train: 86.40, nan
Highest Valid: 86.33, nan
  Final Train: 86.40, nan
   Final Test: 86.46, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 114.9428, Train: 51.74%, Valid: 51.74%, Test: 51.65%
Epoch: 25, Loss: 12.1570, Train: 56.08%, Valid: 55.98%, Test: 56.07%
Epoch: 50, Loss: 15.7203, Train: 79.03%, Valid: 78.84%, Test: 78.96%
Epoch: 75, Loss: 12.4660, Train: 82.25%, Valid: 82.14%, Test: 82.22%
Epoch: 100, Loss: 8.1645, Train: 79.05%, Valid: 79.05%, Test: 79.26%
Epoch: 125, Loss: 10.2144, Train: 81.27%, Valid: 81.14%, Test: 81.23%
Epoch: 150, Loss: 8.0551, Train: 80.44%, Valid: 80.35%, Test: 80.41%
Epoch: 175, Loss: 7.4481, Train: 77.33%, Valid: 77.27%, Test: 77.37%
Run 01:
Highest Train: 83.06
Highest Valid: 82.98
  Final Train: 83.06
   Final Test: 83.08
All runs:
Highest Train: 83.06, nan
Highest Valid: 82.98, nan
  Final Train: 83.06, nan
   Final Test: 83.08, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 3.9314, Train: 85.54%, Valid: 85.45%, Test: 85.54%
Epoch: 25, Loss: 0.5556, Train: 86.64%, Valid: 86.63%, Test: 86.77%
Epoch: 50, Loss: 0.4998, Train: 86.85%, Valid: 86.90%, Test: 86.96%
Epoch: 75, Loss: 0.4392, Train: 86.88%, Valid: 86.93%, Test: 86.96%
Epoch: 100, Loss: 0.4312, Train: 85.95%, Valid: 85.97%, Test: 86.01%
Epoch: 125, Loss: 0.4119, Train: 86.20%, Valid: 86.22%, Test: 86.21%
Epoch: 150, Loss: 0.3992, Train: 85.71%, Valid: 85.74%, Test: 85.78%
Epoch: 175, Loss: 0.3918, Train: 85.66%, Valid: 85.67%, Test: 85.71%
Run 01:
Highest Train: 86.97
Highest Valid: 87.02
  Final Train: 86.97
   Final Test: 87.06
All runs:
Highest Train: 86.97, nan
Highest Valid: 87.02, nan
  Final Train: 86.97, nan
   Final Test: 87.06, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 0.4228, Train: 84.86%, Valid: 84.73%, Test: 84.95%
Epoch: 25, Loss: 0.3841, Train: 85.33%, Valid: 85.24%, Test: 85.37%
Epoch: 50, Loss: 0.3726, Train: 85.48%, Valid: 85.33%, Test: 85.55%
Epoch: 75, Loss: 0.3677, Train: 85.56%, Valid: 85.41%, Test: 85.62%
Epoch: 100, Loss: 0.3653, Train: 85.57%, Valid: 85.42%, Test: 85.63%
Epoch: 125, Loss: 0.3603, Train: 85.70%, Valid: 85.53%, Test: 85.78%
Epoch: 150, Loss: 0.3583, Train: 86.62%, Valid: 86.53%, Test: 86.69%
Epoch: 175, Loss: 0.3564, Train: 85.56%, Valid: 85.39%, Test: 85.62%
Run 01:
Highest Train: 86.65
Highest Valid: 86.53
  Final Train: 86.65
   Final Test: 86.68
All runs:
Highest Train: 86.65, nan
Highest Valid: 86.53, nan
  Final Train: 86.65, nan
   Final Test: 86.68, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 84.7993, Train: 48.01%, Valid: 47.95%, Test: 48.01%
Epoch: 25, Loss: 11.9768, Train: 48.17%, Valid: 48.19%, Test: 48.22%
Epoch: 50, Loss: 10.3084, Train: 51.33%, Valid: 51.38%, Test: 51.29%
Epoch: 75, Loss: 46.0509, Train: 43.33%, Valid: 43.53%, Test: 43.57%
Epoch: 100, Loss: 7.4549, Train: 50.49%, Valid: 50.51%, Test: 50.49%
Epoch: 125, Loss: 8.1249, Train: 52.64%, Valid: 52.73%, Test: 52.64%
Epoch: 150, Loss: 9.0392, Train: 50.04%, Valid: 50.04%, Test: 50.04%
Epoch: 175, Loss: 8.6944, Train: 49.53%, Valid: 49.53%, Test: 49.53%
Run 01:
Highest Train: 83.21
Highest Valid: 83.09
  Final Train: 83.21
   Final Test: 83.23
All runs:
Highest Train: 83.21, nan
Highest Valid: 83.09, nan
  Final Train: 83.21, nan
   Final Test: 83.23, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 5.8610, Train: 85.29%, Valid: 85.18%, Test: 85.30%
Epoch: 25, Loss: 0.6669, Train: 85.84%, Valid: 85.67%, Test: 85.81%
Epoch: 50, Loss: 0.5876, Train: 85.84%, Valid: 85.68%, Test: 85.82%
Epoch: 75, Loss: 0.5202, Train: 85.82%, Valid: 85.68%, Test: 85.81%
Epoch: 100, Loss: 0.4796, Train: 85.82%, Valid: 85.69%, Test: 85.79%
Epoch: 125, Loss: 0.4649, Train: 85.81%, Valid: 85.68%, Test: 85.79%
Epoch: 150, Loss: 0.4421, Train: 87.22%, Valid: 87.11%, Test: 87.18%
Epoch: 175, Loss: 0.4233, Train: 88.43%, Valid: 88.51%, Test: 88.47%
Run 01:
Highest Train: 88.45
Highest Valid: 88.53
  Final Train: 88.45
   Final Test: 88.48
All runs:
Highest Train: 88.45, nan
Highest Valid: 88.53, nan
  Final Train: 88.45, nan
   Final Test: 88.48, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 14.3595, Train: 41.72%, Valid: 41.50%, Test: 41.35%
Epoch: 25, Loss: 0.6213, Train: 85.55%, Valid: 85.54%, Test: 85.63%
Epoch: 50, Loss: 0.6036, Train: 85.51%, Valid: 85.51%, Test: 85.60%
Epoch: 75, Loss: 0.5750, Train: 85.91%, Valid: 85.91%, Test: 86.00%
Epoch: 100, Loss: 0.5108, Train: 86.11%, Valid: 86.12%, Test: 86.21%
Epoch: 125, Loss: 0.4701, Train: 86.07%, Valid: 86.09%, Test: 86.16%
Epoch: 150, Loss: 0.4445, Train: 85.97%, Valid: 85.80%, Test: 85.98%
Epoch: 175, Loss: 0.4408, Train: 85.91%, Valid: 85.75%, Test: 85.97%
Run 01:
Highest Train: 86.48
Highest Valid: 86.50
  Final Train: 86.48
   Final Test: 86.57
All runs:
Highest Train: 86.48, nan
Highest Valid: 86.50, nan
  Final Train: 86.48, nan
   Final Test: 86.57, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=128, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
)
Epoch: 00, Loss: 307.9000, Train: 53.04%, Valid: 52.78%, Test: 52.73%
Epoch: 25, Loss: 253.5224, Train: 58.46%, Valid: 58.26%, Test: 58.19%
Epoch: 50, Loss: 86.2581, Train: 30.33%, Valid: 30.22%, Test: 29.93%
Epoch: 75, Loss: 71.6335, Train: 29.32%, Valid: 29.18%, Test: 28.89%
Epoch: 100, Loss: 66.5562, Train: 29.62%, Valid: 29.48%, Test: 29.20%
Epoch: 125, Loss: 66.8008, Train: 29.08%, Valid: 28.88%, Test: 28.69%
Epoch: 150, Loss: 53.1171, Train: 29.58%, Valid: 29.42%, Test: 29.15%
Epoch: 175, Loss: 61.6898, Train: 30.10%, Valid: 29.93%, Test: 29.71%
Run 01:
Highest Train: 58.53
Highest Valid: 58.35
  Final Train: 58.53
   Final Test: 58.25
All runs:
Highest Train: 58.53, nan
Highest Valid: 58.35, nan
  Final Train: 58.53, nan
   Final Test: 58.25, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 14.7453, Train: 15.53%, Valid: 15.43%, Test: 15.46%
Epoch: 25, Loss: 0.6529, Train: 86.80%, Valid: 86.88%, Test: 86.87%
Epoch: 50, Loss: 0.6455, Train: 86.80%, Valid: 86.82%, Test: 86.82%
Epoch: 75, Loss: 0.5736, Train: 87.11%, Valid: 87.12%, Test: 87.11%
Epoch: 100, Loss: 0.4367, Train: 85.91%, Valid: 85.91%, Test: 85.92%
Epoch: 125, Loss: 0.3556, Train: 85.63%, Valid: 85.46%, Test: 85.66%
Epoch: 150, Loss: 0.3520, Train: 87.08%, Valid: 87.09%, Test: 87.12%
Epoch: 175, Loss: 0.3456, Train: 87.14%, Valid: 87.12%, Test: 87.19%
Run 01:
Highest Train: 87.92
Highest Valid: 87.95
  Final Train: 87.92
   Final Test: 87.93
All runs:
Highest Train: 87.92, nan
Highest Valid: 87.95, nan
  Final Train: 87.92, nan
   Final Test: 87.93, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 5.0536, Train: 84.82%, Valid: 84.71%, Test: 84.87%
Epoch: 25, Loss: 0.6081, Train: 86.87%, Valid: 86.82%, Test: 86.76%
Epoch: 50, Loss: 0.5473, Train: 86.83%, Valid: 86.72%, Test: 86.72%
Epoch: 75, Loss: 0.3791, Train: 85.32%, Valid: 85.20%, Test: 85.38%
Epoch: 100, Loss: 0.3648, Train: 85.29%, Valid: 85.16%, Test: 85.37%
Epoch: 125, Loss: 0.3611, Train: 85.32%, Valid: 85.18%, Test: 85.40%
Epoch: 150, Loss: 0.3576, Train: 85.30%, Valid: 85.16%, Test: 85.37%
Epoch: 175, Loss: 0.3532, Train: 85.29%, Valid: 85.14%, Test: 85.37%
Run 01:
Highest Train: 87.13
Highest Valid: 87.06
  Final Train: 87.13
   Final Test: 87.04
All runs:
Highest Train: 87.13, nan
Highest Valid: 87.06, nan
  Final Train: 87.13, nan
   Final Test: 87.04, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 71950.7031, Train: 42.26%, Valid: 42.45%, Test: 42.52%
Epoch: 25, Loss: 3293.5632, Train: 67.42%, Valid: 67.69%, Test: 67.77%
Epoch: 50, Loss: 170.0521, Train: 78.72%, Valid: 78.50%, Test: 78.63%
Epoch: 75, Loss: 145.7189, Train: 80.49%, Valid: 80.33%, Test: 80.41%
Epoch: 100, Loss: 95.1384, Train: 81.66%, Valid: 81.52%, Test: 81.63%
Epoch: 125, Loss: 83.5940, Train: 81.95%, Valid: 81.82%, Test: 81.93%
Epoch: 150, Loss: 75.2150, Train: 82.45%, Valid: 82.37%, Test: 82.48%
Epoch: 175, Loss: 68.0609, Train: 82.62%, Valid: 82.55%, Test: 82.65%
Run 01:
Highest Train: 83.52
Highest Valid: 83.48
  Final Train: 83.52
   Final Test: 83.57
All runs:
Highest Train: 83.52, nan
Highest Valid: 83.48, nan
  Final Train: 83.52, nan
   Final Test: 83.57, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 4.4678, Train: 84.31%, Valid: 84.12%, Test: 84.38%
Epoch: 25, Loss: 0.5661, Train: 87.54%, Valid: 87.60%, Test: 87.56%
Epoch: 50, Loss: 0.4621, Train: 86.49%, Valid: 86.61%, Test: 86.59%
Epoch: 75, Loss: 0.3892, Train: 86.13%, Valid: 86.16%, Test: 86.21%
Epoch: 100, Loss: 0.3812, Train: 85.45%, Valid: 85.49%, Test: 85.57%
Epoch: 125, Loss: 0.3807, Train: 85.51%, Valid: 85.56%, Test: 85.62%
Epoch: 150, Loss: 0.3725, Train: 85.44%, Valid: 85.47%, Test: 85.52%
Epoch: 175, Loss: 0.3679, Train: 85.49%, Valid: 85.51%, Test: 85.58%
Run 01:
Highest Train: 87.54
Highest Valid: 87.60
  Final Train: 87.54
   Final Test: 87.56
All runs:
Highest Train: 87.54, nan
Highest Valid: 87.60, nan
  Final Train: 87.54, nan
   Final Test: 87.56, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 3.2530, Train: 86.59%, Valid: 86.47%, Test: 86.57%
Epoch: 25, Loss: 0.5908, Train: 86.92%, Valid: 86.85%, Test: 86.89%
Epoch: 50, Loss: 0.4778, Train: 86.66%, Valid: 86.62%, Test: 86.71%
Epoch: 75, Loss: 0.3900, Train: 85.51%, Valid: 85.34%, Test: 85.57%
Epoch: 100, Loss: 0.3846, Train: 85.61%, Valid: 85.42%, Test: 85.68%
Epoch: 125, Loss: 0.3770, Train: 85.36%, Valid: 85.24%, Test: 85.49%
Epoch: 150, Loss: 0.3706, Train: 85.56%, Valid: 85.39%, Test: 85.63%
Epoch: 175, Loss: 0.3693, Train: 85.50%, Valid: 85.30%, Test: 85.57%
Run 01:
Highest Train: 88.29
Highest Valid: 88.16
  Final Train: 88.29
   Final Test: 88.24
All runs:
Highest Train: 88.29, nan
Highest Valid: 88.16, nan
  Final Train: 88.29, nan
   Final Test: 88.24, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 613.2853, Train: 59.19%, Valid: 59.06%, Test: 58.90%
Epoch: 25, Loss: 134.5934, Train: 49.93%, Valid: 49.66%, Test: 49.62%
Epoch: 50, Loss: 32.7659, Train: 59.97%, Valid: 59.84%, Test: 59.72%
Epoch: 75, Loss: 25.0502, Train: 78.72%, Valid: 78.51%, Test: 78.63%
Epoch: 100, Loss: 14.5981, Train: 81.24%, Valid: 81.10%, Test: 81.18%
Epoch: 125, Loss: 11.9010, Train: 81.92%, Valid: 81.80%, Test: 81.88%
Epoch: 150, Loss: 7.5859, Train: 82.86%, Valid: 82.77%, Test: 82.88%
Epoch: 175, Loss: 11.4329, Train: 83.21%, Valid: 83.12%, Test: 83.25%
Run 01:
Highest Train: 83.33
Highest Valid: 83.24
  Final Train: 83.33
   Final Test: 83.37
All runs:
Highest Train: 83.33, nan
Highest Valid: 83.24, nan
  Final Train: 83.33, nan
   Final Test: 83.37, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 14.7852, Train: 16.49%, Valid: 16.41%, Test: 16.38%
Epoch: 25, Loss: 0.8074, Train: 85.73%, Valid: 85.63%, Test: 85.79%
Epoch: 50, Loss: 0.6972, Train: 85.71%, Valid: 85.60%, Test: 85.76%
Epoch: 75, Loss: 0.5662, Train: 85.71%, Valid: 85.61%, Test: 85.77%
Epoch: 100, Loss: 0.4633, Train: 85.66%, Valid: 85.56%, Test: 85.72%
Epoch: 125, Loss: 0.4125, Train: 87.13%, Valid: 87.15%, Test: 87.20%
Epoch: 150, Loss: 0.3941, Train: 85.69%, Valid: 85.56%, Test: 85.75%
Epoch: 175, Loss: 0.3892, Train: 85.67%, Valid: 85.55%, Test: 85.74%
Run 01:
Highest Train: 87.14
Highest Valid: 87.16
  Final Train: 87.14
   Final Test: 87.20
All runs:
Highest Train: 87.14, nan
Highest Valid: 87.16, nan
  Final Train: 87.14, nan
   Final Test: 87.20, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 0.8540, Train: 85.15%, Valid: 84.99%, Test: 85.13%
Epoch: 25, Loss: 0.5087, Train: 86.01%, Valid: 85.90%, Test: 86.08%
Epoch: 50, Loss: 0.4197, Train: 86.19%, Valid: 86.08%, Test: 86.23%
Epoch: 75, Loss: 0.3821, Train: 86.42%, Valid: 86.37%, Test: 86.56%
Epoch: 100, Loss: 0.3752, Train: 85.57%, Valid: 85.40%, Test: 85.62%
Epoch: 125, Loss: 0.3685, Train: 86.19%, Valid: 86.00%, Test: 86.23%
Epoch: 150, Loss: 0.3628, Train: 86.26%, Valid: 86.25%, Test: 86.33%
Epoch: 175, Loss: 0.3585, Train: 85.70%, Valid: 85.55%, Test: 85.76%
Run 01:
Highest Train: 86.98
Highest Valid: 86.88
  Final Train: 86.98
   Final Test: 87.05
All runs:
Highest Train: 86.98, nan
Highest Valid: 86.88, nan
  Final Train: 86.98, nan
   Final Test: 87.05, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 998.3413, Train: 44.08%, Valid: 43.81%, Test: 43.61%
Epoch: 25, Loss: 240.9361, Train: 34.39%, Valid: 34.15%, Test: 33.93%
Epoch: 50, Loss: 259.5134, Train: 35.89%, Valid: 35.64%, Test: 35.42%
Epoch: 75, Loss: 139.3138, Train: 37.54%, Valid: 37.28%, Test: 37.02%
Epoch: 100, Loss: 116.7194, Train: 77.15%, Valid: 76.95%, Test: 77.04%
Epoch: 125, Loss: 4.9811, Train: 83.10%, Valid: 83.01%, Test: 83.13%
Epoch: 150, Loss: 30.8347, Train: 83.62%, Valid: 83.52%, Test: 83.70%
Epoch: 175, Loss: 6.7178, Train: 83.92%, Valid: 83.84%, Test: 84.00%
Run 01:
Highest Train: 84.00
Highest Valid: 83.92
  Final Train: 84.00
   Final Test: 84.09
All runs:
Highest Train: 84.00, nan
Highest Valid: 83.92, nan
  Final Train: 84.00, nan
   Final Test: 84.09, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 3.6531, Train: 84.13%, Valid: 83.95%, Test: 84.25%
Epoch: 25, Loss: 0.7467, Train: 87.43%, Valid: 87.52%, Test: 87.44%
Epoch: 50, Loss: 0.5011, Train: 86.27%, Valid: 86.30%, Test: 86.35%
Epoch: 75, Loss: 0.4365, Train: 85.94%, Valid: 86.02%, Test: 86.05%
Epoch: 100, Loss: 0.4181, Train: 85.86%, Valid: 85.91%, Test: 86.00%
Epoch: 125, Loss: 0.3992, Train: 85.77%, Valid: 85.83%, Test: 85.89%
Epoch: 150, Loss: 0.3833, Train: 86.14%, Valid: 86.20%, Test: 86.24%
Epoch: 175, Loss: 0.3830, Train: 87.78%, Valid: 87.81%, Test: 87.87%
Run 01:
Highest Train: 88.28
Highest Valid: 88.35
  Final Train: 88.28
   Final Test: 88.33
All runs:
Highest Train: 88.28, nan
Highest Valid: 88.35, nan
  Final Train: 88.28, nan
   Final Test: 88.33, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 7.9965, Train: 84.57%, Valid: 84.47%, Test: 84.66%
Epoch: 25, Loss: 0.7755, Train: 86.63%, Valid: 86.60%, Test: 86.68%
Epoch: 50, Loss: 0.6506, Train: 86.71%, Valid: 86.73%, Test: 86.78%
Epoch: 75, Loss: 0.5459, Train: 86.75%, Valid: 86.74%, Test: 86.79%
Epoch: 100, Loss: 0.4454, Train: 86.36%, Valid: 86.34%, Test: 86.46%
Epoch: 125, Loss: 0.4277, Train: 85.02%, Valid: 85.03%, Test: 85.21%
Epoch: 150, Loss: 0.4175, Train: 85.38%, Valid: 85.36%, Test: 85.56%
Epoch: 175, Loss: 0.3986, Train: 86.55%, Valid: 86.53%, Test: 86.61%
Run 01:
Highest Train: 86.86
Highest Valid: 86.86
  Final Train: 86.86
   Final Test: 86.88
All runs:
Highest Train: 86.86, nan
Highest Valid: 86.86, nan
  Final Train: 86.86, nan
   Final Test: 86.88, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 474.4239, Train: 61.11%, Valid: 60.99%, Test: 60.87%
Epoch: 25, Loss: 18.7834, Train: 77.38%, Valid: 77.18%, Test: 77.26%
Epoch: 50, Loss: 70.1144, Train: 82.37%, Valid: 82.28%, Test: 82.37%
Epoch: 75, Loss: 3.2759, Train: 83.08%, Valid: 82.99%, Test: 83.11%
Epoch: 100, Loss: 2.7360, Train: 83.46%, Valid: 83.34%, Test: 83.51%
Epoch: 125, Loss: 4.2642, Train: 83.76%, Valid: 83.67%, Test: 83.82%
Epoch: 150, Loss: 3.2513, Train: 83.99%, Valid: 83.91%, Test: 84.07%
Epoch: 175, Loss: 5.6005, Train: 84.05%, Valid: 83.96%, Test: 84.12%
Run 01:
Highest Train: 84.14
Highest Valid: 84.06
  Final Train: 84.14
   Final Test: 84.23
All runs:
Highest Train: 84.14, nan
Highest Valid: 84.06, nan
  Final Train: 84.14, nan
   Final Test: 84.23, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 10.9882, Train: 85.82%, Valid: 85.74%, Test: 85.80%
Epoch: 25, Loss: 0.6243, Train: 87.47%, Valid: 87.52%, Test: 87.52%
Epoch: 50, Loss: 0.5044, Train: 87.22%, Valid: 87.31%, Test: 87.25%
Epoch: 75, Loss: 0.3656, Train: 86.06%, Valid: 86.05%, Test: 86.11%
Epoch: 100, Loss: 0.3575, Train: 85.85%, Valid: 85.82%, Test: 85.93%
Epoch: 125, Loss: 0.3529, Train: 85.82%, Valid: 85.81%, Test: 85.92%
Epoch: 150, Loss: 0.3498, Train: 85.76%, Valid: 85.75%, Test: 85.84%
Epoch: 175, Loss: 0.3382, Train: 85.65%, Valid: 85.66%, Test: 85.75%
Run 01:
Highest Train: 87.57
Highest Valid: 87.62
  Final Train: 87.57
   Final Test: 87.63
All runs:
Highest Train: 87.57, nan
Highest Valid: 87.62, nan
  Final Train: 87.57, nan
   Final Test: 87.63, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.0305, Train: 83.87%, Valid: 83.80%, Test: 83.97%
Epoch: 25, Loss: 0.6567, Train: 85.83%, Valid: 85.71%, Test: 85.86%
Epoch: 50, Loss: 0.5525, Train: 85.89%, Valid: 85.78%, Test: 85.91%
Epoch: 75, Loss: 0.4603, Train: 85.30%, Valid: 85.16%, Test: 85.36%
Epoch: 100, Loss: 0.8716, Train: 85.54%, Valid: 85.36%, Test: 85.59%
Epoch: 125, Loss: 0.9589, Train: 86.71%, Valid: 86.77%, Test: 86.79%
Epoch: 150, Loss: 0.7613, Train: 86.61%, Valid: 86.65%, Test: 86.78%
Epoch: 175, Loss: 0.6479, Train: 86.50%, Valid: 86.52%, Test: 86.67%
Run 01:
Highest Train: 88.13
Highest Valid: 88.04
  Final Train: 88.13
   Final Test: 88.10
All runs:
Highest Train: 88.13, nan
Highest Valid: 88.04, nan
  Final Train: 88.13, nan
   Final Test: 88.10, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 239797.7188, Train: 58.84%, Valid: 58.72%, Test: 58.60%
Epoch: 25, Loss: 608828.9375, Train: 33.65%, Valid: 33.74%, Test: 33.84%
Epoch: 50, Loss: 55500.4805, Train: 32.01%, Valid: 32.10%, Test: 32.19%
Epoch: 75, Loss: 113653.2109, Train: 58.44%, Valid: 58.29%, Test: 58.16%
Epoch: 100, Loss: 54770.0078, Train: 25.75%, Valid: 25.94%, Test: 25.83%
Epoch: 125, Loss: 35634.3750, Train: 58.37%, Valid: 58.22%, Test: 58.07%
Epoch: 150, Loss: 34217.1055, Train: 46.37%, Valid: 46.08%, Test: 45.94%
Epoch: 175, Loss: 48162.1133, Train: 58.57%, Valid: 58.43%, Test: 58.31%
Run 01:
Highest Train: 60.22
Highest Valid: 60.08
  Final Train: 60.22
   Final Test: 59.96
All runs:
Highest Train: 60.22, nan
Highest Valid: 60.08, nan
  Final Train: 60.22, nan
   Final Test: 59.96, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.3929, Train: 86.44%, Valid: 86.30%, Test: 86.47%
Epoch: 25, Loss: 0.6326, Train: 86.59%, Valid: 86.65%, Test: 86.62%
Epoch: 50, Loss: 0.5242, Train: 86.37%, Valid: 86.40%, Test: 86.42%
Epoch: 75, Loss: 0.4108, Train: 86.01%, Valid: 86.07%, Test: 86.08%
Epoch: 100, Loss: 0.3709, Train: 86.96%, Valid: 87.01%, Test: 87.02%
Epoch: 125, Loss: 0.3661, Train: 87.15%, Valid: 87.13%, Test: 87.18%
Epoch: 150, Loss: 0.3626, Train: 87.13%, Valid: 87.16%, Test: 87.22%
Epoch: 175, Loss: 0.3624, Train: 87.19%, Valid: 87.21%, Test: 87.25%
Run 01:
Highest Train: 87.19
Highest Valid: 87.21
  Final Train: 87.18
   Final Test: 87.24
All runs:
Highest Train: 87.19, nan
Highest Valid: 87.21, nan
  Final Train: 87.18, nan
   Final Test: 87.24, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.8704, Train: 86.29%, Valid: 86.25%, Test: 86.39%
Epoch: 25, Loss: 0.6264, Train: 85.16%, Valid: 85.20%, Test: 85.22%
Epoch: 50, Loss: 0.4105, Train: 85.44%, Valid: 85.29%, Test: 85.51%
Epoch: 75, Loss: 0.3880, Train: 86.33%, Valid: 86.18%, Test: 86.34%
Epoch: 100, Loss: 0.3839, Train: 86.58%, Valid: 86.61%, Test: 86.70%
Epoch: 125, Loss: 0.3773, Train: 85.40%, Valid: 85.26%, Test: 85.50%
Epoch: 150, Loss: 0.3711, Train: 85.51%, Valid: 85.33%, Test: 85.58%
Epoch: 175, Loss: 0.3637, Train: 85.97%, Valid: 85.94%, Test: 86.04%
Run 01:
Highest Train: 86.87
Highest Valid: 86.91
  Final Train: 86.87
   Final Test: 86.91
All runs:
Highest Train: 86.87, nan
Highest Valid: 86.91, nan
  Final Train: 86.87, nan
   Final Test: 86.91, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 97.2997, Train: 73.77%, Valid: 73.58%, Test: 73.61%
Epoch: 25, Loss: 83.5677, Train: 76.75%, Valid: 76.53%, Test: 76.64%
Epoch: 50, Loss: 17.0084, Train: 81.62%, Valid: 81.50%, Test: 81.58%
Epoch: 75, Loss: 22.3605, Train: 82.43%, Valid: 82.34%, Test: 82.43%
Epoch: 100, Loss: 11.5335, Train: 81.65%, Valid: 81.52%, Test: 81.59%
Epoch: 125, Loss: 9.5752, Train: 82.29%, Valid: 82.21%, Test: 82.28%
Epoch: 150, Loss: 7.9113, Train: 82.96%, Valid: 82.88%, Test: 82.97%
Epoch: 175, Loss: 9.8259, Train: 83.03%, Valid: 82.95%, Test: 83.05%
Run 01:
Highest Train: 83.11
Highest Valid: 83.03
  Final Train: 83.11
   Final Test: 83.14
All runs:
Highest Train: 83.11, nan
Highest Valid: 83.03, nan
  Final Train: 83.11, nan
   Final Test: 83.14, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 10.8449, Train: 85.26%, Valid: 85.08%, Test: 85.36%
Epoch: 25, Loss: 0.7747, Train: 85.78%, Valid: 85.61%, Test: 85.82%
Epoch: 50, Loss: 0.6226, Train: 86.79%, Valid: 86.83%, Test: 86.87%
Epoch: 75, Loss: 0.4525, Train: 86.55%, Valid: 86.62%, Test: 86.62%
Epoch: 100, Loss: 0.4400, Train: 86.48%, Valid: 86.55%, Test: 86.51%
Epoch: 125, Loss: 0.4482, Train: 85.90%, Valid: 85.96%, Test: 85.99%
Epoch: 150, Loss: 0.4105, Train: 85.76%, Valid: 85.80%, Test: 85.85%
Epoch: 175, Loss: 0.3935, Train: 86.66%, Valid: 86.69%, Test: 86.74%
Run 01:
Highest Train: 87.64
Highest Valid: 87.64
  Final Train: 87.64
   Final Test: 87.67
All runs:
Highest Train: 87.64, nan
Highest Valid: 87.64, nan
  Final Train: 87.64, nan
   Final Test: 87.67, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 5.4116, Train: 85.44%, Valid: 85.29%, Test: 85.52%
Epoch: 25, Loss: 0.6345, Train: 84.42%, Valid: 84.47%, Test: 84.58%
Epoch: 50, Loss: 0.5417, Train: 85.31%, Valid: 85.32%, Test: 85.40%
Epoch: 75, Loss: 0.4252, Train: 86.15%, Valid: 86.14%, Test: 86.17%
Epoch: 100, Loss: 0.4030, Train: 86.42%, Valid: 86.40%, Test: 86.51%
Epoch: 125, Loss: 0.3933, Train: 86.51%, Valid: 86.45%, Test: 86.60%
Epoch: 150, Loss: 0.3863, Train: 84.99%, Valid: 84.95%, Test: 85.09%
Epoch: 175, Loss: 0.3788, Train: 85.23%, Valid: 85.17%, Test: 85.31%
Run 01:
Highest Train: 87.63
Highest Valid: 87.59
  Final Train: 87.63
   Final Test: 87.62
All runs:
Highest Train: 87.63, nan
Highest Valid: 87.59, nan
  Final Train: 87.63, nan
   Final Test: 87.62, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 419.4415, Train: 30.60%, Valid: 30.88%, Test: 30.85%
Epoch: 25, Loss: 37.9646, Train: 21.75%, Valid: 22.00%, Test: 21.84%
Epoch: 50, Loss: 146.1500, Train: 49.64%, Valid: 49.30%, Test: 49.29%
Epoch: 75, Loss: 162.0176, Train: 41.76%, Valid: 41.53%, Test: 41.28%
Epoch: 100, Loss: 49.0407, Train: 36.35%, Valid: 36.11%, Test: 35.90%
Epoch: 125, Loss: 29.3952, Train: 38.23%, Valid: 37.99%, Test: 37.76%
Epoch: 150, Loss: 23.8666, Train: 48.96%, Valid: 48.90%, Test: 48.56%
Epoch: 175, Loss: 24.5359, Train: 74.64%, Valid: 74.46%, Test: 74.51%
Run 01:
Highest Train: 80.68
Highest Valid: 80.55
  Final Train: 80.68
   Final Test: 80.63
All runs:
Highest Train: 80.68, nan
Highest Valid: 80.55, nan
  Final Train: 80.68, nan
   Final Test: 80.63, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 8.0303, Train: 88.23%, Valid: 88.25%, Test: 88.26%
Epoch: 25, Loss: 0.7512, Train: 85.42%, Valid: 85.50%, Test: 85.48%
Epoch: 50, Loss: 0.5912, Train: 86.24%, Valid: 86.27%, Test: 86.22%
Epoch: 75, Loss: 0.4842, Train: 85.89%, Valid: 86.01%, Test: 85.97%
Epoch: 100, Loss: 0.4207, Train: 85.46%, Valid: 85.54%, Test: 85.54%
Epoch: 125, Loss: 0.4042, Train: 85.56%, Valid: 85.60%, Test: 85.62%
Epoch: 150, Loss: 0.4043, Train: 85.61%, Valid: 85.66%, Test: 85.69%
Epoch: 175, Loss: 0.3897, Train: 85.60%, Valid: 85.66%, Test: 85.68%
Run 01:
Highest Train: 88.23
Highest Valid: 88.25
  Final Train: 88.23
   Final Test: 88.26
All runs:
Highest Train: 88.23, nan
Highest Valid: 88.25, nan
  Final Train: 88.23, nan
   Final Test: 88.26, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 2.4460, Train: 85.32%, Valid: 85.15%, Test: 85.37%
Epoch: 25, Loss: 0.6405, Train: 86.27%, Valid: 86.24%, Test: 86.31%
Epoch: 50, Loss: 0.4657, Train: 86.55%, Valid: 86.54%, Test: 86.61%
Epoch: 75, Loss: 0.4079, Train: 85.47%, Valid: 85.31%, Test: 85.54%
Epoch: 100, Loss: 0.4015, Train: 85.58%, Valid: 85.42%, Test: 85.65%
Epoch: 125, Loss: 0.3935, Train: 85.64%, Valid: 85.47%, Test: 85.70%
Epoch: 150, Loss: 0.3872, Train: 85.63%, Valid: 85.45%, Test: 85.70%
Epoch: 175, Loss: 0.3797, Train: 85.58%, Valid: 85.40%, Test: 85.66%
Run 01:
Highest Train: 87.55
Highest Valid: 87.52
  Final Train: 87.55
   Final Test: 87.56
All runs:
Highest Train: 87.55, nan
Highest Valid: 87.52, nan
  Final Train: 87.55, nan
   Final Test: 87.56, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 211.6825, Train: 60.09%, Valid: 59.92%, Test: 59.80%
Epoch: 25, Loss: 45.2821, Train: 62.20%, Valid: 61.92%, Test: 61.86%
Epoch: 50, Loss: 565.3829, Train: 82.74%, Valid: 82.63%, Test: 82.80%
Epoch: 75, Loss: 8.8527, Train: 56.92%, Valid: 56.89%, Test: 56.72%
Epoch: 100, Loss: 110.3514, Train: 45.35%, Valid: 45.54%, Test: 45.72%
Epoch: 125, Loss: 60.9074, Train: 80.73%, Valid: 80.50%, Test: 80.70%
Epoch: 150, Loss: 7.9420, Train: 82.51%, Valid: 82.37%, Test: 82.56%
Epoch: 175, Loss: 20.5658, Train: 80.82%, Valid: 80.63%, Test: 80.78%
Run 01:
Highest Train: 82.95
Highest Valid: 82.93
  Final Train: 82.94
   Final Test: 83.11
All runs:
Highest Train: 82.95, nan
Highest Valid: 82.93, nan
  Final Train: 82.94, nan
   Final Test: 83.11, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 9.9263, Train: 84.84%, Valid: 84.87%, Test: 84.96%
Epoch: 25, Loss: 0.5447, Train: 86.93%, Valid: 86.92%, Test: 86.99%
Epoch: 50, Loss: 0.4193, Train: 86.39%, Valid: 86.42%, Test: 86.41%
Epoch: 75, Loss: 0.3602, Train: 85.58%, Valid: 85.59%, Test: 85.67%
Epoch: 100, Loss: 0.3564, Train: 85.76%, Valid: 85.76%, Test: 85.82%
Epoch: 125, Loss: 0.3514, Train: 85.70%, Valid: 85.73%, Test: 85.79%
Epoch: 150, Loss: 0.3453, Train: 85.59%, Valid: 85.61%, Test: 85.70%
Epoch: 175, Loss: 0.3358, Train: 85.51%, Valid: 85.50%, Test: 85.66%
Run 01:
Highest Train: 88.10
Highest Valid: 88.17
  Final Train: 88.10
   Final Test: 88.14
All runs:
Highest Train: 88.10, nan
Highest Valid: 88.17, nan
  Final Train: 88.10, nan
   Final Test: 88.14, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 4.9368, Train: 84.69%, Valid: 84.63%, Test: 84.80%
Epoch: 25, Loss: 0.6277, Train: 85.80%, Valid: 85.65%, Test: 85.87%
Epoch: 50, Loss: 0.5653, Train: 85.87%, Valid: 85.75%, Test: 85.95%
Epoch: 75, Loss: 0.4756, Train: 85.75%, Valid: 85.62%, Test: 85.82%
Epoch: 100, Loss: 0.4456, Train: 85.42%, Valid: 85.26%, Test: 85.47%
Epoch: 125, Loss: 0.3760, Train: 85.56%, Valid: 85.39%, Test: 85.61%
Epoch: 150, Loss: 0.3628, Train: 85.57%, Valid: 85.39%, Test: 85.63%
Epoch: 175, Loss: 0.3576, Train: 85.52%, Valid: 85.35%, Test: 85.59%
Run 01:
Highest Train: 86.13
Highest Valid: 86.05
  Final Train: 86.13
   Final Test: 86.20
All runs:
Highest Train: 86.13, nan
Highest Valid: 86.05, nan
  Final Train: 86.13, nan
   Final Test: 86.20, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.0, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 798338.4375, Train: 52.24%, Valid: 52.34%, Test: 52.20%
Epoch: 25, Loss: 112831.5156, Train: 48.72%, Valid: 48.79%, Test: 48.74%
Epoch: 50, Loss: 84365296.0000, Train: 47.59%, Valid: 47.57%, Test: 47.69%
Epoch: 75, Loss: 1238165.2500, Train: 29.92%, Valid: 30.13%, Test: 30.12%
Epoch: 100, Loss: 23660914.0000, Train: 48.54%, Valid: 48.55%, Test: 48.63%
Epoch: 125, Loss: 97779.5078, Train: 41.73%, Valid: 41.78%, Test: 41.75%
Epoch: 150, Loss: 78376352.0000, Train: 51.48%, Valid: 51.51%, Test: 51.42%
Epoch: 175, Loss: 15783561.0000, Train: 48.97%, Valid: 49.02%, Test: 49.01%
Run 01:
Highest Train: 60.25
Highest Valid: 60.07
  Final Train: 60.25
   Final Test: 60.01
All runs:
Highest Train: 60.25, nan
Highest Valid: 60.07, nan
  Final Train: 60.25, nan
   Final Test: 60.01, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 3.7746, Train: 84.23%, Valid: 84.09%, Test: 84.40%
Epoch: 25, Loss: 0.6079, Train: 86.33%, Valid: 86.39%, Test: 86.36%
Epoch: 50, Loss: 0.5031, Train: 86.68%, Valid: 86.75%, Test: 86.74%
Epoch: 75, Loss: 0.4132, Train: 86.15%, Valid: 86.23%, Test: 86.21%
Epoch: 100, Loss: 0.3806, Train: 87.10%, Valid: 86.96%, Test: 87.10%
Epoch: 125, Loss: 0.3752, Train: 85.74%, Valid: 85.76%, Test: 85.90%
Epoch: 150, Loss: 0.3665, Train: 85.93%, Valid: 85.94%, Test: 86.08%
Epoch: 175, Loss: 0.3641, Train: 85.51%, Valid: 85.52%, Test: 85.67%
Run 01:
Highest Train: 87.17
Highest Valid: 87.06
  Final Train: 87.17
   Final Test: 87.19
All runs:
Highest Train: 87.17, nan
Highest Valid: 87.06, nan
  Final Train: 87.17, nan
   Final Test: 87.19, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.2661, Train: 84.00%, Valid: 84.00%, Test: 84.12%
Epoch: 25, Loss: 0.5448, Train: 86.04%, Valid: 85.98%, Test: 86.15%
Epoch: 50, Loss: 0.4071, Train: 85.45%, Valid: 85.27%, Test: 85.51%
Epoch: 75, Loss: 0.3878, Train: 85.45%, Valid: 85.25%, Test: 85.52%
Epoch: 100, Loss: 0.3761, Train: 85.47%, Valid: 85.28%, Test: 85.54%
Epoch: 125, Loss: 0.3705, Train: 85.54%, Valid: 85.36%, Test: 85.61%
Epoch: 150, Loss: 0.3663, Train: 85.72%, Valid: 85.62%, Test: 85.74%
Epoch: 175, Loss: 0.3638, Train: 86.27%, Valid: 86.16%, Test: 86.33%
Run 01:
Highest Train: 86.67
Highest Valid: 86.55
  Final Train: 86.67
   Final Test: 86.76
All runs:
Highest Train: 86.67, nan
Highest Valid: 86.55, nan
  Final Train: 86.67, nan
   Final Test: 86.76, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.1, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 18.2153, Train: 51.89%, Valid: 51.89%, Test: 51.82%
Epoch: 25, Loss: 9.5002, Train: 48.26%, Valid: 48.18%, Test: 48.30%
Epoch: 50, Loss: 7.8514, Train: 48.00%, Valid: 47.97%, Test: 48.08%
Epoch: 75, Loss: 8.2052, Train: 50.17%, Valid: 50.17%, Test: 50.21%
Epoch: 100, Loss: 30.2215, Train: 56.59%, Valid: 56.51%, Test: 56.47%
Epoch: 125, Loss: 7.8097, Train: 53.93%, Valid: 53.94%, Test: 53.86%
Epoch: 150, Loss: 8.1265, Train: 77.85%, Valid: 77.67%, Test: 77.76%
Epoch: 175, Loss: 164.0840, Train: 83.44%, Valid: 83.34%, Test: 83.50%
Run 01:
Highest Train: 83.80
Highest Valid: 83.72
  Final Train: 83.80
   Final Test: 83.86
All runs:
Highest Train: 83.80, nan
Highest Valid: 83.72, nan
  Final Train: 83.80, nan
   Final Test: 83.86, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 13.9319, Train: 85.10%, Valid: 85.05%, Test: 85.27%
Epoch: 25, Loss: 0.8313, Train: 86.38%, Valid: 86.41%, Test: 86.43%
Epoch: 50, Loss: 0.7000, Train: 86.70%, Valid: 86.73%, Test: 86.75%
Epoch: 75, Loss: 0.5789, Train: 86.75%, Valid: 86.79%, Test: 86.80%
Epoch: 100, Loss: 0.4540, Train: 86.94%, Valid: 86.81%, Test: 86.99%
Epoch: 125, Loss: 0.4304, Train: 85.73%, Valid: 85.77%, Test: 85.78%
Epoch: 150, Loss: 0.4213, Train: 85.78%, Valid: 85.82%, Test: 85.83%
Epoch: 175, Loss: 0.4027, Train: 85.76%, Valid: 85.78%, Test: 85.82%
Run 01:
Highest Train: 86.94
Highest Valid: 86.81
  Final Train: 86.94
   Final Test: 86.99
All runs:
Highest Train: 86.94, nan
Highest Valid: 86.81, nan
  Final Train: 86.94, nan
   Final Test: 86.99, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 1.1914, Train: 85.48%, Valid: 85.32%, Test: 85.53%
Epoch: 25, Loss: 0.4881, Train: 86.39%, Valid: 86.28%, Test: 86.48%
Epoch: 50, Loss: 0.4086, Train: 85.61%, Valid: 85.46%, Test: 85.70%
Epoch: 75, Loss: 0.3791, Train: 85.49%, Valid: 85.32%, Test: 85.55%
Epoch: 100, Loss: 0.3769, Train: 85.75%, Valid: 85.59%, Test: 85.81%
Epoch: 125, Loss: 0.3730, Train: 85.67%, Valid: 85.50%, Test: 85.73%
Epoch: 150, Loss: 0.3659, Train: 85.67%, Valid: 85.49%, Test: 85.73%
Epoch: 175, Loss: 0.3642, Train: 85.64%, Valid: 85.47%, Test: 85.70%
Run 01:
Highest Train: 86.43
Highest Valid: 86.38
  Final Train: 86.43
   Final Test: 86.52
All runs:
Highest Train: 86.43, nan
Highest Valid: 86.38, nan
  Final Train: 86.43, nan
   Final Test: 86.52, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.2, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 91.0642, Train: 38.71%, Valid: 38.79%, Test: 38.93%
Epoch: 25, Loss: 287.9528, Train: 41.45%, Valid: 41.57%, Test: 41.78%
Epoch: 50, Loss: 33.7098, Train: 39.35%, Valid: 39.09%, Test: 38.84%
Epoch: 75, Loss: 21.6274, Train: 81.88%, Valid: 81.76%, Test: 81.84%
Epoch: 100, Loss: 25.5164, Train: 57.90%, Valid: 57.79%, Test: 57.65%
Epoch: 125, Loss: 17.5864, Train: 82.97%, Valid: 82.89%, Test: 83.02%
Epoch: 150, Loss: 9.6635, Train: 83.65%, Valid: 83.56%, Test: 83.70%
Epoch: 175, Loss: 10.6617, Train: 83.77%, Valid: 83.68%, Test: 83.82%
Run 01:
Highest Train: 83.97
Highest Valid: 83.89
  Final Train: 83.97
   Final Test: 84.01
All runs:
Highest Train: 83.97, nan
Highest Valid: 83.89, nan
  Final Train: 83.97, nan
   Final Test: 84.01, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 7.9907, Train: 86.15%, Valid: 86.21%, Test: 86.28%
Epoch: 25, Loss: 0.7929, Train: 86.06%, Valid: 86.13%, Test: 86.18%
Epoch: 50, Loss: 0.6275, Train: 88.04%, Valid: 88.12%, Test: 88.08%
Epoch: 75, Loss: 0.5239, Train: 87.63%, Valid: 87.70%, Test: 87.66%
Epoch: 100, Loss: 0.4410, Train: 87.57%, Valid: 87.69%, Test: 87.63%
Epoch: 125, Loss: 0.4354, Train: 87.48%, Valid: 87.57%, Test: 87.49%
Epoch: 150, Loss: 0.4149, Train: 87.52%, Valid: 87.64%, Test: 87.67%
Epoch: 175, Loss: 0.4045, Train: 87.39%, Valid: 87.46%, Test: 87.50%
Run 01:
Highest Train: 88.25
Highest Valid: 88.31
  Final Train: 88.25
   Final Test: 88.29
All runs:
Highest Train: 88.25, nan
Highest Valid: 88.31, nan
  Final Train: 88.25, nan
   Final Test: 88.29, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 13.4780, Train: 72.53%, Valid: 72.70%, Test: 72.92%
Epoch: 25, Loss: 0.8523, Train: 85.98%, Valid: 85.82%, Test: 86.05%
Epoch: 50, Loss: 0.7088, Train: 86.03%, Valid: 85.89%, Test: 86.10%
Epoch: 75, Loss: 0.5700, Train: 85.97%, Valid: 85.82%, Test: 86.02%
Epoch: 100, Loss: 0.4551, Train: 85.70%, Valid: 85.54%, Test: 85.76%
Epoch: 125, Loss: 0.4331, Train: 85.55%, Valid: 85.40%, Test: 85.61%
Epoch: 150, Loss: 0.4170, Train: 85.70%, Valid: 85.54%, Test: 85.75%
Epoch: 175, Loss: 0.4076, Train: 86.00%, Valid: 85.83%, Test: 86.07%
Run 01:
Highest Train: 86.13
Highest Valid: 86.08
  Final Train: 86.13
   Final Test: 86.19
All runs:
Highest Train: 86.13, nan
Highest Valid: 86.08, nan
  Final Train: 86.13, nan
   Final Test: 86.19, nan
Saving results to results/genius.csv
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='genius', directed=False, display_step=25, dropout=0.3, epochs=200, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.0005, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=5, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.0)
num nodes 421961 | num classes 2 | num node feats 12
MODEL: MLPNORM(
  (fc1): Linear(in_features=12, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=2, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
)
Epoch: 00, Loss: 336.0331, Train: 46.37%, Valid: 46.41%, Test: 46.50%
Epoch: 25, Loss: 93.5316, Train: 45.11%, Valid: 44.76%, Test: 44.67%
Epoch: 50, Loss: 39.7615, Train: 30.73%, Valid: 30.57%, Test: 30.36%
Epoch: 75, Loss: 50.0558, Train: 29.93%, Valid: 29.78%, Test: 29.57%
Epoch: 100, Loss: 351.0437, Train: 32.42%, Valid: 32.26%, Test: 32.06%
Epoch: 125, Loss: 46.2985, Train: 31.97%, Valid: 31.79%, Test: 31.62%
Epoch: 150, Loss: 36.3590, Train: 34.54%, Valid: 34.29%, Test: 34.20%
Epoch: 175, Loss: 35.4554, Train: 35.05%, Valid: 34.80%, Test: 34.68%
Run 01:
Highest Train: 80.20
Highest Valid: 80.01
  Final Train: 80.20
   Final Test: 80.16
All runs:
Highest Train: 80.20, nan
Highest Valid: 80.01, nan
  Final Train: 80.20, nan
   Final Test: 80.16, nan
Saving results to results/genius.csv
20211124-09:47 ---> 20211124-22:18 Totl:45058 seconds
