nohup: ignoring input
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.1, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7484, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.5411, Train: 27.40%, Valid: 27.37%, Test: 27.88%
Epoch: 50, Loss: 1.4983, Train: 34.55%, Valid: 34.35%, Test: 34.64%
Epoch: 75, Loss: 1.4874, Train: 35.08%, Valid: 34.66%, Test: 34.85%
Epoch: 100, Loss: 1.5097, Train: 34.42%, Valid: 34.01%, Test: 34.25%
Epoch: 125, Loss: 1.4644, Train: 36.37%, Valid: 35.67%, Test: 36.01%
Epoch: 150, Loss: 1.5769, Train: 34.60%, Valid: 34.33%, Test: 34.82%
Epoch: 175, Loss: 1.4408, Train: 36.90%, Valid: 36.47%, Test: 36.74%
Epoch: 200, Loss: 1.4200, Train: 38.12%, Valid: 37.52%, Test: 37.68%
Epoch: 225, Loss: 3.7341, Train: 17.06%, Valid: 17.06%, Test: 17.04%
Epoch: 250, Loss: 1.6057, Train: 30.44%, Valid: 29.98%, Test: 30.39%
Epoch: 275, Loss: 1.4813, Train: 35.26%, Valid: 35.01%, Test: 35.38%
Epoch: 300, Loss: 1.4596, Train: 36.85%, Valid: 36.35%, Test: 36.60%
Epoch: 325, Loss: 1.4470, Train: 37.30%, Valid: 36.74%, Test: 36.97%
Epoch: 350, Loss: 1.4546, Train: 36.67%, Valid: 36.10%, Test: 36.37%
Epoch: 375, Loss: 1.4631, Train: 36.71%, Valid: 36.17%, Test: 36.37%
Epoch: 400, Loss: 1.4312, Train: 38.98%, Valid: 38.09%, Test: 38.20%
Epoch: 425, Loss: 1.4210, Train: 39.67%, Valid: 38.87%, Test: 38.88%
Epoch: 450, Loss: 1.4286, Train: 39.85%, Valid: 38.92%, Test: 39.14%
Epoch: 475, Loss: 1.4167, Train: 39.65%, Valid: 38.71%, Test: 38.75%
Epoch: 500, Loss: 1.4086, Train: 39.87%, Valid: 38.79%, Test: 38.96%
Epoch: 525, Loss: 1.4270, Train: 40.04%, Valid: 38.97%, Test: 39.10%
Epoch: 550, Loss: 1.4155, Train: 38.79%, Valid: 37.71%, Test: 37.78%
Epoch: 575, Loss: 1.8371, Train: 33.80%, Valid: 32.95%, Test: 33.17%
Epoch: 600, Loss: 1.4443, Train: 37.48%, Valid: 36.43%, Test: 36.76%
Epoch: 625, Loss: 1.4138, Train: 39.53%, Valid: 38.08%, Test: 38.28%
Epoch: 650, Loss: 1.4152, Train: 39.88%, Valid: 38.42%, Test: 38.65%
Epoch: 675, Loss: 1.3804, Train: 41.42%, Valid: 39.73%, Test: 39.98%
Epoch: 700, Loss: 1.4572, Train: 34.07%, Valid: 33.12%, Test: 33.03%
Epoch: 725, Loss: 1.3889, Train: 40.86%, Valid: 39.39%, Test: 39.56%
Epoch: 750, Loss: 1.4340, Train: 37.49%, Valid: 36.65%, Test: 36.71%
Epoch: 775, Loss: 1.6393, Train: 32.03%, Valid: 31.21%, Test: 31.56%
Epoch: 800, Loss: 1.4204, Train: 38.96%, Valid: 37.67%, Test: 37.96%
Epoch: 825, Loss: 1.3971, Train: 40.64%, Valid: 38.84%, Test: 39.36%
Epoch: 850, Loss: 1.4003, Train: 39.18%, Valid: 37.88%, Test: 38.04%
Epoch: 875, Loss: 1.7006, Train: 22.93%, Valid: 22.73%, Test: 23.27%
Epoch: 900, Loss: 1.4210, Train: 38.07%, Valid: 36.74%, Test: 36.81%
Epoch: 925, Loss: 1.4382, Train: 38.04%, Valid: 36.73%, Test: 37.05%
Epoch: 950, Loss: 1.4381, Train: 38.74%, Valid: 37.18%, Test: 37.43%
Epoch: 975, Loss: 1.3710, Train: 41.27%, Valid: 39.14%, Test: 39.54%
Run 01:
Highest Train: 42.46
Highest Valid: 41.11
  Final Train: 42.30
   Final Test: 41.01
All runs:
Highest Train: 42.46, nan
Highest Valid: 41.11, nan
  Final Train: 42.30, nan
   Final Test: 41.01, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.1, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7256, Train: 17.64%, Valid: 17.78%, Test: 17.32%
Epoch: 25, Loss: 1.5581, Train: 29.49%, Valid: 29.23%, Test: 29.23%
Epoch: 50, Loss: 1.4541, Train: 22.32%, Valid: 22.24%, Test: 22.38%
Epoch: 75, Loss: 1.5176, Train: 34.70%, Valid: 34.43%, Test: 34.58%
Epoch: 100, Loss: 1.4421, Train: 37.30%, Valid: 36.90%, Test: 37.31%
Epoch: 125, Loss: 1.4038, Train: 38.94%, Valid: 38.57%, Test: 38.77%
Epoch: 150, Loss: 4.3970, Train: 18.98%, Valid: 19.23%, Test: 19.31%
Epoch: 175, Loss: 2.6830, Train: 34.10%, Valid: 33.79%, Test: 34.01%
Epoch: 200, Loss: 1.7715, Train: 35.14%, Valid: 34.71%, Test: 34.89%
Epoch: 225, Loss: 1.6393, Train: 31.01%, Valid: 30.55%, Test: 31.02%
Epoch: 250, Loss: 1.5475, Train: 35.91%, Valid: 34.99%, Test: 35.50%
Epoch: 275, Loss: 1.5119, Train: 31.13%, Valid: 30.65%, Test: 31.24%
Epoch: 300, Loss: 1.5882, Train: 28.14%, Valid: 27.56%, Test: 28.03%
Epoch: 325, Loss: 1.4507, Train: 37.21%, Valid: 36.52%, Test: 36.82%
Epoch: 350, Loss: 1.4704, Train: 38.63%, Valid: 37.55%, Test: 37.93%
Epoch: 375, Loss: 1.4469, Train: 35.54%, Valid: 34.79%, Test: 35.36%
Epoch: 400, Loss: 1.4903, Train: 36.29%, Valid: 35.71%, Test: 35.86%
Epoch: 425, Loss: 1.4393, Train: 38.07%, Valid: 37.11%, Test: 37.55%
Epoch: 450, Loss: 1.5097, Train: 38.41%, Valid: 37.53%, Test: 37.74%
Epoch: 475, Loss: 1.5570, Train: 34.66%, Valid: 33.96%, Test: 34.37%
Epoch: 500, Loss: 1.4074, Train: 39.88%, Valid: 38.76%, Test: 39.16%
Epoch: 525, Loss: 1.4663, Train: 35.75%, Valid: 34.80%, Test: 35.18%
Epoch: 550, Loss: 1.3926, Train: 40.95%, Valid: 39.64%, Test: 39.78%
Epoch: 575, Loss: 1.7305, Train: 20.11%, Valid: 19.88%, Test: 19.47%
Epoch: 600, Loss: 1.4438, Train: 38.39%, Valid: 37.25%, Test: 37.75%
Epoch: 625, Loss: 1.4160, Train: 38.66%, Valid: 37.54%, Test: 37.77%
Epoch: 650, Loss: 1.3903, Train: 40.02%, Valid: 38.45%, Test: 38.64%
Epoch: 675, Loss: 1.4687, Train: 38.78%, Valid: 37.88%, Test: 37.96%
Epoch: 700, Loss: 1.4328, Train: 40.23%, Valid: 38.93%, Test: 39.14%
Epoch: 725, Loss: 1.3840, Train: 39.80%, Valid: 38.12%, Test: 38.59%
Epoch: 750, Loss: 1.3398, Train: 42.48%, Valid: 40.63%, Test: 40.76%
Epoch: 775, Loss: 1.4010, Train: 36.76%, Valid: 35.50%, Test: 35.76%
Epoch: 800, Loss: 1.4162, Train: 32.16%, Valid: 30.84%, Test: 31.06%
Epoch: 825, Loss: 1.3307, Train: 42.66%, Valid: 40.97%, Test: 41.00%
Epoch: 850, Loss: 1.3535, Train: 41.98%, Valid: 39.83%, Test: 40.16%
Epoch: 875, Loss: 1.3690, Train: 40.58%, Valid: 39.01%, Test: 39.22%
Epoch: 900, Loss: 1.3972, Train: 41.30%, Valid: 40.16%, Test: 40.27%
Epoch: 925, Loss: 1.4661, Train: 26.73%, Valid: 25.92%, Test: 26.13%
Epoch: 950, Loss: 1.3852, Train: 40.65%, Valid: 39.27%, Test: 39.28%
Epoch: 975, Loss: 1.3794, Train: 40.66%, Valid: 38.24%, Test: 38.78%
Run 01:
Highest Train: 43.68
Highest Valid: 41.65
  Final Train: 43.68
   Final Test: 41.92
All runs:
Highest Train: 43.68, nan
Highest Valid: 41.65, nan
  Final Train: 43.68, nan
   Final Test: 41.92, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.1, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7186, Train: 17.64%, Valid: 17.78%, Test: 17.32%
Epoch: 25, Loss: 1.5168, Train: 27.80%, Valid: 27.81%, Test: 27.78%
Epoch: 50, Loss: 1.4763, Train: 31.18%, Valid: 31.41%, Test: 31.06%
Epoch: 75, Loss: 1.4119, Train: 36.80%, Valid: 36.56%, Test: 37.05%
Epoch: 100, Loss: 4.0367, Train: 25.06%, Valid: 25.02%, Test: 25.11%
Epoch: 125, Loss: 1.5788, Train: 32.28%, Valid: 32.19%, Test: 32.15%
Epoch: 150, Loss: 1.5183, Train: 29.83%, Valid: 29.75%, Test: 30.52%
Epoch: 175, Loss: 1.4663, Train: 31.85%, Valid: 31.64%, Test: 32.01%
Epoch: 200, Loss: 1.4174, Train: 33.95%, Valid: 33.55%, Test: 33.97%
Epoch: 225, Loss: 1.3719, Train: 41.21%, Valid: 40.59%, Test: 40.91%
Epoch: 250, Loss: 1.3792, Train: 40.39%, Valid: 39.65%, Test: 39.97%
Epoch: 275, Loss: 1.3572, Train: 41.70%, Valid: 40.88%, Test: 41.35%
Epoch: 300, Loss: 1.4151, Train: 41.19%, Valid: 40.62%, Test: 40.82%
Epoch: 325, Loss: 1.3610, Train: 41.41%, Valid: 40.59%, Test: 41.03%
Epoch: 350, Loss: 1.3371, Train: 41.79%, Valid: 40.80%, Test: 41.09%
Epoch: 375, Loss: 1.3333, Train: 42.62%, Valid: 41.85%, Test: 41.97%
Epoch: 400, Loss: 1.3407, Train: 42.09%, Valid: 41.21%, Test: 41.51%
Epoch: 425, Loss: 1.3197, Train: 43.39%, Valid: 42.43%, Test: 42.51%
Epoch: 450, Loss: 1.3819, Train: 40.40%, Valid: 39.52%, Test: 39.79%
Epoch: 475, Loss: 1.3590, Train: 41.02%, Valid: 40.25%, Test: 40.51%
Epoch: 500, Loss: 1.3284, Train: 39.39%, Valid: 38.51%, Test: 39.26%
Epoch: 525, Loss: 1.3394, Train: 42.39%, Valid: 41.40%, Test: 41.61%
Epoch: 550, Loss: 1.3353, Train: 42.50%, Valid: 41.65%, Test: 41.98%
Epoch: 575, Loss: 1.3736, Train: 38.87%, Valid: 38.41%, Test: 38.38%
Epoch: 600, Loss: 1.3398, Train: 42.74%, Valid: 41.63%, Test: 41.69%
Epoch: 625, Loss: 1.3317, Train: 42.71%, Valid: 41.57%, Test: 41.76%
Epoch: 650, Loss: 1.3131, Train: 43.70%, Valid: 42.57%, Test: 42.82%
Epoch: 675, Loss: 1.3138, Train: 43.71%, Valid: 42.22%, Test: 42.49%
Epoch: 700, Loss: 1.3073, Train: 41.93%, Valid: 40.98%, Test: 41.28%
Epoch: 725, Loss: 1.3223, Train: 43.53%, Valid: 42.06%, Test: 42.22%
Epoch: 750, Loss: 1.2989, Train: 42.08%, Valid: 40.97%, Test: 41.12%
Epoch: 775, Loss: 1.3550, Train: 42.36%, Valid: 41.18%, Test: 41.65%
Epoch: 800, Loss: 1.3156, Train: 43.78%, Valid: 42.40%, Test: 42.57%
Epoch: 825, Loss: 1.3870, Train: 40.63%, Valid: 39.24%, Test: 39.77%
Epoch: 850, Loss: 1.3247, Train: 43.12%, Valid: 41.53%, Test: 41.92%
Epoch: 875, Loss: 1.3852, Train: 40.69%, Valid: 39.73%, Test: 39.80%
Epoch: 900, Loss: 1.3317, Train: 42.78%, Valid: 40.98%, Test: 41.42%
Epoch: 925, Loss: 1.3105, Train: 43.97%, Valid: 42.23%, Test: 42.49%
Epoch: 950, Loss: 1.2964, Train: 43.99%, Valid: 42.32%, Test: 42.48%
Epoch: 975, Loss: 1.3022, Train: 44.41%, Valid: 42.46%, Test: 42.72%
Run 01:
Highest Train: 44.86
Highest Valid: 43.17
  Final Train: 44.72
   Final Test: 43.22
All runs:
Highest Train: 44.86, nan
Highest Valid: 43.17, nan
  Final Train: 44.72, nan
   Final Test: 43.22, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.1, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7191, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.5663, Train: 31.26%, Valid: 30.85%, Test: 31.25%
Epoch: 50, Loss: 1.5020, Train: 34.22%, Valid: 34.03%, Test: 34.29%
Epoch: 75, Loss: 1.4918, Train: 34.72%, Valid: 34.57%, Test: 34.91%
Epoch: 100, Loss: 1.4819, Train: 35.59%, Valid: 35.41%, Test: 35.54%
Epoch: 125, Loss: 1.4536, Train: 36.99%, Valid: 36.54%, Test: 36.73%
Epoch: 150, Loss: 1.4852, Train: 33.56%, Valid: 32.96%, Test: 33.40%
Epoch: 175, Loss: 2.0564, Train: 31.53%, Valid: 31.04%, Test: 31.10%
Epoch: 200, Loss: 1.4794, Train: 35.90%, Valid: 35.42%, Test: 35.52%
Epoch: 225, Loss: 1.4628, Train: 36.77%, Valid: 35.91%, Test: 36.22%
Epoch: 250, Loss: 1.4415, Train: 37.70%, Valid: 36.75%, Test: 37.02%
Epoch: 275, Loss: 1.4307, Train: 37.27%, Valid: 36.66%, Test: 36.95%
Epoch: 300, Loss: 1.4622, Train: 38.14%, Valid: 36.92%, Test: 37.27%
Epoch: 325, Loss: 1.4494, Train: 26.41%, Valid: 25.78%, Test: 25.79%
Epoch: 350, Loss: 1.4501, Train: 37.23%, Valid: 36.15%, Test: 36.48%
Epoch: 375, Loss: 1.5571, Train: 27.67%, Valid: 27.07%, Test: 27.35%
Epoch: 400, Loss: 1.5425, Train: 29.64%, Valid: 29.29%, Test: 29.54%
Epoch: 425, Loss: 1.4531, Train: 37.72%, Valid: 36.51%, Test: 36.92%
Epoch: 450, Loss: 1.4499, Train: 37.23%, Valid: 36.13%, Test: 36.58%
Epoch: 475, Loss: 1.4173, Train: 39.73%, Valid: 38.29%, Test: 38.56%
Epoch: 500, Loss: 1.4347, Train: 37.39%, Valid: 36.22%, Test: 36.62%
Epoch: 525, Loss: 1.4464, Train: 32.64%, Valid: 31.46%, Test: 31.84%
Epoch: 550, Loss: 1.6658, Train: 20.56%, Valid: 20.47%, Test: 20.29%
Epoch: 575, Loss: 1.4493, Train: 32.54%, Valid: 31.44%, Test: 31.62%
Epoch: 600, Loss: 1.8730, Train: 28.98%, Valid: 28.61%, Test: 29.21%
Epoch: 625, Loss: 1.4728, Train: 33.58%, Valid: 33.19%, Test: 33.33%
Epoch: 650, Loss: 1.4344, Train: 37.50%, Valid: 36.40%, Test: 36.74%
Epoch: 675, Loss: 1.4679, Train: 35.04%, Valid: 34.02%, Test: 34.13%
Epoch: 700, Loss: 1.4194, Train: 38.63%, Valid: 37.48%, Test: 37.95%
Epoch: 725, Loss: 1.4460, Train: 37.72%, Valid: 36.56%, Test: 37.04%
Epoch: 750, Loss: 1.4010, Train: 38.99%, Valid: 37.43%, Test: 37.62%
Epoch: 775, Loss: 1.4532, Train: 39.49%, Valid: 38.41%, Test: 38.41%
Epoch: 800, Loss: 1.6025, Train: 31.86%, Valid: 30.93%, Test: 31.45%
Epoch: 825, Loss: 1.6220, Train: 23.58%, Valid: 23.35%, Test: 22.92%
Epoch: 850, Loss: 1.4339, Train: 38.84%, Valid: 37.38%, Test: 37.56%
Epoch: 875, Loss: 1.3959, Train: 40.71%, Valid: 39.07%, Test: 39.14%
Epoch: 900, Loss: 1.4537, Train: 38.31%, Valid: 36.56%, Test: 36.71%
Epoch: 925, Loss: 1.4999, Train: 28.55%, Valid: 27.71%, Test: 27.01%
Epoch: 950, Loss: 1.3850, Train: 35.49%, Valid: 34.18%, Test: 34.22%
Epoch: 975, Loss: 1.3890, Train: 38.41%, Valid: 36.25%, Test: 36.61%
Run 01:
Highest Train: 43.45
Highest Valid: 41.71
  Final Train: 43.22
   Final Test: 41.62
All runs:
Highest Train: 43.45, nan
Highest Valid: 41.71, nan
  Final Train: 43.22, nan
   Final Test: 41.62, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.1, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7340, Train: 17.64%, Valid: 17.78%, Test: 17.32%
Epoch: 25, Loss: 1.5613, Train: 30.04%, Valid: 30.02%, Test: 30.23%
Epoch: 50, Loss: 1.7800, Train: 29.44%, Valid: 29.11%, Test: 29.48%
Epoch: 75, Loss: 1.4527, Train: 36.25%, Valid: 35.89%, Test: 36.05%
Epoch: 100, Loss: 1.4577, Train: 36.62%, Valid: 36.02%, Test: 36.51%
Epoch: 125, Loss: 2.7150, Train: 30.55%, Valid: 30.20%, Test: 30.82%
Epoch: 150, Loss: 1.9991, Train: 32.37%, Valid: 32.11%, Test: 32.31%
Epoch: 175, Loss: 1.4945, Train: 36.75%, Valid: 36.44%, Test: 36.54%
Epoch: 200, Loss: 1.5294, Train: 32.13%, Valid: 32.23%, Test: 32.50%
Epoch: 225, Loss: 1.4449, Train: 37.77%, Valid: 37.16%, Test: 37.75%
Epoch: 250, Loss: 1.4283, Train: 38.77%, Valid: 38.40%, Test: 38.72%
Epoch: 275, Loss: 1.4920, Train: 36.63%, Valid: 36.20%, Test: 36.54%
Epoch: 300, Loss: 1.4297, Train: 37.59%, Valid: 36.91%, Test: 37.14%
Epoch: 325, Loss: 1.4107, Train: 39.44%, Valid: 39.13%, Test: 39.37%
Epoch: 350, Loss: 1.3823, Train: 36.51%, Valid: 35.94%, Test: 35.99%
Epoch: 375, Loss: 1.4679, Train: 31.38%, Valid: 30.86%, Test: 31.17%
Epoch: 400, Loss: 1.4462, Train: 38.87%, Valid: 38.21%, Test: 38.63%
Epoch: 425, Loss: 1.4012, Train: 38.90%, Valid: 38.05%, Test: 38.20%
Epoch: 450, Loss: 1.3803, Train: 40.63%, Valid: 39.55%, Test: 39.79%
Epoch: 475, Loss: 1.3645, Train: 40.16%, Valid: 39.20%, Test: 39.57%
Epoch: 500, Loss: 1.3692, Train: 40.76%, Valid: 39.42%, Test: 39.79%
Epoch: 525, Loss: 1.3796, Train: 37.13%, Valid: 36.36%, Test: 36.60%
Epoch: 550, Loss: 1.4012, Train: 39.32%, Valid: 38.30%, Test: 38.47%
Epoch: 575, Loss: 1.4060, Train: 31.86%, Valid: 30.95%, Test: 31.36%
Epoch: 600, Loss: 1.4125, Train: 38.11%, Valid: 37.00%, Test: 37.30%
Epoch: 625, Loss: 1.3475, Train: 40.82%, Valid: 39.59%, Test: 39.59%
Epoch: 650, Loss: 1.3612, Train: 41.50%, Valid: 39.97%, Test: 40.44%
Epoch: 675, Loss: 1.3739, Train: 38.89%, Valid: 37.65%, Test: 38.05%
Epoch: 700, Loss: 1.3885, Train: 40.88%, Valid: 39.42%, Test: 39.70%
Epoch: 725, Loss: 2.2358, Train: 28.01%, Valid: 27.98%, Test: 27.72%
Epoch: 750, Loss: 1.4917, Train: 37.61%, Valid: 36.95%, Test: 37.22%
Epoch: 775, Loss: 1.3955, Train: 38.26%, Valid: 37.15%, Test: 37.55%
Epoch: 800, Loss: 1.4522, Train: 38.21%, Valid: 37.24%, Test: 37.17%
Epoch: 825, Loss: 1.3927, Train: 40.28%, Valid: 38.96%, Test: 39.45%
Epoch: 850, Loss: 1.3746, Train: 41.10%, Valid: 39.62%, Test: 40.01%
Epoch: 875, Loss: 1.4674, Train: 34.63%, Valid: 33.97%, Test: 33.81%
Epoch: 900, Loss: 1.3799, Train: 40.64%, Valid: 39.49%, Test: 40.08%
Epoch: 925, Loss: 1.4858, Train: 33.25%, Valid: 31.83%, Test: 32.72%
Epoch: 950, Loss: 1.3709, Train: 41.50%, Valid: 39.77%, Test: 40.19%
Epoch: 975, Loss: 1.3539, Train: 41.83%, Valid: 40.12%, Test: 40.42%
Run 01:
Highest Train: 43.04
Highest Valid: 41.68
  Final Train: 42.64
   Final Test: 41.59
All runs:
Highest Train: 43.04, nan
Highest Valid: 41.68, nan
  Final Train: 42.64, nan
   Final Test: 41.59, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.1, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7514, Train: 12.51%, Valid: 12.59%, Test: 12.43%
Epoch: 25, Loss: 1.5483, Train: 30.39%, Valid: 30.14%, Test: 30.13%
Epoch: 50, Loss: 1.4505, Train: 37.53%, Valid: 37.37%, Test: 37.62%
Epoch: 75, Loss: 1.6216, Train: 31.99%, Valid: 31.53%, Test: 31.92%
Epoch: 100, Loss: 1.4409, Train: 36.78%, Valid: 36.36%, Test: 36.60%
Epoch: 125, Loss: 1.4850, Train: 31.89%, Valid: 31.50%, Test: 31.73%
Epoch: 150, Loss: 1.3888, Train: 39.31%, Valid: 38.76%, Test: 39.05%
Epoch: 175, Loss: 1.3984, Train: 39.48%, Valid: 38.74%, Test: 39.11%
Epoch: 200, Loss: 1.4673, Train: 38.74%, Valid: 38.16%, Test: 38.31%
Epoch: 225, Loss: 1.4114, Train: 38.18%, Valid: 37.60%, Test: 37.98%
Epoch: 250, Loss: 1.3615, Train: 40.91%, Valid: 40.20%, Test: 40.55%
Epoch: 275, Loss: 1.3403, Train: 42.15%, Valid: 41.29%, Test: 41.51%
Epoch: 300, Loss: 5.3402, Train: 24.13%, Valid: 23.92%, Test: 24.19%
Epoch: 325, Loss: 1.7380, Train: 27.70%, Valid: 27.54%, Test: 28.28%
Epoch: 350, Loss: 1.6306, Train: 36.37%, Valid: 35.82%, Test: 36.28%
Epoch: 375, Loss: 1.4091, Train: 38.76%, Valid: 38.16%, Test: 38.48%
Epoch: 400, Loss: 1.3830, Train: 40.47%, Valid: 39.91%, Test: 40.13%
Epoch: 425, Loss: 2.2029, Train: 28.75%, Valid: 28.68%, Test: 29.46%
Epoch: 450, Loss: 1.4940, Train: 30.87%, Valid: 30.37%, Test: 30.68%
Epoch: 475, Loss: 1.4218, Train: 39.17%, Valid: 38.52%, Test: 38.73%
Epoch: 500, Loss: 1.4062, Train: 39.51%, Valid: 38.66%, Test: 39.02%
Epoch: 525, Loss: 1.3799, Train: 38.37%, Valid: 37.16%, Test: 37.48%
Epoch: 550, Loss: 1.4010, Train: 38.47%, Valid: 37.49%, Test: 37.92%
Epoch: 575, Loss: 1.3838, Train: 35.58%, Valid: 34.75%, Test: 35.02%
Epoch: 600, Loss: 1.4230, Train: 39.68%, Valid: 38.47%, Test: 38.71%
Epoch: 625, Loss: 1.3649, Train: 40.50%, Valid: 39.69%, Test: 39.91%
Epoch: 650, Loss: 1.3573, Train: 40.67%, Valid: 39.81%, Test: 39.87%
Epoch: 675, Loss: 1.5954, Train: 39.67%, Valid: 39.07%, Test: 39.22%
Epoch: 700, Loss: 1.3980, Train: 40.30%, Valid: 39.27%, Test: 39.42%
Epoch: 725, Loss: 1.3772, Train: 40.94%, Valid: 39.57%, Test: 39.78%
Epoch: 750, Loss: 3.9093, Train: 28.92%, Valid: 28.52%, Test: 28.89%
Epoch: 775, Loss: 4.5766, Train: 15.90%, Valid: 15.95%, Test: 15.97%
Epoch: 800, Loss: 1.6007, Train: 28.19%, Valid: 28.01%, Test: 28.03%
Epoch: 825, Loss: 1.4489, Train: 34.90%, Valid: 34.49%, Test: 34.91%
Epoch: 850, Loss: 1.3943, Train: 40.47%, Valid: 39.66%, Test: 40.06%
Epoch: 875, Loss: 1.3643, Train: 40.61%, Valid: 39.97%, Test: 40.32%
Epoch: 900, Loss: 6.9227, Train: 26.08%, Valid: 26.18%, Test: 25.62%
Epoch: 925, Loss: 2.2544, Train: 28.42%, Valid: 28.35%, Test: 29.03%
Epoch: 950, Loss: 1.6325, Train: 29.21%, Valid: 28.95%, Test: 29.18%
Epoch: 975, Loss: 1.4918, Train: 31.82%, Valid: 31.26%, Test: 31.99%
Run 01:
Highest Train: 42.81
Highest Valid: 41.50
  Final Train: 42.21
   Final Test: 41.67
All runs:
Highest Train: 42.81, nan
Highest Valid: 41.50, nan
  Final Train: 42.21, nan
   Final Test: 41.67, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.1, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7393, Train: 12.51%, Valid: 12.59%, Test: 12.43%
Epoch: 25, Loss: 1.5424, Train: 29.29%, Valid: 29.11%, Test: 29.44%
Epoch: 50, Loss: 1.4998, Train: 34.30%, Valid: 34.07%, Test: 34.52%
Epoch: 75, Loss: 1.4867, Train: 35.19%, Valid: 34.91%, Test: 35.02%
Epoch: 100, Loss: 1.6138, Train: 29.33%, Valid: 29.03%, Test: 29.33%
Epoch: 125, Loss: 1.4808, Train: 35.37%, Valid: 34.90%, Test: 35.27%
Epoch: 150, Loss: 1.4554, Train: 37.12%, Valid: 36.63%, Test: 36.93%
Epoch: 175, Loss: 1.4590, Train: 38.08%, Valid: 37.98%, Test: 38.07%
Epoch: 200, Loss: 1.5196, Train: 34.02%, Valid: 33.21%, Test: 33.70%
Epoch: 225, Loss: 1.4532, Train: 37.43%, Valid: 36.53%, Test: 37.09%
Epoch: 250, Loss: 1.4600, Train: 36.40%, Valid: 35.37%, Test: 35.82%
Epoch: 275, Loss: 1.4984, Train: 30.62%, Valid: 29.86%, Test: 30.19%
Epoch: 300, Loss: 1.4618, Train: 36.65%, Valid: 36.27%, Test: 36.48%
Epoch: 325, Loss: 1.4861, Train: 34.97%, Valid: 33.63%, Test: 34.34%
Epoch: 350, Loss: 1.4991, Train: 32.68%, Valid: 32.16%, Test: 32.51%
Epoch: 375, Loss: 1.4430, Train: 37.06%, Valid: 36.18%, Test: 36.58%
Epoch: 400, Loss: 1.4526, Train: 37.21%, Valid: 36.48%, Test: 36.63%
Epoch: 425, Loss: 1.4270, Train: 39.20%, Valid: 38.15%, Test: 38.19%
Epoch: 450, Loss: 1.5256, Train: 33.88%, Valid: 33.37%, Test: 33.42%
Epoch: 475, Loss: 1.4262, Train: 39.00%, Valid: 38.00%, Test: 37.90%
Epoch: 500, Loss: 1.5915, Train: 27.19%, Valid: 26.91%, Test: 27.18%
Epoch: 525, Loss: 1.4488, Train: 37.44%, Valid: 36.49%, Test: 36.70%
Epoch: 550, Loss: 1.4940, Train: 29.73%, Valid: 29.08%, Test: 29.48%
Epoch: 575, Loss: 1.4542, Train: 36.92%, Valid: 36.02%, Test: 36.36%
Epoch: 600, Loss: 1.4398, Train: 38.25%, Valid: 37.03%, Test: 37.25%
Epoch: 625, Loss: 1.4155, Train: 40.10%, Valid: 38.74%, Test: 38.64%
Epoch: 650, Loss: 1.3942, Train: 39.07%, Valid: 37.96%, Test: 37.96%
Epoch: 675, Loss: 1.4199, Train: 36.84%, Valid: 35.41%, Test: 35.84%
Epoch: 700, Loss: 1.4167, Train: 35.59%, Valid: 34.50%, Test: 34.67%
Epoch: 725, Loss: 2.2140, Train: 24.13%, Valid: 24.01%, Test: 24.38%
Epoch: 750, Loss: 1.4900, Train: 35.66%, Valid: 34.96%, Test: 35.18%
Epoch: 775, Loss: 1.4397, Train: 38.22%, Valid: 36.79%, Test: 37.01%
Epoch: 800, Loss: 1.4104, Train: 39.29%, Valid: 37.99%, Test: 38.30%
Epoch: 825, Loss: 1.5058, Train: 27.57%, Valid: 27.19%, Test: 26.65%
Epoch: 850, Loss: 1.4449, Train: 37.57%, Valid: 35.75%, Test: 36.29%
Epoch: 875, Loss: 1.4146, Train: 38.61%, Valid: 36.98%, Test: 37.28%
Epoch: 900, Loss: 1.4233, Train: 39.13%, Valid: 37.45%, Test: 37.95%
Epoch: 925, Loss: 1.7438, Train: 28.31%, Valid: 27.78%, Test: 27.19%
Epoch: 950, Loss: 1.4762, Train: 37.45%, Valid: 35.97%, Test: 35.89%
Epoch: 975, Loss: 1.4174, Train: 39.30%, Valid: 37.36%, Test: 37.69%
Run 01:
Highest Train: 41.96
Highest Valid: 40.60
  Final Train: 41.96
   Final Test: 40.71
All runs:
Highest Train: 41.96, nan
Highest Valid: 40.60, nan
  Final Train: 41.96, nan
   Final Test: 40.71, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.1, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7032, Train: 15.25%, Valid: 15.01%, Test: 15.17%
Epoch: 25, Loss: 1.5617, Train: 31.84%, Valid: 31.64%, Test: 31.92%
Epoch: 50, Loss: 1.4901, Train: 34.61%, Valid: 34.34%, Test: 34.57%
Epoch: 75, Loss: 1.5184, Train: 34.71%, Valid: 34.49%, Test: 34.79%
Epoch: 100, Loss: 1.4588, Train: 22.62%, Valid: 22.54%, Test: 23.20%
Epoch: 125, Loss: 1.5407, Train: 36.59%, Valid: 36.36%, Test: 36.63%
Epoch: 150, Loss: 1.4332, Train: 38.51%, Valid: 38.18%, Test: 38.30%
Epoch: 175, Loss: 1.4062, Train: 37.77%, Valid: 37.52%, Test: 37.95%
Epoch: 200, Loss: 1.4892, Train: 26.24%, Valid: 26.20%, Test: 26.59%
Epoch: 225, Loss: 1.4083, Train: 40.14%, Valid: 39.57%, Test: 39.94%
Epoch: 250, Loss: 1.5873, Train: 31.53%, Valid: 30.85%, Test: 31.56%
Epoch: 275, Loss: 1.4319, Train: 37.28%, Valid: 36.88%, Test: 37.05%
Epoch: 300, Loss: 1.4107, Train: 39.13%, Valid: 38.55%, Test: 38.67%
Epoch: 325, Loss: 1.5496, Train: 36.78%, Valid: 36.62%, Test: 36.89%
Epoch: 350, Loss: 1.4085, Train: 39.75%, Valid: 39.09%, Test: 39.37%
Epoch: 375, Loss: 1.4456, Train: 37.60%, Valid: 37.00%, Test: 37.25%
Epoch: 400, Loss: 1.5355, Train: 30.14%, Valid: 29.82%, Test: 30.08%
Epoch: 425, Loss: 1.7222, Train: 25.62%, Valid: 25.61%, Test: 25.20%
Epoch: 450, Loss: 1.6309, Train: 32.72%, Valid: 32.50%, Test: 32.87%
Epoch: 475, Loss: 1.4340, Train: 37.49%, Valid: 36.91%, Test: 37.31%
Epoch: 500, Loss: 1.4009, Train: 39.62%, Valid: 39.20%, Test: 39.46%
Epoch: 525, Loss: 1.3965, Train: 41.72%, Valid: 40.96%, Test: 40.92%
Epoch: 550, Loss: 1.4470, Train: 41.72%, Valid: 40.80%, Test: 41.12%
Epoch: 575, Loss: 1.3712, Train: 39.94%, Valid: 39.32%, Test: 39.44%
Epoch: 600, Loss: 1.3703, Train: 40.34%, Valid: 39.54%, Test: 39.65%
Epoch: 625, Loss: 1.3866, Train: 37.89%, Valid: 36.40%, Test: 36.95%
Epoch: 650, Loss: 1.3797, Train: 41.29%, Valid: 40.20%, Test: 40.58%
Epoch: 675, Loss: 1.3638, Train: 33.48%, Valid: 32.53%, Test: 33.09%
Epoch: 700, Loss: 1.3750, Train: 40.96%, Valid: 39.89%, Test: 39.96%
Epoch: 725, Loss: 1.3298, Train: 41.92%, Valid: 40.56%, Test: 40.38%
Epoch: 750, Loss: 1.3846, Train: 40.52%, Valid: 39.36%, Test: 39.75%
Epoch: 775, Loss: 1.4727, Train: 37.80%, Valid: 36.71%, Test: 36.80%
Epoch: 800, Loss: 1.3713, Train: 39.54%, Valid: 38.46%, Test: 38.77%
Epoch: 825, Loss: 1.3524, Train: 39.82%, Valid: 38.46%, Test: 38.53%
Epoch: 850, Loss: 2.3744, Train: 32.55%, Valid: 32.52%, Test: 32.32%
Epoch: 875, Loss: 1.5145, Train: 36.68%, Valid: 36.43%, Test: 36.38%
Epoch: 900, Loss: 1.5248, Train: 33.13%, Valid: 32.75%, Test: 32.89%
Epoch: 925, Loss: 1.4152, Train: 38.95%, Valid: 38.12%, Test: 38.39%
Epoch: 950, Loss: 1.3772, Train: 39.04%, Valid: 38.43%, Test: 38.70%
Epoch: 975, Loss: 1.4277, Train: 37.38%, Valid: 35.97%, Test: 36.41%
Run 01:
Highest Train: 43.89
Highest Valid: 42.42
  Final Train: 43.89
   Final Test: 42.49
All runs:
Highest Train: 43.89, nan
Highest Valid: 42.42, nan
  Final Train: 43.89, nan
   Final Test: 42.49, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.1, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7919, Train: 22.16%, Valid: 22.17%, Test: 22.74%
Epoch: 25, Loss: 1.5514, Train: 33.17%, Valid: 32.80%, Test: 33.42%
Epoch: 50, Loss: 1.4804, Train: 36.23%, Valid: 35.65%, Test: 36.19%
Epoch: 75, Loss: 1.3831, Train: 39.13%, Valid: 38.67%, Test: 38.75%
Epoch: 100, Loss: 1.5346, Train: 30.85%, Valid: 30.80%, Test: 30.91%
Epoch: 125, Loss: 1.6329, Train: 32.20%, Valid: 31.82%, Test: 32.64%
Epoch: 150, Loss: 1.4120, Train: 39.68%, Valid: 39.08%, Test: 39.42%
Epoch: 175, Loss: 1.3616, Train: 40.65%, Valid: 39.86%, Test: 40.36%
Epoch: 200, Loss: 1.3699, Train: 41.10%, Valid: 40.15%, Test: 40.65%
Epoch: 225, Loss: 1.3581, Train: 41.27%, Valid: 40.70%, Test: 40.69%
Epoch: 250, Loss: 1.3638, Train: 40.57%, Valid: 40.03%, Test: 40.33%
Epoch: 275, Loss: 1.3438, Train: 42.20%, Valid: 41.38%, Test: 41.52%
Epoch: 300, Loss: 1.3360, Train: 41.47%, Valid: 40.70%, Test: 41.01%
Epoch: 325, Loss: 7.8487, Train: 24.93%, Valid: 24.79%, Test: 24.79%
Epoch: 350, Loss: 1.9644, Train: 28.91%, Valid: 28.70%, Test: 29.09%
Epoch: 375, Loss: 2.8378, Train: 26.18%, Valid: 26.14%, Test: 26.86%
Epoch: 400, Loss: 1.5871, Train: 27.35%, Valid: 27.05%, Test: 27.11%
Epoch: 425, Loss: 1.4740, Train: 36.83%, Valid: 36.30%, Test: 37.19%
Epoch: 450, Loss: 1.4520, Train: 36.93%, Valid: 36.53%, Test: 36.63%
Epoch: 475, Loss: 1.5235, Train: 36.32%, Valid: 35.83%, Test: 35.76%
Epoch: 500, Loss: 1.4171, Train: 39.57%, Valid: 39.04%, Test: 39.36%
Epoch: 525, Loss: 1.4095, Train: 37.37%, Valid: 36.91%, Test: 37.08%
Epoch: 550, Loss: 1.4278, Train: 38.94%, Valid: 38.19%, Test: 38.53%
Epoch: 575, Loss: 1.3800, Train: 40.60%, Valid: 39.93%, Test: 40.05%
Epoch: 600, Loss: 1.3709, Train: 40.09%, Valid: 39.31%, Test: 39.62%
Epoch: 625, Loss: 1.3841, Train: 40.48%, Valid: 39.81%, Test: 39.96%
Epoch: 650, Loss: 1.3632, Train: 41.50%, Valid: 40.89%, Test: 40.93%
Epoch: 675, Loss: 1.3716, Train: 38.90%, Valid: 38.50%, Test: 38.47%
Epoch: 700, Loss: 1.3671, Train: 41.45%, Valid: 40.59%, Test: 40.97%
Epoch: 725, Loss: 1.3706, Train: 36.47%, Valid: 35.50%, Test: 36.01%
Epoch: 750, Loss: 1.3467, Train: 42.14%, Valid: 41.30%, Test: 41.48%
Epoch: 775, Loss: 1.4386, Train: 35.06%, Valid: 34.56%, Test: 35.06%
Epoch: 800, Loss: 1.3874, Train: 38.15%, Valid: 37.72%, Test: 37.68%
Epoch: 825, Loss: 1.3587, Train: 41.33%, Valid: 40.41%, Test: 40.52%
Epoch: 850, Loss: 1.3535, Train: 41.68%, Valid: 40.77%, Test: 41.12%
Epoch: 875, Loss: 1.4637, Train: 35.88%, Valid: 35.67%, Test: 35.41%
Epoch: 900, Loss: 1.3876, Train: 39.53%, Valid: 38.97%, Test: 39.20%
Epoch: 925, Loss: 1.3975, Train: 40.89%, Valid: 40.13%, Test: 40.39%
Epoch: 950, Loss: 1.4092, Train: 39.22%, Valid: 38.53%, Test: 38.78%
Epoch: 975, Loss: 1.3367, Train: 42.72%, Valid: 41.69%, Test: 41.83%
Run 01:
Highest Train: 43.08
Highest Valid: 42.15
  Final Train: 43.08
   Final Test: 42.32
All runs:
Highest Train: 43.08, nan
Highest Valid: 42.15, nan
  Final Train: 43.08, nan
   Final Test: 42.32, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.2, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7609, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.5529, Train: 31.41%, Valid: 30.80%, Test: 31.20%
Epoch: 50, Loss: 1.5028, Train: 34.46%, Valid: 34.19%, Test: 34.38%
Epoch: 75, Loss: 1.4937, Train: 35.03%, Valid: 34.61%, Test: 34.85%
Epoch: 100, Loss: 1.4876, Train: 35.25%, Valid: 34.81%, Test: 35.12%
Epoch: 125, Loss: 1.4804, Train: 35.89%, Valid: 35.45%, Test: 35.61%
Epoch: 150, Loss: 1.4976, Train: 35.89%, Valid: 35.33%, Test: 35.64%
Epoch: 175, Loss: 1.5349, Train: 32.90%, Valid: 32.62%, Test: 32.83%
Epoch: 200, Loss: 1.4517, Train: 37.24%, Valid: 36.36%, Test: 36.66%
Epoch: 225, Loss: 1.4498, Train: 37.03%, Valid: 36.46%, Test: 36.57%
Epoch: 250, Loss: 1.4571, Train: 37.78%, Valid: 37.05%, Test: 37.24%
Epoch: 275, Loss: 1.4356, Train: 37.99%, Valid: 37.40%, Test: 37.43%
Epoch: 300, Loss: 1.4472, Train: 37.75%, Valid: 36.32%, Test: 37.13%
Epoch: 325, Loss: 1.7841, Train: 27.66%, Valid: 27.56%, Test: 27.55%
Epoch: 350, Loss: 1.4728, Train: 36.54%, Valid: 35.85%, Test: 36.04%
Epoch: 375, Loss: 1.4755, Train: 36.38%, Valid: 35.85%, Test: 36.01%
Epoch: 400, Loss: 1.4379, Train: 37.60%, Valid: 36.49%, Test: 36.87%
Epoch: 425, Loss: 1.4343, Train: 36.15%, Valid: 34.97%, Test: 35.20%
Epoch: 450, Loss: 1.4145, Train: 28.62%, Valid: 27.52%, Test: 27.71%
Epoch: 475, Loss: 1.4725, Train: 37.44%, Valid: 36.58%, Test: 36.74%
Epoch: 500, Loss: 1.4121, Train: 38.87%, Valid: 37.68%, Test: 38.00%
Epoch: 525, Loss: 1.4173, Train: 38.05%, Valid: 36.63%, Test: 36.85%
Epoch: 550, Loss: 1.4063, Train: 39.52%, Valid: 38.15%, Test: 38.31%
Epoch: 575, Loss: 1.4345, Train: 35.24%, Valid: 34.35%, Test: 34.18%
Epoch: 600, Loss: 1.5190, Train: 28.73%, Valid: 27.93%, Test: 27.37%
Epoch: 625, Loss: 1.4319, Train: 39.95%, Valid: 38.54%, Test: 38.62%
Epoch: 650, Loss: 1.3909, Train: 41.01%, Valid: 39.24%, Test: 39.18%
Epoch: 675, Loss: 1.4160, Train: 37.98%, Valid: 35.93%, Test: 36.16%
Epoch: 700, Loss: 1.3708, Train: 41.86%, Valid: 39.55%, Test: 39.48%
Epoch: 725, Loss: 1.3867, Train: 40.45%, Valid: 38.03%, Test: 38.38%
Epoch: 750, Loss: 1.4785, Train: 41.75%, Valid: 39.90%, Test: 39.62%
Epoch: 775, Loss: 1.3864, Train: 40.83%, Valid: 38.59%, Test: 38.88%
Epoch: 800, Loss: 1.3821, Train: 42.57%, Valid: 40.11%, Test: 40.16%
Epoch: 825, Loss: 1.3927, Train: 37.72%, Valid: 35.72%, Test: 35.51%
Epoch: 850, Loss: 1.3923, Train: 40.75%, Valid: 38.56%, Test: 38.34%
Epoch: 875, Loss: 1.3715, Train: 41.20%, Valid: 38.64%, Test: 38.76%
Epoch: 900, Loss: 1.5270, Train: 38.80%, Valid: 36.96%, Test: 36.82%
Epoch: 925, Loss: 1.3971, Train: 39.31%, Valid: 36.30%, Test: 36.60%
Epoch: 950, Loss: 1.3769, Train: 42.61%, Valid: 39.51%, Test: 39.33%
Epoch: 975, Loss: 1.4034, Train: 37.13%, Valid: 34.43%, Test: 34.59%
Run 01:
Highest Train: 43.85
Highest Valid: 41.65
  Final Train: 43.85
   Final Test: 41.32
All runs:
Highest Train: 43.85, nan
Highest Valid: 41.65, nan
  Final Train: 43.85, nan
   Final Test: 41.32, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.2, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7509, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.5315, Train: 27.38%, Valid: 27.78%, Test: 27.76%
Epoch: 50, Loss: 1.4481, Train: 36.69%, Valid: 36.71%, Test: 37.10%
Epoch: 75, Loss: 2.0854, Train: 30.31%, Valid: 30.12%, Test: 30.30%
Epoch: 100, Loss: 1.8006, Train: 30.17%, Valid: 29.64%, Test: 30.23%
Epoch: 125, Loss: 1.4787, Train: 38.11%, Valid: 37.90%, Test: 38.48%
Epoch: 150, Loss: 1.4406, Train: 38.39%, Valid: 37.88%, Test: 38.56%
Epoch: 175, Loss: 1.4508, Train: 37.54%, Valid: 37.12%, Test: 37.49%
Epoch: 200, Loss: 1.4056, Train: 39.45%, Valid: 38.65%, Test: 39.05%
Epoch: 225, Loss: 1.4175, Train: 38.71%, Valid: 38.06%, Test: 38.44%
Epoch: 250, Loss: 1.6507, Train: 32.33%, Valid: 32.21%, Test: 32.12%
Epoch: 275, Loss: 1.4248, Train: 39.60%, Valid: 38.65%, Test: 39.15%
Epoch: 300, Loss: 1.5190, Train: 33.04%, Valid: 32.39%, Test: 32.91%
Epoch: 325, Loss: 1.4464, Train: 37.50%, Valid: 36.81%, Test: 37.14%
Epoch: 350, Loss: 1.4444, Train: 38.66%, Valid: 37.86%, Test: 38.26%
Epoch: 375, Loss: 1.4129, Train: 39.02%, Valid: 38.46%, Test: 38.39%
Epoch: 400, Loss: 1.4144, Train: 40.76%, Valid: 39.94%, Test: 40.06%
Epoch: 425, Loss: 1.6332, Train: 30.48%, Valid: 30.01%, Test: 30.60%
Epoch: 450, Loss: 1.3952, Train: 40.71%, Valid: 40.17%, Test: 40.32%
Epoch: 475, Loss: 1.3844, Train: 40.36%, Valid: 39.69%, Test: 39.88%
Epoch: 500, Loss: 1.4192, Train: 38.13%, Valid: 37.21%, Test: 37.90%
Epoch: 525, Loss: 1.4185, Train: 39.19%, Valid: 38.25%, Test: 38.58%
Epoch: 550, Loss: 4.9904, Train: 28.73%, Valid: 28.55%, Test: 28.82%
Epoch: 575, Loss: 1.5899, Train: 30.09%, Valid: 29.97%, Test: 30.09%
Epoch: 600, Loss: 1.4508, Train: 37.16%, Valid: 36.78%, Test: 37.07%
Epoch: 625, Loss: 1.4179, Train: 39.45%, Valid: 38.89%, Test: 39.17%
Epoch: 650, Loss: 1.3972, Train: 40.42%, Valid: 39.58%, Test: 40.15%
Epoch: 675, Loss: 1.4454, Train: 36.28%, Valid: 35.80%, Test: 35.80%
Epoch: 700, Loss: 1.3800, Train: 38.10%, Valid: 37.31%, Test: 37.40%
Epoch: 725, Loss: 1.4389, Train: 38.80%, Valid: 37.93%, Test: 38.10%
Epoch: 750, Loss: 1.3826, Train: 41.48%, Valid: 40.59%, Test: 40.68%
Epoch: 775, Loss: 1.3852, Train: 39.60%, Valid: 38.67%, Test: 38.97%
Epoch: 800, Loss: 1.4598, Train: 39.21%, Valid: 38.51%, Test: 38.62%
Epoch: 825, Loss: 1.3996, Train: 40.05%, Valid: 39.49%, Test: 39.50%
Epoch: 850, Loss: 1.4099, Train: 38.13%, Valid: 37.28%, Test: 37.36%
Epoch: 875, Loss: 1.3642, Train: 42.04%, Valid: 40.95%, Test: 41.00%
Epoch: 900, Loss: 3.7500, Train: 31.63%, Valid: 31.14%, Test: 31.71%
Epoch: 925, Loss: 1.5179, Train: 28.48%, Valid: 27.97%, Test: 28.27%
Epoch: 950, Loss: 1.4494, Train: 38.00%, Valid: 37.33%, Test: 37.64%
Epoch: 975, Loss: 2.5184, Train: 16.60%, Valid: 16.67%, Test: 16.70%
Run 01:
Highest Train: 43.18
Highest Valid: 41.94
  Final Train: 42.93
   Final Test: 42.21
All runs:
Highest Train: 43.18, nan
Highest Valid: 41.94, nan
  Final Train: 42.93, nan
   Final Test: 42.21, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.2, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7349, Train: 17.64%, Valid: 17.78%, Test: 17.32%
Epoch: 25, Loss: 1.5237, Train: 31.25%, Valid: 30.92%, Test: 31.10%
Epoch: 50, Loss: 1.4547, Train: 37.36%, Valid: 36.89%, Test: 37.42%
Epoch: 75, Loss: 1.3763, Train: 40.80%, Valid: 40.20%, Test: 40.75%
Epoch: 100, Loss: 1.3758, Train: 40.35%, Valid: 39.63%, Test: 40.07%
Epoch: 125, Loss: 1.3757, Train: 40.43%, Valid: 39.83%, Test: 40.18%
Epoch: 150, Loss: 3.8032, Train: 22.91%, Valid: 22.78%, Test: 23.36%
Epoch: 175, Loss: 2.1846, Train: 34.60%, Valid: 34.62%, Test: 35.17%
Epoch: 200, Loss: 1.6390, Train: 36.23%, Valid: 35.87%, Test: 36.09%
Epoch: 225, Loss: 1.6111, Train: 32.78%, Valid: 32.32%, Test: 33.02%
Epoch: 250, Loss: 1.7408, Train: 35.75%, Valid: 35.55%, Test: 35.69%
Epoch: 275, Loss: 1.4527, Train: 37.10%, Valid: 36.75%, Test: 36.92%
Epoch: 300, Loss: 1.4099, Train: 38.86%, Valid: 38.24%, Test: 38.39%
Epoch: 325, Loss: 1.3970, Train: 40.33%, Valid: 39.61%, Test: 39.95%
Epoch: 350, Loss: 1.4539, Train: 33.73%, Valid: 33.32%, Test: 33.70%
Epoch: 375, Loss: 1.4259, Train: 33.80%, Valid: 33.14%, Test: 33.14%
Epoch: 400, Loss: 1.3725, Train: 41.33%, Valid: 40.81%, Test: 40.87%
Epoch: 425, Loss: 1.5292, Train: 28.21%, Valid: 28.23%, Test: 27.97%
Epoch: 450, Loss: 1.3692, Train: 39.72%, Valid: 39.02%, Test: 39.14%
Epoch: 475, Loss: 1.3948, Train: 41.54%, Valid: 40.75%, Test: 40.94%
Epoch: 500, Loss: 1.7620, Train: 35.60%, Valid: 35.36%, Test: 35.52%
Epoch: 525, Loss: 1.5150, Train: 34.43%, Valid: 33.73%, Test: 34.35%
Epoch: 550, Loss: 1.4395, Train: 34.61%, Valid: 33.90%, Test: 34.23%
Epoch: 575, Loss: 1.4278, Train: 38.68%, Valid: 37.72%, Test: 38.06%
Epoch: 600, Loss: 1.4068, Train: 40.33%, Valid: 39.44%, Test: 39.87%
Epoch: 625, Loss: 1.3970, Train: 39.78%, Valid: 38.99%, Test: 39.06%
Epoch: 650, Loss: 2.5018, Train: 18.94%, Valid: 18.97%, Test: 18.50%
Epoch: 675, Loss: 5.8042, Train: 19.09%, Valid: 19.09%, Test: 18.77%
Epoch: 700, Loss: 1.8378, Train: 29.14%, Valid: 28.40%, Test: 29.16%
Epoch: 725, Loss: 1.4838, Train: 37.97%, Valid: 37.49%, Test: 37.93%
Epoch: 750, Loss: 1.4342, Train: 38.84%, Valid: 37.88%, Test: 38.50%
Epoch: 775, Loss: 1.4041, Train: 40.12%, Valid: 39.40%, Test: 39.71%
Epoch: 800, Loss: 1.3933, Train: 39.45%, Valid: 38.71%, Test: 38.98%
Epoch: 825, Loss: 1.3785, Train: 38.24%, Valid: 37.45%, Test: 37.84%
Epoch: 850, Loss: 1.3623, Train: 41.43%, Valid: 40.74%, Test: 41.15%
Epoch: 875, Loss: 1.3603, Train: 41.66%, Valid: 40.85%, Test: 41.12%
Epoch: 900, Loss: 1.4889, Train: 37.58%, Valid: 37.14%, Test: 37.37%
Epoch: 925, Loss: 1.3825, Train: 41.64%, Valid: 40.71%, Test: 41.12%
Epoch: 950, Loss: 1.3724, Train: 40.97%, Valid: 40.13%, Test: 40.66%
Epoch: 975, Loss: 1.3635, Train: 41.21%, Valid: 40.37%, Test: 40.58%
Run 01:
Highest Train: 42.38
Highest Valid: 41.65
  Final Train: 42.36
   Final Test: 41.73
All runs:
Highest Train: 42.38, nan
Highest Valid: 41.65, nan
  Final Train: 42.36, nan
   Final Test: 41.73, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.2, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7289, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.5494, Train: 33.42%, Valid: 33.15%, Test: 33.64%
Epoch: 50, Loss: 1.4990, Train: 34.47%, Valid: 34.25%, Test: 34.32%
Epoch: 75, Loss: 1.5160, Train: 35.40%, Valid: 34.91%, Test: 35.22%
Epoch: 100, Loss: 1.4815, Train: 35.42%, Valid: 35.08%, Test: 35.29%
Epoch: 125, Loss: 1.4596, Train: 36.49%, Valid: 35.99%, Test: 36.29%
Epoch: 150, Loss: 1.5169, Train: 29.54%, Valid: 29.47%, Test: 29.27%
Epoch: 175, Loss: 1.4638, Train: 36.19%, Valid: 35.35%, Test: 35.95%
Epoch: 200, Loss: 1.4871, Train: 36.29%, Valid: 35.72%, Test: 35.93%
Epoch: 225, Loss: 1.4481, Train: 38.67%, Valid: 37.79%, Test: 38.06%
Epoch: 250, Loss: 1.4279, Train: 39.02%, Valid: 38.29%, Test: 38.37%
Epoch: 275, Loss: 1.8134, Train: 29.91%, Valid: 29.89%, Test: 29.85%
Epoch: 300, Loss: 1.4719, Train: 36.41%, Valid: 35.72%, Test: 35.95%
Epoch: 325, Loss: 1.4671, Train: 35.12%, Valid: 34.62%, Test: 34.84%
Epoch: 350, Loss: 1.4352, Train: 40.42%, Valid: 39.36%, Test: 39.54%
Epoch: 375, Loss: 1.4146, Train: 40.38%, Valid: 39.17%, Test: 39.26%
Epoch: 400, Loss: 1.4463, Train: 37.08%, Valid: 35.96%, Test: 36.37%
Epoch: 425, Loss: 1.4414, Train: 39.29%, Valid: 38.49%, Test: 38.76%
Epoch: 450, Loss: 1.5059, Train: 36.35%, Valid: 35.44%, Test: 35.75%
Epoch: 475, Loss: 1.4622, Train: 36.55%, Valid: 35.54%, Test: 35.71%
Epoch: 500, Loss: 1.4581, Train: 38.21%, Valid: 37.11%, Test: 37.42%
Epoch: 525, Loss: 1.4433, Train: 33.03%, Valid: 32.31%, Test: 32.11%
Epoch: 550, Loss: 1.5127, Train: 33.95%, Valid: 33.53%, Test: 33.50%
Epoch: 575, Loss: 1.6748, Train: 36.70%, Valid: 35.30%, Test: 35.94%
Epoch: 600, Loss: 1.4515, Train: 36.82%, Valid: 35.83%, Test: 36.18%
Epoch: 625, Loss: 1.4305, Train: 38.84%, Valid: 37.42%, Test: 37.79%
Epoch: 650, Loss: 1.5307, Train: 37.96%, Valid: 36.97%, Test: 37.26%
Epoch: 675, Loss: 1.7189, Train: 29.86%, Valid: 29.51%, Test: 29.87%
Epoch: 700, Loss: 1.4987, Train: 37.21%, Valid: 36.22%, Test: 36.36%
Epoch: 725, Loss: 1.4580, Train: 39.36%, Valid: 38.17%, Test: 38.62%
Epoch: 750, Loss: 1.4986, Train: 30.81%, Valid: 30.16%, Test: 30.08%
Epoch: 775, Loss: 1.4453, Train: 38.39%, Valid: 36.79%, Test: 37.54%
Epoch: 800, Loss: 1.3997, Train: 25.35%, Valid: 25.02%, Test: 24.79%
Epoch: 825, Loss: 1.8863, Train: 27.40%, Valid: 27.35%, Test: 27.61%
Epoch: 850, Loss: 1.5494, Train: 35.25%, Valid: 35.06%, Test: 35.34%
Epoch: 875, Loss: 1.4706, Train: 35.80%, Valid: 35.24%, Test: 35.70%
Epoch: 900, Loss: 1.4567, Train: 37.08%, Valid: 36.56%, Test: 36.69%
Epoch: 925, Loss: 1.4765, Train: 14.23%, Valid: 14.12%, Test: 14.15%
Epoch: 950, Loss: 1.4801, Train: 36.94%, Valid: 36.19%, Test: 36.57%
Epoch: 975, Loss: 1.4328, Train: 39.11%, Valid: 38.33%, Test: 38.80%
Run 01:
Highest Train: 41.86
Highest Valid: 40.42
  Final Train: 41.86
   Final Test: 40.82
All runs:
Highest Train: 41.86, nan
Highest Valid: 40.42, nan
  Final Train: 41.86, nan
   Final Test: 40.82, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.2, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7674, Train: 19.26%, Valid: 19.05%, Test: 19.59%
Epoch: 25, Loss: 1.5414, Train: 31.28%, Valid: 30.45%, Test: 31.27%
Epoch: 50, Loss: 1.4783, Train: 36.30%, Valid: 35.92%, Test: 36.18%
Epoch: 75, Loss: 1.4217, Train: 38.05%, Valid: 37.64%, Test: 37.89%
Epoch: 100, Loss: 1.4384, Train: 39.09%, Valid: 38.43%, Test: 39.17%
Epoch: 125, Loss: 1.3951, Train: 39.89%, Valid: 39.24%, Test: 39.53%
Epoch: 150, Loss: 1.4716, Train: 37.81%, Valid: 37.38%, Test: 37.87%
Epoch: 175, Loss: 1.8705, Train: 26.79%, Valid: 26.43%, Test: 26.66%
Epoch: 200, Loss: 1.4824, Train: 35.29%, Valid: 34.94%, Test: 35.66%
Epoch: 225, Loss: 1.4682, Train: 31.71%, Valid: 31.39%, Test: 31.98%
Epoch: 250, Loss: 1.4361, Train: 38.99%, Valid: 38.47%, Test: 38.92%
Epoch: 275, Loss: 3.2273, Train: 12.53%, Valid: 12.59%, Test: 12.45%
Epoch: 300, Loss: 1.5619, Train: 34.10%, Valid: 34.09%, Test: 34.43%
Epoch: 325, Loss: 1.4535, Train: 37.37%, Valid: 37.15%, Test: 37.71%
Epoch: 350, Loss: 1.4243, Train: 38.66%, Valid: 38.33%, Test: 38.90%
Epoch: 375, Loss: 1.4204, Train: 38.23%, Valid: 38.17%, Test: 38.51%
Epoch: 400, Loss: 1.5112, Train: 37.10%, Valid: 36.81%, Test: 37.09%
Epoch: 425, Loss: 1.4008, Train: 40.76%, Valid: 40.15%, Test: 40.28%
Epoch: 450, Loss: 1.4191, Train: 39.61%, Valid: 39.15%, Test: 39.39%
Epoch: 475, Loss: 1.3965, Train: 40.94%, Valid: 40.32%, Test: 40.52%
Epoch: 500, Loss: 1.4492, Train: 30.69%, Valid: 30.38%, Test: 30.92%
Epoch: 525, Loss: 1.9130, Train: 23.16%, Valid: 23.08%, Test: 23.74%
Epoch: 550, Loss: 1.4754, Train: 35.70%, Valid: 35.54%, Test: 35.80%
Epoch: 575, Loss: 2.2871, Train: 19.33%, Valid: 19.27%, Test: 19.02%
Epoch: 600, Loss: 1.7999, Train: 33.66%, Valid: 33.26%, Test: 33.65%
Epoch: 625, Loss: 1.4688, Train: 37.32%, Valid: 36.95%, Test: 37.30%
Epoch: 650, Loss: 1.4421, Train: 38.42%, Valid: 37.82%, Test: 38.28%
Epoch: 675, Loss: 1.4411, Train: 38.67%, Valid: 38.13%, Test: 38.56%
Epoch: 700, Loss: 1.5616, Train: 37.08%, Valid: 36.66%, Test: 36.91%
Epoch: 725, Loss: 1.4482, Train: 38.12%, Valid: 37.88%, Test: 37.98%
Epoch: 750, Loss: 1.3897, Train: 40.68%, Valid: 39.73%, Test: 40.27%
Epoch: 775, Loss: 3.8333, Train: 22.20%, Valid: 22.19%, Test: 22.76%
Epoch: 800, Loss: 1.5815, Train: 34.98%, Valid: 34.58%, Test: 34.76%
Epoch: 825, Loss: 1.4828, Train: 35.45%, Valid: 35.00%, Test: 35.43%
Epoch: 850, Loss: 1.4317, Train: 38.96%, Valid: 38.36%, Test: 38.54%
Epoch: 875, Loss: 1.4194, Train: 39.99%, Valid: 39.21%, Test: 39.54%
Epoch: 900, Loss: 1.3981, Train: 40.98%, Valid: 40.27%, Test: 40.64%
Epoch: 925, Loss: 1.3970, Train: 39.46%, Valid: 38.57%, Test: 38.99%
Epoch: 950, Loss: 1.4171, Train: 39.65%, Valid: 39.15%, Test: 39.14%
Epoch: 975, Loss: 1.4063, Train: 39.94%, Valid: 39.22%, Test: 39.73%
Run 01:
Highest Train: 41.92
Highest Valid: 41.17
  Final Train: 41.92
   Final Test: 41.57
All runs:
Highest Train: 41.92, nan
Highest Valid: 41.17, nan
  Final Train: 41.92, nan
   Final Test: 41.57, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.2, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7529, Train: 12.51%, Valid: 12.59%, Test: 12.43%
Epoch: 25, Loss: 1.5096, Train: 27.33%, Valid: 26.91%, Test: 27.21%
Epoch: 50, Loss: 1.4745, Train: 35.24%, Valid: 34.85%, Test: 35.62%
Epoch: 75, Loss: 1.4281, Train: 29.76%, Valid: 29.82%, Test: 29.92%
Epoch: 100, Loss: 1.4679, Train: 35.47%, Valid: 35.06%, Test: 35.24%
Epoch: 125, Loss: 1.4896, Train: 31.23%, Valid: 31.10%, Test: 31.63%
Epoch: 150, Loss: 1.4256, Train: 35.07%, Valid: 34.95%, Test: 35.34%
Epoch: 175, Loss: 1.3963, Train: 40.91%, Valid: 40.32%, Test: 40.78%
Epoch: 200, Loss: 1.3711, Train: 40.21%, Valid: 39.63%, Test: 39.84%
Epoch: 225, Loss: 1.3825, Train: 36.21%, Valid: 35.89%, Test: 36.05%
Epoch: 250, Loss: 1.3567, Train: 41.79%, Valid: 41.23%, Test: 41.45%
Epoch: 275, Loss: 1.3710, Train: 41.76%, Valid: 40.82%, Test: 41.26%
Epoch: 300, Loss: 1.3528, Train: 42.23%, Valid: 41.52%, Test: 41.80%
Epoch: 325, Loss: 1.3903, Train: 41.25%, Valid: 40.37%, Test: 40.67%
Epoch: 350, Loss: 1.4291, Train: 25.99%, Valid: 25.91%, Test: 25.69%
Epoch: 375, Loss: 1.8432, Train: 22.62%, Valid: 22.53%, Test: 23.19%
Epoch: 400, Loss: 1.8814, Train: 34.77%, Valid: 34.37%, Test: 34.47%
Epoch: 425, Loss: 1.5620, Train: 38.04%, Valid: 37.64%, Test: 37.61%
Epoch: 450, Loss: 1.4200, Train: 38.22%, Valid: 37.94%, Test: 38.10%
Epoch: 475, Loss: 1.3593, Train: 41.90%, Valid: 41.40%, Test: 41.22%
Epoch: 500, Loss: 1.3873, Train: 41.00%, Valid: 40.26%, Test: 40.25%
Epoch: 525, Loss: 1.8346, Train: 13.64%, Valid: 13.72%, Test: 13.52%
Epoch: 550, Loss: 1.4763, Train: 37.47%, Valid: 36.97%, Test: 37.55%
Epoch: 575, Loss: 1.4433, Train: 38.59%, Valid: 38.05%, Test: 38.33%
Epoch: 600, Loss: 1.3830, Train: 31.53%, Valid: 31.19%, Test: 31.44%
Epoch: 625, Loss: 1.4897, Train: 36.91%, Valid: 36.37%, Test: 36.82%
Epoch: 650, Loss: 1.3863, Train: 40.25%, Valid: 39.36%, Test: 39.78%
Epoch: 675, Loss: 1.3882, Train: 41.11%, Valid: 40.14%, Test: 40.39%
Epoch: 700, Loss: 1.3890, Train: 40.26%, Valid: 39.61%, Test: 39.85%
Epoch: 725, Loss: 1.3467, Train: 41.33%, Valid: 40.34%, Test: 40.74%
Epoch: 750, Loss: 1.4100, Train: 35.04%, Valid: 34.04%, Test: 34.45%
Epoch: 775, Loss: 1.3548, Train: 41.86%, Valid: 40.81%, Test: 41.09%
Epoch: 800, Loss: 1.3612, Train: 42.26%, Valid: 41.21%, Test: 41.25%
Epoch: 825, Loss: 1.3390, Train: 42.52%, Valid: 41.53%, Test: 41.76%
Epoch: 850, Loss: 1.3442, Train: 42.24%, Valid: 41.21%, Test: 41.22%
Epoch: 875, Loss: 1.3591, Train: 40.79%, Valid: 39.90%, Test: 40.17%
Epoch: 900, Loss: 1.3301, Train: 43.11%, Valid: 41.99%, Test: 42.16%
Epoch: 925, Loss: 1.3799, Train: 35.16%, Valid: 34.80%, Test: 34.88%
Epoch: 950, Loss: 1.3551, Train: 42.56%, Valid: 41.37%, Test: 41.51%
Epoch: 975, Loss: 1.3398, Train: 42.07%, Valid: 41.29%, Test: 41.36%
Run 01:
Highest Train: 43.61
Highest Valid: 42.44
  Final Train: 43.51
   Final Test: 42.57
All runs:
Highest Train: 43.61, nan
Highest Valid: 42.44, nan
  Final Train: 43.51, nan
   Final Test: 42.57, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.2, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7219, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.5302, Train: 33.00%, Valid: 32.76%, Test: 33.15%
Epoch: 50, Loss: 1.4994, Train: 34.43%, Valid: 34.23%, Test: 34.40%
Epoch: 75, Loss: 1.4854, Train: 35.20%, Valid: 34.98%, Test: 35.13%
Epoch: 100, Loss: 1.4651, Train: 36.67%, Valid: 36.42%, Test: 36.55%
Epoch: 125, Loss: 1.4843, Train: 37.24%, Valid: 36.93%, Test: 37.52%
Epoch: 150, Loss: 1.7555, Train: 18.69%, Valid: 18.80%, Test: 18.26%
Epoch: 175, Loss: 1.4831, Train: 36.12%, Valid: 35.59%, Test: 35.90%
Epoch: 200, Loss: 1.4633, Train: 38.09%, Valid: 37.46%, Test: 37.53%
Epoch: 225, Loss: 1.5221, Train: 30.80%, Valid: 30.28%, Test: 30.77%
Epoch: 250, Loss: 1.4537, Train: 37.86%, Valid: 36.98%, Test: 37.35%
Epoch: 275, Loss: 1.4313, Train: 38.67%, Valid: 38.11%, Test: 38.29%
Epoch: 300, Loss: 5.7375, Train: 23.41%, Valid: 23.36%, Test: 23.33%
Epoch: 325, Loss: 1.5794, Train: 31.49%, Valid: 31.24%, Test: 31.54%
Epoch: 350, Loss: 1.4875, Train: 35.31%, Valid: 34.93%, Test: 35.35%
Epoch: 375, Loss: 1.4964, Train: 34.65%, Valid: 34.39%, Test: 34.61%
Epoch: 400, Loss: 1.4757, Train: 35.62%, Valid: 35.11%, Test: 35.44%
Epoch: 425, Loss: 1.4683, Train: 36.66%, Valid: 35.86%, Test: 36.20%
Epoch: 450, Loss: 1.4591, Train: 36.09%, Valid: 35.61%, Test: 35.70%
Epoch: 475, Loss: 1.4614, Train: 37.25%, Valid: 36.25%, Test: 36.58%
Epoch: 500, Loss: 1.4644, Train: 35.02%, Valid: 34.20%, Test: 34.43%
Epoch: 525, Loss: 1.4276, Train: 35.86%, Valid: 35.23%, Test: 35.04%
Epoch: 550, Loss: 1.4744, Train: 37.33%, Valid: 36.11%, Test: 36.48%
Epoch: 575, Loss: 1.4415, Train: 38.65%, Valid: 37.36%, Test: 37.93%
Epoch: 600, Loss: 1.4587, Train: 39.28%, Valid: 38.21%, Test: 38.34%
Epoch: 625, Loss: 1.4931, Train: 36.06%, Valid: 35.25%, Test: 35.56%
Epoch: 650, Loss: 1.4286, Train: 38.44%, Valid: 37.28%, Test: 37.66%
Epoch: 675, Loss: 1.4321, Train: 38.45%, Valid: 37.19%, Test: 37.52%
Epoch: 700, Loss: 1.4306, Train: 38.48%, Valid: 37.36%, Test: 37.70%
Epoch: 725, Loss: 1.4281, Train: 39.55%, Valid: 38.52%, Test: 38.63%
Epoch: 750, Loss: 1.5386, Train: 33.59%, Valid: 32.55%, Test: 32.39%
Epoch: 775, Loss: 1.4332, Train: 38.83%, Valid: 37.55%, Test: 37.90%
Epoch: 800, Loss: 1.4383, Train: 39.33%, Valid: 37.97%, Test: 38.33%
Epoch: 825, Loss: 1.4862, Train: 38.00%, Valid: 37.08%, Test: 37.38%
Epoch: 850, Loss: 1.4467, Train: 38.21%, Valid: 37.05%, Test: 37.33%
Epoch: 875, Loss: 1.4680, Train: 38.47%, Valid: 37.37%, Test: 37.62%
Epoch: 900, Loss: 1.4141, Train: 38.16%, Valid: 37.01%, Test: 37.19%
Epoch: 925, Loss: 1.9218, Train: 31.30%, Valid: 30.88%, Test: 31.07%
Epoch: 950, Loss: 1.5573, Train: 36.16%, Valid: 35.50%, Test: 35.57%
Epoch: 975, Loss: 1.4600, Train: 37.04%, Valid: 35.64%, Test: 36.01%
Run 01:
Highest Train: 40.89
Highest Valid: 39.76
  Final Train: 40.87
   Final Test: 39.96
All runs:
Highest Train: 40.89, nan
Highest Valid: 39.76, nan
  Final Train: 40.87, nan
   Final Test: 39.96, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.2, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7503, Train: 18.96%, Valid: 18.92%, Test: 18.69%
Epoch: 25, Loss: 1.5734, Train: 30.68%, Valid: 30.52%, Test: 30.61%
Epoch: 50, Loss: 1.4499, Train: 31.43%, Valid: 31.46%, Test: 31.82%
Epoch: 75, Loss: 1.5657, Train: 34.08%, Valid: 33.69%, Test: 33.86%
Epoch: 100, Loss: 1.4375, Train: 38.47%, Valid: 38.07%, Test: 38.29%
Epoch: 125, Loss: 1.4876, Train: 33.93%, Valid: 33.23%, Test: 33.99%
Epoch: 150, Loss: 1.4407, Train: 37.64%, Valid: 37.15%, Test: 37.44%
Epoch: 175, Loss: 1.4750, Train: 36.21%, Valid: 35.74%, Test: 36.20%
Epoch: 200, Loss: 2.2101, Train: 25.70%, Valid: 25.78%, Test: 25.25%
Epoch: 225, Loss: 1.5761, Train: 32.31%, Valid: 31.72%, Test: 32.61%
Epoch: 250, Loss: 1.4383, Train: 37.89%, Valid: 37.37%, Test: 38.08%
Epoch: 275, Loss: 1.4475, Train: 38.48%, Valid: 37.85%, Test: 38.22%
Epoch: 300, Loss: 1.4060, Train: 39.27%, Valid: 38.37%, Test: 38.69%
Epoch: 325, Loss: 1.5099, Train: 37.43%, Valid: 36.89%, Test: 37.30%
Epoch: 350, Loss: 1.5216, Train: 36.64%, Valid: 36.20%, Test: 36.49%
Epoch: 375, Loss: 1.3926, Train: 41.01%, Valid: 40.13%, Test: 40.28%
Epoch: 400, Loss: 1.6888, Train: 34.25%, Valid: 34.13%, Test: 34.63%
Epoch: 425, Loss: 1.4806, Train: 39.11%, Valid: 38.62%, Test: 38.98%
Epoch: 450, Loss: 1.4101, Train: 40.90%, Valid: 40.20%, Test: 40.52%
Epoch: 475, Loss: 1.5496, Train: 35.69%, Valid: 35.20%, Test: 35.58%
Epoch: 500, Loss: 1.4480, Train: 35.73%, Valid: 35.23%, Test: 35.56%
Epoch: 525, Loss: 1.3949, Train: 36.88%, Valid: 36.52%, Test: 36.68%
Epoch: 550, Loss: 1.4279, Train: 39.56%, Valid: 38.81%, Test: 39.10%
Epoch: 575, Loss: 1.4528, Train: 38.31%, Valid: 37.78%, Test: 37.96%
Epoch: 600, Loss: 1.4650, Train: 32.57%, Valid: 31.84%, Test: 32.39%
Epoch: 625, Loss: 1.4562, Train: 35.14%, Valid: 34.73%, Test: 35.19%
Epoch: 650, Loss: 1.3872, Train: 38.79%, Valid: 37.81%, Test: 38.32%
Epoch: 675, Loss: 1.4028, Train: 40.26%, Valid: 39.58%, Test: 39.64%
Epoch: 700, Loss: 1.4157, Train: 39.96%, Valid: 39.44%, Test: 39.58%
Epoch: 725, Loss: 1.4155, Train: 35.36%, Valid: 35.06%, Test: 35.30%
Epoch: 750, Loss: 1.5746, Train: 31.97%, Valid: 31.39%, Test: 32.13%
Epoch: 775, Loss: 1.7732, Train: 30.75%, Valid: 30.73%, Test: 30.78%
Epoch: 800, Loss: 1.4895, Train: 36.13%, Valid: 35.59%, Test: 35.99%
Epoch: 825, Loss: 1.6654, Train: 21.77%, Valid: 21.64%, Test: 21.06%
Epoch: 850, Loss: 1.4760, Train: 36.76%, Valid: 36.19%, Test: 36.42%
Epoch: 875, Loss: 1.4739, Train: 35.33%, Valid: 34.53%, Test: 34.56%
Epoch: 900, Loss: 1.3911, Train: 40.65%, Valid: 39.54%, Test: 39.84%
Epoch: 925, Loss: 15.4356, Train: 28.73%, Valid: 28.53%, Test: 28.83%
Epoch: 950, Loss: 1.8906, Train: 26.99%, Valid: 27.15%, Test: 27.26%
Epoch: 975, Loss: 1.4774, Train: 35.52%, Valid: 34.97%, Test: 35.57%
Run 01:
Highest Train: 42.65
Highest Valid: 41.66
  Final Train: 42.65
   Final Test: 41.73
All runs:
Highest Train: 42.65, nan
Highest Valid: 41.66, nan
  Final Train: 42.65, nan
   Final Test: 41.73, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.2, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7035, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.5362, Train: 31.66%, Valid: 31.46%, Test: 31.96%
Epoch: 50, Loss: 1.5622, Train: 29.87%, Valid: 29.97%, Test: 30.18%
Epoch: 75, Loss: 5.1894, Train: 25.81%, Valid: 26.00%, Test: 25.57%
Epoch: 100, Loss: 1.7604, Train: 22.27%, Valid: 22.46%, Test: 22.39%
Epoch: 125, Loss: 1.4529, Train: 37.37%, Valid: 37.00%, Test: 37.57%
Epoch: 150, Loss: 1.3970, Train: 39.19%, Valid: 38.65%, Test: 39.06%
Epoch: 175, Loss: 1.3863, Train: 39.53%, Valid: 38.95%, Test: 39.42%
Epoch: 200, Loss: 1.4962, Train: 36.97%, Valid: 36.57%, Test: 36.89%
Epoch: 225, Loss: 1.4011, Train: 37.92%, Valid: 37.51%, Test: 37.83%
Epoch: 250, Loss: 1.3682, Train: 41.60%, Valid: 40.83%, Test: 41.12%
Epoch: 275, Loss: 1.4307, Train: 38.28%, Valid: 37.96%, Test: 38.18%
Epoch: 300, Loss: 6.0564, Train: 33.30%, Valid: 33.36%, Test: 33.40%
Epoch: 325, Loss: 2.4772, Train: 31.38%, Valid: 31.03%, Test: 31.34%
Epoch: 350, Loss: 2.9930, Train: 23.63%, Valid: 23.73%, Test: 23.58%
Epoch: 375, Loss: 1.6143, Train: 16.76%, Valid: 16.86%, Test: 16.61%
Epoch: 400, Loss: 1.4808, Train: 33.55%, Valid: 33.03%, Test: 33.79%
Epoch: 425, Loss: 1.4777, Train: 37.28%, Valid: 36.89%, Test: 37.40%
Epoch: 450, Loss: 1.3995, Train: 40.12%, Valid: 39.41%, Test: 39.74%
Epoch: 475, Loss: 1.4195, Train: 39.91%, Valid: 39.52%, Test: 39.72%
Epoch: 500, Loss: 1.3985, Train: 39.01%, Valid: 38.31%, Test: 38.95%
Epoch: 525, Loss: 1.4406, Train: 38.11%, Valid: 37.80%, Test: 38.12%
Epoch: 550, Loss: 1.3754, Train: 39.41%, Valid: 38.54%, Test: 38.86%
Epoch: 575, Loss: 1.4089, Train: 38.86%, Valid: 38.07%, Test: 38.10%
Epoch: 600, Loss: 1.3934, Train: 34.66%, Valid: 33.96%, Test: 34.03%
Epoch: 625, Loss: 1.5125, Train: 27.64%, Valid: 27.35%, Test: 27.51%
Epoch: 650, Loss: 1.3708, Train: 34.19%, Valid: 33.58%, Test: 33.87%
Epoch: 675, Loss: 1.3823, Train: 35.72%, Valid: 35.07%, Test: 35.72%
Epoch: 700, Loss: 1.3992, Train: 39.77%, Valid: 39.25%, Test: 39.37%
Epoch: 725, Loss: 1.4728, Train: 38.99%, Valid: 38.42%, Test: 38.67%
Epoch: 750, Loss: 1.3796, Train: 40.91%, Valid: 39.93%, Test: 40.08%
Epoch: 775, Loss: 1.3873, Train: 39.82%, Valid: 39.20%, Test: 39.49%
Epoch: 800, Loss: 1.4212, Train: 38.28%, Valid: 37.17%, Test: 37.38%
Epoch: 825, Loss: 1.3794, Train: 41.44%, Valid: 40.63%, Test: 40.79%
Epoch: 850, Loss: 1.8423, Train: 27.96%, Valid: 27.84%, Test: 28.59%
Epoch: 875, Loss: 1.8990, Train: 29.46%, Valid: 29.11%, Test: 29.41%
Epoch: 900, Loss: 1.4580, Train: 33.84%, Valid: 33.70%, Test: 34.27%
Epoch: 925, Loss: 1.4422, Train: 37.34%, Valid: 36.92%, Test: 37.34%
Epoch: 950, Loss: 1.4500, Train: 39.19%, Valid: 38.61%, Test: 38.91%
Epoch: 975, Loss: 1.4072, Train: 40.49%, Valid: 39.70%, Test: 40.14%
Run 01:
Highest Train: 42.99
Highest Valid: 42.09
  Final Train: 42.99
   Final Test: 42.07
All runs:
Highest Train: 42.99, nan
Highest Valid: 42.09, nan
  Final Train: 42.99, nan
   Final Test: 42.07, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.3, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7391, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.5429, Train: 28.88%, Valid: 28.43%, Test: 29.14%
Epoch: 50, Loss: 1.4998, Train: 34.35%, Valid: 34.18%, Test: 34.47%
Epoch: 75, Loss: 1.5278, Train: 32.81%, Valid: 32.70%, Test: 33.04%
Epoch: 100, Loss: 1.4758, Train: 35.59%, Valid: 35.27%, Test: 35.65%
Epoch: 125, Loss: 1.4644, Train: 36.43%, Valid: 36.13%, Test: 36.27%
Epoch: 150, Loss: 1.4967, Train: 35.59%, Valid: 35.24%, Test: 35.54%
Epoch: 175, Loss: 1.6111, Train: 26.09%, Valid: 25.92%, Test: 26.62%
Epoch: 200, Loss: 1.4513, Train: 35.83%, Valid: 35.49%, Test: 35.74%
Epoch: 225, Loss: 1.4880, Train: 35.45%, Valid: 34.68%, Test: 35.34%
Epoch: 250, Loss: 1.4331, Train: 39.08%, Valid: 38.40%, Test: 38.70%
Epoch: 275, Loss: 1.4842, Train: 35.59%, Valid: 35.13%, Test: 35.62%
Epoch: 300, Loss: 1.4940, Train: 37.33%, Valid: 36.68%, Test: 36.85%
Epoch: 325, Loss: 1.4443, Train: 38.80%, Valid: 38.10%, Test: 38.31%
Epoch: 350, Loss: 1.4433, Train: 39.07%, Valid: 38.22%, Test: 38.60%
Epoch: 375, Loss: 1.5749, Train: 35.60%, Valid: 35.16%, Test: 35.41%
Epoch: 400, Loss: 1.4625, Train: 37.10%, Valid: 36.27%, Test: 36.36%
Epoch: 425, Loss: 1.4430, Train: 37.48%, Valid: 36.42%, Test: 36.58%
Epoch: 450, Loss: 1.6253, Train: 32.41%, Valid: 31.97%, Test: 31.96%
Epoch: 475, Loss: 1.5694, Train: 30.22%, Valid: 29.28%, Test: 29.41%
Epoch: 500, Loss: 1.4668, Train: 37.71%, Valid: 36.31%, Test: 36.50%
Epoch: 525, Loss: 1.4332, Train: 38.49%, Valid: 37.22%, Test: 37.43%
Epoch: 550, Loss: 2.1470, Train: 29.71%, Valid: 29.50%, Test: 29.79%
Epoch: 575, Loss: 1.4815, Train: 37.01%, Valid: 36.00%, Test: 36.29%
Epoch: 600, Loss: 1.4322, Train: 37.09%, Valid: 36.33%, Test: 36.48%
Epoch: 625, Loss: 1.6817, Train: 24.57%, Valid: 24.06%, Test: 24.42%
Epoch: 650, Loss: 1.4578, Train: 38.68%, Valid: 36.70%, Test: 37.46%
Epoch: 675, Loss: 1.5288, Train: 30.38%, Valid: 29.82%, Test: 29.27%
Epoch: 700, Loss: 1.4936, Train: 34.68%, Valid: 33.88%, Test: 34.06%
Epoch: 725, Loss: 1.4764, Train: 40.08%, Valid: 39.01%, Test: 39.03%
Epoch: 750, Loss: 6.5426, Train: 29.14%, Valid: 28.98%, Test: 29.25%
Epoch: 775, Loss: 2.5350, Train: 25.18%, Valid: 25.47%, Test: 25.70%
Epoch: 800, Loss: 1.5426, Train: 32.78%, Valid: 32.43%, Test: 32.85%
Epoch: 825, Loss: 1.5090, Train: 35.08%, Valid: 34.82%, Test: 34.92%
Epoch: 850, Loss: 1.4988, Train: 34.95%, Valid: 34.64%, Test: 34.63%
Epoch: 875, Loss: 1.4807, Train: 37.09%, Valid: 36.75%, Test: 37.02%
Epoch: 900, Loss: 1.4776, Train: 37.03%, Valid: 36.61%, Test: 36.95%
Epoch: 925, Loss: 1.4753, Train: 34.61%, Valid: 34.22%, Test: 34.67%
Epoch: 950, Loss: 1.5302, Train: 36.25%, Valid: 35.96%, Test: 35.93%
Epoch: 975, Loss: 1.5547, Train: 35.32%, Valid: 34.61%, Test: 35.26%
Run 01:
Highest Train: 42.18
Highest Valid: 41.20
  Final Train: 42.11
   Final Test: 41.03
All runs:
Highest Train: 42.18, nan
Highest Valid: 41.20, nan
  Final Train: 42.11, nan
   Final Test: 41.03, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.3, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7626, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.5338, Train: 32.73%, Valid: 32.42%, Test: 32.70%
Epoch: 50, Loss: 1.4647, Train: 36.76%, Valid: 36.19%, Test: 36.67%
Epoch: 75, Loss: 1.4308, Train: 38.06%, Valid: 37.77%, Test: 38.36%
Epoch: 100, Loss: 1.4443, Train: 35.58%, Valid: 35.24%, Test: 35.48%
Epoch: 125, Loss: 1.4186, Train: 34.71%, Valid: 34.56%, Test: 34.78%
Epoch: 150, Loss: 1.4107, Train: 40.25%, Valid: 39.44%, Test: 40.04%
Epoch: 175, Loss: 1.3871, Train: 40.80%, Valid: 40.07%, Test: 40.58%
Epoch: 200, Loss: 2.6212, Train: 24.51%, Valid: 24.32%, Test: 24.62%
Epoch: 225, Loss: 1.5283, Train: 28.55%, Valid: 28.38%, Test: 28.66%
Epoch: 250, Loss: 1.4802, Train: 37.34%, Valid: 37.30%, Test: 37.54%
Epoch: 275, Loss: 1.4257, Train: 38.67%, Valid: 38.14%, Test: 38.62%
Epoch: 300, Loss: 1.4466, Train: 38.01%, Valid: 37.41%, Test: 37.69%
Epoch: 325, Loss: 1.4104, Train: 37.26%, Valid: 36.49%, Test: 36.99%
Epoch: 350, Loss: 1.4329, Train: 37.16%, Valid: 36.61%, Test: 36.81%
Epoch: 375, Loss: 1.3867, Train: 41.26%, Valid: 40.55%, Test: 40.68%
Epoch: 400, Loss: 1.4366, Train: 34.81%, Valid: 34.13%, Test: 34.25%
Epoch: 425, Loss: 1.4053, Train: 37.64%, Valid: 37.25%, Test: 37.65%
Epoch: 450, Loss: 1.3860, Train: 40.75%, Valid: 39.98%, Test: 40.02%
Epoch: 475, Loss: 1.4717, Train: 33.13%, Valid: 32.49%, Test: 32.81%
Epoch: 500, Loss: 1.4602, Train: 35.87%, Valid: 35.05%, Test: 35.71%
Epoch: 525, Loss: 1.4224, Train: 34.01%, Valid: 33.71%, Test: 33.57%
Epoch: 550, Loss: 1.3991, Train: 39.48%, Valid: 38.79%, Test: 39.04%
Epoch: 575, Loss: 1.4175, Train: 39.18%, Valid: 38.50%, Test: 38.65%
Epoch: 600, Loss: 1.4224, Train: 38.76%, Valid: 38.28%, Test: 38.64%
Epoch: 625, Loss: 4.2093, Train: 22.18%, Valid: 22.18%, Test: 22.75%
Epoch: 650, Loss: 1.5745, Train: 29.08%, Valid: 28.87%, Test: 29.15%
Epoch: 675, Loss: 1.7113, Train: 30.57%, Valid: 30.32%, Test: 30.55%
Epoch: 700, Loss: 1.4432, Train: 38.78%, Valid: 38.47%, Test: 38.82%
Epoch: 725, Loss: 1.8699, Train: 29.95%, Valid: 29.84%, Test: 29.96%
Epoch: 750, Loss: 1.4425, Train: 37.89%, Valid: 37.49%, Test: 37.99%
Epoch: 775, Loss: 1.4188, Train: 39.55%, Valid: 38.78%, Test: 38.97%
Epoch: 800, Loss: 1.4206, Train: 39.60%, Valid: 38.93%, Test: 39.06%
Epoch: 825, Loss: 1.4253, Train: 39.74%, Valid: 39.25%, Test: 39.31%
Epoch: 850, Loss: 1.3941, Train: 40.97%, Valid: 40.01%, Test: 40.36%
Epoch: 875, Loss: 1.4154, Train: 39.30%, Valid: 38.55%, Test: 38.82%
Epoch: 900, Loss: 1.7190, Train: 29.72%, Valid: 29.45%, Test: 29.70%
Epoch: 925, Loss: 1.4702, Train: 36.43%, Valid: 35.88%, Test: 36.42%
Epoch: 950, Loss: 1.4313, Train: 38.11%, Valid: 37.39%, Test: 37.95%
Epoch: 975, Loss: 1.5038, Train: 34.55%, Valid: 33.99%, Test: 34.41%
Run 01:
Highest Train: 42.27
Highest Valid: 41.45
  Final Train: 42.27
   Final Test: 41.48
All runs:
Highest Train: 42.27, nan
Highest Valid: 41.45, nan
  Final Train: 42.27, nan
   Final Test: 41.48, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.3, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7381, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.5786, Train: 26.00%, Valid: 25.69%, Test: 26.13%
Epoch: 50, Loss: 1.5305, Train: 31.66%, Valid: 31.70%, Test: 32.13%
Epoch: 75, Loss: 1.7688, Train: 30.76%, Valid: 30.65%, Test: 31.02%
Epoch: 100, Loss: 1.8283, Train: 32.14%, Valid: 31.69%, Test: 32.45%
Epoch: 125, Loss: 1.4597, Train: 36.85%, Valid: 36.55%, Test: 36.67%
Epoch: 150, Loss: 1.4396, Train: 37.53%, Valid: 37.29%, Test: 37.85%
Epoch: 175, Loss: 1.4008, Train: 38.32%, Valid: 37.97%, Test: 38.48%
Epoch: 200, Loss: 1.3891, Train: 39.26%, Valid: 38.64%, Test: 38.94%
Epoch: 225, Loss: 1.4065, Train: 38.05%, Valid: 37.67%, Test: 37.94%
Epoch: 250, Loss: 1.4010, Train: 40.71%, Valid: 40.24%, Test: 40.35%
Epoch: 275, Loss: 1.4087, Train: 40.78%, Valid: 40.09%, Test: 40.26%
Epoch: 300, Loss: 1.3745, Train: 40.35%, Valid: 39.82%, Test: 40.02%
Epoch: 325, Loss: 1.4507, Train: 40.64%, Valid: 39.92%, Test: 40.21%
Epoch: 350, Loss: 1.3676, Train: 42.08%, Valid: 41.40%, Test: 41.62%
Epoch: 375, Loss: 1.3636, Train: 39.34%, Valid: 38.94%, Test: 38.99%
Epoch: 400, Loss: 1.3768, Train: 40.94%, Valid: 40.09%, Test: 40.40%
Epoch: 425, Loss: 1.3444, Train: 42.41%, Valid: 41.44%, Test: 41.81%
Epoch: 450, Loss: 1.3742, Train: 40.47%, Valid: 39.70%, Test: 39.96%
Epoch: 475, Loss: 1.5655, Train: 33.36%, Valid: 33.30%, Test: 33.17%
Epoch: 500, Loss: 1.3557, Train: 41.79%, Valid: 41.01%, Test: 41.08%
Epoch: 525, Loss: 1.4034, Train: 41.65%, Valid: 40.71%, Test: 41.06%
Epoch: 550, Loss: 1.3619, Train: 41.78%, Valid: 41.15%, Test: 41.50%
Epoch: 575, Loss: 21.4301, Train: 32.84%, Valid: 32.79%, Test: 32.98%
Epoch: 600, Loss: 2.1272, Train: 27.19%, Valid: 27.19%, Test: 27.42%
Epoch: 625, Loss: 1.5065, Train: 36.24%, Valid: 35.91%, Test: 36.05%
Epoch: 650, Loss: 1.4562, Train: 37.05%, Valid: 36.75%, Test: 36.98%
Epoch: 675, Loss: 1.4617, Train: 39.51%, Valid: 39.32%, Test: 39.50%
Epoch: 700, Loss: 1.3907, Train: 39.72%, Valid: 39.40%, Test: 39.60%
Epoch: 725, Loss: 1.3802, Train: 40.40%, Valid: 39.70%, Test: 40.17%
Epoch: 750, Loss: 1.4052, Train: 41.39%, Valid: 40.79%, Test: 41.06%
Epoch: 775, Loss: 1.3791, Train: 41.09%, Valid: 40.35%, Test: 40.70%
Epoch: 800, Loss: 1.3805, Train: 40.86%, Valid: 40.30%, Test: 40.50%
Epoch: 825, Loss: 1.3745, Train: 37.61%, Valid: 36.64%, Test: 37.26%
Epoch: 850, Loss: 1.3815, Train: 41.44%, Valid: 40.73%, Test: 41.09%
Epoch: 875, Loss: 1.3540, Train: 42.70%, Valid: 41.85%, Test: 42.09%
Epoch: 900, Loss: 1.3439, Train: 41.97%, Valid: 41.11%, Test: 41.44%
Epoch: 925, Loss: 1.3467, Train: 42.72%, Valid: 41.96%, Test: 42.13%
Epoch: 950, Loss: 1.3738, Train: 41.84%, Valid: 40.89%, Test: 41.26%
Epoch: 975, Loss: 1.3491, Train: 42.58%, Valid: 41.58%, Test: 41.85%
Run 01:
Highest Train: 43.21
Highest Valid: 42.33
  Final Train: 43.21
   Final Test: 42.55
All runs:
Highest Train: 43.21, nan
Highest Valid: 42.33, nan
  Final Train: 43.21, nan
   Final Test: 42.55, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.3, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7632, Train: 22.16%, Valid: 22.17%, Test: 22.74%
Epoch: 25, Loss: 1.6016, Train: 31.80%, Valid: 31.69%, Test: 32.02%
Epoch: 50, Loss: 1.5080, Train: 34.30%, Valid: 33.88%, Test: 34.36%
Epoch: 75, Loss: 1.4958, Train: 34.74%, Valid: 34.36%, Test: 34.67%
Epoch: 100, Loss: 1.4832, Train: 35.43%, Valid: 35.00%, Test: 35.19%
Epoch: 125, Loss: 1.4981, Train: 33.76%, Valid: 33.66%, Test: 33.97%
Epoch: 150, Loss: 1.4871, Train: 34.24%, Valid: 34.06%, Test: 34.31%
Epoch: 175, Loss: 1.5458, Train: 29.78%, Valid: 29.45%, Test: 29.81%
Epoch: 200, Loss: 1.4737, Train: 34.68%, Valid: 34.44%, Test: 34.82%
Epoch: 225, Loss: 1.4993, Train: 35.91%, Valid: 35.50%, Test: 35.73%
Epoch: 250, Loss: 1.5045, Train: 36.58%, Valid: 36.05%, Test: 36.15%
Epoch: 275, Loss: 1.5253, Train: 36.31%, Valid: 36.05%, Test: 36.20%
Epoch: 300, Loss: 1.8750, Train: 28.97%, Valid: 28.77%, Test: 29.03%
Epoch: 325, Loss: 1.5203, Train: 34.98%, Valid: 34.11%, Test: 34.83%
Epoch: 350, Loss: 1.4731, Train: 36.76%, Valid: 36.10%, Test: 36.61%
Epoch: 375, Loss: 1.7712, Train: 30.75%, Valid: 30.50%, Test: 30.78%
Epoch: 400, Loss: 1.4905, Train: 35.89%, Valid: 35.02%, Test: 35.34%
Epoch: 425, Loss: 1.5133, Train: 30.83%, Valid: 30.22%, Test: 30.62%
Epoch: 450, Loss: 1.4594, Train: 37.37%, Valid: 36.40%, Test: 36.69%
Epoch: 475, Loss: 1.4522, Train: 37.72%, Valid: 36.92%, Test: 37.28%
Epoch: 500, Loss: 1.5320, Train: 22.73%, Valid: 22.61%, Test: 22.35%
Epoch: 525, Loss: 1.4892, Train: 36.62%, Valid: 36.08%, Test: 36.35%
Epoch: 550, Loss: 1.4198, Train: 40.48%, Valid: 39.94%, Test: 39.95%
Epoch: 575, Loss: 1.8112, Train: 26.00%, Valid: 25.59%, Test: 25.74%
Epoch: 600, Loss: 1.4948, Train: 36.61%, Valid: 35.98%, Test: 36.25%
Epoch: 625, Loss: 1.4539, Train: 37.29%, Valid: 36.60%, Test: 36.86%
Epoch: 650, Loss: 1.4614, Train: 38.45%, Valid: 37.60%, Test: 37.90%
Epoch: 675, Loss: 1.4821, Train: 33.84%, Valid: 33.06%, Test: 33.26%
Epoch: 700, Loss: 1.4346, Train: 39.44%, Valid: 38.20%, Test: 38.59%
Epoch: 725, Loss: 1.6155, Train: 31.92%, Valid: 31.10%, Test: 31.68%
Epoch: 750, Loss: 1.4539, Train: 34.10%, Valid: 33.10%, Test: 33.83%
Epoch: 775, Loss: 1.5593, Train: 22.43%, Valid: 22.33%, Test: 22.93%
Epoch: 800, Loss: 1.4202, Train: 38.99%, Valid: 37.67%, Test: 37.92%
Epoch: 825, Loss: 1.3989, Train: 39.31%, Valid: 38.12%, Test: 38.29%
Epoch: 850, Loss: 1.4161, Train: 40.01%, Valid: 38.96%, Test: 38.94%
Epoch: 875, Loss: 1.7510, Train: 30.48%, Valid: 29.59%, Test: 29.47%
Epoch: 900, Loss: 1.4763, Train: 37.47%, Valid: 36.16%, Test: 36.52%
Epoch: 925, Loss: 1.4825, Train: 33.17%, Valid: 31.93%, Test: 32.43%
Epoch: 950, Loss: 1.4228, Train: 35.29%, Valid: 34.07%, Test: 33.95%
Epoch: 975, Loss: 1.4821, Train: 37.22%, Valid: 35.58%, Test: 36.05%
Run 01:
Highest Train: 41.42
Highest Valid: 40.90
  Final Train: 41.35
   Final Test: 40.85
All runs:
Highest Train: 41.42, nan
Highest Valid: 40.90, nan
  Final Train: 41.35, nan
   Final Test: 40.85, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.3, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7550, Train: 18.90%, Valid: 18.84%, Test: 18.62%
Epoch: 25, Loss: 1.5431, Train: 27.27%, Valid: 27.00%, Test: 27.75%
Epoch: 50, Loss: 1.4828, Train: 34.31%, Valid: 33.97%, Test: 34.36%
Epoch: 75, Loss: 1.4629, Train: 36.50%, Valid: 35.85%, Test: 36.60%
Epoch: 100, Loss: 1.4554, Train: 37.67%, Valid: 37.09%, Test: 37.47%
Epoch: 125, Loss: 1.4372, Train: 36.02%, Valid: 35.84%, Test: 36.48%
Epoch: 150, Loss: 1.4055, Train: 39.63%, Valid: 39.18%, Test: 39.47%
Epoch: 175, Loss: 1.4227, Train: 39.55%, Valid: 39.03%, Test: 39.22%
Epoch: 200, Loss: 1.4242, Train: 38.40%, Valid: 38.05%, Test: 38.29%
Epoch: 225, Loss: 1.3861, Train: 39.75%, Valid: 39.31%, Test: 39.56%
Epoch: 250, Loss: 1.3671, Train: 41.79%, Valid: 40.85%, Test: 41.25%
Epoch: 275, Loss: 1.7462, Train: 34.92%, Valid: 34.70%, Test: 34.83%
Epoch: 300, Loss: 1.4740, Train: 36.89%, Valid: 36.56%, Test: 36.85%
Epoch: 325, Loss: 1.4399, Train: 36.97%, Valid: 36.44%, Test: 36.84%
Epoch: 350, Loss: 1.4093, Train: 39.49%, Valid: 38.96%, Test: 39.22%
Epoch: 375, Loss: 1.4085, Train: 39.46%, Valid: 38.87%, Test: 39.35%
Epoch: 400, Loss: 1.5392, Train: 37.42%, Valid: 36.97%, Test: 37.04%
Epoch: 425, Loss: 1.4217, Train: 37.95%, Valid: 37.26%, Test: 37.92%
Epoch: 450, Loss: 1.3770, Train: 40.25%, Valid: 39.79%, Test: 39.94%
Epoch: 475, Loss: 1.4925, Train: 27.71%, Valid: 27.39%, Test: 28.12%
Epoch: 500, Loss: 1.4285, Train: 39.93%, Valid: 38.99%, Test: 39.25%
Epoch: 525, Loss: 1.3900, Train: 39.59%, Valid: 38.63%, Test: 39.01%
Epoch: 550, Loss: 1.4519, Train: 39.84%, Valid: 39.31%, Test: 39.59%
Epoch: 575, Loss: 1.4243, Train: 33.43%, Valid: 32.74%, Test: 33.33%
Epoch: 600, Loss: 1.5100, Train: 33.29%, Valid: 32.40%, Test: 33.33%
Epoch: 625, Loss: 1.4877, Train: 25.72%, Valid: 25.44%, Test: 25.39%
Epoch: 650, Loss: 1.4024, Train: 40.51%, Valid: 39.80%, Test: 39.75%
Epoch: 675, Loss: 1.3795, Train: 41.91%, Valid: 40.96%, Test: 40.90%
Epoch: 700, Loss: 1.3890, Train: 38.60%, Valid: 37.76%, Test: 38.21%
Epoch: 725, Loss: 1.4272, Train: 39.04%, Valid: 38.42%, Test: 38.60%
Epoch: 750, Loss: 1.3652, Train: 41.31%, Valid: 40.59%, Test: 40.52%
Epoch: 775, Loss: 1.4027, Train: 39.25%, Valid: 38.18%, Test: 38.45%
Epoch: 800, Loss: 1.4099, Train: 37.13%, Valid: 36.50%, Test: 36.61%
Epoch: 825, Loss: 1.3741, Train: 39.65%, Valid: 38.58%, Test: 39.16%
Epoch: 850, Loss: 1.3734, Train: 40.29%, Valid: 38.94%, Test: 39.39%
Epoch: 875, Loss: 1.3703, Train: 39.93%, Valid: 39.04%, Test: 39.34%
Epoch: 900, Loss: 1.3977, Train: 35.49%, Valid: 34.48%, Test: 34.89%
Epoch: 925, Loss: 3.6409, Train: 28.75%, Valid: 28.56%, Test: 28.86%
Epoch: 950, Loss: 1.5072, Train: 33.27%, Valid: 32.89%, Test: 33.07%
Epoch: 975, Loss: 1.4815, Train: 36.06%, Valid: 35.64%, Test: 35.99%
Run 01:
Highest Train: 42.81
Highest Valid: 41.90
  Final Train: 42.81
   Final Test: 42.28
All runs:
Highest Train: 42.81, nan
Highest Valid: 41.90, nan
  Final Train: 42.81, nan
   Final Test: 42.28, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.3, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7330, Train: 19.00%, Valid: 18.95%, Test: 18.72%
Epoch: 25, Loss: 1.5503, Train: 32.36%, Valid: 32.04%, Test: 32.32%
Epoch: 50, Loss: 1.5268, Train: 33.21%, Valid: 32.78%, Test: 33.51%
Epoch: 75, Loss: 1.4375, Train: 34.55%, Valid: 34.58%, Test: 35.19%
Epoch: 100, Loss: 1.4679, Train: 33.70%, Valid: 33.79%, Test: 33.70%
Epoch: 125, Loss: 1.3926, Train: 38.32%, Valid: 38.15%, Test: 38.46%
Epoch: 150, Loss: 1.5695, Train: 32.70%, Valid: 32.92%, Test: 32.77%
Epoch: 175, Loss: 1.4510, Train: 36.44%, Valid: 36.18%, Test: 36.48%
Epoch: 200, Loss: 1.4200, Train: 38.94%, Valid: 38.32%, Test: 38.65%
Epoch: 225, Loss: 1.3814, Train: 40.34%, Valid: 39.92%, Test: 40.29%
Epoch: 250, Loss: 1.3758, Train: 34.55%, Valid: 33.81%, Test: 34.22%
Epoch: 275, Loss: 1.4170, Train: 39.69%, Valid: 39.33%, Test: 39.67%
Epoch: 300, Loss: 1.4053, Train: 34.13%, Valid: 34.09%, Test: 34.48%
Epoch: 325, Loss: 1.4468, Train: 30.37%, Valid: 30.07%, Test: 30.25%
Epoch: 350, Loss: 1.3629, Train: 41.16%, Valid: 40.47%, Test: 40.47%
Epoch: 375, Loss: 1.3497, Train: 41.49%, Valid: 40.79%, Test: 40.86%
Epoch: 400, Loss: 3.9304, Train: 29.90%, Valid: 29.57%, Test: 29.97%
Epoch: 425, Loss: 1.9506, Train: 25.53%, Valid: 25.69%, Test: 26.18%
Epoch: 450, Loss: 1.4989, Train: 35.24%, Valid: 35.24%, Test: 35.71%
Epoch: 475, Loss: 1.4390, Train: 37.67%, Valid: 37.16%, Test: 37.50%
Epoch: 500, Loss: 1.4242, Train: 39.62%, Valid: 38.79%, Test: 39.10%
Epoch: 525, Loss: 1.4265, Train: 39.64%, Valid: 38.67%, Test: 39.20%
Epoch: 550, Loss: 1.4242, Train: 39.53%, Valid: 38.72%, Test: 38.86%
Epoch: 575, Loss: 1.4084, Train: 37.61%, Valid: 37.14%, Test: 37.41%
Epoch: 600, Loss: 1.4032, Train: 39.85%, Valid: 38.79%, Test: 39.20%
Epoch: 625, Loss: 1.4258, Train: 38.88%, Valid: 37.97%, Test: 38.46%
Epoch: 650, Loss: 1.3863, Train: 39.54%, Valid: 38.86%, Test: 38.98%
Epoch: 675, Loss: 1.4005, Train: 40.82%, Valid: 39.93%, Test: 40.09%
Epoch: 700, Loss: 1.3974, Train: 41.11%, Valid: 39.88%, Test: 40.11%
Epoch: 725, Loss: 1.3868, Train: 40.46%, Valid: 39.61%, Test: 39.88%
Epoch: 750, Loss: 1.4210, Train: 39.11%, Valid: 38.29%, Test: 38.39%
Epoch: 775, Loss: 1.3775, Train: 42.73%, Valid: 41.58%, Test: 41.90%
Epoch: 800, Loss: 1.3535, Train: 41.41%, Valid: 40.25%, Test: 40.65%
Epoch: 825, Loss: 1.3867, Train: 39.63%, Valid: 39.18%, Test: 39.22%
Epoch: 850, Loss: 1.3859, Train: 39.80%, Valid: 38.35%, Test: 38.87%
Epoch: 875, Loss: 1.3545, Train: 42.20%, Valid: 41.14%, Test: 41.55%
Epoch: 900, Loss: 1.3539, Train: 42.34%, Valid: 41.49%, Test: 41.92%
Epoch: 925, Loss: 1.3291, Train: 43.74%, Valid: 42.42%, Test: 42.68%
Epoch: 950, Loss: 1.3310, Train: 42.47%, Valid: 41.44%, Test: 41.45%
Epoch: 975, Loss: 1.3457, Train: 42.69%, Valid: 41.24%, Test: 41.53%
Run 01:
Highest Train: 43.78
Highest Valid: 42.50
  Final Train: 43.66
   Final Test: 42.58
All runs:
Highest Train: 43.78, nan
Highest Valid: 42.50, nan
  Final Train: 43.66, nan
   Final Test: 42.58, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.3, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7496, Train: 22.16%, Valid: 22.17%, Test: 22.74%
Epoch: 25, Loss: 1.5419, Train: 29.14%, Valid: 28.97%, Test: 29.22%
Epoch: 50, Loss: 1.5012, Train: 34.41%, Valid: 34.12%, Test: 34.63%
Epoch: 75, Loss: 1.4669, Train: 35.32%, Valid: 35.10%, Test: 35.38%
Epoch: 100, Loss: 1.4712, Train: 36.39%, Valid: 35.75%, Test: 36.15%
Epoch: 125, Loss: 1.4752, Train: 36.09%, Valid: 35.63%, Test: 36.02%
Epoch: 150, Loss: 1.4418, Train: 36.65%, Valid: 36.12%, Test: 36.56%
Epoch: 175, Loss: 1.4643, Train: 37.04%, Valid: 36.33%, Test: 36.98%
Epoch: 200, Loss: 1.4316, Train: 38.64%, Valid: 37.73%, Test: 38.29%
Epoch: 225, Loss: 1.6269, Train: 27.78%, Valid: 27.71%, Test: 27.64%
Epoch: 250, Loss: 1.4743, Train: 36.30%, Valid: 35.77%, Test: 36.07%
Epoch: 275, Loss: 1.5397, Train: 36.85%, Valid: 36.12%, Test: 36.27%
Epoch: 300, Loss: 1.5670, Train: 24.08%, Valid: 24.01%, Test: 24.01%
Epoch: 325, Loss: 1.4631, Train: 37.35%, Valid: 36.42%, Test: 36.73%
Epoch: 350, Loss: 1.4223, Train: 38.30%, Valid: 37.67%, Test: 37.72%
Epoch: 375, Loss: 1.4175, Train: 39.18%, Valid: 38.37%, Test: 38.38%
Epoch: 400, Loss: 1.4793, Train: 35.98%, Valid: 35.25%, Test: 35.77%
Epoch: 425, Loss: 1.4475, Train: 37.15%, Valid: 36.54%, Test: 36.67%
Epoch: 450, Loss: 1.4197, Train: 39.58%, Valid: 38.61%, Test: 38.67%
Epoch: 475, Loss: 1.4493, Train: 38.87%, Valid: 37.59%, Test: 38.11%
Epoch: 500, Loss: 1.5761, Train: 27.90%, Valid: 27.47%, Test: 26.99%
Epoch: 525, Loss: 1.4821, Train: 35.54%, Valid: 34.72%, Test: 35.25%
Epoch: 550, Loss: 1.4569, Train: 37.48%, Valid: 36.32%, Test: 36.59%
Epoch: 575, Loss: 1.4578, Train: 37.52%, Valid: 36.45%, Test: 36.53%
Epoch: 600, Loss: 1.6870, Train: 35.36%, Valid: 34.47%, Test: 34.45%
Epoch: 625, Loss: 1.4425, Train: 36.69%, Valid: 35.41%, Test: 35.93%
Epoch: 650, Loss: 1.4382, Train: 38.22%, Valid: 37.12%, Test: 37.12%
Epoch: 675, Loss: 1.4279, Train: 39.84%, Valid: 38.37%, Test: 38.47%
Epoch: 700, Loss: 1.4555, Train: 36.77%, Valid: 35.59%, Test: 36.05%
Epoch: 725, Loss: 1.4336, Train: 38.99%, Valid: 37.06%, Test: 37.59%
Epoch: 750, Loss: 1.4038, Train: 40.32%, Valid: 38.68%, Test: 38.72%
Epoch: 775, Loss: 4.5837, Train: 11.71%, Valid: 11.70%, Test: 11.25%
Epoch: 800, Loss: 1.6159, Train: 33.34%, Valid: 33.07%, Test: 33.42%
Epoch: 825, Loss: 1.4903, Train: 36.68%, Valid: 36.18%, Test: 36.46%
Epoch: 850, Loss: 1.4569, Train: 37.88%, Valid: 37.39%, Test: 37.46%
Epoch: 875, Loss: 1.4876, Train: 36.18%, Valid: 35.21%, Test: 35.77%
Epoch: 900, Loss: 1.4536, Train: 36.83%, Valid: 36.58%, Test: 36.69%
Epoch: 925, Loss: 1.4932, Train: 36.54%, Valid: 36.07%, Test: 36.35%
Epoch: 950, Loss: 1.4363, Train: 38.32%, Valid: 37.60%, Test: 37.86%
Epoch: 975, Loss: 1.4178, Train: 39.96%, Valid: 38.97%, Test: 39.25%
Run 01:
Highest Train: 41.65
Highest Valid: 40.04
  Final Train: 41.65
   Final Test: 40.19
All runs:
Highest Train: 41.65, nan
Highest Valid: 40.04, nan
  Final Train: 41.65, nan
   Final Test: 40.19, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.3, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7689, Train: 12.51%, Valid: 12.59%, Test: 12.43%
Epoch: 25, Loss: 1.5371, Train: 30.77%, Valid: 30.51%, Test: 30.74%
Epoch: 50, Loss: 1.4616, Train: 29.78%, Valid: 30.11%, Test: 30.41%
Epoch: 75, Loss: 1.6743, Train: 33.11%, Valid: 33.09%, Test: 32.91%
Epoch: 100, Loss: 1.5233, Train: 35.63%, Valid: 35.17%, Test: 35.53%
Epoch: 125, Loss: 1.4576, Train: 36.48%, Valid: 35.93%, Test: 36.37%
Epoch: 150, Loss: 1.6168, Train: 35.53%, Valid: 35.57%, Test: 35.52%
Epoch: 175, Loss: 1.4980, Train: 32.24%, Valid: 31.98%, Test: 32.35%
Epoch: 200, Loss: 1.4133, Train: 39.40%, Valid: 38.72%, Test: 39.11%
Epoch: 225, Loss: 1.3977, Train: 40.23%, Valid: 39.75%, Test: 39.88%
Epoch: 250, Loss: 9.8066, Train: 27.88%, Valid: 27.93%, Test: 28.24%
Epoch: 275, Loss: 1.7654, Train: 28.60%, Valid: 28.37%, Test: 28.36%
Epoch: 300, Loss: 1.4760, Train: 35.24%, Valid: 34.69%, Test: 35.40%
Epoch: 325, Loss: 1.4425, Train: 38.90%, Valid: 38.31%, Test: 38.85%
Epoch: 350, Loss: 1.4404, Train: 38.92%, Valid: 38.25%, Test: 38.68%
Epoch: 375, Loss: 1.4268, Train: 38.69%, Valid: 38.19%, Test: 38.44%
Epoch: 400, Loss: 1.4298, Train: 39.26%, Valid: 38.78%, Test: 39.06%
Epoch: 425, Loss: 1.4661, Train: 38.75%, Valid: 38.15%, Test: 38.40%
Epoch: 450, Loss: 1.4580, Train: 36.50%, Valid: 35.85%, Test: 36.34%
Epoch: 475, Loss: 1.4065, Train: 40.94%, Valid: 40.32%, Test: 40.54%
Epoch: 500, Loss: 1.4846, Train: 35.32%, Valid: 34.65%, Test: 35.48%
Epoch: 525, Loss: 1.4183, Train: 38.60%, Valid: 38.20%, Test: 38.59%
Epoch: 550, Loss: 1.3992, Train: 39.56%, Valid: 39.06%, Test: 39.28%
Epoch: 575, Loss: 4.7978, Train: 17.51%, Valid: 17.80%, Test: 17.24%
Epoch: 600, Loss: 2.1741, Train: 21.74%, Valid: 21.94%, Test: 21.45%
Epoch: 625, Loss: 2.3717, Train: 24.57%, Valid: 24.57%, Test: 24.55%
Epoch: 650, Loss: 1.5424, Train: 29.27%, Valid: 29.11%, Test: 29.54%
Epoch: 675, Loss: 1.5301, Train: 37.51%, Valid: 36.92%, Test: 37.55%
Epoch: 700, Loss: 1.4252, Train: 39.35%, Valid: 38.93%, Test: 39.37%
Epoch: 725, Loss: 1.4155, Train: 38.49%, Valid: 38.32%, Test: 38.56%
Epoch: 750, Loss: 1.4039, Train: 40.40%, Valid: 39.93%, Test: 40.23%
Epoch: 775, Loss: 1.4386, Train: 39.09%, Valid: 38.58%, Test: 38.66%
Epoch: 800, Loss: 1.4068, Train: 40.23%, Valid: 39.54%, Test: 39.90%
Epoch: 825, Loss: 1.4512, Train: 36.68%, Valid: 36.31%, Test: 36.57%
Epoch: 850, Loss: 1.4195, Train: 39.68%, Valid: 39.21%, Test: 39.33%
Epoch: 875, Loss: 1.4383, Train: 38.20%, Valid: 37.22%, Test: 37.52%
Epoch: 900, Loss: 1.4001, Train: 41.14%, Valid: 40.33%, Test: 40.55%
Epoch: 925, Loss: 1.4297, Train: 39.67%, Valid: 39.45%, Test: 39.61%
Epoch: 950, Loss: 1.5875, Train: 38.28%, Valid: 37.94%, Test: 38.19%
Epoch: 975, Loss: 1.4345, Train: 39.80%, Valid: 39.30%, Test: 39.67%
Run 01:
Highest Train: 41.72
Highest Valid: 41.10
  Final Train: 41.58
   Final Test: 41.22
All runs:
Highest Train: 41.72, nan
Highest Valid: 41.10, nan
  Final Train: 41.58, nan
   Final Test: 41.22, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.3, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7224, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.5101, Train: 22.94%, Valid: 23.20%, Test: 23.53%
Epoch: 50, Loss: 1.4164, Train: 33.27%, Valid: 33.33%, Test: 33.47%
Epoch: 75, Loss: 1.4155, Train: 36.89%, Valid: 36.28%, Test: 36.98%
Epoch: 100, Loss: 1.4142, Train: 39.37%, Valid: 38.72%, Test: 39.10%
Epoch: 125, Loss: 1.3926, Train: 40.60%, Valid: 39.99%, Test: 40.73%
Epoch: 150, Loss: 1.4088, Train: 38.32%, Valid: 37.85%, Test: 38.46%
Epoch: 175, Loss: 1.3864, Train: 40.18%, Valid: 39.31%, Test: 39.80%
Epoch: 200, Loss: 1.4280, Train: 39.74%, Valid: 39.27%, Test: 39.37%
Epoch: 225, Loss: 1.4140, Train: 40.31%, Valid: 39.75%, Test: 40.17%
Epoch: 250, Loss: 1.3991, Train: 40.64%, Valid: 39.63%, Test: 40.23%
Epoch: 275, Loss: 1.3638, Train: 41.58%, Valid: 41.01%, Test: 41.19%
Epoch: 300, Loss: 1.4224, Train: 41.06%, Valid: 40.32%, Test: 40.71%
Epoch: 325, Loss: 1.3696, Train: 41.15%, Valid: 40.18%, Test: 40.64%
Epoch: 350, Loss: 1.4449, Train: 31.99%, Valid: 31.60%, Test: 31.75%
Epoch: 375, Loss: 1.4296, Train: 38.32%, Valid: 37.80%, Test: 37.99%
Epoch: 400, Loss: 1.6158, Train: 34.22%, Valid: 33.41%, Test: 34.31%
Epoch: 425, Loss: 1.7978, Train: 30.52%, Valid: 30.24%, Test: 30.85%
Epoch: 450, Loss: 1.4357, Train: 38.38%, Valid: 37.99%, Test: 38.48%
Epoch: 475, Loss: 1.3958, Train: 40.40%, Valid: 39.66%, Test: 39.76%
Epoch: 500, Loss: 1.4147, Train: 36.27%, Valid: 35.36%, Test: 36.08%
Epoch: 525, Loss: 1.3856, Train: 41.33%, Valid: 40.56%, Test: 40.88%
Epoch: 550, Loss: 1.3457, Train: 41.43%, Valid: 40.55%, Test: 40.74%
Epoch: 575, Loss: 1.3478, Train: 41.83%, Valid: 41.06%, Test: 41.33%
Epoch: 600, Loss: 1.4049, Train: 40.62%, Valid: 40.06%, Test: 40.02%
Epoch: 625, Loss: 1.3918, Train: 41.04%, Valid: 40.06%, Test: 40.47%
Epoch: 650, Loss: 1.3679, Train: 42.71%, Valid: 41.78%, Test: 41.91%
Epoch: 675, Loss: 1.3512, Train: 41.95%, Valid: 40.93%, Test: 41.26%
Epoch: 700, Loss: 1.3433, Train: 43.35%, Valid: 42.41%, Test: 42.63%
Epoch: 725, Loss: 1.3750, Train: 38.21%, Valid: 37.47%, Test: 37.75%
Epoch: 750, Loss: 1.3553, Train: 40.97%, Valid: 40.13%, Test: 40.45%
Epoch: 775, Loss: 1.4160, Train: 40.98%, Valid: 39.65%, Test: 40.16%
Epoch: 800, Loss: 1.3382, Train: 42.95%, Valid: 41.87%, Test: 42.17%
Epoch: 825, Loss: 1.3236, Train: 43.26%, Valid: 42.23%, Test: 42.51%
Epoch: 850, Loss: 1.3181, Train: 42.30%, Valid: 41.57%, Test: 41.77%
Epoch: 875, Loss: 1.3091, Train: 43.40%, Valid: 42.15%, Test: 42.48%
Epoch: 900, Loss: 1.3496, Train: 41.15%, Valid: 39.90%, Test: 40.28%
Epoch: 925, Loss: 1.3080, Train: 44.32%, Valid: 43.02%, Test: 43.10%
Epoch: 950, Loss: 1.3759, Train: 38.45%, Valid: 37.88%, Test: 38.45%
Epoch: 975, Loss: 1.3889, Train: 41.46%, Valid: 40.60%, Test: 40.72%
Run 01:
Highest Train: 44.35
Highest Valid: 43.08
  Final Train: 44.35
   Final Test: 43.19
All runs:
Highest Train: 44.35, nan
Highest Valid: 43.08, nan
  Final Train: 44.35, nan
   Final Test: 43.19, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.4, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7598, Train: 28.72%, Valid: 28.53%, Test: 28.82%
Epoch: 25, Loss: 1.5685, Train: 32.99%, Valid: 32.82%, Test: 33.03%
Epoch: 50, Loss: 1.5048, Train: 34.42%, Valid: 34.16%, Test: 34.52%
Epoch: 75, Loss: 1.4922, Train: 34.84%, Valid: 34.55%, Test: 34.96%
Epoch: 100, Loss: 1.4917, Train: 35.84%, Valid: 35.22%, Test: 36.13%
Epoch: 125, Loss: 1.4950, Train: 36.80%, Valid: 36.36%, Test: 36.86%
Epoch: 150, Loss: 1.4606, Train: 34.28%, Valid: 33.93%, Test: 34.27%
Epoch: 175, Loss: 3.2791, Train: 17.96%, Valid: 18.05%, Test: 17.60%
Epoch: 200, Loss: 1.5249, Train: 33.73%, Valid: 33.53%, Test: 33.73%
Epoch: 225, Loss: 1.4688, Train: 36.01%, Valid: 35.68%, Test: 35.78%
Epoch: 250, Loss: 1.4852, Train: 35.99%, Valid: 35.38%, Test: 35.72%
Epoch: 275, Loss: 1.4505, Train: 37.68%, Valid: 37.02%, Test: 37.36%
Epoch: 300, Loss: 1.5405, Train: 34.91%, Valid: 34.22%, Test: 34.74%
Epoch: 325, Loss: 1.4531, Train: 37.91%, Valid: 36.92%, Test: 37.29%
Epoch: 350, Loss: 1.4395, Train: 39.04%, Valid: 38.37%, Test: 38.71%
Epoch: 375, Loss: 1.4356, Train: 38.62%, Valid: 37.89%, Test: 37.95%
Epoch: 400, Loss: 1.4985, Train: 35.54%, Valid: 34.72%, Test: 35.43%
Epoch: 425, Loss: 1.4426, Train: 38.27%, Valid: 37.47%, Test: 37.73%
Epoch: 450, Loss: 1.4301, Train: 25.92%, Valid: 25.64%, Test: 25.17%
Epoch: 475, Loss: 1.4420, Train: 38.26%, Valid: 37.36%, Test: 37.40%
Epoch: 500, Loss: 1.4592, Train: 38.56%, Valid: 37.61%, Test: 37.93%
Epoch: 525, Loss: 1.4327, Train: 39.58%, Valid: 38.72%, Test: 39.00%
Epoch: 550, Loss: 1.4714, Train: 28.76%, Valid: 28.04%, Test: 28.12%
Epoch: 575, Loss: 2.1719, Train: 23.49%, Valid: 23.28%, Test: 23.52%
Epoch: 600, Loss: 1.5173, Train: 35.15%, Valid: 34.48%, Test: 35.05%
Epoch: 625, Loss: 1.4674, Train: 37.44%, Valid: 36.61%, Test: 36.97%
Epoch: 650, Loss: 1.4747, Train: 39.22%, Valid: 38.18%, Test: 38.67%
Epoch: 675, Loss: 1.6621, Train: 27.93%, Valid: 27.83%, Test: 27.41%
Epoch: 700, Loss: 1.4632, Train: 36.62%, Valid: 36.13%, Test: 36.25%
Epoch: 725, Loss: 1.4422, Train: 38.31%, Valid: 37.49%, Test: 37.67%
Epoch: 750, Loss: 1.9424, Train: 29.78%, Valid: 29.39%, Test: 29.64%
Epoch: 775, Loss: 1.4995, Train: 35.47%, Valid: 35.11%, Test: 35.38%
Epoch: 800, Loss: 1.5148, Train: 30.99%, Valid: 30.60%, Test: 30.96%
Epoch: 825, Loss: 1.4613, Train: 38.59%, Valid: 37.80%, Test: 37.88%
Epoch: 850, Loss: 1.4434, Train: 38.04%, Valid: 37.39%, Test: 37.65%
Epoch: 875, Loss: 1.4422, Train: 39.06%, Valid: 38.29%, Test: 38.39%
Epoch: 900, Loss: 1.4443, Train: 36.65%, Valid: 35.73%, Test: 36.08%
Epoch: 925, Loss: 1.4639, Train: 37.40%, Valid: 36.56%, Test: 36.93%
Epoch: 950, Loss: 1.4396, Train: 40.51%, Valid: 39.53%, Test: 39.79%
Epoch: 975, Loss: 1.4385, Train: 37.90%, Valid: 37.04%, Test: 37.19%
Run 01:
Highest Train: 40.94
Highest Valid: 40.26
  Final Train: 40.94
   Final Test: 40.53
All runs:
Highest Train: 40.94, nan
Highest Valid: 40.26, nan
  Final Train: 40.94, nan
   Final Test: 40.53, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.4, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7966, Train: 18.95%, Valid: 18.92%, Test: 18.69%
Epoch: 25, Loss: 1.5558, Train: 29.28%, Valid: 29.16%, Test: 29.47%
Epoch: 50, Loss: 1.4822, Train: 34.40%, Valid: 34.24%, Test: 34.49%
Epoch: 75, Loss: 1.4636, Train: 37.00%, Valid: 36.96%, Test: 37.61%
Epoch: 100, Loss: 1.4559, Train: 36.80%, Valid: 36.45%, Test: 36.60%
Epoch: 125, Loss: 1.5321, Train: 22.79%, Valid: 22.80%, Test: 23.03%
Epoch: 150, Loss: 1.4479, Train: 37.04%, Valid: 36.52%, Test: 36.97%
Epoch: 175, Loss: 1.4327, Train: 38.02%, Valid: 37.47%, Test: 37.84%
Epoch: 200, Loss: 1.4221, Train: 36.78%, Valid: 36.24%, Test: 36.51%
Epoch: 225, Loss: 1.4279, Train: 38.79%, Valid: 38.52%, Test: 38.64%
Epoch: 250, Loss: 1.4011, Train: 40.04%, Valid: 39.49%, Test: 39.80%
Epoch: 275, Loss: 1.4009, Train: 39.62%, Valid: 39.34%, Test: 39.57%
Epoch: 300, Loss: 1.4166, Train: 40.24%, Valid: 39.56%, Test: 40.09%
Epoch: 325, Loss: 1.6318, Train: 28.22%, Valid: 28.07%, Test: 27.95%
Epoch: 350, Loss: 2.5925, Train: 22.19%, Valid: 22.23%, Test: 22.81%
Epoch: 375, Loss: 1.5079, Train: 37.19%, Valid: 36.99%, Test: 37.17%
Epoch: 400, Loss: 1.4538, Train: 38.45%, Valid: 38.03%, Test: 38.37%
Epoch: 425, Loss: 1.4456, Train: 38.06%, Valid: 37.59%, Test: 37.54%
Epoch: 450, Loss: 1.5072, Train: 37.88%, Valid: 37.24%, Test: 37.57%
Epoch: 475, Loss: 1.4462, Train: 39.54%, Valid: 38.77%, Test: 39.13%
Epoch: 500, Loss: 1.4130, Train: 36.60%, Valid: 36.28%, Test: 36.54%
Epoch: 525, Loss: 1.4121, Train: 38.35%, Valid: 37.88%, Test: 38.28%
Epoch: 550, Loss: 1.6679, Train: 35.32%, Valid: 35.04%, Test: 35.22%
Epoch: 575, Loss: 1.4748, Train: 35.60%, Valid: 35.53%, Test: 35.56%
Epoch: 600, Loss: 1.4222, Train: 37.19%, Valid: 36.58%, Test: 37.08%
Epoch: 625, Loss: 2.2916, Train: 31.34%, Valid: 31.21%, Test: 31.56%
Epoch: 650, Loss: 1.8431, Train: 22.75%, Valid: 22.73%, Test: 23.35%
Epoch: 675, Loss: 1.5072, Train: 36.75%, Valid: 36.40%, Test: 36.66%
Epoch: 700, Loss: 1.4934, Train: 31.95%, Valid: 31.58%, Test: 31.85%
Epoch: 725, Loss: 1.4332, Train: 38.92%, Valid: 38.41%, Test: 38.65%
Epoch: 750, Loss: 2.8135, Train: 23.10%, Valid: 23.04%, Test: 23.55%
Epoch: 775, Loss: 2.7093, Train: 14.95%, Valid: 15.13%, Test: 15.02%
Epoch: 800, Loss: 1.7284, Train: 35.87%, Valid: 35.73%, Test: 35.89%
Epoch: 825, Loss: 1.4918, Train: 35.42%, Valid: 35.14%, Test: 35.07%
Epoch: 850, Loss: 1.4513, Train: 37.83%, Valid: 37.47%, Test: 38.11%
Epoch: 875, Loss: 1.4422, Train: 35.60%, Valid: 35.25%, Test: 35.89%
Epoch: 900, Loss: 1.4341, Train: 39.37%, Valid: 38.80%, Test: 39.21%
Epoch: 925, Loss: 1.4559, Train: 37.38%, Valid: 37.13%, Test: 37.40%
Epoch: 950, Loss: 1.4243, Train: 34.46%, Valid: 33.97%, Test: 34.64%
Epoch: 975, Loss: 1.4344, Train: 39.25%, Valid: 38.65%, Test: 39.04%
Run 01:
Highest Train: 41.86
Highest Valid: 41.13
  Final Train: 41.86
   Final Test: 41.34
All runs:
Highest Train: 41.86, nan
Highest Valid: 41.13, nan
  Final Train: 41.86, nan
   Final Test: 41.34, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.4, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7659, Train: 22.13%, Valid: 22.12%, Test: 22.73%
Epoch: 25, Loss: 1.5338, Train: 30.17%, Valid: 29.60%, Test: 30.46%
Epoch: 50, Loss: 1.4892, Train: 34.34%, Valid: 34.25%, Test: 34.34%
Epoch: 75, Loss: 1.4511, Train: 37.48%, Valid: 37.23%, Test: 37.71%
Epoch: 100, Loss: 1.7123, Train: 25.79%, Valid: 25.48%, Test: 25.76%
Epoch: 125, Loss: 4.0723, Train: 26.08%, Valid: 26.25%, Test: 26.27%
Epoch: 150, Loss: 2.1624, Train: 36.01%, Valid: 35.50%, Test: 35.84%
Epoch: 175, Loss: 1.6790, Train: 31.93%, Valid: 31.70%, Test: 32.00%
Epoch: 200, Loss: 1.7445, Train: 33.50%, Valid: 33.08%, Test: 33.17%
Epoch: 225, Loss: 1.4865, Train: 37.33%, Valid: 36.92%, Test: 37.10%
Epoch: 250, Loss: 1.4205, Train: 36.34%, Valid: 35.79%, Test: 36.62%
Epoch: 275, Loss: 1.4190, Train: 35.60%, Valid: 35.09%, Test: 35.74%
Epoch: 300, Loss: 1.4349, Train: 30.57%, Valid: 30.10%, Test: 30.06%
Epoch: 325, Loss: 1.4088, Train: 38.86%, Valid: 38.12%, Test: 38.50%
Epoch: 350, Loss: 1.3881, Train: 40.69%, Valid: 39.89%, Test: 40.17%
Epoch: 375, Loss: 1.4161, Train: 40.10%, Valid: 39.70%, Test: 39.72%
Epoch: 400, Loss: 1.3802, Train: 41.78%, Valid: 40.84%, Test: 41.03%
Epoch: 425, Loss: 1.4311, Train: 35.66%, Valid: 34.80%, Test: 35.35%
Epoch: 450, Loss: 1.4024, Train: 39.87%, Valid: 39.22%, Test: 39.08%
Epoch: 475, Loss: 2.6031, Train: 24.14%, Valid: 23.70%, Test: 23.77%
Epoch: 500, Loss: 1.4704, Train: 39.20%, Valid: 38.57%, Test: 38.83%
Epoch: 525, Loss: 1.3900, Train: 40.75%, Valid: 39.85%, Test: 40.06%
Epoch: 550, Loss: 1.3616, Train: 42.23%, Valid: 41.27%, Test: 41.55%
Epoch: 575, Loss: 1.3623, Train: 41.05%, Valid: 40.24%, Test: 40.33%
Epoch: 600, Loss: 1.3497, Train: 42.18%, Valid: 41.15%, Test: 41.46%
Epoch: 625, Loss: 1.3984, Train: 39.31%, Valid: 38.77%, Test: 38.83%
Epoch: 650, Loss: 1.3572, Train: 42.47%, Valid: 41.52%, Test: 41.72%
Epoch: 675, Loss: 1.4175, Train: 36.80%, Valid: 35.79%, Test: 36.07%
Epoch: 700, Loss: 1.3542, Train: 42.62%, Valid: 41.89%, Test: 42.02%
Epoch: 725, Loss: 1.3679, Train: 39.14%, Valid: 38.37%, Test: 38.66%
Epoch: 750, Loss: 1.3627, Train: 40.47%, Valid: 39.61%, Test: 39.72%
Epoch: 775, Loss: 2.5406, Train: 29.67%, Valid: 29.59%, Test: 30.03%
Epoch: 800, Loss: 1.5031, Train: 34.48%, Valid: 34.25%, Test: 34.88%
Epoch: 825, Loss: 1.4533, Train: 38.10%, Valid: 37.81%, Test: 38.16%
Epoch: 850, Loss: 1.4626, Train: 28.53%, Valid: 28.45%, Test: 28.45%
Epoch: 875, Loss: 1.4100, Train: 40.06%, Valid: 39.44%, Test: 39.77%
Epoch: 900, Loss: 1.3937, Train: 39.54%, Valid: 38.86%, Test: 38.99%
Epoch: 925, Loss: 1.4579, Train: 41.01%, Valid: 40.36%, Test: 40.62%
Epoch: 950, Loss: 1.4030, Train: 40.33%, Valid: 39.64%, Test: 39.78%
Epoch: 975, Loss: 1.4108, Train: 41.32%, Valid: 40.34%, Test: 40.53%
Run 01:
Highest Train: 43.58
Highest Valid: 42.46
  Final Train: 43.58
   Final Test: 42.79
All runs:
Highest Train: 43.58, nan
Highest Valid: 42.46, nan
  Final Train: 43.58, nan
   Final Test: 42.79, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.4, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7566, Train: 21.65%, Valid: 21.80%, Test: 21.89%
Epoch: 25, Loss: 1.5329, Train: 32.37%, Valid: 32.26%, Test: 32.64%
Epoch: 50, Loss: 1.5018, Train: 34.45%, Valid: 34.16%, Test: 34.40%
Epoch: 75, Loss: 1.5065, Train: 35.08%, Valid: 34.75%, Test: 35.01%
Epoch: 100, Loss: 1.5658, Train: 35.66%, Valid: 35.20%, Test: 35.60%
Epoch: 125, Loss: 1.4695, Train: 37.80%, Valid: 37.34%, Test: 37.66%
Epoch: 150, Loss: 1.4669, Train: 35.90%, Valid: 35.32%, Test: 35.59%
Epoch: 175, Loss: 1.4859, Train: 35.85%, Valid: 35.21%, Test: 35.80%
Epoch: 200, Loss: 1.4611, Train: 37.66%, Valid: 37.07%, Test: 37.56%
Epoch: 225, Loss: 1.4974, Train: 35.51%, Valid: 35.13%, Test: 35.65%
Epoch: 250, Loss: 1.4552, Train: 36.89%, Valid: 36.05%, Test: 36.65%
Epoch: 275, Loss: 1.5132, Train: 27.68%, Valid: 27.61%, Test: 27.95%
Epoch: 300, Loss: 1.4685, Train: 37.05%, Valid: 36.06%, Test: 36.46%
Epoch: 325, Loss: 1.4695, Train: 35.93%, Valid: 35.54%, Test: 35.67%
Epoch: 350, Loss: 1.7270, Train: 27.22%, Valid: 27.17%, Test: 26.57%
Epoch: 375, Loss: 1.4687, Train: 37.14%, Valid: 36.25%, Test: 36.40%
Epoch: 400, Loss: 1.4342, Train: 39.58%, Valid: 38.53%, Test: 38.67%
Epoch: 425, Loss: 1.4631, Train: 36.70%, Valid: 35.95%, Test: 36.40%
Epoch: 450, Loss: 1.4626, Train: 38.92%, Valid: 38.03%, Test: 38.09%
Epoch: 475, Loss: 1.5194, Train: 37.40%, Valid: 36.51%, Test: 36.52%
Epoch: 500, Loss: 1.4433, Train: 39.46%, Valid: 38.76%, Test: 38.81%
Epoch: 525, Loss: 1.5623, Train: 32.80%, Valid: 32.44%, Test: 32.58%
Epoch: 550, Loss: 1.4721, Train: 37.34%, Valid: 36.44%, Test: 36.72%
Epoch: 575, Loss: 1.4630, Train: 37.46%, Valid: 36.44%, Test: 36.54%
Epoch: 600, Loss: 4.3356, Train: 30.08%, Valid: 29.61%, Test: 30.09%
Epoch: 625, Loss: 1.5835, Train: 28.31%, Valid: 28.13%, Test: 28.28%
Epoch: 650, Loss: 1.5562, Train: 34.73%, Valid: 34.60%, Test: 34.77%
Epoch: 675, Loss: 1.4572, Train: 37.53%, Valid: 36.92%, Test: 37.44%
Epoch: 700, Loss: 1.5172, Train: 33.84%, Valid: 33.75%, Test: 33.98%
Epoch: 725, Loss: 1.4964, Train: 36.11%, Valid: 35.59%, Test: 35.90%
Epoch: 750, Loss: 1.4673, Train: 36.81%, Valid: 36.21%, Test: 36.46%
Epoch: 775, Loss: 1.4592, Train: 35.06%, Valid: 34.47%, Test: 34.73%
Epoch: 800, Loss: 1.5187, Train: 36.45%, Valid: 36.01%, Test: 36.61%
Epoch: 825, Loss: 1.4597, Train: 38.25%, Valid: 37.44%, Test: 37.76%
Epoch: 850, Loss: 1.4585, Train: 36.65%, Valid: 36.42%, Test: 36.69%
Epoch: 875, Loss: 1.4586, Train: 37.78%, Valid: 37.06%, Test: 37.26%
Epoch: 900, Loss: 1.4417, Train: 39.50%, Valid: 38.84%, Test: 39.03%
Epoch: 925, Loss: 1.4478, Train: 37.87%, Valid: 37.14%, Test: 37.25%
Epoch: 950, Loss: 1.5418, Train: 38.79%, Valid: 38.12%, Test: 38.42%
Epoch: 975, Loss: 1.4727, Train: 38.07%, Valid: 37.33%, Test: 37.76%
Run 01:
Highest Train: 40.52
Highest Valid: 39.83
  Final Train: 40.52
   Final Test: 39.95
All runs:
Highest Train: 40.52, nan
Highest Valid: 39.83, nan
  Final Train: 40.52, nan
   Final Test: 39.95, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.4, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7927, Train: 12.59%, Valid: 12.66%, Test: 12.54%
Epoch: 25, Loss: 1.5472, Train: 34.19%, Valid: 33.85%, Test: 34.19%
Epoch: 50, Loss: 1.4631, Train: 35.69%, Valid: 35.49%, Test: 35.79%
Epoch: 75, Loss: 1.5750, Train: 28.91%, Valid: 28.69%, Test: 28.94%
Epoch: 100, Loss: 1.4436, Train: 37.27%, Valid: 36.85%, Test: 37.06%
Epoch: 125, Loss: 1.4101, Train: 39.30%, Valid: 38.84%, Test: 39.06%
Epoch: 150, Loss: 1.4145, Train: 39.63%, Valid: 39.16%, Test: 39.37%
Epoch: 175, Loss: 1.5649, Train: 34.58%, Valid: 34.14%, Test: 34.36%
Epoch: 200, Loss: 1.4046, Train: 39.29%, Valid: 39.05%, Test: 39.50%
Epoch: 225, Loss: 1.4046, Train: 37.19%, Valid: 36.80%, Test: 37.08%
Epoch: 250, Loss: 1.3994, Train: 39.26%, Valid: 38.92%, Test: 39.09%
Epoch: 275, Loss: 1.3992, Train: 40.09%, Valid: 39.60%, Test: 40.14%
Epoch: 300, Loss: 1.4087, Train: 40.36%, Valid: 39.69%, Test: 40.18%
Epoch: 325, Loss: 1.3967, Train: 37.54%, Valid: 37.21%, Test: 37.56%
Epoch: 350, Loss: 1.3797, Train: 40.31%, Valid: 39.61%, Test: 40.09%
Epoch: 375, Loss: 1.6822, Train: 27.46%, Valid: 27.14%, Test: 27.53%
Epoch: 400, Loss: 2.2710, Train: 18.97%, Valid: 19.16%, Test: 18.85%
Epoch: 425, Loss: 1.6833, Train: 31.57%, Valid: 31.53%, Test: 31.22%
Epoch: 450, Loss: 1.4743, Train: 35.16%, Valid: 35.12%, Test: 35.36%
Epoch: 475, Loss: 1.4273, Train: 38.98%, Valid: 38.37%, Test: 38.77%
Epoch: 500, Loss: 1.4088, Train: 39.78%, Valid: 39.18%, Test: 39.50%
Epoch: 525, Loss: 1.4195, Train: 38.53%, Valid: 37.89%, Test: 38.31%
Epoch: 550, Loss: 1.3866, Train: 40.53%, Valid: 40.00%, Test: 40.48%
Epoch: 575, Loss: 1.4945, Train: 28.43%, Valid: 28.15%, Test: 28.46%
Epoch: 600, Loss: 1.5273, Train: 34.67%, Valid: 34.11%, Test: 34.49%
Epoch: 625, Loss: 1.4155, Train: 41.28%, Valid: 40.70%, Test: 41.04%
Epoch: 650, Loss: 1.8619, Train: 29.44%, Valid: 29.14%, Test: 29.46%
Epoch: 675, Loss: 1.4753, Train: 34.55%, Valid: 34.31%, Test: 34.58%
Epoch: 700, Loss: 1.4135, Train: 39.99%, Valid: 39.32%, Test: 39.83%
Epoch: 725, Loss: 1.4197, Train: 42.02%, Valid: 41.51%, Test: 41.73%
Epoch: 750, Loss: 1.4401, Train: 38.51%, Valid: 37.79%, Test: 38.51%
Epoch: 775, Loss: 1.7636, Train: 29.13%, Valid: 28.85%, Test: 29.63%
Epoch: 800, Loss: 1.6415, Train: 34.32%, Valid: 34.00%, Test: 34.47%
Epoch: 825, Loss: 1.4470, Train: 32.99%, Valid: 32.45%, Test: 32.85%
Epoch: 850, Loss: 1.3932, Train: 38.06%, Valid: 37.56%, Test: 38.17%
Epoch: 875, Loss: 1.4649, Train: 33.60%, Valid: 32.96%, Test: 33.53%
Epoch: 900, Loss: 1.4183, Train: 40.33%, Valid: 39.69%, Test: 40.23%
Epoch: 925, Loss: 1.4109, Train: 38.28%, Valid: 37.86%, Test: 38.04%
Epoch: 950, Loss: 1.5044, Train: 39.93%, Valid: 39.21%, Test: 39.54%
Epoch: 975, Loss: 1.4778, Train: 32.10%, Valid: 31.70%, Test: 32.08%
Run 01:
Highest Train: 42.04
Highest Valid: 41.51
  Final Train: 42.02
   Final Test: 41.73
All runs:
Highest Train: 42.04, nan
Highest Valid: 41.51, nan
  Final Train: 42.02, nan
   Final Test: 41.73, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.4, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7566, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.4989, Train: 28.29%, Valid: 28.25%, Test: 28.17%
Epoch: 50, Loss: 1.4816, Train: 35.58%, Valid: 35.82%, Test: 35.96%
Epoch: 75, Loss: 1.3950, Train: 40.08%, Valid: 39.37%, Test: 39.98%
Epoch: 100, Loss: 1.3818, Train: 40.10%, Valid: 39.64%, Test: 40.04%
Epoch: 125, Loss: 1.5703, Train: 28.77%, Valid: 28.60%, Test: 28.88%
Epoch: 150, Loss: 1.5480, Train: 36.95%, Valid: 36.64%, Test: 37.17%
Epoch: 175, Loss: 1.4756, Train: 36.10%, Valid: 35.88%, Test: 36.00%
Epoch: 200, Loss: 1.4234, Train: 38.02%, Valid: 37.41%, Test: 37.82%
Epoch: 225, Loss: 1.4187, Train: 38.87%, Valid: 38.33%, Test: 38.93%
Epoch: 250, Loss: 1.4159, Train: 38.39%, Valid: 37.85%, Test: 38.10%
Epoch: 275, Loss: 1.4294, Train: 39.11%, Valid: 38.85%, Test: 39.02%
Epoch: 300, Loss: 1.3845, Train: 40.40%, Valid: 39.74%, Test: 40.08%
Epoch: 325, Loss: 1.3836, Train: 41.17%, Valid: 40.40%, Test: 40.78%
Epoch: 350, Loss: 1.3859, Train: 38.12%, Valid: 37.74%, Test: 37.81%
Epoch: 375, Loss: 1.4158, Train: 39.04%, Valid: 38.59%, Test: 38.93%
Epoch: 400, Loss: 1.3569, Train: 41.67%, Valid: 40.89%, Test: 41.50%
Epoch: 425, Loss: 1.4346, Train: 33.56%, Valid: 33.14%, Test: 33.56%
Epoch: 450, Loss: 1.3762, Train: 38.13%, Valid: 37.41%, Test: 37.44%
Epoch: 475, Loss: 1.4197, Train: 42.18%, Valid: 41.25%, Test: 41.75%
Epoch: 500, Loss: 1.4749, Train: 28.21%, Valid: 28.01%, Test: 27.82%
Epoch: 525, Loss: 1.4683, Train: 36.75%, Valid: 35.83%, Test: 36.29%
Epoch: 550, Loss: 1.3932, Train: 39.26%, Valid: 38.43%, Test: 38.90%
Epoch: 575, Loss: 1.3747, Train: 40.99%, Valid: 40.30%, Test: 40.48%
Epoch: 600, Loss: 1.3477, Train: 42.00%, Valid: 41.23%, Test: 41.66%
Epoch: 625, Loss: 1.3638, Train: 41.89%, Valid: 40.91%, Test: 41.25%
Epoch: 650, Loss: 1.4939, Train: 37.53%, Valid: 36.99%, Test: 37.22%
Epoch: 675, Loss: 1.3574, Train: 42.39%, Valid: 41.55%, Test: 41.95%
Epoch: 700, Loss: 1.3556, Train: 42.37%, Valid: 41.28%, Test: 41.70%
Epoch: 725, Loss: 1.3744, Train: 42.06%, Valid: 41.32%, Test: 41.46%
Epoch: 750, Loss: 1.4432, Train: 39.90%, Valid: 39.21%, Test: 39.43%
Epoch: 775, Loss: 1.4481, Train: 37.26%, Valid: 36.64%, Test: 37.15%
Epoch: 800, Loss: 1.4954, Train: 22.21%, Valid: 22.23%, Test: 22.79%
Epoch: 825, Loss: 1.4276, Train: 37.07%, Valid: 36.42%, Test: 36.58%
Epoch: 850, Loss: 1.4110, Train: 37.86%, Valid: 37.42%, Test: 37.46%
Epoch: 875, Loss: 1.4186, Train: 40.20%, Valid: 39.65%, Test: 39.79%
Epoch: 900, Loss: 1.3834, Train: 38.46%, Valid: 37.93%, Test: 38.34%
Epoch: 925, Loss: 1.3776, Train: 41.18%, Valid: 40.36%, Test: 40.68%
Epoch: 950, Loss: 1.3517, Train: 40.77%, Valid: 39.93%, Test: 40.29%
Epoch: 975, Loss: 1.3691, Train: 40.65%, Valid: 39.94%, Test: 40.09%
Run 01:
Highest Train: 43.18
Highest Valid: 42.36
  Final Train: 43.01
   Final Test: 42.42
All runs:
Highest Train: 43.18, nan
Highest Valid: 42.36, nan
  Final Train: 43.01, nan
   Final Test: 42.42, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.4, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7881, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.5503, Train: 31.86%, Valid: 31.75%, Test: 32.07%
Epoch: 50, Loss: 1.5044, Train: 34.32%, Valid: 34.14%, Test: 34.48%
Epoch: 75, Loss: 1.4876, Train: 35.14%, Valid: 34.88%, Test: 35.15%
Epoch: 100, Loss: 1.5986, Train: 34.69%, Valid: 34.55%, Test: 34.73%
Epoch: 125, Loss: 1.4766, Train: 35.95%, Valid: 35.39%, Test: 36.06%
Epoch: 150, Loss: 1.4644, Train: 35.40%, Valid: 34.98%, Test: 35.35%
Epoch: 175, Loss: 1.4601, Train: 34.64%, Valid: 33.69%, Test: 34.51%
Epoch: 200, Loss: 1.4752, Train: 37.64%, Valid: 37.00%, Test: 37.39%
Epoch: 225, Loss: 1.4687, Train: 39.13%, Valid: 38.33%, Test: 38.66%
Epoch: 250, Loss: 1.4500, Train: 37.89%, Valid: 37.13%, Test: 37.59%
Epoch: 275, Loss: 1.8700, Train: 34.12%, Valid: 33.75%, Test: 33.96%
Epoch: 300, Loss: 1.4916, Train: 33.79%, Valid: 33.37%, Test: 33.81%
Epoch: 325, Loss: 1.4654, Train: 36.91%, Valid: 36.25%, Test: 36.54%
Epoch: 350, Loss: 1.4541, Train: 37.75%, Valid: 37.04%, Test: 37.45%
Epoch: 375, Loss: 1.4739, Train: 33.16%, Valid: 32.82%, Test: 33.06%
Epoch: 400, Loss: 1.5678, Train: 28.77%, Valid: 28.71%, Test: 28.65%
Epoch: 425, Loss: 1.4973, Train: 35.94%, Valid: 35.26%, Test: 35.46%
Epoch: 450, Loss: 1.6529, Train: 36.20%, Valid: 35.68%, Test: 35.87%
Epoch: 475, Loss: 1.6119, Train: 33.32%, Valid: 32.84%, Test: 33.12%
Epoch: 500, Loss: 1.4679, Train: 37.51%, Valid: 36.55%, Test: 36.91%
Epoch: 525, Loss: 1.4956, Train: 34.56%, Valid: 34.11%, Test: 33.94%
Epoch: 550, Loss: 1.4324, Train: 39.82%, Valid: 38.89%, Test: 39.28%
Epoch: 575, Loss: 1.4767, Train: 35.42%, Valid: 34.89%, Test: 35.33%
Epoch: 600, Loss: 1.5325, Train: 28.44%, Valid: 28.03%, Test: 28.75%
Epoch: 625, Loss: 1.4337, Train: 39.07%, Valid: 38.12%, Test: 38.38%
Epoch: 650, Loss: 1.5041, Train: 33.49%, Valid: 32.45%, Test: 33.17%
Epoch: 675, Loss: 1.4512, Train: 38.86%, Valid: 37.64%, Test: 37.93%
Epoch: 700, Loss: 1.6838, Train: 29.70%, Valid: 29.49%, Test: 29.70%
Epoch: 725, Loss: 1.4803, Train: 35.70%, Valid: 35.19%, Test: 35.58%
Epoch: 750, Loss: 1.4745, Train: 35.33%, Valid: 34.59%, Test: 34.97%
Epoch: 775, Loss: 1.4513, Train: 37.55%, Valid: 36.56%, Test: 37.01%
Epoch: 800, Loss: 1.4397, Train: 40.07%, Valid: 38.89%, Test: 39.38%
Epoch: 825, Loss: 1.4606, Train: 38.04%, Valid: 37.04%, Test: 37.67%
Epoch: 850, Loss: 1.4517, Train: 39.61%, Valid: 38.59%, Test: 38.90%
Epoch: 875, Loss: 7.3234, Train: 29.76%, Valid: 29.54%, Test: 29.82%
Epoch: 900, Loss: 3.4938, Train: 29.46%, Valid: 29.23%, Test: 29.54%
Epoch: 925, Loss: 1.5791, Train: 24.88%, Valid: 24.45%, Test: 25.24%
Epoch: 950, Loss: 1.5011, Train: 34.84%, Valid: 34.73%, Test: 34.84%
Epoch: 975, Loss: 1.4899, Train: 35.81%, Valid: 35.36%, Test: 35.53%
Run 01:
Highest Train: 40.54
Highest Valid: 39.54
  Final Train: 40.36
   Final Test: 39.72
All runs:
Highest Train: 40.54, nan
Highest Valid: 39.54, nan
  Final Train: 40.36, nan
   Final Test: 39.72, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.4, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7752, Train: 12.51%, Valid: 12.59%, Test: 12.43%
Epoch: 25, Loss: 1.5542, Train: 28.36%, Valid: 27.77%, Test: 28.31%
Epoch: 50, Loss: 1.4705, Train: 34.37%, Valid: 34.24%, Test: 34.72%
Epoch: 75, Loss: 1.4520, Train: 36.78%, Valid: 36.27%, Test: 36.62%
Epoch: 100, Loss: 1.4443, Train: 38.17%, Valid: 37.93%, Test: 38.53%
Epoch: 125, Loss: 1.4046, Train: 38.57%, Valid: 38.34%, Test: 38.71%
Epoch: 150, Loss: 1.4711, Train: 36.50%, Valid: 36.60%, Test: 36.76%
Epoch: 175, Loss: 2.2056, Train: 26.36%, Valid: 26.09%, Test: 26.09%
Epoch: 200, Loss: 1.5308, Train: 34.51%, Valid: 34.36%, Test: 34.43%
Epoch: 225, Loss: 1.4787, Train: 36.11%, Valid: 35.60%, Test: 35.96%
Epoch: 250, Loss: 1.4614, Train: 36.34%, Valid: 36.04%, Test: 36.70%
Epoch: 275, Loss: 1.4481, Train: 37.72%, Valid: 37.17%, Test: 37.46%
Epoch: 300, Loss: 1.4563, Train: 36.97%, Valid: 36.48%, Test: 36.84%
Epoch: 325, Loss: 1.4361, Train: 39.79%, Valid: 39.12%, Test: 39.41%
Epoch: 350, Loss: 1.4498, Train: 38.04%, Valid: 37.49%, Test: 37.96%
Epoch: 375, Loss: 2.9079, Train: 30.35%, Valid: 30.24%, Test: 30.57%
Epoch: 400, Loss: 1.5402, Train: 36.29%, Valid: 35.56%, Test: 36.09%
Epoch: 425, Loss: 1.4773, Train: 35.87%, Valid: 35.51%, Test: 35.83%
Epoch: 450, Loss: 1.4932, Train: 37.85%, Valid: 37.11%, Test: 37.40%
Epoch: 475, Loss: 1.4552, Train: 36.82%, Valid: 36.43%, Test: 36.59%
Epoch: 500, Loss: 2.4562, Train: 19.74%, Valid: 19.79%, Test: 19.50%
Epoch: 525, Loss: 2.0629, Train: 13.27%, Valid: 13.31%, Test: 13.15%
Epoch: 550, Loss: 1.5234, Train: 33.62%, Valid: 32.87%, Test: 33.61%
Epoch: 575, Loss: 1.4480, Train: 38.67%, Valid: 37.96%, Test: 38.24%
Epoch: 600, Loss: 1.4299, Train: 38.22%, Valid: 37.76%, Test: 37.94%
Epoch: 625, Loss: 1.4216, Train: 37.99%, Valid: 37.58%, Test: 37.77%
Epoch: 650, Loss: 1.4952, Train: 38.09%, Valid: 37.44%, Test: 37.81%
Epoch: 675, Loss: 1.4511, Train: 38.60%, Valid: 38.02%, Test: 38.32%
Epoch: 700, Loss: 1.4343, Train: 35.72%, Valid: 35.06%, Test: 35.43%
Epoch: 725, Loss: 1.6715, Train: 37.35%, Valid: 37.02%, Test: 37.01%
Epoch: 750, Loss: 1.4305, Train: 39.96%, Valid: 39.41%, Test: 39.85%
Epoch: 775, Loss: 1.4190, Train: 41.04%, Valid: 40.54%, Test: 40.79%
Epoch: 800, Loss: 1.4821, Train: 35.22%, Valid: 34.90%, Test: 34.71%
Epoch: 825, Loss: 1.3999, Train: 39.87%, Valid: 39.14%, Test: 39.56%
Epoch: 850, Loss: 1.8642, Train: 28.07%, Valid: 27.97%, Test: 28.82%
Epoch: 875, Loss: 1.5034, Train: 32.53%, Valid: 31.99%, Test: 32.42%
Epoch: 900, Loss: 1.5039, Train: 35.95%, Valid: 35.62%, Test: 35.92%
Epoch: 925, Loss: 1.4330, Train: 36.47%, Valid: 35.81%, Test: 36.24%
Epoch: 950, Loss: 1.4488, Train: 38.63%, Valid: 38.10%, Test: 38.24%
Epoch: 975, Loss: 1.4303, Train: 38.81%, Valid: 38.44%, Test: 38.71%
Run 01:
Highest Train: 41.26
Highest Valid: 40.62
  Final Train: 41.26
   Final Test: 40.99
All runs:
Highest Train: 41.26, nan
Highest Valid: 40.62, nan
  Final Train: 41.26, nan
   Final Test: 40.99, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.4, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7882, Train: 12.51%, Valid: 12.59%, Test: 12.43%
Epoch: 25, Loss: 1.5036, Train: 34.04%, Valid: 34.15%, Test: 34.67%
Epoch: 50, Loss: 1.4914, Train: 32.73%, Valid: 32.33%, Test: 32.65%
Epoch: 75, Loss: 1.3957, Train: 38.62%, Valid: 38.23%, Test: 38.43%
Epoch: 100, Loss: 1.4316, Train: 39.65%, Valid: 39.31%, Test: 39.24%
Epoch: 125, Loss: 1.4125, Train: 38.32%, Valid: 38.06%, Test: 38.39%
Epoch: 150, Loss: 1.3694, Train: 40.48%, Valid: 40.01%, Test: 40.33%
Epoch: 175, Loss: 2.0328, Train: 29.23%, Valid: 29.04%, Test: 29.31%
Epoch: 200, Loss: 1.5121, Train: 38.32%, Valid: 38.26%, Test: 38.51%
Epoch: 225, Loss: 1.5179, Train: 37.99%, Valid: 37.67%, Test: 37.94%
Epoch: 250, Loss: 1.4311, Train: 39.10%, Valid: 38.49%, Test: 38.99%
Epoch: 275, Loss: 1.4367, Train: 38.93%, Valid: 38.29%, Test: 38.94%
Epoch: 300, Loss: 1.4324, Train: 39.32%, Valid: 38.84%, Test: 39.02%
Epoch: 325, Loss: 1.4491, Train: 38.15%, Valid: 37.57%, Test: 37.95%
Epoch: 350, Loss: 1.4186, Train: 39.47%, Valid: 38.96%, Test: 39.48%
Epoch: 375, Loss: 1.4095, Train: 39.89%, Valid: 39.15%, Test: 39.60%
Epoch: 400, Loss: 1.4897, Train: 38.71%, Valid: 38.31%, Test: 38.46%
Epoch: 425, Loss: 1.4479, Train: 38.82%, Valid: 38.27%, Test: 38.51%
Epoch: 450, Loss: 1.4108, Train: 38.54%, Valid: 37.80%, Test: 37.85%
Epoch: 475, Loss: 1.4093, Train: 41.14%, Valid: 40.29%, Test: 40.50%
Epoch: 500, Loss: 1.3784, Train: 34.85%, Valid: 34.37%, Test: 34.75%
Epoch: 525, Loss: 1.4381, Train: 38.11%, Valid: 37.57%, Test: 37.80%
Epoch: 550, Loss: 1.3909, Train: 41.47%, Valid: 40.60%, Test: 40.83%
Epoch: 575, Loss: 1.3637, Train: 42.47%, Valid: 41.61%, Test: 41.91%
Epoch: 600, Loss: 1.3784, Train: 31.71%, Valid: 31.23%, Test: 31.39%
Epoch: 625, Loss: 1.3566, Train: 42.26%, Valid: 41.25%, Test: 41.43%
Epoch: 650, Loss: 1.3726, Train: 41.20%, Valid: 40.47%, Test: 40.86%
Epoch: 675, Loss: 1.3603, Train: 42.14%, Valid: 41.37%, Test: 41.48%
Epoch: 700, Loss: 1.3728, Train: 42.24%, Valid: 41.08%, Test: 41.18%
Epoch: 725, Loss: 1.3515, Train: 42.68%, Valid: 41.64%, Test: 41.64%
Epoch: 750, Loss: 1.3678, Train: 41.11%, Valid: 40.39%, Test: 40.87%
Epoch: 775, Loss: 1.3445, Train: 42.92%, Valid: 41.95%, Test: 42.02%
Epoch: 800, Loss: 1.3458, Train: 42.77%, Valid: 41.78%, Test: 42.15%
Epoch: 825, Loss: 1.3796, Train: 42.11%, Valid: 41.09%, Test: 41.36%
Epoch: 850, Loss: 1.3248, Train: 43.85%, Valid: 42.79%, Test: 42.85%
Epoch: 875, Loss: 1.3195, Train: 44.00%, Valid: 42.90%, Test: 43.19%
Epoch: 900, Loss: 1.3363, Train: 43.22%, Valid: 42.23%, Test: 42.49%
Epoch: 925, Loss: 1.3531, Train: 42.31%, Valid: 41.47%, Test: 41.57%
Epoch: 950, Loss: 1.3429, Train: 43.23%, Valid: 42.39%, Test: 42.53%
Epoch: 975, Loss: 1.3178, Train: 43.85%, Valid: 42.79%, Test: 43.07%
Run 01:
Highest Train: 44.61
Highest Valid: 43.49
  Final Train: 44.61
   Final Test: 43.67
All runs:
Highest Train: 44.61, nan
Highest Valid: 43.49, nan
  Final Train: 44.61, nan
   Final Test: 43.67, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.8214, Train: 12.51%, Valid: 12.59%, Test: 12.43%
Epoch: 25, Loss: 1.5900, Train: 28.25%, Valid: 27.73%, Test: 28.25%
Epoch: 50, Loss: 1.5072, Train: 33.96%, Valid: 33.89%, Test: 34.15%
Epoch: 75, Loss: 1.4957, Train: 34.75%, Valid: 34.38%, Test: 34.70%
Epoch: 100, Loss: 1.4648, Train: 36.45%, Valid: 35.85%, Test: 36.36%
Epoch: 125, Loss: 1.4854, Train: 36.13%, Valid: 35.73%, Test: 36.51%
Epoch: 150, Loss: 1.4770, Train: 36.64%, Valid: 36.40%, Test: 36.88%
Epoch: 175, Loss: 1.4647, Train: 37.99%, Valid: 37.38%, Test: 37.75%
Epoch: 200, Loss: 1.4665, Train: 37.13%, Valid: 36.30%, Test: 36.87%
Epoch: 225, Loss: 1.4321, Train: 39.06%, Valid: 38.22%, Test: 38.63%
Epoch: 250, Loss: 1.4694, Train: 37.20%, Valid: 36.54%, Test: 37.00%
Epoch: 275, Loss: 1.4594, Train: 38.17%, Valid: 37.65%, Test: 37.77%
Epoch: 300, Loss: 1.6775, Train: 34.58%, Valid: 34.26%, Test: 34.71%
Epoch: 325, Loss: 1.5309, Train: 35.77%, Valid: 35.45%, Test: 35.73%
Epoch: 350, Loss: 1.4851, Train: 37.14%, Valid: 36.56%, Test: 36.82%
Epoch: 375, Loss: 1.4612, Train: 38.08%, Valid: 37.48%, Test: 37.57%
Epoch: 400, Loss: 1.4369, Train: 38.65%, Valid: 37.97%, Test: 38.26%
Epoch: 425, Loss: 1.4705, Train: 36.34%, Valid: 36.04%, Test: 36.22%
Epoch: 450, Loss: 1.4470, Train: 38.28%, Valid: 37.54%, Test: 38.30%
Epoch: 475, Loss: 1.4762, Train: 37.53%, Valid: 36.89%, Test: 37.64%
Epoch: 500, Loss: 1.4278, Train: 39.07%, Valid: 38.71%, Test: 38.89%
Epoch: 525, Loss: 1.5054, Train: 24.01%, Valid: 23.84%, Test: 24.42%
Epoch: 550, Loss: 1.4944, Train: 37.55%, Valid: 36.75%, Test: 37.14%
Epoch: 575, Loss: 1.5208, Train: 37.62%, Valid: 36.88%, Test: 37.11%
Epoch: 600, Loss: 1.4527, Train: 38.13%, Valid: 37.15%, Test: 37.72%
Epoch: 625, Loss: 1.4441, Train: 39.63%, Valid: 38.86%, Test: 39.04%
Epoch: 650, Loss: 1.4411, Train: 38.16%, Valid: 37.33%, Test: 37.61%
Epoch: 675, Loss: 1.4543, Train: 38.02%, Valid: 37.31%, Test: 37.57%
Epoch: 700, Loss: 1.4695, Train: 38.70%, Valid: 37.93%, Test: 38.23%
Epoch: 725, Loss: 1.4750, Train: 32.47%, Valid: 32.11%, Test: 32.33%
Epoch: 750, Loss: 1.5781, Train: 35.01%, Valid: 33.98%, Test: 34.78%
Epoch: 775, Loss: 1.4915, Train: 38.61%, Valid: 37.73%, Test: 38.05%
Epoch: 800, Loss: 1.4761, Train: 35.99%, Valid: 35.01%, Test: 35.58%
Epoch: 825, Loss: 1.5579, Train: 34.22%, Valid: 33.96%, Test: 34.15%
Epoch: 850, Loss: 1.4751, Train: 36.90%, Valid: 35.94%, Test: 36.30%
Epoch: 875, Loss: 1.4466, Train: 38.49%, Valid: 37.57%, Test: 38.04%
Epoch: 900, Loss: 1.4366, Train: 38.57%, Valid: 37.88%, Test: 37.97%
Epoch: 925, Loss: 1.4404, Train: 37.51%, Valid: 36.79%, Test: 37.03%
Epoch: 950, Loss: 1.4434, Train: 32.62%, Valid: 31.91%, Test: 32.16%
Epoch: 975, Loss: 1.4767, Train: 37.50%, Valid: 36.95%, Test: 37.11%
Run 01:
Highest Train: 40.75
Highest Valid: 39.87
  Final Train: 40.75
   Final Test: 40.12
All runs:
Highest Train: 40.75, nan
Highest Valid: 39.87, nan
  Final Train: 40.75, nan
   Final Test: 40.12, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.8002, Train: 17.97%, Valid: 18.08%, Test: 17.70%
Epoch: 25, Loss: 1.5760, Train: 31.24%, Valid: 31.03%, Test: 31.62%
Epoch: 50, Loss: 1.4891, Train: 35.79%, Valid: 35.48%, Test: 35.60%
Epoch: 75, Loss: 1.4500, Train: 38.91%, Valid: 38.21%, Test: 38.59%
Epoch: 100, Loss: 1.4919, Train: 34.06%, Valid: 33.67%, Test: 34.02%
Epoch: 125, Loss: 1.4691, Train: 35.57%, Valid: 35.20%, Test: 35.54%
Epoch: 150, Loss: 1.4420, Train: 37.05%, Valid: 36.94%, Test: 37.02%
Epoch: 175, Loss: 1.4442, Train: 37.25%, Valid: 36.63%, Test: 36.98%
Epoch: 200, Loss: 1.7445, Train: 34.65%, Valid: 34.32%, Test: 35.03%
Epoch: 225, Loss: 1.4861, Train: 35.82%, Valid: 35.61%, Test: 36.28%
Epoch: 250, Loss: 1.4634, Train: 36.88%, Valid: 36.59%, Test: 36.76%
Epoch: 275, Loss: 1.4432, Train: 37.43%, Valid: 36.89%, Test: 37.25%
Epoch: 300, Loss: 1.5025, Train: 34.96%, Valid: 34.84%, Test: 35.03%
Epoch: 325, Loss: 1.4205, Train: 36.37%, Valid: 36.02%, Test: 36.16%
Epoch: 350, Loss: 1.4404, Train: 39.36%, Valid: 38.88%, Test: 39.13%
Epoch: 375, Loss: 1.4005, Train: 38.47%, Valid: 38.05%, Test: 38.15%
Epoch: 400, Loss: 1.4633, Train: 33.52%, Valid: 33.25%, Test: 33.35%
Epoch: 425, Loss: 1.4527, Train: 38.34%, Valid: 37.82%, Test: 37.89%
Epoch: 450, Loss: 1.4169, Train: 38.08%, Valid: 37.63%, Test: 37.76%
Epoch: 475, Loss: 1.4254, Train: 38.27%, Valid: 37.96%, Test: 38.19%
Epoch: 500, Loss: 1.6302, Train: 31.11%, Valid: 30.69%, Test: 31.29%
Epoch: 525, Loss: 1.4636, Train: 35.57%, Valid: 35.38%, Test: 35.55%
Epoch: 550, Loss: 1.4156, Train: 40.24%, Valid: 39.75%, Test: 40.00%
Epoch: 575, Loss: 1.4685, Train: 37.95%, Valid: 37.38%, Test: 37.79%
Epoch: 600, Loss: 1.3840, Train: 40.40%, Valid: 39.52%, Test: 39.89%
Epoch: 625, Loss: 1.4076, Train: 40.74%, Valid: 40.00%, Test: 40.45%
Epoch: 650, Loss: 1.3593, Train: 42.24%, Valid: 41.67%, Test: 41.78%
Epoch: 675, Loss: 1.8754, Train: 30.04%, Valid: 29.60%, Test: 30.20%
Epoch: 700, Loss: 1.4654, Train: 36.93%, Valid: 36.56%, Test: 37.19%
Epoch: 725, Loss: 1.4819, Train: 33.27%, Valid: 33.06%, Test: 33.25%
Epoch: 750, Loss: 1.4933, Train: 36.72%, Valid: 36.31%, Test: 36.46%
Epoch: 775, Loss: 1.4475, Train: 37.68%, Valid: 37.17%, Test: 37.39%
Epoch: 800, Loss: 1.4300, Train: 40.15%, Valid: 39.34%, Test: 39.95%
Epoch: 825, Loss: 1.4033, Train: 37.84%, Valid: 37.32%, Test: 37.08%
Epoch: 850, Loss: 1.7093, Train: 28.30%, Valid: 28.16%, Test: 28.35%
Epoch: 875, Loss: 1.5983, Train: 34.87%, Valid: 34.72%, Test: 34.62%
Epoch: 900, Loss: 1.7389, Train: 36.60%, Valid: 35.99%, Test: 36.39%
Epoch: 925, Loss: 1.4762, Train: 36.26%, Valid: 35.90%, Test: 36.13%
Epoch: 950, Loss: 1.4620, Train: 38.22%, Valid: 37.84%, Test: 38.00%
Epoch: 975, Loss: 1.4241, Train: 38.34%, Valid: 37.76%, Test: 38.02%
Run 01:
Highest Train: 42.42
Highest Valid: 41.67
  Final Train: 42.24
   Final Test: 41.78
All runs:
Highest Train: 42.42, nan
Highest Valid: 41.67, nan
  Final Train: 42.24, nan
   Final Test: 41.78, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.8016, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.5074, Train: 30.86%, Valid: 31.36%, Test: 31.45%
Epoch: 50, Loss: 1.4528, Train: 37.72%, Valid: 37.69%, Test: 37.97%
Epoch: 75, Loss: 1.4887, Train: 37.43%, Valid: 37.36%, Test: 37.59%
Epoch: 100, Loss: 1.4182, Train: 40.20%, Valid: 39.70%, Test: 39.85%
Epoch: 125, Loss: 1.3880, Train: 40.75%, Valid: 40.18%, Test: 40.37%
Epoch: 150, Loss: 1.3650, Train: 41.13%, Valid: 40.48%, Test: 40.68%
Epoch: 175, Loss: 1.3704, Train: 41.13%, Valid: 40.55%, Test: 40.78%
Epoch: 200, Loss: 1.3697, Train: 40.96%, Valid: 40.32%, Test: 40.62%
Epoch: 225, Loss: 4.2783, Train: 19.55%, Valid: 19.40%, Test: 19.44%
Epoch: 250, Loss: 1.5524, Train: 34.45%, Valid: 34.13%, Test: 34.45%
Epoch: 275, Loss: 1.4782, Train: 36.41%, Valid: 36.02%, Test: 36.41%
Epoch: 300, Loss: 1.4494, Train: 36.65%, Valid: 36.17%, Test: 36.55%
Epoch: 325, Loss: 1.4273, Train: 38.04%, Valid: 37.60%, Test: 37.79%
Epoch: 350, Loss: 1.4144, Train: 39.34%, Valid: 38.80%, Test: 39.06%
Epoch: 375, Loss: 1.3872, Train: 41.09%, Valid: 40.42%, Test: 40.80%
Epoch: 400, Loss: 1.3852, Train: 40.54%, Valid: 39.94%, Test: 40.35%
Epoch: 425, Loss: 1.3876, Train: 40.10%, Valid: 39.57%, Test: 39.89%
Epoch: 450, Loss: 1.3683, Train: 41.93%, Valid: 41.17%, Test: 41.63%
Epoch: 475, Loss: 1.3575, Train: 42.65%, Valid: 42.06%, Test: 42.38%
Epoch: 500, Loss: 1.3537, Train: 42.23%, Valid: 41.62%, Test: 42.01%
Epoch: 525, Loss: 1.5726, Train: 31.26%, Valid: 31.11%, Test: 31.15%
Epoch: 550, Loss: 1.4396, Train: 38.12%, Valid: 37.41%, Test: 37.91%
Epoch: 575, Loss: 1.4009, Train: 36.67%, Valid: 36.27%, Test: 36.58%
Epoch: 600, Loss: 1.4233, Train: 38.68%, Valid: 38.10%, Test: 38.60%
Epoch: 625, Loss: 1.3657, Train: 40.29%, Valid: 39.73%, Test: 39.96%
Epoch: 650, Loss: 1.3621, Train: 41.97%, Valid: 41.20%, Test: 41.45%
Epoch: 675, Loss: 1.3357, Train: 42.84%, Valid: 42.21%, Test: 42.37%
Epoch: 700, Loss: 1.3769, Train: 42.09%, Valid: 41.38%, Test: 41.64%
Epoch: 725, Loss: 1.3642, Train: 42.51%, Valid: 41.78%, Test: 42.22%
Epoch: 750, Loss: 1.3618, Train: 41.50%, Valid: 40.68%, Test: 41.22%
Epoch: 775, Loss: 1.3759, Train: 40.71%, Valid: 40.21%, Test: 40.25%
Epoch: 800, Loss: 1.3982, Train: 42.26%, Valid: 41.66%, Test: 41.70%
Epoch: 825, Loss: 1.3498, Train: 42.79%, Valid: 42.09%, Test: 42.29%
Epoch: 850, Loss: 1.3631, Train: 40.84%, Valid: 40.30%, Test: 40.46%
Epoch: 875, Loss: 1.3275, Train: 43.19%, Valid: 42.42%, Test: 42.75%
Epoch: 900, Loss: 1.3451, Train: 42.13%, Valid: 41.31%, Test: 41.50%
Epoch: 925, Loss: 1.3499, Train: 40.75%, Valid: 40.15%, Test: 40.21%
Epoch: 950, Loss: 1.3809, Train: 40.42%, Valid: 39.68%, Test: 40.06%
Epoch: 975, Loss: 1.4044, Train: 41.25%, Valid: 40.58%, Test: 40.70%
Run 01:
Highest Train: 44.42
Highest Valid: 43.70
  Final Train: 44.42
   Final Test: 43.85
All runs:
Highest Train: 44.42, nan
Highest Valid: 43.70, nan
  Final Train: 44.42, nan
   Final Test: 43.85, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.8005, Train: 12.51%, Valid: 12.59%, Test: 12.43%
Epoch: 25, Loss: 1.5811, Train: 31.80%, Valid: 31.58%, Test: 31.89%
Epoch: 50, Loss: 1.5132, Train: 34.10%, Valid: 33.87%, Test: 34.00%
Epoch: 75, Loss: 1.4999, Train: 34.26%, Valid: 34.07%, Test: 34.37%
Epoch: 100, Loss: 1.4719, Train: 36.68%, Valid: 36.33%, Test: 36.64%
Epoch: 125, Loss: 1.4848, Train: 35.72%, Valid: 35.57%, Test: 35.81%
Epoch: 150, Loss: 1.4683, Train: 36.47%, Valid: 36.38%, Test: 36.76%
Epoch: 175, Loss: 1.4980, Train: 36.43%, Valid: 35.91%, Test: 36.41%
Epoch: 200, Loss: 1.4784, Train: 36.72%, Valid: 36.30%, Test: 36.47%
Epoch: 225, Loss: 1.5000, Train: 36.06%, Valid: 35.47%, Test: 36.18%
Epoch: 250, Loss: 1.4765, Train: 36.25%, Valid: 35.79%, Test: 36.09%
Epoch: 275, Loss: 1.4659, Train: 34.36%, Valid: 34.17%, Test: 34.45%
Epoch: 300, Loss: 1.5450, Train: 34.61%, Valid: 34.00%, Test: 34.65%
Epoch: 325, Loss: 1.5455, Train: 26.59%, Valid: 26.67%, Test: 26.91%
Epoch: 350, Loss: 1.4638, Train: 37.26%, Valid: 36.48%, Test: 36.93%
Epoch: 375, Loss: 1.4504, Train: 36.86%, Valid: 36.43%, Test: 36.43%
Epoch: 400, Loss: 1.5265, Train: 31.62%, Valid: 31.30%, Test: 31.59%
Epoch: 425, Loss: 1.4524, Train: 38.00%, Valid: 37.39%, Test: 38.01%
Epoch: 450, Loss: 1.4402, Train: 39.20%, Valid: 38.37%, Test: 39.14%
Epoch: 475, Loss: 1.5101, Train: 29.19%, Valid: 28.91%, Test: 29.52%
Epoch: 500, Loss: 1.4530, Train: 38.13%, Valid: 37.38%, Test: 37.74%
Epoch: 525, Loss: 1.4899, Train: 33.98%, Valid: 33.19%, Test: 34.09%
Epoch: 550, Loss: 1.4421, Train: 38.70%, Valid: 37.94%, Test: 38.20%
Epoch: 575, Loss: 1.7438, Train: 36.39%, Valid: 35.97%, Test: 35.96%
Epoch: 600, Loss: 1.4592, Train: 38.39%, Valid: 37.53%, Test: 37.92%
Epoch: 625, Loss: 1.4571, Train: 39.40%, Valid: 38.34%, Test: 38.72%
Epoch: 650, Loss: 1.4530, Train: 37.85%, Valid: 37.17%, Test: 37.48%
Epoch: 675, Loss: 1.5064, Train: 32.69%, Valid: 31.80%, Test: 32.38%
Epoch: 700, Loss: 1.4700, Train: 38.32%, Valid: 37.54%, Test: 37.80%
Epoch: 725, Loss: 1.4443, Train: 34.79%, Valid: 34.38%, Test: 34.81%
Epoch: 750, Loss: 1.5593, Train: 36.69%, Valid: 36.37%, Test: 36.45%
Epoch: 775, Loss: 1.4499, Train: 38.56%, Valid: 37.87%, Test: 38.11%
Epoch: 800, Loss: 1.5025, Train: 36.66%, Valid: 36.19%, Test: 36.29%
Epoch: 825, Loss: 1.5160, Train: 31.25%, Valid: 30.92%, Test: 31.08%
Epoch: 850, Loss: 1.4739, Train: 36.77%, Valid: 36.42%, Test: 36.58%
Epoch: 875, Loss: 1.4743, Train: 38.88%, Valid: 37.87%, Test: 38.34%
Epoch: 900, Loss: 1.5157, Train: 38.53%, Valid: 37.65%, Test: 37.94%
Epoch: 925, Loss: 1.4417, Train: 37.12%, Valid: 36.68%, Test: 36.87%
Epoch: 950, Loss: 1.4666, Train: 39.56%, Valid: 38.55%, Test: 38.77%
Epoch: 975, Loss: 1.4455, Train: 38.46%, Valid: 37.73%, Test: 37.88%
Run 01:
Highest Train: 40.69
Highest Valid: 39.76
  Final Train: 40.69
   Final Test: 39.89
All runs:
Highest Train: 40.69, nan
Highest Valid: 39.76, nan
  Final Train: 40.69, nan
   Final Test: 39.89, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.8137, Train: 22.57%, Valid: 22.83%, Test: 22.71%
Epoch: 25, Loss: 1.5736, Train: 30.52%, Valid: 30.51%, Test: 30.73%
Epoch: 50, Loss: 1.4777, Train: 35.74%, Valid: 35.65%, Test: 36.14%
Epoch: 75, Loss: 1.5984, Train: 30.66%, Valid: 30.36%, Test: 30.39%
Epoch: 100, Loss: 1.4954, Train: 35.85%, Valid: 35.80%, Test: 36.49%
Epoch: 125, Loss: 1.4505, Train: 37.74%, Valid: 37.40%, Test: 37.66%
Epoch: 150, Loss: 1.4357, Train: 38.28%, Valid: 38.12%, Test: 38.41%
Epoch: 175, Loss: 1.4291, Train: 39.60%, Valid: 39.51%, Test: 39.69%
Epoch: 200, Loss: 1.7197, Train: 33.18%, Valid: 32.88%, Test: 33.50%
Epoch: 225, Loss: 1.4664, Train: 37.75%, Valid: 37.37%, Test: 37.73%
Epoch: 250, Loss: 1.4975, Train: 35.83%, Valid: 35.61%, Test: 35.77%
Epoch: 275, Loss: 1.4073, Train: 39.45%, Valid: 38.67%, Test: 39.09%
Epoch: 300, Loss: 1.4301, Train: 37.97%, Valid: 37.60%, Test: 37.67%
Epoch: 325, Loss: 1.4408, Train: 40.63%, Valid: 40.07%, Test: 40.43%
Epoch: 350, Loss: 1.8623, Train: 18.96%, Valid: 18.93%, Test: 18.68%
Epoch: 375, Loss: 1.6317, Train: 34.59%, Valid: 34.22%, Test: 34.71%
Epoch: 400, Loss: 1.4695, Train: 38.09%, Valid: 38.19%, Test: 38.30%
Epoch: 425, Loss: 1.5939, Train: 34.68%, Valid: 34.37%, Test: 34.51%
Epoch: 450, Loss: 1.4730, Train: 36.83%, Valid: 36.49%, Test: 36.83%
Epoch: 475, Loss: 1.5043, Train: 36.09%, Valid: 35.79%, Test: 36.06%
Epoch: 500, Loss: 1.6857, Train: 26.28%, Valid: 26.12%, Test: 26.39%
Epoch: 525, Loss: 1.4727, Train: 35.96%, Valid: 35.66%, Test: 35.89%
Epoch: 550, Loss: 1.4556, Train: 37.21%, Valid: 36.51%, Test: 36.95%
Epoch: 575, Loss: 1.4995, Train: 37.98%, Valid: 37.48%, Test: 37.86%
Epoch: 600, Loss: 1.4153, Train: 39.26%, Valid: 38.99%, Test: 39.28%
Epoch: 625, Loss: 1.4526, Train: 38.01%, Valid: 37.37%, Test: 37.67%
Epoch: 650, Loss: 2.1972, Train: 35.33%, Valid: 34.86%, Test: 35.48%
Epoch: 675, Loss: 1.5369, Train: 34.52%, Valid: 34.15%, Test: 34.48%
Epoch: 700, Loss: 1.5398, Train: 33.42%, Valid: 33.32%, Test: 33.62%
Epoch: 725, Loss: 1.4543, Train: 36.78%, Valid: 36.59%, Test: 36.91%
Epoch: 750, Loss: 1.4396, Train: 36.92%, Valid: 36.49%, Test: 36.79%
Epoch: 775, Loss: 1.4538, Train: 38.24%, Valid: 38.02%, Test: 38.48%
Epoch: 800, Loss: 1.4089, Train: 39.03%, Valid: 38.73%, Test: 39.11%
Epoch: 825, Loss: 1.5216, Train: 39.19%, Valid: 38.73%, Test: 39.02%
Epoch: 850, Loss: 1.4811, Train: 38.09%, Valid: 37.79%, Test: 38.14%
Epoch: 875, Loss: 1.5259, Train: 31.65%, Valid: 31.49%, Test: 32.11%
Epoch: 900, Loss: 1.4800, Train: 35.78%, Valid: 35.60%, Test: 35.70%
Epoch: 925, Loss: 1.4401, Train: 39.65%, Valid: 39.11%, Test: 39.65%
Epoch: 950, Loss: 1.4109, Train: 39.53%, Valid: 38.89%, Test: 39.40%
Epoch: 975, Loss: 1.4153, Train: 38.28%, Valid: 37.78%, Test: 38.19%
Run 01:
Highest Train: 40.96
Highest Valid: 40.53
  Final Train: 40.95
   Final Test: 40.59
All runs:
Highest Train: 40.96, nan
Highest Valid: 40.53, nan
  Final Train: 40.95, nan
   Final Test: 40.59, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.8197, Train: 22.11%, Valid: 22.14%, Test: 22.70%
Epoch: 25, Loss: 1.5912, Train: 33.61%, Valid: 33.48%, Test: 34.00%
Epoch: 50, Loss: 1.4304, Train: 39.10%, Valid: 38.89%, Test: 39.32%
Epoch: 75, Loss: 1.4070, Train: 38.71%, Valid: 38.38%, Test: 38.59%
Epoch: 100, Loss: 1.4437, Train: 36.97%, Valid: 36.61%, Test: 36.45%
Epoch: 125, Loss: 1.4711, Train: 31.16%, Valid: 30.62%, Test: 31.56%
Epoch: 150, Loss: 1.4180, Train: 39.08%, Valid: 38.45%, Test: 38.87%
Epoch: 175, Loss: 1.3766, Train: 39.85%, Valid: 39.27%, Test: 39.50%
Epoch: 200, Loss: 1.3676, Train: 41.02%, Valid: 40.67%, Test: 40.83%
Epoch: 225, Loss: 1.3702, Train: 40.90%, Valid: 40.10%, Test: 40.43%
Epoch: 250, Loss: 2.7892, Train: 30.01%, Valid: 29.73%, Test: 29.79%
Epoch: 275, Loss: 1.5190, Train: 33.14%, Valid: 32.76%, Test: 33.63%
Epoch: 300, Loss: 1.4955, Train: 36.68%, Valid: 36.39%, Test: 36.40%
Epoch: 325, Loss: 1.5065, Train: 33.04%, Valid: 32.83%, Test: 33.71%
Epoch: 350, Loss: 1.4891, Train: 36.01%, Valid: 35.59%, Test: 35.84%
Epoch: 375, Loss: 1.4860, Train: 37.81%, Valid: 37.33%, Test: 37.78%
Epoch: 400, Loss: 1.4458, Train: 38.96%, Valid: 38.80%, Test: 39.11%
Epoch: 425, Loss: 1.4552, Train: 38.17%, Valid: 38.08%, Test: 38.14%
Epoch: 450, Loss: 1.4266, Train: 39.71%, Valid: 39.16%, Test: 39.47%
Epoch: 475, Loss: 1.4639, Train: 39.19%, Valid: 38.86%, Test: 39.14%
Epoch: 500, Loss: 1.3897, Train: 36.42%, Valid: 36.20%, Test: 36.07%
Epoch: 525, Loss: 1.4764, Train: 39.40%, Valid: 38.86%, Test: 39.07%
Epoch: 550, Loss: 1.4187, Train: 41.05%, Valid: 40.40%, Test: 40.68%
Epoch: 575, Loss: 3.9547, Train: 31.11%, Valid: 30.67%, Test: 31.27%
Epoch: 600, Loss: 1.6520, Train: 28.00%, Valid: 28.02%, Test: 27.91%
Epoch: 625, Loss: 1.5384, Train: 33.41%, Valid: 32.72%, Test: 33.64%
Epoch: 650, Loss: 1.4736, Train: 38.21%, Valid: 37.78%, Test: 38.31%
Epoch: 675, Loss: 1.4605, Train: 28.83%, Valid: 28.78%, Test: 28.80%
Epoch: 700, Loss: 1.4475, Train: 36.81%, Valid: 36.48%, Test: 36.71%
Epoch: 725, Loss: 1.4498, Train: 36.55%, Valid: 36.34%, Test: 36.48%
Epoch: 750, Loss: 1.3974, Train: 40.80%, Valid: 40.36%, Test: 40.63%
Epoch: 775, Loss: 1.4801, Train: 39.07%, Valid: 38.69%, Test: 38.97%
Epoch: 800, Loss: 1.4311, Train: 37.39%, Valid: 37.25%, Test: 37.65%
Epoch: 825, Loss: 2.2018, Train: 19.16%, Valid: 19.08%, Test: 18.92%
Epoch: 850, Loss: 1.8130, Train: 31.47%, Valid: 31.41%, Test: 31.40%
Epoch: 875, Loss: 1.5444, Train: 32.07%, Valid: 31.89%, Test: 32.29%
Epoch: 900, Loss: 1.5098, Train: 33.89%, Valid: 33.76%, Test: 34.30%
Epoch: 925, Loss: 1.5059, Train: 34.19%, Valid: 33.73%, Test: 34.22%
Epoch: 950, Loss: 1.4708, Train: 36.26%, Valid: 36.01%, Test: 36.13%
Epoch: 975, Loss: 1.4682, Train: 37.70%, Valid: 37.22%, Test: 37.58%
Run 01:
Highest Train: 42.42
Highest Valid: 41.74
  Final Train: 42.42
   Final Test: 41.89
All runs:
Highest Train: 42.42, nan
Highest Valid: 41.74, nan
  Final Train: 42.42, nan
   Final Test: 41.89, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7813, Train: 21.55%, Valid: 21.40%, Test: 21.46%
Epoch: 25, Loss: 1.5822, Train: 32.53%, Valid: 32.28%, Test: 32.59%
Epoch: 50, Loss: 1.5125, Train: 34.33%, Valid: 34.10%, Test: 34.40%
Epoch: 75, Loss: 1.5018, Train: 34.66%, Valid: 34.41%, Test: 34.62%
Epoch: 100, Loss: 1.4944, Train: 35.29%, Valid: 34.94%, Test: 35.11%
Epoch: 125, Loss: 1.5658, Train: 33.52%, Valid: 33.14%, Test: 33.57%
Epoch: 150, Loss: 1.4788, Train: 35.98%, Valid: 35.80%, Test: 35.96%
Epoch: 175, Loss: 1.5226, Train: 35.26%, Valid: 34.89%, Test: 35.14%
Epoch: 200, Loss: 1.4628, Train: 38.08%, Valid: 37.29%, Test: 37.88%
Epoch: 225, Loss: 1.4642, Train: 32.87%, Valid: 32.64%, Test: 32.59%
Epoch: 250, Loss: 1.5000, Train: 37.74%, Valid: 37.15%, Test: 37.59%
Epoch: 275, Loss: 1.8754, Train: 28.76%, Valid: 28.61%, Test: 28.92%
Epoch: 300, Loss: 1.5500, Train: 30.32%, Valid: 30.10%, Test: 30.44%
Epoch: 325, Loss: 1.5064, Train: 35.17%, Valid: 34.81%, Test: 35.35%
Epoch: 350, Loss: 1.4938, Train: 35.62%, Valid: 35.17%, Test: 35.53%
Epoch: 375, Loss: 1.5158, Train: 35.47%, Valid: 35.11%, Test: 35.35%
Epoch: 400, Loss: 1.4917, Train: 35.97%, Valid: 35.67%, Test: 35.87%
Epoch: 425, Loss: 1.5051, Train: 34.70%, Valid: 34.17%, Test: 34.67%
Epoch: 450, Loss: 1.4624, Train: 36.27%, Valid: 35.93%, Test: 36.33%
Epoch: 475, Loss: 1.5080, Train: 36.06%, Valid: 35.69%, Test: 35.86%
Epoch: 500, Loss: 1.4859, Train: 33.65%, Valid: 33.25%, Test: 33.63%
Epoch: 525, Loss: 1.5014, Train: 33.38%, Valid: 32.72%, Test: 33.32%
Epoch: 550, Loss: 1.4888, Train: 36.94%, Valid: 36.40%, Test: 36.82%
Epoch: 575, Loss: 1.5263, Train: 31.92%, Valid: 31.65%, Test: 31.89%
Epoch: 600, Loss: 1.7464, Train: 33.65%, Valid: 33.34%, Test: 33.72%
Epoch: 625, Loss: 1.5663, Train: 33.75%, Valid: 33.37%, Test: 33.96%
Epoch: 650, Loss: 1.5388, Train: 36.10%, Valid: 35.84%, Test: 36.01%
Epoch: 675, Loss: 1.4916, Train: 35.70%, Valid: 35.49%, Test: 35.65%
Epoch: 700, Loss: 1.4831, Train: 37.72%, Valid: 36.95%, Test: 37.24%
Epoch: 725, Loss: 1.4362, Train: 38.28%, Valid: 37.76%, Test: 38.47%
Epoch: 750, Loss: 1.4496, Train: 38.36%, Valid: 37.90%, Test: 38.08%
Epoch: 775, Loss: 1.4844, Train: 37.40%, Valid: 37.01%, Test: 37.19%
Epoch: 800, Loss: 2.4756, Train: 22.16%, Valid: 22.18%, Test: 22.74%
Epoch: 825, Loss: 3.6817, Train: 20.72%, Valid: 20.81%, Test: 20.52%
Epoch: 850, Loss: 1.5925, Train: 28.77%, Valid: 28.77%, Test: 28.27%
Epoch: 875, Loss: 1.5062, Train: 35.74%, Valid: 35.56%, Test: 35.71%
Epoch: 900, Loss: 1.4830, Train: 36.63%, Valid: 36.07%, Test: 36.40%
Epoch: 925, Loss: 1.4974, Train: 34.94%, Valid: 34.65%, Test: 34.97%
Epoch: 950, Loss: 1.5396, Train: 30.30%, Valid: 30.17%, Test: 30.56%
Epoch: 975, Loss: 1.4701, Train: 38.11%, Valid: 37.61%, Test: 37.91%
Run 01:
Highest Train: 40.31
Highest Valid: 39.89
  Final Train: 40.25
   Final Test: 40.01
All runs:
Highest Train: 40.31, nan
Highest Valid: 39.89, nan
  Final Train: 40.25, nan
   Final Test: 40.01, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.8090, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.5957, Train: 28.15%, Valid: 28.22%, Test: 28.28%
Epoch: 50, Loss: 1.4706, Train: 35.93%, Valid: 35.75%, Test: 35.86%
Epoch: 75, Loss: 1.4695, Train: 37.85%, Valid: 37.63%, Test: 37.58%
Epoch: 100, Loss: 1.4400, Train: 36.09%, Valid: 35.74%, Test: 36.36%
Epoch: 125, Loss: 1.4184, Train: 39.20%, Valid: 38.73%, Test: 38.95%
Epoch: 150, Loss: 1.5664, Train: 33.39%, Valid: 33.59%, Test: 33.56%
Epoch: 175, Loss: 1.4759, Train: 36.87%, Valid: 36.77%, Test: 37.18%
Epoch: 200, Loss: 1.4626, Train: 36.36%, Valid: 36.12%, Test: 36.23%
Epoch: 225, Loss: 1.4565, Train: 37.93%, Valid: 37.22%, Test: 37.74%
Epoch: 250, Loss: 1.5053, Train: 37.64%, Valid: 37.44%, Test: 37.67%
Epoch: 275, Loss: 1.4624, Train: 38.17%, Valid: 37.85%, Test: 38.28%
Epoch: 300, Loss: 1.4500, Train: 35.68%, Valid: 35.35%, Test: 35.22%
Epoch: 325, Loss: 1.4386, Train: 38.63%, Valid: 38.16%, Test: 38.25%
Epoch: 350, Loss: 1.4459, Train: 37.54%, Valid: 37.21%, Test: 37.80%
Epoch: 375, Loss: 1.4323, Train: 38.61%, Valid: 38.18%, Test: 38.42%
Epoch: 400, Loss: 1.4947, Train: 35.50%, Valid: 35.35%, Test: 35.60%
Epoch: 425, Loss: 1.4354, Train: 37.98%, Valid: 37.36%, Test: 37.66%
Epoch: 450, Loss: 1.4567, Train: 36.28%, Valid: 36.02%, Test: 36.13%
Epoch: 475, Loss: 1.4339, Train: 35.68%, Valid: 35.56%, Test: 35.91%
Epoch: 500, Loss: 1.4266, Train: 38.32%, Valid: 38.03%, Test: 38.53%
Epoch: 525, Loss: 1.4359, Train: 38.37%, Valid: 37.92%, Test: 38.08%
Epoch: 550, Loss: 1.4420, Train: 38.62%, Valid: 38.10%, Test: 38.34%
Epoch: 575, Loss: 1.4288, Train: 38.55%, Valid: 38.09%, Test: 38.43%
Epoch: 600, Loss: 1.4485, Train: 36.31%, Valid: 36.02%, Test: 36.55%
Epoch: 625, Loss: 1.4425, Train: 39.07%, Valid: 38.67%, Test: 38.97%
Epoch: 650, Loss: 1.4838, Train: 37.92%, Valid: 37.43%, Test: 37.91%
Epoch: 675, Loss: 1.4646, Train: 37.22%, Valid: 36.62%, Test: 36.71%
Epoch: 700, Loss: 1.5386, Train: 35.89%, Valid: 35.51%, Test: 35.64%
Epoch: 725, Loss: 1.4662, Train: 39.33%, Valid: 38.60%, Test: 38.91%
Epoch: 750, Loss: 1.4662, Train: 39.27%, Valid: 38.65%, Test: 38.98%
Epoch: 775, Loss: 1.4516, Train: 39.71%, Valid: 38.98%, Test: 39.25%
Epoch: 800, Loss: 1.4294, Train: 39.68%, Valid: 39.04%, Test: 39.16%
Epoch: 825, Loss: 1.4511, Train: 38.97%, Valid: 38.23%, Test: 38.52%
Epoch: 850, Loss: 2.5620, Train: 31.66%, Valid: 31.66%, Test: 31.58%
Epoch: 875, Loss: 1.4890, Train: 31.15%, Valid: 30.89%, Test: 30.98%
Epoch: 900, Loss: 1.4943, Train: 36.12%, Valid: 35.81%, Test: 36.12%
Epoch: 925, Loss: 1.5049, Train: 34.06%, Valid: 33.58%, Test: 34.12%
Epoch: 950, Loss: 1.4909, Train: 34.89%, Valid: 34.68%, Test: 34.94%
Epoch: 975, Loss: 1.4397, Train: 37.95%, Valid: 37.26%, Test: 37.35%
Run 01:
Highest Train: 40.96
Highest Valid: 40.11
  Final Train: 40.96
   Final Test: 40.60
All runs:
Highest Train: 40.96, nan
Highest Valid: 40.11, nan
  Final Train: 40.96, nan
   Final Test: 40.60, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=1, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.8135, Train: 13.44%, Valid: 13.28%, Test: 13.03%
Epoch: 25, Loss: 1.5372, Train: 33.03%, Valid: 33.03%, Test: 33.45%
Epoch: 50, Loss: 1.5254, Train: 30.99%, Valid: 30.66%, Test: 31.04%
Epoch: 75, Loss: 1.4364, Train: 37.98%, Valid: 37.84%, Test: 38.29%
Epoch: 100, Loss: 1.4420, Train: 37.83%, Valid: 37.45%, Test: 37.53%
Epoch: 125, Loss: 1.4053, Train: 39.17%, Valid: 38.92%, Test: 39.32%
Epoch: 150, Loss: 1.3961, Train: 38.00%, Valid: 37.32%, Test: 37.83%
Epoch: 175, Loss: 1.5266, Train: 36.02%, Valid: 35.57%, Test: 35.83%
Epoch: 200, Loss: 1.4231, Train: 39.94%, Valid: 39.43%, Test: 39.77%
Epoch: 225, Loss: 1.4069, Train: 39.40%, Valid: 38.93%, Test: 39.45%
Epoch: 250, Loss: 1.3664, Train: 41.66%, Valid: 40.97%, Test: 41.29%
Epoch: 275, Loss: 1.3674, Train: 42.15%, Valid: 41.38%, Test: 41.73%
Epoch: 300, Loss: 1.3803, Train: 41.24%, Valid: 40.60%, Test: 40.90%
Epoch: 325, Loss: 1.3967, Train: 41.20%, Valid: 40.23%, Test: 40.74%
Epoch: 350, Loss: 1.3958, Train: 40.25%, Valid: 39.53%, Test: 40.06%
Epoch: 375, Loss: 1.4323, Train: 36.00%, Valid: 35.67%, Test: 35.56%
Epoch: 400, Loss: 1.6342, Train: 23.14%, Valid: 23.31%, Test: 23.13%
Epoch: 425, Loss: 1.5050, Train: 36.34%, Valid: 35.93%, Test: 36.28%
Epoch: 450, Loss: 1.4342, Train: 40.18%, Valid: 39.63%, Test: 39.84%
Epoch: 475, Loss: 1.3943, Train: 41.06%, Valid: 40.53%, Test: 40.54%
Epoch: 500, Loss: 1.3877, Train: 37.70%, Valid: 37.53%, Test: 37.47%
Epoch: 525, Loss: 1.4056, Train: 38.62%, Valid: 38.03%, Test: 38.42%
Epoch: 550, Loss: 1.4051, Train: 40.03%, Valid: 39.54%, Test: 39.84%
Epoch: 575, Loss: 1.5226, Train: 33.50%, Valid: 33.25%, Test: 33.75%
Epoch: 600, Loss: 1.4142, Train: 37.92%, Valid: 37.52%, Test: 37.52%
Epoch: 625, Loss: 8.7130, Train: 29.62%, Valid: 29.15%, Test: 29.78%
Epoch: 650, Loss: 4.3473, Train: 13.64%, Valid: 13.29%, Test: 13.25%
Epoch: 675, Loss: 1.8795, Train: 24.91%, Valid: 24.99%, Test: 24.40%
Epoch: 700, Loss: 1.5503, Train: 30.02%, Valid: 30.15%, Test: 30.49%
Epoch: 725, Loss: 1.5976, Train: 30.89%, Valid: 30.60%, Test: 30.91%
Epoch: 750, Loss: 1.4715, Train: 32.61%, Valid: 32.46%, Test: 32.70%
Epoch: 775, Loss: 1.5034, Train: 29.99%, Valid: 29.87%, Test: 30.02%
Epoch: 800, Loss: 1.4945, Train: 37.03%, Valid: 37.09%, Test: 37.22%
Epoch: 825, Loss: 1.4507, Train: 34.92%, Valid: 34.68%, Test: 34.89%
Epoch: 850, Loss: 1.4477, Train: 36.84%, Valid: 36.79%, Test: 37.11%
Epoch: 875, Loss: 1.4521, Train: 37.64%, Valid: 37.35%, Test: 37.81%
Epoch: 900, Loss: 1.4252, Train: 35.37%, Valid: 35.18%, Test: 35.30%
Epoch: 925, Loss: 1.4082, Train: 40.16%, Valid: 39.62%, Test: 39.86%
Epoch: 950, Loss: 1.4969, Train: 39.17%, Valid: 38.84%, Test: 39.27%
Epoch: 975, Loss: 1.4256, Train: 37.95%, Valid: 37.60%, Test: 37.78%
Run 01:
Highest Train: 42.30
Highest Valid: 41.47
  Final Train: 42.30
   Final Test: 41.76
All runs:
Highest Train: 42.30, nan
Highest Valid: 41.47, nan
  Final Train: 42.30, nan
   Final Test: 41.76, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.1, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7062, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.5510, Train: 32.35%, Valid: 31.60%, Test: 32.48%
Epoch: 50, Loss: 1.4979, Train: 34.58%, Valid: 34.31%, Test: 34.44%
Epoch: 75, Loss: 1.5353, Train: 33.92%, Valid: 33.62%, Test: 33.95%
Epoch: 100, Loss: 1.4807, Train: 35.04%, Valid: 34.60%, Test: 34.91%
Epoch: 125, Loss: 1.4962, Train: 34.56%, Valid: 34.44%, Test: 34.84%
Epoch: 150, Loss: 1.8062, Train: 27.62%, Valid: 27.56%, Test: 27.45%
Epoch: 175, Loss: 1.4901, Train: 34.08%, Valid: 33.74%, Test: 34.16%
Epoch: 200, Loss: 1.4672, Train: 36.01%, Valid: 35.23%, Test: 35.86%
Epoch: 225, Loss: 1.5235, Train: 29.69%, Valid: 29.13%, Test: 29.68%
Epoch: 250, Loss: 1.4603, Train: 36.89%, Valid: 36.13%, Test: 36.40%
Epoch: 275, Loss: 1.4602, Train: 36.53%, Valid: 35.67%, Test: 35.97%
Epoch: 300, Loss: 1.4382, Train: 37.32%, Valid: 36.43%, Test: 36.58%
Epoch: 325, Loss: 1.4246, Train: 35.18%, Valid: 34.29%, Test: 34.58%
Epoch: 350, Loss: 1.3866, Train: 38.95%, Valid: 37.85%, Test: 38.07%
Epoch: 375, Loss: 1.4616, Train: 37.90%, Valid: 36.64%, Test: 37.14%
Epoch: 400, Loss: 1.4000, Train: 40.78%, Valid: 39.69%, Test: 39.86%
Epoch: 425, Loss: 1.4362, Train: 38.63%, Valid: 37.60%, Test: 37.80%
Epoch: 450, Loss: 1.3753, Train: 41.21%, Valid: 39.94%, Test: 40.35%
Epoch: 475, Loss: 1.4180, Train: 38.23%, Valid: 36.65%, Test: 37.07%
Epoch: 500, Loss: 1.4122, Train: 39.37%, Valid: 37.89%, Test: 38.08%
Epoch: 525, Loss: 1.4269, Train: 36.64%, Valid: 35.66%, Test: 36.24%
Epoch: 550, Loss: 1.3939, Train: 40.69%, Valid: 39.17%, Test: 39.36%
Epoch: 575, Loss: 1.3955, Train: 39.88%, Valid: 38.54%, Test: 38.90%
Epoch: 600, Loss: 1.4297, Train: 37.96%, Valid: 36.00%, Test: 36.52%
Epoch: 625, Loss: 1.5450, Train: 32.01%, Valid: 31.14%, Test: 31.31%
Epoch: 650, Loss: 1.3834, Train: 40.73%, Valid: 38.99%, Test: 39.18%
Epoch: 675, Loss: 1.3951, Train: 40.35%, Valid: 38.75%, Test: 38.98%
Epoch: 700, Loss: 1.5960, Train: 37.67%, Valid: 35.95%, Test: 36.63%
Epoch: 725, Loss: 1.4057, Train: 40.44%, Valid: 38.36%, Test: 38.61%
Epoch: 750, Loss: 1.3713, Train: 40.10%, Valid: 37.69%, Test: 37.67%
Epoch: 775, Loss: 1.3825, Train: 42.18%, Valid: 39.68%, Test: 40.17%
Epoch: 800, Loss: 1.3599, Train: 43.18%, Valid: 40.27%, Test: 40.56%
Epoch: 825, Loss: 1.3543, Train: 42.14%, Valid: 39.33%, Test: 39.42%
Epoch: 850, Loss: 1.3987, Train: 38.94%, Valid: 36.40%, Test: 37.12%
Epoch: 875, Loss: 1.3650, Train: 43.44%, Valid: 40.50%, Test: 40.94%
Epoch: 900, Loss: 1.3314, Train: 44.03%, Valid: 41.71%, Test: 41.80%
Epoch: 925, Loss: 1.3638, Train: 40.23%, Valid: 37.17%, Test: 37.49%
Epoch: 950, Loss: 1.3783, Train: 41.68%, Valid: 38.29%, Test: 38.87%
Epoch: 975, Loss: 1.3339, Train: 44.12%, Valid: 40.51%, Test: 40.74%
Run 01:
Highest Train: 44.86
Highest Valid: 41.71
  Final Train: 44.03
   Final Test: 41.80
All runs:
Highest Train: 44.86, nan
Highest Valid: 41.71, nan
  Final Train: 44.03, nan
   Final Test: 41.80, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.1, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7179, Train: 17.72%, Valid: 17.86%, Test: 17.42%
Epoch: 25, Loss: 1.5326, Train: 29.89%, Valid: 29.63%, Test: 30.20%
Epoch: 50, Loss: 1.4448, Train: 36.35%, Valid: 35.84%, Test: 36.24%
Epoch: 75, Loss: 1.3803, Train: 39.87%, Valid: 39.43%, Test: 39.80%
Epoch: 100, Loss: 1.3841, Train: 40.48%, Valid: 40.10%, Test: 40.48%
Epoch: 125, Loss: 1.3807, Train: 39.86%, Valid: 39.11%, Test: 39.56%
Epoch: 150, Loss: 1.4684, Train: 33.51%, Valid: 33.24%, Test: 33.42%
Epoch: 175, Loss: 3.0591, Train: 31.14%, Valid: 30.75%, Test: 31.21%
Epoch: 200, Loss: 1.5677, Train: 34.92%, Valid: 34.37%, Test: 34.72%
Epoch: 225, Loss: 1.4568, Train: 36.26%, Valid: 36.23%, Test: 36.50%
Epoch: 250, Loss: 1.4413, Train: 33.44%, Valid: 32.97%, Test: 33.55%
Epoch: 275, Loss: 1.4444, Train: 34.25%, Valid: 33.73%, Test: 34.28%
Epoch: 300, Loss: 1.3643, Train: 39.94%, Valid: 39.37%, Test: 39.41%
Epoch: 325, Loss: 1.3846, Train: 40.16%, Valid: 39.48%, Test: 39.68%
Epoch: 350, Loss: 1.3549, Train: 39.77%, Valid: 39.07%, Test: 39.40%
Epoch: 375, Loss: 1.3614, Train: 41.00%, Valid: 40.17%, Test: 40.47%
Epoch: 400, Loss: 1.5271, Train: 26.05%, Valid: 26.00%, Test: 25.72%
Epoch: 425, Loss: 1.4812, Train: 34.82%, Valid: 34.33%, Test: 34.53%
Epoch: 450, Loss: 1.4305, Train: 38.80%, Valid: 38.07%, Test: 38.24%
Epoch: 475, Loss: 1.4143, Train: 37.66%, Valid: 37.04%, Test: 37.56%
Epoch: 500, Loss: 1.5746, Train: 36.56%, Valid: 36.02%, Test: 36.32%
Epoch: 525, Loss: 1.4168, Train: 38.65%, Valid: 37.91%, Test: 38.44%
Epoch: 550, Loss: 1.4084, Train: 39.94%, Valid: 39.21%, Test: 39.36%
Epoch: 575, Loss: 1.3816, Train: 40.87%, Valid: 39.91%, Test: 40.15%
Epoch: 600, Loss: 1.4649, Train: 29.59%, Valid: 29.02%, Test: 29.69%
Epoch: 625, Loss: 1.3949, Train: 40.05%, Valid: 39.14%, Test: 39.24%
Epoch: 650, Loss: 3.7701, Train: 25.78%, Valid: 25.97%, Test: 26.05%
Epoch: 675, Loss: 3.6436, Train: 18.44%, Valid: 18.35%, Test: 18.12%
Epoch: 700, Loss: 1.5830, Train: 28.73%, Valid: 28.53%, Test: 28.82%
Epoch: 725, Loss: 1.4996, Train: 35.32%, Valid: 34.99%, Test: 35.42%
Epoch: 750, Loss: 1.4173, Train: 39.19%, Valid: 38.67%, Test: 38.94%
Epoch: 775, Loss: 1.4363, Train: 39.82%, Valid: 39.30%, Test: 39.48%
Epoch: 800, Loss: 1.3967, Train: 41.29%, Valid: 40.45%, Test: 40.87%
Epoch: 825, Loss: 1.3846, Train: 32.46%, Valid: 31.86%, Test: 32.17%
Epoch: 850, Loss: 14.5116, Train: 25.10%, Valid: 25.02%, Test: 25.04%
Epoch: 875, Loss: 7.7451, Train: 30.34%, Valid: 29.81%, Test: 30.54%
Epoch: 900, Loss: 4.5602, Train: 31.11%, Valid: 30.58%, Test: 31.24%
Epoch: 925, Loss: 3.7085, Train: 34.07%, Valid: 33.83%, Test: 33.93%
Epoch: 950, Loss: 1.5829, Train: 34.20%, Valid: 33.94%, Test: 34.31%
Epoch: 975, Loss: 1.4817, Train: 37.70%, Valid: 37.08%, Test: 37.46%
Run 01:
Highest Train: 42.95
Highest Valid: 42.02
  Final Train: 42.95
   Final Test: 42.37
All runs:
Highest Train: 42.95, nan
Highest Valid: 42.02, nan
  Final Train: 42.95, nan
   Final Test: 42.37, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.1, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7057, Train: 22.16%, Valid: 22.17%, Test: 22.74%
Epoch: 25, Loss: 1.4980, Train: 30.30%, Valid: 30.55%, Test: 30.68%
Epoch: 50, Loss: 1.4365, Train: 37.61%, Valid: 37.23%, Test: 37.83%
Epoch: 75, Loss: 1.4394, Train: 39.42%, Valid: 39.18%, Test: 39.38%
Epoch: 100, Loss: 1.4340, Train: 37.75%, Valid: 37.63%, Test: 37.57%
Epoch: 125, Loss: 1.9320, Train: 29.12%, Valid: 28.90%, Test: 29.19%
Epoch: 150, Loss: 1.4082, Train: 39.28%, Valid: 38.90%, Test: 39.24%
Epoch: 175, Loss: 1.3736, Train: 40.97%, Valid: 40.28%, Test: 40.62%
Epoch: 200, Loss: 1.3817, Train: 40.80%, Valid: 40.25%, Test: 40.37%
Epoch: 225, Loss: 1.4360, Train: 37.88%, Valid: 37.43%, Test: 37.70%
Epoch: 250, Loss: 1.3589, Train: 41.22%, Valid: 40.60%, Test: 40.79%
Epoch: 275, Loss: 1.4306, Train: 38.41%, Valid: 38.07%, Test: 38.12%
Epoch: 300, Loss: 1.3627, Train: 40.80%, Valid: 39.85%, Test: 40.26%
Epoch: 325, Loss: 1.3578, Train: 41.29%, Valid: 40.43%, Test: 40.59%
Epoch: 350, Loss: 1.3796, Train: 38.75%, Valid: 38.37%, Test: 38.47%
Epoch: 375, Loss: 1.3509, Train: 42.12%, Valid: 41.11%, Test: 41.27%
Epoch: 400, Loss: 1.3243, Train: 42.90%, Valid: 41.66%, Test: 41.93%
Epoch: 425, Loss: 1.3410, Train: 42.13%, Valid: 41.02%, Test: 41.14%
Epoch: 450, Loss: 1.3206, Train: 42.52%, Valid: 41.16%, Test: 41.56%
Epoch: 475, Loss: 1.3760, Train: 40.64%, Valid: 39.96%, Test: 40.16%
Epoch: 500, Loss: 1.3492, Train: 41.73%, Valid: 40.70%, Test: 41.12%
Epoch: 525, Loss: 1.3341, Train: 42.77%, Valid: 41.41%, Test: 41.72%
Epoch: 550, Loss: 1.3344, Train: 42.70%, Valid: 41.45%, Test: 41.87%
Epoch: 575, Loss: 1.3515, Train: 41.87%, Valid: 40.59%, Test: 40.90%
Epoch: 600, Loss: 1.4260, Train: 41.18%, Valid: 39.93%, Test: 40.20%
Epoch: 625, Loss: 4.0516, Train: 26.05%, Valid: 26.01%, Test: 26.20%
Epoch: 650, Loss: 1.7519, Train: 33.37%, Valid: 33.15%, Test: 33.40%
Epoch: 675, Loss: 1.4270, Train: 36.21%, Valid: 36.15%, Test: 36.33%
Epoch: 700, Loss: 1.4016, Train: 39.09%, Valid: 38.46%, Test: 38.90%
Epoch: 725, Loss: 1.3542, Train: 41.73%, Valid: 40.82%, Test: 41.21%
Epoch: 750, Loss: 1.3698, Train: 40.97%, Valid: 40.20%, Test: 40.53%
Epoch: 775, Loss: 1.3827, Train: 39.27%, Valid: 38.55%, Test: 38.60%
Epoch: 800, Loss: 1.3215, Train: 42.44%, Valid: 41.68%, Test: 41.97%
Epoch: 825, Loss: 1.3837, Train: 40.99%, Valid: 40.21%, Test: 40.32%
Epoch: 850, Loss: 1.3284, Train: 42.84%, Valid: 41.62%, Test: 41.88%
Epoch: 875, Loss: 1.4746, Train: 38.65%, Valid: 38.33%, Test: 38.39%
Epoch: 900, Loss: 1.3529, Train: 42.18%, Valid: 41.04%, Test: 41.26%
Epoch: 925, Loss: 1.3813, Train: 40.10%, Valid: 38.94%, Test: 39.05%
Epoch: 950, Loss: 1.3401, Train: 42.36%, Valid: 41.05%, Test: 41.32%
Epoch: 975, Loss: 1.5937, Train: 28.29%, Valid: 27.58%, Test: 27.80%
Run 01:
Highest Train: 44.26
Highest Valid: 43.06
  Final Train: 44.26
   Final Test: 43.27
All runs:
Highest Train: 44.26, nan
Highest Valid: 43.06, nan
  Final Train: 44.26, nan
   Final Test: 43.27, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.1, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7227, Train: 22.16%, Valid: 22.17%, Test: 22.74%
Epoch: 25, Loss: 1.5697, Train: 29.96%, Valid: 29.84%, Test: 29.76%
Epoch: 50, Loss: 1.5140, Train: 27.69%, Valid: 27.67%, Test: 27.27%
Epoch: 75, Loss: 1.4997, Train: 33.50%, Valid: 33.13%, Test: 33.48%
Epoch: 100, Loss: 1.4489, Train: 28.69%, Valid: 28.39%, Test: 28.45%
Epoch: 125, Loss: 1.5720, Train: 29.74%, Valid: 29.25%, Test: 29.60%
Epoch: 150, Loss: 1.5031, Train: 34.81%, Valid: 34.60%, Test: 34.91%
Epoch: 175, Loss: 1.5479, Train: 34.73%, Valid: 34.07%, Test: 34.49%
Epoch: 200, Loss: 1.4558, Train: 36.40%, Valid: 35.94%, Test: 36.08%
Epoch: 225, Loss: 1.4388, Train: 37.75%, Valid: 37.07%, Test: 37.44%
Epoch: 250, Loss: 1.4467, Train: 37.73%, Valid: 36.79%, Test: 37.43%
Epoch: 275, Loss: 1.7816, Train: 22.64%, Valid: 22.47%, Test: 22.96%
Epoch: 300, Loss: 1.5113, Train: 34.61%, Valid: 34.01%, Test: 34.54%
Epoch: 325, Loss: 1.4671, Train: 36.57%, Valid: 35.67%, Test: 36.03%
Epoch: 350, Loss: 1.4559, Train: 37.02%, Valid: 35.81%, Test: 36.41%
Epoch: 375, Loss: 1.4446, Train: 37.75%, Valid: 36.52%, Test: 36.91%
Epoch: 400, Loss: 1.4209, Train: 38.97%, Valid: 37.58%, Test: 37.93%
Epoch: 425, Loss: 1.4364, Train: 38.74%, Valid: 37.77%, Test: 37.85%
Epoch: 450, Loss: 1.5147, Train: 27.63%, Valid: 27.26%, Test: 27.02%
Epoch: 475, Loss: 1.4475, Train: 37.09%, Valid: 36.12%, Test: 36.37%
Epoch: 500, Loss: 1.4251, Train: 39.69%, Valid: 38.40%, Test: 38.63%
Epoch: 525, Loss: 1.4302, Train: 38.81%, Valid: 37.24%, Test: 37.53%
Epoch: 550, Loss: 1.4815, Train: 31.35%, Valid: 30.86%, Test: 31.02%
Epoch: 575, Loss: 1.4207, Train: 36.87%, Valid: 35.88%, Test: 36.02%
Epoch: 600, Loss: 1.4014, Train: 40.57%, Valid: 38.74%, Test: 38.92%
Epoch: 625, Loss: 1.4405, Train: 37.21%, Valid: 35.98%, Test: 36.28%
Epoch: 650, Loss: 1.4177, Train: 38.59%, Valid: 37.02%, Test: 37.13%
Epoch: 675, Loss: 1.6392, Train: 32.67%, Valid: 32.01%, Test: 32.33%
Epoch: 700, Loss: 1.5084, Train: 35.55%, Valid: 34.63%, Test: 34.79%
Epoch: 725, Loss: 1.4266, Train: 38.56%, Valid: 36.76%, Test: 37.18%
Epoch: 750, Loss: 1.5625, Train: 36.23%, Valid: 35.50%, Test: 35.76%
Epoch: 775, Loss: 1.4529, Train: 36.60%, Valid: 34.43%, Test: 35.47%
Epoch: 800, Loss: 1.4219, Train: 38.36%, Valid: 36.35%, Test: 36.72%
Epoch: 825, Loss: 1.4272, Train: 38.14%, Valid: 35.78%, Test: 36.29%
Epoch: 850, Loss: 1.4066, Train: 40.47%, Valid: 38.20%, Test: 38.50%
Epoch: 875, Loss: 1.4227, Train: 39.91%, Valid: 37.90%, Test: 38.34%
Epoch: 900, Loss: 1.4691, Train: 33.85%, Valid: 32.19%, Test: 32.16%
Epoch: 925, Loss: 1.4131, Train: 39.20%, Valid: 37.06%, Test: 37.19%
Epoch: 950, Loss: 1.4367, Train: 34.50%, Valid: 32.60%, Test: 33.19%
Epoch: 975, Loss: 1.6186, Train: 32.73%, Valid: 31.76%, Test: 32.01%
Run 01:
Highest Train: 41.49
Highest Valid: 39.72
  Final Train: 41.08
   Final Test: 39.89
All runs:
Highest Train: 41.49, nan
Highest Valid: 39.72, nan
  Final Train: 41.08, nan
   Final Test: 39.89, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.1, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7226, Train: 12.51%, Valid: 12.59%, Test: 12.43%
Epoch: 25, Loss: 1.5473, Train: 28.45%, Valid: 28.19%, Test: 28.78%
Epoch: 50, Loss: 1.4703, Train: 35.37%, Valid: 34.79%, Test: 35.54%
Epoch: 75, Loss: 1.6933, Train: 29.54%, Valid: 29.36%, Test: 29.46%
Epoch: 100, Loss: 1.4589, Train: 35.71%, Valid: 35.41%, Test: 35.65%
Epoch: 125, Loss: 1.4356, Train: 38.20%, Valid: 38.02%, Test: 38.24%
Epoch: 150, Loss: 1.7306, Train: 19.42%, Valid: 19.21%, Test: 19.50%
Epoch: 175, Loss: 1.4708, Train: 35.81%, Valid: 35.94%, Test: 36.09%
Epoch: 200, Loss: 1.4013, Train: 38.16%, Valid: 37.87%, Test: 38.14%
Epoch: 225, Loss: 1.4150, Train: 36.84%, Valid: 36.63%, Test: 36.60%
Epoch: 250, Loss: 8.4966, Train: 22.29%, Valid: 22.42%, Test: 22.94%
Epoch: 275, Loss: 4.0878, Train: 15.92%, Valid: 15.96%, Test: 15.76%
Epoch: 300, Loss: 2.0402, Train: 34.99%, Valid: 34.73%, Test: 34.93%
Epoch: 325, Loss: 1.7139, Train: 36.19%, Valid: 35.65%, Test: 36.06%
Epoch: 350, Loss: 1.6411, Train: 30.27%, Valid: 29.99%, Test: 30.82%
Epoch: 375, Loss: 2.5608, Train: 32.28%, Valid: 31.57%, Test: 32.41%
Epoch: 400, Loss: 1.5519, Train: 34.37%, Valid: 33.70%, Test: 34.25%
Epoch: 425, Loss: 1.4438, Train: 36.19%, Valid: 35.59%, Test: 36.32%
Epoch: 450, Loss: 1.4424, Train: 39.05%, Valid: 38.59%, Test: 38.74%
Epoch: 475, Loss: 1.4444, Train: 35.26%, Valid: 34.12%, Test: 35.00%
Epoch: 500, Loss: 1.4235, Train: 38.56%, Valid: 37.94%, Test: 38.23%
Epoch: 525, Loss: 1.5122, Train: 36.37%, Valid: 35.91%, Test: 36.45%
Epoch: 550, Loss: 1.4843, Train: 39.10%, Valid: 38.46%, Test: 38.60%
Epoch: 575, Loss: 1.4150, Train: 39.81%, Valid: 39.00%, Test: 39.24%
Epoch: 600, Loss: 1.4823, Train: 35.09%, Valid: 34.57%, Test: 34.92%
Epoch: 625, Loss: 1.4301, Train: 39.06%, Valid: 38.18%, Test: 38.37%
Epoch: 650, Loss: 1.4173, Train: 39.18%, Valid: 38.13%, Test: 38.68%
Epoch: 675, Loss: 1.4222, Train: 39.23%, Valid: 38.42%, Test: 38.67%
Epoch: 700, Loss: 1.4287, Train: 39.12%, Valid: 38.24%, Test: 38.63%
Epoch: 725, Loss: 1.4825, Train: 31.43%, Valid: 31.29%, Test: 31.70%
Epoch: 750, Loss: 1.4140, Train: 38.54%, Valid: 37.77%, Test: 38.20%
Epoch: 775, Loss: 1.4003, Train: 40.46%, Valid: 39.74%, Test: 39.78%
Epoch: 800, Loss: 1.3884, Train: 42.56%, Valid: 41.42%, Test: 41.76%
Epoch: 825, Loss: 1.4452, Train: 39.39%, Valid: 38.15%, Test: 38.71%
Epoch: 850, Loss: 1.4308, Train: 37.25%, Valid: 36.49%, Test: 36.61%
Epoch: 875, Loss: 1.4361, Train: 33.39%, Valid: 32.50%, Test: 33.43%
Epoch: 900, Loss: 1.3898, Train: 40.02%, Valid: 39.27%, Test: 39.47%
Epoch: 925, Loss: 1.3632, Train: 41.03%, Valid: 40.31%, Test: 40.29%
Epoch: 950, Loss: 1.3410, Train: 42.62%, Valid: 41.45%, Test: 41.85%
Epoch: 975, Loss: 1.3594, Train: 39.41%, Valid: 38.35%, Test: 38.39%
Run 01:
Highest Train: 43.17
Highest Valid: 42.10
  Final Train: 43.17
   Final Test: 42.30
All runs:
Highest Train: 43.17, nan
Highest Valid: 42.10, nan
  Final Train: 43.17, nan
   Final Test: 42.30, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.1, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7307, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.4929, Train: 25.82%, Valid: 25.41%, Test: 26.01%
Epoch: 50, Loss: 1.4723, Train: 35.54%, Valid: 35.29%, Test: 35.59%
Epoch: 75, Loss: 1.4295, Train: 33.06%, Valid: 32.58%, Test: 32.95%
Epoch: 100, Loss: 1.3882, Train: 40.10%, Valid: 39.76%, Test: 40.08%
Epoch: 125, Loss: 1.4000, Train: 38.95%, Valid: 38.76%, Test: 39.10%
Epoch: 150, Loss: 1.3635, Train: 40.60%, Valid: 40.12%, Test: 40.24%
Epoch: 175, Loss: 1.3732, Train: 40.59%, Valid: 40.06%, Test: 40.07%
Epoch: 200, Loss: 1.3779, Train: 40.46%, Valid: 40.00%, Test: 40.16%
Epoch: 225, Loss: 1.3714, Train: 40.12%, Valid: 39.42%, Test: 39.57%
Epoch: 250, Loss: 1.3440, Train: 41.89%, Valid: 41.46%, Test: 41.71%
Epoch: 275, Loss: 1.3811, Train: 38.75%, Valid: 37.96%, Test: 37.98%
Epoch: 300, Loss: 1.3441, Train: 42.82%, Valid: 41.99%, Test: 42.18%
Epoch: 325, Loss: 1.3262, Train: 43.16%, Valid: 42.23%, Test: 42.44%
Epoch: 350, Loss: 1.3410, Train: 41.64%, Valid: 40.82%, Test: 40.91%
Epoch: 375, Loss: 1.3413, Train: 42.50%, Valid: 41.36%, Test: 41.58%
Epoch: 400, Loss: 1.3392, Train: 42.32%, Valid: 41.24%, Test: 41.54%
Epoch: 425, Loss: 1.3435, Train: 42.11%, Valid: 41.00%, Test: 41.34%
Epoch: 450, Loss: 1.3292, Train: 41.93%, Valid: 41.11%, Test: 41.49%
Epoch: 475, Loss: 1.4016, Train: 39.84%, Valid: 38.81%, Test: 39.02%
Epoch: 500, Loss: 1.3428, Train: 42.59%, Valid: 41.61%, Test: 41.95%
Epoch: 525, Loss: 1.4911, Train: 38.38%, Valid: 37.88%, Test: 37.96%
Epoch: 550, Loss: 1.3902, Train: 40.31%, Valid: 39.77%, Test: 39.94%
Epoch: 575, Loss: 1.4039, Train: 38.46%, Valid: 37.85%, Test: 37.97%
Epoch: 600, Loss: 1.3550, Train: 43.08%, Valid: 42.06%, Test: 42.51%
Epoch: 625, Loss: 1.3967, Train: 42.12%, Valid: 40.94%, Test: 41.60%
Epoch: 650, Loss: 1.3286, Train: 42.44%, Valid: 41.60%, Test: 41.80%
Epoch: 675, Loss: 1.3328, Train: 42.36%, Valid: 41.31%, Test: 41.42%
Epoch: 700, Loss: 1.3011, Train: 43.11%, Valid: 41.92%, Test: 42.48%
Epoch: 725, Loss: 1.3154, Train: 43.59%, Valid: 42.44%, Test: 42.54%
Epoch: 750, Loss: 1.3493, Train: 40.87%, Valid: 40.23%, Test: 40.15%
Epoch: 775, Loss: 1.3498, Train: 39.16%, Valid: 38.29%, Test: 38.52%
Epoch: 800, Loss: 1.3219, Train: 42.53%, Valid: 41.32%, Test: 41.75%
Epoch: 825, Loss: 1.3055, Train: 44.75%, Valid: 43.60%, Test: 43.60%
Epoch: 850, Loss: 1.3547, Train: 40.25%, Valid: 39.19%, Test: 39.56%
Epoch: 875, Loss: 1.3247, Train: 43.20%, Valid: 41.77%, Test: 41.89%
Epoch: 900, Loss: 1.3169, Train: 42.45%, Valid: 41.32%, Test: 41.24%
Epoch: 925, Loss: 1.3391, Train: 39.02%, Valid: 37.78%, Test: 38.04%
Epoch: 950, Loss: 1.3086, Train: 43.04%, Valid: 41.50%, Test: 42.06%
Epoch: 975, Loss: 1.3328, Train: 40.88%, Valid: 38.70%, Test: 38.90%
Run 01:
Highest Train: 45.00
Highest Valid: 43.91
  Final Train: 44.96
   Final Test: 43.90
All runs:
Highest Train: 45.00, nan
Highest Valid: 43.91, nan
  Final Train: 44.96, nan
   Final Test: 43.90, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.1, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7465, Train: 16.13%, Valid: 15.95%, Test: 16.05%
Epoch: 25, Loss: 1.5354, Train: 26.68%, Valid: 26.18%, Test: 26.81%
Epoch: 50, Loss: 1.5084, Train: 29.85%, Valid: 29.97%, Test: 29.88%
Epoch: 75, Loss: 1.4679, Train: 35.96%, Valid: 35.56%, Test: 35.66%
Epoch: 100, Loss: 1.5062, Train: 35.05%, Valid: 34.67%, Test: 34.94%
Epoch: 125, Loss: 1.4785, Train: 35.58%, Valid: 35.17%, Test: 35.74%
Epoch: 150, Loss: 1.4905, Train: 35.28%, Valid: 34.93%, Test: 35.03%
Epoch: 175, Loss: 1.4839, Train: 35.78%, Valid: 35.24%, Test: 35.77%
Epoch: 200, Loss: 1.4530, Train: 36.82%, Valid: 36.65%, Test: 36.74%
Epoch: 225, Loss: 1.4897, Train: 35.30%, Valid: 34.98%, Test: 35.20%
Epoch: 250, Loss: 1.5050, Train: 37.17%, Valid: 36.54%, Test: 36.78%
Epoch: 275, Loss: 1.5420, Train: 33.12%, Valid: 32.85%, Test: 33.07%
Epoch: 300, Loss: 1.4396, Train: 37.06%, Valid: 36.16%, Test: 36.58%
Epoch: 325, Loss: 1.4388, Train: 37.28%, Valid: 36.35%, Test: 36.75%
Epoch: 350, Loss: 1.4011, Train: 39.07%, Valid: 38.29%, Test: 38.19%
Epoch: 375, Loss: 1.3844, Train: 39.86%, Valid: 38.92%, Test: 39.11%
Epoch: 400, Loss: 1.4471, Train: 36.73%, Valid: 35.69%, Test: 36.13%
Epoch: 425, Loss: 1.7772, Train: 30.48%, Valid: 30.14%, Test: 29.65%
Epoch: 450, Loss: 1.4220, Train: 38.31%, Valid: 36.89%, Test: 37.31%
Epoch: 475, Loss: 1.5090, Train: 36.92%, Valid: 35.90%, Test: 35.98%
Epoch: 500, Loss: 1.4409, Train: 37.54%, Valid: 35.70%, Test: 36.13%
Epoch: 525, Loss: 1.4534, Train: 38.96%, Valid: 37.15%, Test: 37.46%
Epoch: 550, Loss: 1.4338, Train: 39.04%, Valid: 37.34%, Test: 37.62%
Epoch: 575, Loss: 1.4744, Train: 34.99%, Valid: 33.94%, Test: 34.15%
Epoch: 600, Loss: 1.4840, Train: 31.36%, Valid: 30.78%, Test: 30.87%
Epoch: 625, Loss: 1.4377, Train: 37.55%, Valid: 35.85%, Test: 36.01%
Epoch: 650, Loss: 1.3957, Train: 40.76%, Valid: 38.34%, Test: 38.75%
Epoch: 675, Loss: 1.4092, Train: 41.04%, Valid: 38.51%, Test: 38.81%
Epoch: 700, Loss: 1.3962, Train: 39.32%, Valid: 37.75%, Test: 38.25%
Epoch: 725, Loss: 1.3530, Train: 41.85%, Valid: 39.80%, Test: 40.08%
Epoch: 750, Loss: 3.5460, Train: 18.74%, Valid: 18.79%, Test: 18.56%
Epoch: 775, Loss: 1.5860, Train: 21.94%, Valid: 21.87%, Test: 21.74%
Epoch: 800, Loss: 1.4775, Train: 36.29%, Valid: 35.47%, Test: 35.99%
Epoch: 825, Loss: 1.4487, Train: 37.65%, Valid: 36.82%, Test: 37.15%
Epoch: 850, Loss: 1.4353, Train: 39.59%, Valid: 38.93%, Test: 39.29%
Epoch: 875, Loss: 3.3609, Train: 22.14%, Valid: 22.07%, Test: 22.69%
Epoch: 900, Loss: 1.6253, Train: 36.03%, Valid: 35.06%, Test: 35.61%
Epoch: 925, Loss: 1.4678, Train: 36.48%, Valid: 35.44%, Test: 36.00%
Epoch: 950, Loss: 1.4488, Train: 37.35%, Valid: 36.56%, Test: 36.99%
Epoch: 975, Loss: 1.4392, Train: 39.59%, Valid: 38.85%, Test: 38.88%
Run 01:
Highest Train: 43.72
Highest Valid: 41.75
  Final Train: 43.72
   Final Test: 42.15
All runs:
Highest Train: 43.72, nan
Highest Valid: 41.75, nan
  Final Train: 43.72, nan
   Final Test: 42.15, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.1, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7358, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.5883, Train: 30.63%, Valid: 30.48%, Test: 30.68%
Epoch: 50, Loss: 1.4723, Train: 36.32%, Valid: 36.07%, Test: 36.36%
Epoch: 75, Loss: 1.5319, Train: 31.24%, Valid: 30.88%, Test: 31.11%
Epoch: 100, Loss: 1.4262, Train: 38.93%, Valid: 38.81%, Test: 39.04%
Epoch: 125, Loss: 1.3834, Train: 40.01%, Valid: 39.39%, Test: 39.66%
Epoch: 150, Loss: 1.3848, Train: 40.00%, Valid: 39.41%, Test: 39.81%
Epoch: 175, Loss: 1.3695, Train: 39.22%, Valid: 38.73%, Test: 38.72%
Epoch: 200, Loss: 1.6846, Train: 18.22%, Valid: 18.38%, Test: 17.87%
Epoch: 225, Loss: 1.4545, Train: 38.25%, Valid: 37.92%, Test: 38.08%
Epoch: 250, Loss: 1.5165, Train: 35.62%, Valid: 35.28%, Test: 35.24%
Epoch: 275, Loss: 1.4069, Train: 38.40%, Valid: 37.97%, Test: 38.13%
Epoch: 300, Loss: 2.4890, Train: 27.66%, Valid: 27.84%, Test: 27.54%
Epoch: 325, Loss: 1.5791, Train: 31.81%, Valid: 31.66%, Test: 32.04%
Epoch: 350, Loss: 1.4616, Train: 36.78%, Valid: 36.17%, Test: 36.67%
Epoch: 375, Loss: 1.4524, Train: 37.64%, Valid: 36.85%, Test: 37.33%
Epoch: 400, Loss: 1.4426, Train: 38.07%, Valid: 37.24%, Test: 37.79%
Epoch: 425, Loss: 1.4121, Train: 39.55%, Valid: 38.87%, Test: 39.08%
Epoch: 450, Loss: 1.4149, Train: 39.09%, Valid: 38.28%, Test: 38.53%
Epoch: 475, Loss: 1.3922, Train: 40.32%, Valid: 39.55%, Test: 39.71%
Epoch: 500, Loss: 1.4259, Train: 37.58%, Valid: 36.64%, Test: 37.30%
Epoch: 525, Loss: 1.3974, Train: 40.28%, Valid: 39.13%, Test: 39.55%
Epoch: 550, Loss: 1.5303, Train: 36.53%, Valid: 36.13%, Test: 36.36%
Epoch: 575, Loss: 1.4372, Train: 38.67%, Valid: 37.79%, Test: 38.14%
Epoch: 600, Loss: 1.4214, Train: 39.49%, Valid: 38.26%, Test: 38.87%
Epoch: 625, Loss: 1.3988, Train: 40.51%, Valid: 39.45%, Test: 39.73%
Epoch: 650, Loss: 1.3856, Train: 40.97%, Valid: 40.08%, Test: 40.19%
Epoch: 675, Loss: 1.9163, Train: 12.84%, Valid: 12.94%, Test: 12.77%
Epoch: 700, Loss: 1.4726, Train: 37.17%, Valid: 36.83%, Test: 37.41%
Epoch: 725, Loss: 1.4123, Train: 38.63%, Valid: 38.09%, Test: 38.54%
Epoch: 750, Loss: 1.4133, Train: 40.01%, Valid: 39.28%, Test: 39.43%
Epoch: 775, Loss: 1.4474, Train: 34.95%, Valid: 34.56%, Test: 34.70%
Epoch: 800, Loss: 1.3648, Train: 39.92%, Valid: 39.40%, Test: 39.46%
Epoch: 825, Loss: 1.5229, Train: 39.55%, Valid: 39.09%, Test: 39.27%
Epoch: 850, Loss: 1.3860, Train: 40.33%, Valid: 39.62%, Test: 39.95%
Epoch: 875, Loss: 1.3531, Train: 41.51%, Valid: 40.69%, Test: 40.80%
Epoch: 900, Loss: 7.2603, Train: 28.76%, Valid: 28.54%, Test: 28.86%
Epoch: 925, Loss: 3.6145, Train: 34.17%, Valid: 33.65%, Test: 34.04%
Epoch: 950, Loss: 1.7795, Train: 35.26%, Valid: 34.68%, Test: 35.46%
Epoch: 975, Loss: 1.5258, Train: 34.94%, Valid: 34.57%, Test: 34.68%
Run 01:
Highest Train: 42.27
Highest Valid: 41.21
  Final Train: 42.27
   Final Test: 41.51
All runs:
Highest Train: 42.27, nan
Highest Valid: 41.21, nan
  Final Train: 42.27, nan
   Final Test: 41.51, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.1, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7375, Train: 12.51%, Valid: 12.59%, Test: 12.43%
Epoch: 25, Loss: 1.5734, Train: 23.18%, Valid: 22.94%, Test: 23.20%
Epoch: 50, Loss: 1.4393, Train: 34.25%, Valid: 33.85%, Test: 33.86%
Epoch: 75, Loss: 1.4785, Train: 35.59%, Valid: 35.66%, Test: 36.16%
Epoch: 100, Loss: 1.9819, Train: 24.47%, Valid: 24.30%, Test: 24.86%
Epoch: 125, Loss: 1.5536, Train: 32.34%, Valid: 32.05%, Test: 32.31%
Epoch: 150, Loss: 1.4531, Train: 35.80%, Valid: 35.43%, Test: 36.11%
Epoch: 175, Loss: 1.5546, Train: 25.94%, Valid: 26.02%, Test: 26.33%
Epoch: 200, Loss: 1.4592, Train: 37.02%, Valid: 36.57%, Test: 37.00%
Epoch: 225, Loss: 1.4292, Train: 38.21%, Valid: 37.49%, Test: 38.01%
Epoch: 250, Loss: 1.4090, Train: 38.54%, Valid: 37.80%, Test: 38.19%
Epoch: 275, Loss: 1.4241, Train: 38.37%, Valid: 37.41%, Test: 38.10%
Epoch: 300, Loss: 1.3875, Train: 40.03%, Valid: 39.19%, Test: 39.55%
Epoch: 325, Loss: 1.3782, Train: 39.72%, Valid: 39.08%, Test: 39.19%
Epoch: 350, Loss: 1.4029, Train: 38.47%, Valid: 37.86%, Test: 38.11%
Epoch: 375, Loss: 1.3826, Train: 40.02%, Valid: 38.99%, Test: 39.32%
Epoch: 400, Loss: 1.4551, Train: 36.85%, Valid: 36.39%, Test: 36.48%
Epoch: 425, Loss: 1.3476, Train: 40.35%, Valid: 39.44%, Test: 39.82%
Epoch: 450, Loss: 1.3311, Train: 42.49%, Valid: 41.50%, Test: 41.52%
Epoch: 475, Loss: 3.0910, Train: 25.95%, Valid: 25.82%, Test: 26.65%
Epoch: 500, Loss: 1.6225, Train: 31.96%, Valid: 31.75%, Test: 31.83%
Epoch: 525, Loss: 1.4526, Train: 37.32%, Valid: 37.34%, Test: 37.56%
Epoch: 550, Loss: 1.4636, Train: 36.12%, Valid: 35.56%, Test: 35.86%
Epoch: 575, Loss: 1.4289, Train: 38.38%, Valid: 37.74%, Test: 38.13%
Epoch: 600, Loss: 1.4114, Train: 38.84%, Valid: 38.27%, Test: 38.64%
Epoch: 625, Loss: 1.3996, Train: 39.72%, Valid: 38.80%, Test: 39.21%
Epoch: 650, Loss: 1.3898, Train: 39.99%, Valid: 38.88%, Test: 39.44%
Epoch: 675, Loss: 1.3652, Train: 38.54%, Valid: 37.73%, Test: 37.52%
Epoch: 700, Loss: 1.3970, Train: 40.22%, Valid: 39.27%, Test: 39.71%
Epoch: 725, Loss: 1.3715, Train: 40.80%, Valid: 39.77%, Test: 40.11%
Epoch: 750, Loss: 1.3711, Train: 40.93%, Valid: 40.15%, Test: 40.33%
Epoch: 775, Loss: 1.3914, Train: 41.23%, Valid: 40.07%, Test: 40.22%
Epoch: 800, Loss: 1.3742, Train: 40.67%, Valid: 39.41%, Test: 39.64%
Epoch: 825, Loss: 1.3385, Train: 42.85%, Valid: 41.17%, Test: 41.61%
Epoch: 850, Loss: 1.3254, Train: 43.52%, Valid: 42.17%, Test: 42.11%
Epoch: 875, Loss: 1.3287, Train: 42.93%, Valid: 41.71%, Test: 41.69%
Epoch: 900, Loss: 1.3436, Train: 42.76%, Valid: 41.42%, Test: 41.38%
Epoch: 925, Loss: 1.3186, Train: 43.06%, Valid: 41.44%, Test: 41.78%
Epoch: 950, Loss: 1.3578, Train: 38.91%, Valid: 37.56%, Test: 37.78%
Epoch: 975, Loss: 1.3032, Train: 44.34%, Valid: 42.64%, Test: 42.75%
Run 01:
Highest Train: 44.91
Highest Valid: 43.23
  Final Train: 44.91
   Final Test: 43.24
All runs:
Highest Train: 44.91, nan
Highest Valid: 43.23, nan
  Final Train: 44.91, nan
   Final Test: 43.24, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.2, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7513, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.5723, Train: 27.99%, Valid: 27.85%, Test: 28.24%
Epoch: 50, Loss: 1.4818, Train: 35.72%, Valid: 35.07%, Test: 35.75%
Epoch: 75, Loss: 1.5181, Train: 32.44%, Valid: 32.27%, Test: 32.36%
Epoch: 100, Loss: 1.6413, Train: 34.96%, Valid: 34.66%, Test: 34.78%
Epoch: 125, Loss: 1.4870, Train: 36.28%, Valid: 35.72%, Test: 36.05%
Epoch: 150, Loss: 1.5268, Train: 35.27%, Valid: 34.89%, Test: 35.26%
Epoch: 175, Loss: 1.4753, Train: 35.96%, Valid: 35.23%, Test: 35.70%
Epoch: 200, Loss: 2.2380, Train: 25.93%, Valid: 25.94%, Test: 26.37%
Epoch: 225, Loss: 1.5056, Train: 35.25%, Valid: 34.95%, Test: 35.13%
Epoch: 250, Loss: 1.4664, Train: 36.59%, Valid: 35.99%, Test: 36.30%
Epoch: 275, Loss: 1.4649, Train: 36.68%, Valid: 35.98%, Test: 36.37%
Epoch: 300, Loss: 1.4505, Train: 37.81%, Valid: 37.08%, Test: 37.56%
Epoch: 325, Loss: 1.4369, Train: 39.55%, Valid: 38.67%, Test: 39.15%
Epoch: 350, Loss: 1.4484, Train: 39.37%, Valid: 38.76%, Test: 38.80%
Epoch: 375, Loss: 1.4227, Train: 39.11%, Valid: 38.19%, Test: 38.34%
Epoch: 400, Loss: 1.4524, Train: 39.09%, Valid: 38.13%, Test: 38.40%
Epoch: 425, Loss: 1.4105, Train: 39.26%, Valid: 38.31%, Test: 38.42%
Epoch: 450, Loss: 1.3999, Train: 39.55%, Valid: 38.53%, Test: 38.65%
Epoch: 475, Loss: 1.4492, Train: 34.85%, Valid: 34.39%, Test: 34.36%
Epoch: 500, Loss: 1.4322, Train: 39.51%, Valid: 38.65%, Test: 38.69%
Epoch: 525, Loss: 1.4402, Train: 37.54%, Valid: 36.27%, Test: 37.16%
Epoch: 550, Loss: 1.4112, Train: 40.04%, Valid: 38.97%, Test: 39.14%
Epoch: 575, Loss: 1.4039, Train: 40.49%, Valid: 39.44%, Test: 39.45%
Epoch: 600, Loss: 1.4430, Train: 39.15%, Valid: 38.50%, Test: 38.52%
Epoch: 625, Loss: 1.4018, Train: 40.75%, Valid: 39.21%, Test: 39.67%
Epoch: 650, Loss: 1.3927, Train: 39.90%, Valid: 38.91%, Test: 38.98%
Epoch: 675, Loss: 1.3661, Train: 42.13%, Valid: 41.07%, Test: 41.03%
Epoch: 700, Loss: 1.3953, Train: 40.88%, Valid: 39.57%, Test: 39.73%
Epoch: 725, Loss: 1.4280, Train: 39.75%, Valid: 38.29%, Test: 38.78%
Epoch: 750, Loss: 1.3842, Train: 42.48%, Valid: 41.10%, Test: 41.12%
Epoch: 775, Loss: 1.3714, Train: 42.02%, Valid: 40.72%, Test: 40.48%
Epoch: 800, Loss: 1.3615, Train: 40.33%, Valid: 38.81%, Test: 38.91%
Epoch: 825, Loss: 1.4929, Train: 30.85%, Valid: 29.81%, Test: 29.38%
Epoch: 850, Loss: 1.3821, Train: 39.43%, Valid: 37.34%, Test: 37.81%
Epoch: 875, Loss: 1.3889, Train: 40.06%, Valid: 37.99%, Test: 38.34%
Epoch: 900, Loss: 1.3795, Train: 42.52%, Valid: 40.56%, Test: 40.74%
Epoch: 925, Loss: 1.3444, Train: 42.48%, Valid: 39.97%, Test: 39.97%
Epoch: 950, Loss: 1.3600, Train: 43.39%, Valid: 40.80%, Test: 41.11%
Epoch: 975, Loss: 1.3468, Train: 43.76%, Valid: 41.24%, Test: 41.53%
Run 01:
Highest Train: 44.24
Highest Valid: 41.90
  Final Train: 44.07
   Final Test: 41.89
All runs:
Highest Train: 44.24, nan
Highest Valid: 41.90, nan
  Final Train: 44.07, nan
   Final Test: 41.89, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.2, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7350, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.6107, Train: 29.20%, Valid: 28.99%, Test: 29.29%
Epoch: 50, Loss: 1.5045, Train: 34.25%, Valid: 34.00%, Test: 34.47%
Epoch: 75, Loss: 1.4702, Train: 22.32%, Valid: 22.22%, Test: 22.50%
Epoch: 100, Loss: 1.4384, Train: 36.35%, Valid: 35.88%, Test: 36.40%
Epoch: 125, Loss: 4.0174, Train: 22.90%, Valid: 23.04%, Test: 23.01%
Epoch: 150, Loss: 2.9147, Train: 33.22%, Valid: 32.94%, Test: 33.37%
Epoch: 175, Loss: 1.8299, Train: 32.10%, Valid: 31.65%, Test: 32.51%
Epoch: 200, Loss: 1.5489, Train: 38.09%, Valid: 37.69%, Test: 37.97%
Epoch: 225, Loss: 1.8168, Train: 24.27%, Valid: 24.36%, Test: 24.04%
Epoch: 250, Loss: 1.4959, Train: 31.39%, Valid: 31.22%, Test: 30.96%
Epoch: 275, Loss: 1.4355, Train: 37.07%, Valid: 36.58%, Test: 37.06%
Epoch: 300, Loss: 1.4173, Train: 38.93%, Valid: 38.84%, Test: 38.75%
Epoch: 325, Loss: 1.5550, Train: 33.59%, Valid: 33.27%, Test: 33.61%
Epoch: 350, Loss: 1.4550, Train: 38.64%, Valid: 38.17%, Test: 38.37%
Epoch: 375, Loss: 1.4270, Train: 39.28%, Valid: 38.49%, Test: 38.75%
Epoch: 400, Loss: 1.4121, Train: 39.31%, Valid: 38.86%, Test: 38.79%
Epoch: 425, Loss: 1.4324, Train: 37.83%, Valid: 37.06%, Test: 37.26%
Epoch: 450, Loss: 1.4007, Train: 40.02%, Valid: 39.21%, Test: 39.28%
Epoch: 475, Loss: 1.4921, Train: 38.86%, Valid: 38.44%, Test: 38.50%
Epoch: 500, Loss: 1.3860, Train: 40.12%, Valid: 39.08%, Test: 39.41%
Epoch: 525, Loss: 6.3400, Train: 22.28%, Valid: 22.32%, Test: 22.24%
Epoch: 550, Loss: 1.7416, Train: 23.99%, Valid: 23.77%, Test: 24.54%
Epoch: 575, Loss: 1.5763, Train: 36.25%, Valid: 35.76%, Test: 36.10%
Epoch: 600, Loss: 1.4500, Train: 38.33%, Valid: 37.90%, Test: 38.04%
Epoch: 625, Loss: 1.4570, Train: 38.20%, Valid: 37.83%, Test: 37.97%
Epoch: 650, Loss: 1.3854, Train: 40.53%, Valid: 39.83%, Test: 40.17%
Epoch: 675, Loss: 1.3992, Train: 40.27%, Valid: 39.65%, Test: 39.88%
Epoch: 700, Loss: 1.4264, Train: 38.86%, Valid: 38.43%, Test: 38.63%
Epoch: 725, Loss: 1.3907, Train: 41.25%, Valid: 40.55%, Test: 40.55%
Epoch: 750, Loss: 1.3851, Train: 38.35%, Valid: 38.00%, Test: 38.16%
Epoch: 775, Loss: 1.3945, Train: 40.42%, Valid: 39.83%, Test: 40.09%
Epoch: 800, Loss: 1.3472, Train: 42.27%, Valid: 41.65%, Test: 41.68%
Epoch: 825, Loss: 1.5232, Train: 35.55%, Valid: 35.29%, Test: 34.99%
Epoch: 850, Loss: 1.3668, Train: 40.85%, Valid: 40.02%, Test: 40.16%
Epoch: 875, Loss: 1.3988, Train: 41.10%, Valid: 40.23%, Test: 40.26%
Epoch: 900, Loss: 1.3650, Train: 41.99%, Valid: 41.01%, Test: 41.25%
Epoch: 925, Loss: 1.3421, Train: 42.67%, Valid: 41.81%, Test: 41.95%
Epoch: 950, Loss: 1.4194, Train: 39.18%, Valid: 38.45%, Test: 38.57%
Epoch: 975, Loss: 1.3855, Train: 39.17%, Valid: 38.35%, Test: 38.24%
Run 01:
Highest Train: 42.88
Highest Valid: 41.93
  Final Train: 42.88
   Final Test: 42.22
All runs:
Highest Train: 42.88, nan
Highest Valid: 41.93, nan
  Final Train: 42.88, nan
   Final Test: 42.22, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.2, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7369, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.7457, Train: 30.12%, Valid: 29.53%, Test: 30.23%
Epoch: 50, Loss: 1.4502, Train: 30.17%, Valid: 29.99%, Test: 30.52%
Epoch: 75, Loss: 1.4476, Train: 37.42%, Valid: 36.91%, Test: 37.36%
Epoch: 100, Loss: 1.4132, Train: 36.48%, Valid: 35.83%, Test: 36.64%
Epoch: 125, Loss: 1.4748, Train: 31.19%, Valid: 30.79%, Test: 31.24%
Epoch: 150, Loss: 1.4428, Train: 37.46%, Valid: 37.08%, Test: 37.52%
Epoch: 175, Loss: 1.4173, Train: 37.12%, Valid: 36.74%, Test: 36.99%
Epoch: 200, Loss: 1.3938, Train: 39.96%, Valid: 39.50%, Test: 39.72%
Epoch: 225, Loss: 1.4968, Train: 34.48%, Valid: 34.34%, Test: 34.38%
Epoch: 250, Loss: 1.4343, Train: 38.82%, Valid: 38.28%, Test: 38.49%
Epoch: 275, Loss: 1.6406, Train: 26.06%, Valid: 26.18%, Test: 25.88%
Epoch: 300, Loss: 1.4869, Train: 35.77%, Valid: 35.58%, Test: 35.74%
Epoch: 325, Loss: 1.4122, Train: 39.57%, Valid: 39.31%, Test: 39.48%
Epoch: 350, Loss: 1.4100, Train: 39.26%, Valid: 38.59%, Test: 38.76%
Epoch: 375, Loss: 1.3582, Train: 42.21%, Valid: 41.50%, Test: 41.54%
Epoch: 400, Loss: 1.3357, Train: 42.56%, Valid: 41.80%, Test: 41.96%
Epoch: 425, Loss: 2.5318, Train: 27.49%, Valid: 27.34%, Test: 28.00%
Epoch: 450, Loss: 1.5431, Train: 33.31%, Valid: 33.39%, Test: 33.64%
Epoch: 475, Loss: 1.4310, Train: 38.74%, Valid: 38.34%, Test: 38.84%
Epoch: 500, Loss: 1.3733, Train: 41.18%, Valid: 40.63%, Test: 40.91%
Epoch: 525, Loss: 1.3643, Train: 42.14%, Valid: 41.48%, Test: 41.84%
Epoch: 550, Loss: 1.3849, Train: 40.43%, Valid: 39.98%, Test: 40.25%
Epoch: 575, Loss: 1.5947, Train: 27.37%, Valid: 27.25%, Test: 27.05%
Epoch: 600, Loss: 1.4649, Train: 36.96%, Valid: 36.56%, Test: 36.84%
Epoch: 625, Loss: 1.4048, Train: 41.21%, Valid: 40.70%, Test: 40.87%
Epoch: 650, Loss: 1.3594, Train: 41.57%, Valid: 40.73%, Test: 41.32%
Epoch: 675, Loss: 1.3922, Train: 41.18%, Valid: 40.37%, Test: 40.70%
Epoch: 700, Loss: 1.3595, Train: 41.11%, Valid: 40.81%, Test: 40.90%
Epoch: 725, Loss: 1.3486, Train: 42.50%, Valid: 41.74%, Test: 42.09%
Epoch: 750, Loss: 1.3444, Train: 41.03%, Valid: 39.93%, Test: 40.38%
Epoch: 775, Loss: 1.4095, Train: 39.13%, Valid: 38.93%, Test: 39.02%
Epoch: 800, Loss: 1.4226, Train: 39.93%, Valid: 39.46%, Test: 39.56%
Epoch: 825, Loss: 1.3613, Train: 41.63%, Valid: 40.99%, Test: 41.14%
Epoch: 850, Loss: 1.3524, Train: 41.06%, Valid: 40.25%, Test: 40.81%
Epoch: 875, Loss: 1.3893, Train: 33.64%, Valid: 33.24%, Test: 33.60%
Epoch: 900, Loss: 1.3665, Train: 41.91%, Valid: 41.18%, Test: 41.45%
Epoch: 925, Loss: 1.3361, Train: 43.05%, Valid: 42.07%, Test: 42.52%
Epoch: 950, Loss: 1.3801, Train: 39.63%, Valid: 38.97%, Test: 39.13%
Epoch: 975, Loss: 1.3613, Train: 42.16%, Valid: 41.39%, Test: 41.78%
Run 01:
Highest Train: 43.59
Highest Valid: 42.95
  Final Train: 43.59
   Final Test: 42.92
All runs:
Highest Train: 43.59, nan
Highest Valid: 42.95, nan
  Final Train: 43.59, nan
   Final Test: 42.92, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.2, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7408, Train: 22.16%, Valid: 22.17%, Test: 22.74%
Epoch: 25, Loss: 1.5530, Train: 28.11%, Valid: 28.07%, Test: 28.11%
Epoch: 50, Loss: 1.5003, Train: 34.23%, Valid: 33.89%, Test: 34.40%
Epoch: 75, Loss: 1.4922, Train: 35.03%, Valid: 34.64%, Test: 34.97%
Epoch: 100, Loss: 1.4590, Train: 33.56%, Valid: 33.23%, Test: 33.62%
Epoch: 125, Loss: 1.4773, Train: 36.84%, Valid: 36.11%, Test: 36.50%
Epoch: 150, Loss: 1.4694, Train: 36.92%, Valid: 36.32%, Test: 37.01%
Epoch: 175, Loss: 1.7692, Train: 26.51%, Valid: 26.63%, Test: 26.13%
Epoch: 200, Loss: 1.4719, Train: 36.75%, Valid: 36.14%, Test: 36.36%
Epoch: 225, Loss: 1.6368, Train: 26.87%, Valid: 26.94%, Test: 26.39%
Epoch: 250, Loss: 1.4731, Train: 37.13%, Valid: 36.33%, Test: 36.63%
Epoch: 275, Loss: 1.4633, Train: 37.31%, Valid: 36.45%, Test: 36.90%
Epoch: 300, Loss: 1.4998, Train: 36.01%, Valid: 35.56%, Test: 35.86%
Epoch: 325, Loss: 1.5267, Train: 36.70%, Valid: 36.03%, Test: 36.28%
Epoch: 350, Loss: 1.4539, Train: 37.05%, Valid: 35.92%, Test: 36.68%
Epoch: 375, Loss: 1.4449, Train: 37.64%, Valid: 37.16%, Test: 37.24%
Epoch: 400, Loss: 1.7455, Train: 36.21%, Valid: 35.69%, Test: 35.99%
Epoch: 425, Loss: 1.5008, Train: 36.37%, Valid: 35.56%, Test: 35.92%
Epoch: 450, Loss: 1.4521, Train: 37.52%, Valid: 36.60%, Test: 36.77%
Epoch: 475, Loss: 1.4293, Train: 36.89%, Valid: 35.95%, Test: 36.22%
Epoch: 500, Loss: 1.4574, Train: 37.11%, Valid: 36.15%, Test: 36.45%
Epoch: 525, Loss: 1.4338, Train: 38.90%, Valid: 37.60%, Test: 37.88%
Epoch: 550, Loss: 1.5277, Train: 33.92%, Valid: 33.27%, Test: 33.57%
Epoch: 575, Loss: 1.4604, Train: 36.81%, Valid: 35.71%, Test: 36.09%
Epoch: 600, Loss: 1.4473, Train: 37.78%, Valid: 36.40%, Test: 36.59%
Epoch: 625, Loss: 1.4401, Train: 38.19%, Valid: 36.69%, Test: 36.88%
Epoch: 650, Loss: 1.4368, Train: 38.72%, Valid: 37.06%, Test: 37.24%
Epoch: 675, Loss: 1.4336, Train: 38.74%, Valid: 37.14%, Test: 37.27%
Epoch: 700, Loss: 1.4297, Train: 38.20%, Valid: 36.69%, Test: 36.76%
Epoch: 725, Loss: 1.4248, Train: 39.08%, Valid: 37.46%, Test: 37.78%
Epoch: 750, Loss: 1.4233, Train: 39.95%, Valid: 38.00%, Test: 38.22%
Epoch: 775, Loss: 1.4134, Train: 39.89%, Valid: 37.99%, Test: 37.90%
Epoch: 800, Loss: 1.4190, Train: 39.97%, Valid: 38.16%, Test: 38.45%
Epoch: 825, Loss: 1.3911, Train: 41.52%, Valid: 39.69%, Test: 39.87%
Epoch: 850, Loss: 1.3987, Train: 41.14%, Valid: 39.36%, Test: 39.73%
Epoch: 875, Loss: 1.3772, Train: 41.74%, Valid: 40.26%, Test: 40.45%
Epoch: 900, Loss: 1.4264, Train: 38.96%, Valid: 37.59%, Test: 37.62%
Epoch: 925, Loss: 1.5082, Train: 32.84%, Valid: 31.68%, Test: 31.95%
Epoch: 950, Loss: 1.5890, Train: 34.72%, Valid: 34.13%, Test: 34.18%
Epoch: 975, Loss: 1.4295, Train: 39.97%, Valid: 38.40%, Test: 38.51%
Run 01:
Highest Train: 42.42
Highest Valid: 40.73
  Final Train: 42.42
   Final Test: 41.04
All runs:
Highest Train: 42.42, nan
Highest Valid: 40.73, nan
  Final Train: 42.42, nan
   Final Test: 41.04, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.2, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7175, Train: 16.63%, Valid: 16.42%, Test: 16.57%
Epoch: 25, Loss: 1.5328, Train: 32.16%, Valid: 31.57%, Test: 32.25%
Epoch: 50, Loss: 1.4936, Train: 33.18%, Valid: 33.03%, Test: 33.42%
Epoch: 75, Loss: 1.4180, Train: 37.99%, Valid: 37.58%, Test: 37.83%
Epoch: 100, Loss: 1.4569, Train: 30.98%, Valid: 30.92%, Test: 31.19%
Epoch: 125, Loss: 1.5834, Train: 20.80%, Valid: 20.80%, Test: 20.54%
Epoch: 150, Loss: 1.4490, Train: 37.58%, Valid: 37.22%, Test: 37.89%
Epoch: 175, Loss: 1.4274, Train: 38.14%, Valid: 37.89%, Test: 38.07%
Epoch: 200, Loss: 1.3936, Train: 41.38%, Valid: 40.69%, Test: 40.82%
Epoch: 225, Loss: 1.3673, Train: 41.33%, Valid: 40.51%, Test: 40.84%
Epoch: 250, Loss: 1.3664, Train: 41.54%, Valid: 41.01%, Test: 41.32%
Epoch: 275, Loss: 1.3274, Train: 42.59%, Valid: 41.85%, Test: 41.85%
Epoch: 300, Loss: 1.4239, Train: 38.53%, Valid: 37.70%, Test: 38.16%
Epoch: 325, Loss: 1.4002, Train: 40.25%, Valid: 39.41%, Test: 39.93%
Epoch: 350, Loss: 1.5577, Train: 17.67%, Valid: 17.88%, Test: 17.82%
Epoch: 375, Loss: 1.4737, Train: 36.73%, Valid: 36.19%, Test: 36.64%
Epoch: 400, Loss: 1.4105, Train: 40.13%, Valid: 39.44%, Test: 39.72%
Epoch: 425, Loss: 1.3636, Train: 41.62%, Valid: 41.11%, Test: 41.32%
Epoch: 450, Loss: 1.4211, Train: 38.47%, Valid: 37.89%, Test: 38.27%
Epoch: 475, Loss: 1.3775, Train: 41.06%, Valid: 40.27%, Test: 40.59%
Epoch: 500, Loss: 1.4263, Train: 38.67%, Valid: 38.03%, Test: 38.33%
Epoch: 525, Loss: 2.1425, Train: 36.18%, Valid: 35.72%, Test: 35.85%
Epoch: 550, Loss: 1.4142, Train: 39.75%, Valid: 38.94%, Test: 39.28%
Epoch: 575, Loss: 1.8054, Train: 31.50%, Valid: 31.32%, Test: 31.15%
Epoch: 600, Loss: 1.6348, Train: 26.92%, Valid: 26.55%, Test: 26.79%
Epoch: 625, Loss: 1.4103, Train: 39.24%, Valid: 38.59%, Test: 38.92%
Epoch: 650, Loss: 1.3819, Train: 41.50%, Valid: 40.62%, Test: 40.68%
Epoch: 675, Loss: 1.4429, Train: 38.79%, Valid: 37.99%, Test: 38.26%
Epoch: 700, Loss: 1.3981, Train: 38.15%, Valid: 37.17%, Test: 37.27%
Epoch: 725, Loss: 1.3618, Train: 42.68%, Valid: 41.63%, Test: 41.94%
Epoch: 750, Loss: 1.3664, Train: 42.70%, Valid: 41.63%, Test: 41.68%
Epoch: 775, Loss: 1.4032, Train: 38.42%, Valid: 37.18%, Test: 37.72%
Epoch: 800, Loss: 1.3935, Train: 41.34%, Valid: 40.48%, Test: 40.61%
Epoch: 825, Loss: 1.4077, Train: 38.69%, Valid: 37.32%, Test: 37.69%
Epoch: 850, Loss: 1.4822, Train: 25.43%, Valid: 25.26%, Test: 25.63%
Epoch: 875, Loss: 1.4429, Train: 40.21%, Valid: 39.05%, Test: 39.50%
Epoch: 900, Loss: 1.3801, Train: 40.88%, Valid: 39.45%, Test: 40.01%
Epoch: 925, Loss: 1.3761, Train: 32.26%, Valid: 31.59%, Test: 31.10%
Epoch: 950, Loss: 1.3730, Train: 41.93%, Valid: 40.04%, Test: 40.76%
Epoch: 975, Loss: 1.3683, Train: 42.75%, Valid: 41.09%, Test: 41.52%
Run 01:
Highest Train: 43.59
Highest Valid: 42.23
  Final Train: 43.59
   Final Test: 42.64
All runs:
Highest Train: 43.59, nan
Highest Valid: 42.23, nan
  Final Train: 43.59, nan
   Final Test: 42.64, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.2, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7696, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.5828, Train: 29.29%, Valid: 29.25%, Test: 29.91%
Epoch: 50, Loss: 1.4597, Train: 35.27%, Valid: 34.82%, Test: 35.28%
Epoch: 75, Loss: 1.5487, Train: 32.03%, Valid: 32.03%, Test: 31.77%
Epoch: 100, Loss: 1.4895, Train: 32.95%, Valid: 32.83%, Test: 33.26%
Epoch: 125, Loss: 1.4026, Train: 38.85%, Valid: 38.25%, Test: 38.73%
Epoch: 150, Loss: 4.0974, Train: 17.59%, Valid: 17.71%, Test: 17.29%
Epoch: 175, Loss: 1.6575, Train: 34.04%, Valid: 33.72%, Test: 33.92%
Epoch: 200, Loss: 1.4645, Train: 35.68%, Valid: 35.34%, Test: 35.59%
Epoch: 225, Loss: 1.4657, Train: 35.95%, Valid: 35.58%, Test: 35.69%
Epoch: 250, Loss: 1.4618, Train: 36.71%, Valid: 36.29%, Test: 36.73%
Epoch: 275, Loss: 1.4342, Train: 37.56%, Valid: 37.15%, Test: 37.35%
Epoch: 300, Loss: 1.4488, Train: 37.02%, Valid: 36.49%, Test: 36.93%
Epoch: 325, Loss: 1.4369, Train: 38.39%, Valid: 37.81%, Test: 38.37%
Epoch: 350, Loss: 1.4292, Train: 37.32%, Valid: 36.72%, Test: 37.23%
Epoch: 375, Loss: 1.4113, Train: 38.70%, Valid: 37.98%, Test: 38.62%
Epoch: 400, Loss: 2.2446, Train: 26.81%, Valid: 26.78%, Test: 27.14%
Epoch: 425, Loss: 1.8192, Train: 13.18%, Valid: 13.24%, Test: 13.10%
Epoch: 450, Loss: 1.4634, Train: 37.39%, Valid: 37.14%, Test: 37.64%
Epoch: 475, Loss: 1.4222, Train: 38.78%, Valid: 38.00%, Test: 38.50%
Epoch: 500, Loss: 1.4181, Train: 38.48%, Valid: 37.94%, Test: 38.23%
Epoch: 525, Loss: 1.5027, Train: 36.15%, Valid: 35.86%, Test: 36.05%
Epoch: 550, Loss: 1.4352, Train: 38.36%, Valid: 37.86%, Test: 38.09%
Epoch: 575, Loss: 1.4346, Train: 38.29%, Valid: 37.66%, Test: 38.07%
Epoch: 600, Loss: 1.4006, Train: 39.50%, Valid: 38.73%, Test: 39.05%
Epoch: 625, Loss: 1.3845, Train: 40.56%, Valid: 39.64%, Test: 40.06%
Epoch: 650, Loss: 1.4293, Train: 39.86%, Valid: 39.10%, Test: 39.58%
Epoch: 675, Loss: 1.4385, Train: 34.23%, Valid: 33.76%, Test: 34.52%
Epoch: 700, Loss: 1.3987, Train: 39.50%, Valid: 38.86%, Test: 39.09%
Epoch: 725, Loss: 1.4382, Train: 37.33%, Valid: 36.85%, Test: 37.09%
Epoch: 750, Loss: 1.4074, Train: 38.96%, Valid: 38.22%, Test: 38.21%
Epoch: 775, Loss: 1.3834, Train: 39.79%, Valid: 38.83%, Test: 38.98%
Epoch: 800, Loss: 1.4443, Train: 39.48%, Valid: 38.70%, Test: 39.14%
Epoch: 825, Loss: 1.3954, Train: 39.64%, Valid: 38.62%, Test: 38.86%
Epoch: 850, Loss: 1.3912, Train: 39.58%, Valid: 38.44%, Test: 38.52%
Epoch: 875, Loss: 1.4822, Train: 37.30%, Valid: 36.73%, Test: 37.01%
Epoch: 900, Loss: 1.4214, Train: 38.92%, Valid: 38.02%, Test: 38.54%
Epoch: 925, Loss: 1.4181, Train: 39.69%, Valid: 38.70%, Test: 38.74%
Epoch: 950, Loss: 1.3992, Train: 38.96%, Valid: 37.95%, Test: 38.34%
Epoch: 975, Loss: 1.3825, Train: 40.73%, Valid: 39.40%, Test: 39.68%
Run 01:
Highest Train: 41.42
Highest Valid: 40.15
  Final Train: 41.21
   Final Test: 40.21
All runs:
Highest Train: 41.42, nan
Highest Valid: 40.15, nan
  Final Train: 41.21, nan
   Final Test: 40.21, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.2, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7292, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.5473, Train: 25.20%, Valid: 25.36%, Test: 25.08%
Epoch: 50, Loss: 1.4970, Train: 34.47%, Valid: 34.32%, Test: 34.36%
Epoch: 75, Loss: 1.4935, Train: 35.63%, Valid: 35.33%, Test: 35.60%
Epoch: 100, Loss: 1.4824, Train: 33.59%, Valid: 33.11%, Test: 33.76%
Epoch: 125, Loss: 1.4621, Train: 36.96%, Valid: 36.45%, Test: 36.80%
Epoch: 150, Loss: 1.4532, Train: 38.14%, Valid: 37.59%, Test: 38.12%
Epoch: 175, Loss: 1.4204, Train: 34.28%, Valid: 34.02%, Test: 34.43%
Epoch: 200, Loss: 1.4704, Train: 36.18%, Valid: 35.71%, Test: 35.93%
Epoch: 225, Loss: 1.4833, Train: 35.91%, Valid: 35.24%, Test: 35.46%
Epoch: 250, Loss: 1.5123, Train: 34.43%, Valid: 33.71%, Test: 33.98%
Epoch: 275, Loss: 1.4588, Train: 35.32%, Valid: 34.82%, Test: 35.23%
Epoch: 300, Loss: 1.4276, Train: 39.52%, Valid: 38.60%, Test: 39.02%
Epoch: 325, Loss: 1.4421, Train: 36.24%, Valid: 35.82%, Test: 36.01%
Epoch: 350, Loss: 1.4103, Train: 38.41%, Valid: 37.57%, Test: 37.74%
Epoch: 375, Loss: 1.4060, Train: 40.28%, Valid: 39.40%, Test: 39.57%
Epoch: 400, Loss: 1.4090, Train: 39.28%, Valid: 38.41%, Test: 38.78%
Epoch: 425, Loss: 1.4114, Train: 41.25%, Valid: 40.41%, Test: 40.25%
Epoch: 450, Loss: 1.4423, Train: 36.84%, Valid: 36.05%, Test: 36.21%
Epoch: 475, Loss: 1.3972, Train: 41.19%, Valid: 39.90%, Test: 39.96%
Epoch: 500, Loss: 1.4021, Train: 38.18%, Valid: 37.07%, Test: 37.13%
Epoch: 525, Loss: 1.4166, Train: 33.82%, Valid: 33.06%, Test: 32.54%
Epoch: 550, Loss: 1.3998, Train: 41.69%, Valid: 40.18%, Test: 40.46%
Epoch: 575, Loss: 1.4082, Train: 38.34%, Valid: 36.82%, Test: 37.06%
Epoch: 600, Loss: 1.3783, Train: 40.92%, Valid: 39.17%, Test: 39.28%
Epoch: 625, Loss: 1.5716, Train: 26.37%, Valid: 25.75%, Test: 26.74%
Epoch: 650, Loss: 1.3982, Train: 41.22%, Valid: 39.57%, Test: 39.72%
Epoch: 675, Loss: 1.4178, Train: 37.91%, Valid: 36.32%, Test: 36.19%
Epoch: 700, Loss: 1.3788, Train: 42.22%, Valid: 40.45%, Test: 40.67%
Epoch: 725, Loss: 1.3646, Train: 42.07%, Valid: 40.10%, Test: 40.19%
Epoch: 750, Loss: 1.3727, Train: 42.72%, Valid: 40.27%, Test: 40.53%
Epoch: 775, Loss: 1.3989, Train: 41.13%, Valid: 38.89%, Test: 38.96%
Epoch: 800, Loss: 1.4090, Train: 41.14%, Valid: 39.38%, Test: 39.15%
Epoch: 825, Loss: 1.3581, Train: 42.30%, Valid: 39.90%, Test: 40.60%
Epoch: 850, Loss: 1.3950, Train: 42.86%, Valid: 40.87%, Test: 40.94%
Epoch: 875, Loss: 1.3498, Train: 42.20%, Valid: 39.83%, Test: 40.28%
Epoch: 900, Loss: 1.3683, Train: 42.02%, Valid: 39.71%, Test: 40.21%
Epoch: 925, Loss: 1.3312, Train: 42.13%, Valid: 39.69%, Test: 39.78%
Epoch: 950, Loss: 1.3243, Train: 42.69%, Valid: 40.25%, Test: 40.32%
Epoch: 975, Loss: 1.3513, Train: 38.18%, Valid: 36.77%, Test: 37.23%
Run 01:
Highest Train: 44.42
Highest Valid: 42.23
  Final Train: 43.75
   Final Test: 42.41
All runs:
Highest Train: 44.42, nan
Highest Valid: 42.23, nan
  Final Train: 43.75, nan
   Final Test: 42.41, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.2, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7427, Train: 12.51%, Valid: 12.59%, Test: 12.43%
Epoch: 25, Loss: 1.5565, Train: 27.69%, Valid: 27.52%, Test: 27.61%
Epoch: 50, Loss: 1.4693, Train: 31.23%, Valid: 31.40%, Test: 31.19%
Epoch: 75, Loss: 1.4144, Train: 35.96%, Valid: 35.67%, Test: 35.80%
Epoch: 100, Loss: 1.4405, Train: 34.19%, Valid: 33.83%, Test: 34.50%
Epoch: 125, Loss: 1.4523, Train: 37.02%, Valid: 36.49%, Test: 37.01%
Epoch: 150, Loss: 1.4201, Train: 37.53%, Valid: 37.28%, Test: 37.49%
Epoch: 175, Loss: 1.3965, Train: 38.69%, Valid: 38.21%, Test: 38.56%
Epoch: 200, Loss: 1.5205, Train: 30.48%, Valid: 30.47%, Test: 30.27%
Epoch: 225, Loss: 1.4594, Train: 34.50%, Valid: 34.08%, Test: 34.61%
Epoch: 250, Loss: 1.4228, Train: 34.86%, Valid: 34.48%, Test: 34.91%
Epoch: 275, Loss: 1.4069, Train: 39.61%, Valid: 39.20%, Test: 39.24%
Epoch: 300, Loss: 1.4464, Train: 40.08%, Valid: 39.62%, Test: 39.92%
Epoch: 325, Loss: 1.4049, Train: 38.73%, Valid: 38.13%, Test: 38.37%
Epoch: 350, Loss: 1.3640, Train: 40.49%, Valid: 40.05%, Test: 40.64%
Epoch: 375, Loss: 1.3729, Train: 39.04%, Valid: 38.66%, Test: 38.72%
Epoch: 400, Loss: 1.3485, Train: 42.70%, Valid: 42.11%, Test: 42.25%
Epoch: 425, Loss: 1.4282, Train: 36.39%, Valid: 36.14%, Test: 36.37%
Epoch: 450, Loss: 13.4140, Train: 18.36%, Valid: 18.76%, Test: 18.62%
Epoch: 475, Loss: 2.2863, Train: 28.92%, Valid: 28.62%, Test: 28.53%
Epoch: 500, Loss: 1.5268, Train: 36.52%, Valid: 36.28%, Test: 36.85%
Epoch: 525, Loss: 1.6694, Train: 26.57%, Valid: 26.61%, Test: 26.16%
Epoch: 550, Loss: 1.4608, Train: 34.37%, Valid: 34.19%, Test: 34.33%
Epoch: 575, Loss: 1.4425, Train: 38.61%, Valid: 37.91%, Test: 38.25%
Epoch: 600, Loss: 1.4342, Train: 36.42%, Valid: 36.24%, Test: 36.44%
Epoch: 625, Loss: 1.5133, Train: 34.45%, Valid: 34.41%, Test: 34.55%
Epoch: 650, Loss: 1.4312, Train: 39.63%, Valid: 39.01%, Test: 39.29%
Epoch: 675, Loss: 2.0704, Train: 31.45%, Valid: 31.17%, Test: 31.37%
Epoch: 700, Loss: 1.4551, Train: 34.72%, Valid: 34.40%, Test: 34.78%
Epoch: 725, Loss: 1.4389, Train: 39.12%, Valid: 38.17%, Test: 38.55%
Epoch: 750, Loss: 1.4218, Train: 39.10%, Valid: 38.24%, Test: 38.59%
Epoch: 775, Loss: 1.4405, Train: 38.79%, Valid: 38.07%, Test: 38.28%
Epoch: 800, Loss: 1.4036, Train: 39.53%, Valid: 38.79%, Test: 39.02%
Epoch: 825, Loss: 1.3966, Train: 40.23%, Valid: 39.32%, Test: 39.72%
Epoch: 850, Loss: 1.3876, Train: 41.01%, Valid: 39.80%, Test: 40.21%
Epoch: 875, Loss: 2.4953, Train: 23.54%, Valid: 23.69%, Test: 23.37%
Epoch: 900, Loss: 5.1696, Train: 24.67%, Valid: 24.74%, Test: 24.28%
Epoch: 925, Loss: 1.6469, Train: 29.88%, Valid: 29.83%, Test: 30.14%
Epoch: 950, Loss: 1.4725, Train: 37.38%, Valid: 37.02%, Test: 37.54%
Epoch: 975, Loss: 1.4205, Train: 39.29%, Valid: 39.03%, Test: 39.31%
Run 01:
Highest Train: 42.70
Highest Valid: 42.11
  Final Train: 42.70
   Final Test: 42.25
All runs:
Highest Train: 42.70, nan
Highest Valid: 42.11, nan
  Final Train: 42.70, nan
   Final Test: 42.25, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.2, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7556, Train: 17.64%, Valid: 17.78%, Test: 17.32%
Epoch: 25, Loss: 1.7370, Train: 27.82%, Valid: 28.03%, Test: 27.69%
Epoch: 50, Loss: 1.4905, Train: 35.70%, Valid: 35.36%, Test: 35.61%
Epoch: 75, Loss: 1.4147, Train: 38.81%, Valid: 38.56%, Test: 38.78%
Epoch: 100, Loss: 1.3827, Train: 36.41%, Valid: 35.95%, Test: 36.69%
Epoch: 125, Loss: 1.3714, Train: 40.76%, Valid: 40.37%, Test: 40.55%
Epoch: 150, Loss: 1.3793, Train: 41.10%, Valid: 40.34%, Test: 40.71%
Epoch: 175, Loss: 1.3488, Train: 42.09%, Valid: 41.46%, Test: 41.51%
Epoch: 200, Loss: 1.3818, Train: 40.43%, Valid: 39.78%, Test: 40.18%
Epoch: 225, Loss: 1.3575, Train: 41.03%, Valid: 40.53%, Test: 40.57%
Epoch: 250, Loss: 1.3273, Train: 42.48%, Valid: 41.79%, Test: 42.16%
Epoch: 275, Loss: 1.3878, Train: 41.12%, Valid: 40.37%, Test: 40.72%
Epoch: 300, Loss: 1.3752, Train: 41.13%, Valid: 40.23%, Test: 40.53%
Epoch: 325, Loss: 1.3645, Train: 41.37%, Valid: 40.44%, Test: 40.65%
Epoch: 350, Loss: 1.3663, Train: 41.38%, Valid: 40.50%, Test: 40.90%
Epoch: 375, Loss: 1.3147, Train: 43.45%, Valid: 42.28%, Test: 42.61%
Epoch: 400, Loss: 1.4807, Train: 39.65%, Valid: 39.10%, Test: 39.46%
Epoch: 425, Loss: 1.3667, Train: 40.51%, Valid: 39.58%, Test: 39.81%
Epoch: 450, Loss: 1.3341, Train: 42.39%, Valid: 41.41%, Test: 41.66%
Epoch: 475, Loss: 1.3322, Train: 42.87%, Valid: 41.94%, Test: 42.09%
Epoch: 500, Loss: 1.3184, Train: 41.50%, Valid: 40.60%, Test: 40.74%
Epoch: 525, Loss: 1.2989, Train: 44.20%, Valid: 43.14%, Test: 43.26%
Epoch: 550, Loss: 1.3227, Train: 43.25%, Valid: 42.15%, Test: 42.12%
Epoch: 575, Loss: 1.3011, Train: 43.44%, Valid: 42.20%, Test: 42.53%
Epoch: 600, Loss: 1.3082, Train: 42.95%, Valid: 41.97%, Test: 42.10%
Epoch: 625, Loss: 1.3237, Train: 41.57%, Valid: 40.73%, Test: 40.75%
Epoch: 650, Loss: 1.4455, Train: 40.16%, Valid: 39.35%, Test: 39.62%
Epoch: 675, Loss: 1.3287, Train: 42.77%, Valid: 41.16%, Test: 41.33%
Epoch: 700, Loss: 1.3129, Train: 42.49%, Valid: 41.50%, Test: 41.72%
Epoch: 725, Loss: 1.3265, Train: 40.06%, Valid: 39.16%, Test: 39.99%
Epoch: 750, Loss: 1.2955, Train: 44.41%, Valid: 43.08%, Test: 43.06%
Epoch: 775, Loss: 1.2822, Train: 42.11%, Valid: 41.06%, Test: 41.31%
Epoch: 800, Loss: 1.2822, Train: 45.20%, Valid: 43.67%, Test: 43.80%
Epoch: 825, Loss: 1.3220, Train: 41.62%, Valid: 40.54%, Test: 40.75%
Epoch: 850, Loss: 1.2941, Train: 44.34%, Valid: 42.63%, Test: 42.95%
Epoch: 875, Loss: 1.8242, Train: 34.65%, Valid: 33.79%, Test: 34.43%
Epoch: 900, Loss: 1.4147, Train: 41.52%, Valid: 40.43%, Test: 40.60%
Epoch: 925, Loss: 1.3188, Train: 44.02%, Valid: 42.67%, Test: 42.77%
Epoch: 950, Loss: 1.3285, Train: 44.16%, Valid: 42.78%, Test: 42.94%
Epoch: 975, Loss: 1.3420, Train: 43.68%, Valid: 42.14%, Test: 42.27%
Run 01:
Highest Train: 45.87
Highest Valid: 44.28
  Final Train: 45.87
   Final Test: 44.39
All runs:
Highest Train: 45.87, nan
Highest Valid: 44.28, nan
  Final Train: 45.87, nan
   Final Test: 44.39, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.3, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7177, Train: 16.70%, Valid: 16.85%, Test: 16.48%
Epoch: 25, Loss: 1.5420, Train: 32.41%, Valid: 32.25%, Test: 32.43%
Epoch: 50, Loss: 1.4921, Train: 35.73%, Valid: 35.56%, Test: 35.66%
Epoch: 75, Loss: 1.4881, Train: 35.04%, Valid: 34.53%, Test: 35.25%
Epoch: 100, Loss: 1.4810, Train: 36.45%, Valid: 35.83%, Test: 36.59%
Epoch: 125, Loss: 1.4647, Train: 36.23%, Valid: 35.75%, Test: 36.06%
Epoch: 150, Loss: 1.5241, Train: 31.85%, Valid: 31.25%, Test: 31.83%
Epoch: 175, Loss: 1.4748, Train: 36.34%, Valid: 35.62%, Test: 36.11%
Epoch: 200, Loss: 1.5993, Train: 30.75%, Valid: 30.68%, Test: 30.56%
Epoch: 225, Loss: 1.4696, Train: 36.87%, Valid: 36.11%, Test: 36.54%
Epoch: 250, Loss: 1.5012, Train: 31.42%, Valid: 30.68%, Test: 31.39%
Epoch: 275, Loss: 1.4707, Train: 36.80%, Valid: 35.90%, Test: 36.13%
Epoch: 300, Loss: 1.4590, Train: 36.82%, Valid: 35.89%, Test: 36.21%
Epoch: 325, Loss: 1.4455, Train: 37.22%, Valid: 36.19%, Test: 36.69%
Epoch: 350, Loss: 1.4235, Train: 37.43%, Valid: 36.73%, Test: 37.11%
Epoch: 375, Loss: 2.3199, Train: 31.58%, Valid: 31.31%, Test: 31.58%
Epoch: 400, Loss: 1.5051, Train: 33.21%, Valid: 33.00%, Test: 33.27%
Epoch: 425, Loss: 1.4607, Train: 36.92%, Valid: 36.02%, Test: 36.36%
Epoch: 450, Loss: 1.4509, Train: 37.14%, Valid: 36.15%, Test: 36.69%
Epoch: 475, Loss: 1.4418, Train: 38.27%, Valid: 37.20%, Test: 37.63%
Epoch: 500, Loss: 1.4396, Train: 38.97%, Valid: 37.94%, Test: 38.11%
Epoch: 525, Loss: 1.4613, Train: 33.39%, Valid: 32.81%, Test: 32.54%
Epoch: 550, Loss: 1.4626, Train: 38.60%, Valid: 37.70%, Test: 38.03%
Epoch: 575, Loss: 1.4245, Train: 37.55%, Valid: 36.68%, Test: 36.66%
Epoch: 600, Loss: 1.4317, Train: 36.17%, Valid: 34.99%, Test: 34.88%
Epoch: 625, Loss: 1.4389, Train: 38.52%, Valid: 37.44%, Test: 37.76%
Epoch: 650, Loss: 1.4437, Train: 35.79%, Valid: 34.44%, Test: 34.76%
Epoch: 675, Loss: 1.4235, Train: 39.27%, Valid: 38.02%, Test: 38.33%
Epoch: 700, Loss: 1.5435, Train: 37.47%, Valid: 36.20%, Test: 36.46%
Epoch: 725, Loss: 1.4281, Train: 40.04%, Valid: 38.63%, Test: 38.81%
Epoch: 750, Loss: 1.4009, Train: 41.32%, Valid: 39.82%, Test: 39.95%
Epoch: 775, Loss: 1.3898, Train: 39.74%, Valid: 38.33%, Test: 38.42%
Epoch: 800, Loss: 1.4035, Train: 40.23%, Valid: 38.90%, Test: 39.19%
Epoch: 825, Loss: 1.3975, Train: 40.11%, Valid: 38.97%, Test: 38.90%
Epoch: 850, Loss: 1.3843, Train: 40.95%, Valid: 39.12%, Test: 39.35%
Epoch: 875, Loss: 1.5180, Train: 33.96%, Valid: 32.14%, Test: 32.51%
Epoch: 900, Loss: 1.4524, Train: 38.51%, Valid: 37.02%, Test: 36.76%
Epoch: 925, Loss: 1.4183, Train: 38.82%, Valid: 37.27%, Test: 37.49%
Epoch: 950, Loss: 1.4159, Train: 37.77%, Valid: 35.99%, Test: 36.39%
Epoch: 975, Loss: 1.4370, Train: 31.49%, Valid: 30.32%, Test: 30.31%
Run 01:
Highest Train: 42.11
Highest Valid: 40.71
  Final Train: 42.11
   Final Test: 40.74
All runs:
Highest Train: 42.11, nan
Highest Valid: 40.71, nan
  Final Train: 42.11, nan
   Final Test: 40.74, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.3, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7297, Train: 17.64%, Valid: 17.78%, Test: 17.32%
Epoch: 25, Loss: 1.5517, Train: 31.40%, Valid: 31.29%, Test: 31.70%
Epoch: 50, Loss: 1.5310, Train: 32.91%, Valid: 32.44%, Test: 32.75%
Epoch: 75, Loss: 1.7410, Train: 22.93%, Valid: 23.17%, Test: 23.44%
Epoch: 100, Loss: 1.4824, Train: 35.41%, Valid: 35.12%, Test: 35.11%
Epoch: 125, Loss: 1.4004, Train: 40.68%, Valid: 39.88%, Test: 40.40%
Epoch: 150, Loss: 1.3972, Train: 40.13%, Valid: 39.43%, Test: 39.88%
Epoch: 175, Loss: 2.2544, Train: 31.98%, Valid: 31.76%, Test: 31.72%
Epoch: 200, Loss: 1.5049, Train: 30.39%, Valid: 30.38%, Test: 30.01%
Epoch: 225, Loss: 1.4554, Train: 36.76%, Valid: 36.32%, Test: 36.80%
Epoch: 250, Loss: 1.4832, Train: 36.03%, Valid: 36.13%, Test: 36.30%
Epoch: 275, Loss: 1.4565, Train: 39.18%, Valid: 38.72%, Test: 39.23%
Epoch: 300, Loss: 1.4136, Train: 39.92%, Valid: 39.17%, Test: 39.61%
Epoch: 325, Loss: 1.4120, Train: 39.48%, Valid: 38.90%, Test: 39.30%
Epoch: 350, Loss: 1.4036, Train: 33.82%, Valid: 33.72%, Test: 33.70%
Epoch: 375, Loss: 1.4335, Train: 39.12%, Valid: 38.60%, Test: 38.93%
Epoch: 400, Loss: 1.3973, Train: 41.30%, Valid: 40.43%, Test: 40.70%
Epoch: 425, Loss: 1.4220, Train: 41.80%, Valid: 40.84%, Test: 41.09%
Epoch: 450, Loss: 1.3842, Train: 41.33%, Valid: 40.85%, Test: 41.03%
Epoch: 475, Loss: 1.3502, Train: 43.92%, Valid: 43.29%, Test: 43.29%
Epoch: 500, Loss: 1.4032, Train: 43.79%, Valid: 42.87%, Test: 43.04%
Epoch: 525, Loss: 1.4044, Train: 36.99%, Valid: 36.51%, Test: 36.60%
Epoch: 550, Loss: 1.3545, Train: 40.66%, Valid: 40.36%, Test: 40.11%
Epoch: 575, Loss: 1.3336, Train: 39.84%, Valid: 38.78%, Test: 39.63%
Epoch: 600, Loss: 1.4052, Train: 40.07%, Valid: 39.55%, Test: 39.63%
Epoch: 625, Loss: 1.3820, Train: 41.59%, Valid: 40.60%, Test: 40.81%
Epoch: 650, Loss: 1.4476, Train: 35.78%, Valid: 35.16%, Test: 35.34%
Epoch: 675, Loss: 1.4771, Train: 40.45%, Valid: 39.41%, Test: 39.50%
Epoch: 700, Loss: 1.3512, Train: 42.76%, Valid: 41.70%, Test: 41.81%
Epoch: 725, Loss: 1.3201, Train: 42.87%, Valid: 42.01%, Test: 42.10%
Epoch: 750, Loss: 1.3538, Train: 43.30%, Valid: 41.98%, Test: 42.40%
Epoch: 775, Loss: 1.3030, Train: 42.83%, Valid: 41.85%, Test: 41.88%
Epoch: 800, Loss: 1.4053, Train: 37.48%, Valid: 36.97%, Test: 36.36%
Epoch: 825, Loss: 1.3793, Train: 35.63%, Valid: 34.68%, Test: 34.74%
Epoch: 850, Loss: 1.3418, Train: 42.92%, Valid: 41.58%, Test: 41.90%
Epoch: 875, Loss: 1.3396, Train: 40.69%, Valid: 39.72%, Test: 39.96%
Epoch: 900, Loss: 1.3136, Train: 42.18%, Valid: 41.02%, Test: 41.41%
Epoch: 925, Loss: 1.3541, Train: 45.97%, Valid: 44.64%, Test: 44.99%
Epoch: 950, Loss: 1.3453, Train: 44.38%, Valid: 43.66%, Test: 43.95%
Epoch: 975, Loss: 1.3032, Train: 44.46%, Valid: 43.28%, Test: 43.43%
Run 01:
Highest Train: 46.08
Highest Valid: 44.96
  Final Train: 46.05
   Final Test: 45.13
All runs:
Highest Train: 46.08, nan
Highest Valid: 44.96, nan
  Final Train: 46.05, nan
   Final Test: 45.13, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.3, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7551, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.5267, Train: 30.21%, Valid: 29.57%, Test: 30.53%
Epoch: 50, Loss: 1.5239, Train: 33.36%, Valid: 33.28%, Test: 33.27%
Epoch: 75, Loss: 1.4320, Train: 38.60%, Valid: 38.37%, Test: 38.83%
Epoch: 100, Loss: 1.4059, Train: 39.76%, Valid: 39.60%, Test: 39.92%
Epoch: 125, Loss: 2.4581, Train: 23.14%, Valid: 23.16%, Test: 23.80%
Epoch: 150, Loss: 1.6031, Train: 33.71%, Valid: 34.04%, Test: 33.95%
Epoch: 175, Loss: 1.4450, Train: 37.90%, Valid: 37.41%, Test: 38.15%
Epoch: 200, Loss: 1.4250, Train: 38.24%, Valid: 38.11%, Test: 38.43%
Epoch: 225, Loss: 1.4078, Train: 39.28%, Valid: 38.59%, Test: 39.09%
Epoch: 250, Loss: 1.3877, Train: 41.05%, Valid: 40.36%, Test: 40.67%
Epoch: 275, Loss: 1.3697, Train: 41.45%, Valid: 40.68%, Test: 41.21%
Epoch: 300, Loss: 1.4086, Train: 40.18%, Valid: 39.63%, Test: 39.94%
Epoch: 325, Loss: 1.3585, Train: 41.19%, Valid: 40.88%, Test: 40.87%
Epoch: 350, Loss: 1.3430, Train: 42.45%, Valid: 41.67%, Test: 41.81%
Epoch: 375, Loss: 1.3757, Train: 40.99%, Valid: 40.30%, Test: 40.54%
Epoch: 400, Loss: 1.3799, Train: 41.18%, Valid: 40.52%, Test: 40.97%
Epoch: 425, Loss: 1.3680, Train: 38.47%, Valid: 37.57%, Test: 38.14%
Epoch: 450, Loss: 1.3313, Train: 42.27%, Valid: 41.38%, Test: 41.78%
Epoch: 475, Loss: 1.3711, Train: 41.70%, Valid: 40.76%, Test: 41.16%
Epoch: 500, Loss: 1.3346, Train: 41.33%, Valid: 40.40%, Test: 41.04%
Epoch: 525, Loss: 1.3589, Train: 41.49%, Valid: 40.69%, Test: 41.02%
Epoch: 550, Loss: 1.4766, Train: 25.82%, Valid: 25.65%, Test: 25.59%
Epoch: 575, Loss: 2.3986, Train: 23.35%, Valid: 23.56%, Test: 23.33%
Epoch: 600, Loss: 1.5597, Train: 34.64%, Valid: 34.34%, Test: 34.69%
Epoch: 625, Loss: 1.4428, Train: 38.20%, Valid: 38.26%, Test: 38.30%
Epoch: 650, Loss: 1.4319, Train: 38.04%, Valid: 37.60%, Test: 37.67%
Epoch: 675, Loss: 1.3987, Train: 40.29%, Valid: 39.93%, Test: 40.19%
Epoch: 700, Loss: 1.3964, Train: 40.60%, Valid: 39.96%, Test: 40.21%
Epoch: 725, Loss: 1.3875, Train: 40.83%, Valid: 40.21%, Test: 40.48%
Epoch: 750, Loss: 1.3883, Train: 40.28%, Valid: 39.98%, Test: 40.07%
Epoch: 775, Loss: 1.3744, Train: 40.01%, Valid: 39.31%, Test: 39.49%
Epoch: 800, Loss: 1.3606, Train: 41.85%, Valid: 41.31%, Test: 41.38%
Epoch: 825, Loss: 1.3565, Train: 39.83%, Valid: 39.33%, Test: 39.43%
Epoch: 850, Loss: 1.3718, Train: 41.69%, Valid: 41.16%, Test: 41.54%
Epoch: 875, Loss: 1.3579, Train: 42.19%, Valid: 41.42%, Test: 41.72%
Epoch: 900, Loss: 1.3413, Train: 42.90%, Valid: 42.17%, Test: 42.21%
Epoch: 925, Loss: 1.3463, Train: 41.36%, Valid: 40.60%, Test: 40.83%
Epoch: 950, Loss: 1.9269, Train: 27.00%, Valid: 26.87%, Test: 27.16%
Epoch: 975, Loss: 1.5648, Train: 29.96%, Valid: 29.58%, Test: 30.00%
Run 01:
Highest Train: 43.91
Highest Valid: 43.13
  Final Train: 43.91
   Final Test: 43.39
All runs:
Highest Train: 43.91, nan
Highest Valid: 43.13, nan
  Final Train: 43.91, nan
   Final Test: 43.39, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.3, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7526, Train: 22.16%, Valid: 22.17%, Test: 22.74%
Epoch: 25, Loss: 1.5430, Train: 32.64%, Valid: 32.01%, Test: 32.35%
Epoch: 50, Loss: 1.5026, Train: 34.25%, Valid: 34.06%, Test: 34.33%
Epoch: 75, Loss: 1.6208, Train: 28.81%, Valid: 28.64%, Test: 28.92%
Epoch: 100, Loss: 1.5049, Train: 35.28%, Valid: 35.02%, Test: 35.22%
Epoch: 125, Loss: 1.4837, Train: 35.54%, Valid: 35.24%, Test: 35.51%
Epoch: 150, Loss: 1.4674, Train: 35.88%, Valid: 35.45%, Test: 35.86%
Epoch: 175, Loss: 1.4734, Train: 36.19%, Valid: 35.68%, Test: 36.04%
Epoch: 200, Loss: 1.4504, Train: 37.24%, Valid: 36.70%, Test: 36.92%
Epoch: 225, Loss: 1.4429, Train: 38.44%, Valid: 37.75%, Test: 37.75%
Epoch: 250, Loss: 1.4352, Train: 37.97%, Valid: 37.28%, Test: 37.41%
Epoch: 275, Loss: 1.4299, Train: 32.70%, Valid: 32.01%, Test: 32.78%
Epoch: 300, Loss: 1.4326, Train: 38.08%, Valid: 37.66%, Test: 37.76%
Epoch: 325, Loss: 1.4386, Train: 26.77%, Valid: 26.12%, Test: 26.81%
Epoch: 350, Loss: 1.4858, Train: 37.27%, Valid: 36.42%, Test: 36.67%
Epoch: 375, Loss: 1.4435, Train: 38.52%, Valid: 37.28%, Test: 37.59%
Epoch: 400, Loss: 1.4384, Train: 39.16%, Valid: 37.98%, Test: 38.18%
Epoch: 425, Loss: 1.5032, Train: 36.09%, Valid: 34.79%, Test: 35.35%
Epoch: 450, Loss: 1.4455, Train: 37.81%, Valid: 36.85%, Test: 37.13%
Epoch: 475, Loss: 1.4176, Train: 40.48%, Valid: 39.43%, Test: 39.74%
Epoch: 500, Loss: 1.4259, Train: 38.63%, Valid: 37.56%, Test: 37.77%
Epoch: 525, Loss: 1.4037, Train: 40.19%, Valid: 38.75%, Test: 38.92%
Epoch: 550, Loss: 1.3977, Train: 40.26%, Valid: 39.02%, Test: 39.06%
Epoch: 575, Loss: 1.4132, Train: 38.44%, Valid: 37.41%, Test: 37.43%
Epoch: 600, Loss: 1.8664, Train: 30.91%, Valid: 30.45%, Test: 30.74%
Epoch: 625, Loss: 1.4641, Train: 37.85%, Valid: 36.31%, Test: 36.77%
Epoch: 650, Loss: 1.4430, Train: 38.51%, Valid: 37.09%, Test: 37.50%
Epoch: 675, Loss: 1.4804, Train: 27.60%, Valid: 26.30%, Test: 26.31%
Epoch: 700, Loss: 1.4450, Train: 38.32%, Valid: 36.53%, Test: 36.94%
Epoch: 725, Loss: 1.4125, Train: 37.74%, Valid: 36.61%, Test: 36.96%
Epoch: 750, Loss: 1.4471, Train: 36.60%, Valid: 35.49%, Test: 35.78%
Epoch: 775, Loss: 1.6391, Train: 37.21%, Valid: 35.68%, Test: 35.96%
Epoch: 800, Loss: 1.4456, Train: 37.91%, Valid: 36.38%, Test: 36.84%
Epoch: 825, Loss: 1.4163, Train: 41.37%, Valid: 39.30%, Test: 39.42%
Epoch: 850, Loss: 1.4033, Train: 41.71%, Valid: 39.80%, Test: 39.76%
Epoch: 875, Loss: 1.4781, Train: 35.62%, Valid: 34.44%, Test: 34.86%
Epoch: 900, Loss: 1.4372, Train: 37.93%, Valid: 36.21%, Test: 36.55%
Epoch: 925, Loss: 1.4238, Train: 38.07%, Valid: 36.33%, Test: 36.61%
Epoch: 950, Loss: 1.5123, Train: 33.40%, Valid: 32.02%, Test: 32.31%
Epoch: 975, Loss: 1.4255, Train: 38.06%, Valid: 36.64%, Test: 36.80%
Run 01:
Highest Train: 42.54
Highest Valid: 41.48
  Final Train: 42.54
   Final Test: 41.50
All runs:
Highest Train: 42.54, nan
Highest Valid: 41.48, nan
  Final Train: 42.54, nan
   Final Test: 41.50, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.3, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7544, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.5417, Train: 31.78%, Valid: 31.52%, Test: 31.86%
Epoch: 50, Loss: 1.4590, Train: 37.26%, Valid: 36.97%, Test: 37.13%
Epoch: 75, Loss: 1.8289, Train: 25.05%, Valid: 25.04%, Test: 24.83%
Epoch: 100, Loss: 1.5181, Train: 35.57%, Valid: 35.25%, Test: 35.48%
Epoch: 125, Loss: 1.4536, Train: 36.33%, Valid: 36.15%, Test: 36.59%
Epoch: 150, Loss: 1.4218, Train: 38.48%, Valid: 37.85%, Test: 38.13%
Epoch: 175, Loss: 1.4466, Train: 38.06%, Valid: 37.81%, Test: 37.98%
Epoch: 200, Loss: 3.5238, Train: 34.89%, Valid: 34.86%, Test: 34.91%
Epoch: 225, Loss: 1.5400, Train: 34.90%, Valid: 34.71%, Test: 34.85%
Epoch: 250, Loss: 1.5131, Train: 35.43%, Valid: 35.05%, Test: 35.26%
Epoch: 275, Loss: 1.4595, Train: 37.62%, Valid: 37.36%, Test: 37.60%
Epoch: 300, Loss: 1.4379, Train: 38.17%, Valid: 37.58%, Test: 37.97%
Epoch: 325, Loss: 1.4657, Train: 38.71%, Valid: 38.03%, Test: 38.49%
Epoch: 350, Loss: 1.4339, Train: 38.99%, Valid: 38.32%, Test: 38.77%
Epoch: 375, Loss: 1.6424, Train: 35.69%, Valid: 35.26%, Test: 35.68%
Epoch: 400, Loss: 1.4388, Train: 38.89%, Valid: 38.38%, Test: 38.66%
Epoch: 425, Loss: 1.4822, Train: 37.91%, Valid: 37.64%, Test: 37.55%
Epoch: 450, Loss: 1.4137, Train: 39.84%, Valid: 39.36%, Test: 39.61%
Epoch: 475, Loss: 1.4116, Train: 38.99%, Valid: 38.31%, Test: 38.68%
Epoch: 500, Loss: 1.3991, Train: 39.95%, Valid: 39.31%, Test: 39.58%
Epoch: 525, Loss: 4.1364, Train: 28.76%, Valid: 28.57%, Test: 28.80%
Epoch: 550, Loss: 1.8867, Train: 12.58%, Valid: 12.68%, Test: 12.53%
Epoch: 575, Loss: 1.5605, Train: 33.48%, Valid: 33.37%, Test: 33.56%
Epoch: 600, Loss: 1.4700, Train: 36.27%, Valid: 35.73%, Test: 36.21%
Epoch: 625, Loss: 1.4403, Train: 37.93%, Valid: 37.27%, Test: 37.76%
Epoch: 650, Loss: 1.4330, Train: 38.53%, Valid: 37.87%, Test: 38.16%
Epoch: 675, Loss: 1.4110, Train: 38.47%, Valid: 37.97%, Test: 37.99%
Epoch: 700, Loss: 1.3769, Train: 39.36%, Valid: 38.68%, Test: 38.98%
Epoch: 725, Loss: 1.4327, Train: 38.19%, Valid: 37.69%, Test: 37.88%
Epoch: 750, Loss: 1.4066, Train: 40.37%, Valid: 39.39%, Test: 39.83%
Epoch: 775, Loss: 1.3718, Train: 40.43%, Valid: 39.84%, Test: 40.01%
Epoch: 800, Loss: 1.4578, Train: 39.26%, Valid: 38.45%, Test: 38.87%
Epoch: 825, Loss: 1.4149, Train: 41.17%, Valid: 40.22%, Test: 40.65%
Epoch: 850, Loss: 1.3571, Train: 41.99%, Valid: 41.20%, Test: 41.30%
Epoch: 875, Loss: 1.3789, Train: 42.51%, Valid: 41.62%, Test: 41.97%
Epoch: 900, Loss: 1.4217, Train: 38.23%, Valid: 37.40%, Test: 37.81%
Epoch: 925, Loss: 1.3825, Train: 41.80%, Valid: 40.83%, Test: 41.13%
Epoch: 950, Loss: 1.3758, Train: 40.92%, Valid: 39.97%, Test: 40.31%
Epoch: 975, Loss: 1.7786, Train: 21.40%, Valid: 21.41%, Test: 20.84%
Run 01:
Highest Train: 42.51
Highest Valid: 41.62
  Final Train: 42.51
   Final Test: 41.97
All runs:
Highest Train: 42.51, nan
Highest Valid: 41.62, nan
  Final Train: 42.51, nan
   Final Test: 41.97, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.3, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7834, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.5624, Train: 25.45%, Valid: 25.38%, Test: 25.68%
Epoch: 50, Loss: 1.6713, Train: 26.20%, Valid: 26.09%, Test: 26.72%
Epoch: 75, Loss: 1.5733, Train: 36.13%, Valid: 35.71%, Test: 36.44%
Epoch: 100, Loss: 1.5565, Train: 35.58%, Valid: 35.21%, Test: 35.44%
Epoch: 125, Loss: 1.4343, Train: 38.61%, Valid: 38.16%, Test: 38.47%
Epoch: 150, Loss: 1.4256, Train: 38.04%, Valid: 37.83%, Test: 38.36%
Epoch: 175, Loss: 2.7787, Train: 25.99%, Valid: 26.02%, Test: 26.44%
Epoch: 200, Loss: 1.5505, Train: 32.95%, Valid: 32.75%, Test: 33.17%
Epoch: 225, Loss: 1.4721, Train: 36.85%, Valid: 36.71%, Test: 36.76%
Epoch: 250, Loss: 1.4352, Train: 37.33%, Valid: 37.04%, Test: 37.17%
Epoch: 275, Loss: 1.4350, Train: 39.47%, Valid: 39.03%, Test: 39.27%
Epoch: 300, Loss: 1.4584, Train: 35.68%, Valid: 35.30%, Test: 35.99%
Epoch: 325, Loss: 1.4109, Train: 40.01%, Valid: 39.32%, Test: 39.59%
Epoch: 350, Loss: 1.4030, Train: 39.79%, Valid: 39.12%, Test: 39.33%
Epoch: 375, Loss: 1.3923, Train: 40.58%, Valid: 39.61%, Test: 40.15%
Epoch: 400, Loss: 1.4147, Train: 39.26%, Valid: 38.59%, Test: 39.13%
Epoch: 425, Loss: 1.3790, Train: 41.49%, Valid: 40.53%, Test: 40.90%
Epoch: 450, Loss: 1.4316, Train: 34.16%, Valid: 33.49%, Test: 33.92%
Epoch: 475, Loss: 1.3693, Train: 41.79%, Valid: 40.79%, Test: 41.06%
Epoch: 500, Loss: 1.3854, Train: 41.65%, Valid: 40.82%, Test: 41.10%
Epoch: 525, Loss: 1.3578, Train: 41.81%, Valid: 40.81%, Test: 41.08%
Epoch: 550, Loss: 1.3651, Train: 39.61%, Valid: 38.92%, Test: 38.97%
Epoch: 575, Loss: 1.3468, Train: 43.12%, Valid: 42.10%, Test: 42.34%
Epoch: 600, Loss: 1.3760, Train: 40.27%, Valid: 39.41%, Test: 39.73%
Epoch: 625, Loss: 1.3129, Train: 42.90%, Valid: 42.35%, Test: 42.14%
Epoch: 650, Loss: 1.3360, Train: 41.72%, Valid: 40.96%, Test: 41.16%
Epoch: 675, Loss: 1.5314, Train: 33.48%, Valid: 33.19%, Test: 33.32%
Epoch: 700, Loss: 1.4398, Train: 38.28%, Valid: 37.65%, Test: 37.94%
Epoch: 725, Loss: 1.3853, Train: 40.44%, Valid: 39.39%, Test: 40.00%
Epoch: 750, Loss: 1.3573, Train: 42.21%, Valid: 41.29%, Test: 41.37%
Epoch: 775, Loss: 1.3503, Train: 43.13%, Valid: 41.98%, Test: 42.25%
Epoch: 800, Loss: 1.3494, Train: 43.16%, Valid: 42.21%, Test: 42.23%
Epoch: 825, Loss: 1.3402, Train: 42.92%, Valid: 41.91%, Test: 42.00%
Epoch: 850, Loss: 1.3381, Train: 42.36%, Valid: 41.48%, Test: 41.73%
Epoch: 875, Loss: 1.4235, Train: 41.38%, Valid: 40.45%, Test: 40.59%
Epoch: 900, Loss: 1.3328, Train: 43.01%, Valid: 41.79%, Test: 42.02%
Epoch: 925, Loss: 1.3083, Train: 38.86%, Valid: 37.87%, Test: 38.29%
Epoch: 950, Loss: 1.3160, Train: 44.54%, Valid: 43.24%, Test: 43.48%
Epoch: 975, Loss: 1.3319, Train: 42.42%, Valid: 41.57%, Test: 41.72%
Run 01:
Highest Train: 45.43
Highest Valid: 44.35
  Final Train: 45.43
   Final Test: 44.37
All runs:
Highest Train: 45.43, nan
Highest Valid: 44.35, nan
  Final Train: 45.43, nan
   Final Test: 44.37, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.3, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7697, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.5611, Train: 28.63%, Valid: 27.94%, Test: 28.70%
Epoch: 50, Loss: 1.4801, Train: 33.79%, Valid: 33.51%, Test: 33.97%
Epoch: 75, Loss: 1.4285, Train: 37.17%, Valid: 36.92%, Test: 37.48%
Epoch: 100, Loss: 1.4640, Train: 37.09%, Valid: 36.69%, Test: 37.27%
Epoch: 125, Loss: 1.4610, Train: 36.37%, Valid: 35.89%, Test: 36.30%
Epoch: 150, Loss: 1.5078, Train: 28.87%, Valid: 28.80%, Test: 28.30%
Epoch: 175, Loss: 1.4702, Train: 37.31%, Valid: 36.89%, Test: 37.09%
Epoch: 200, Loss: 1.4354, Train: 38.11%, Valid: 37.62%, Test: 37.59%
Epoch: 225, Loss: 1.5867, Train: 22.76%, Valid: 22.85%, Test: 22.70%
Epoch: 250, Loss: 1.4894, Train: 35.46%, Valid: 34.87%, Test: 35.31%
Epoch: 275, Loss: 1.4359, Train: 38.25%, Valid: 37.50%, Test: 37.84%
Epoch: 300, Loss: 1.4904, Train: 35.92%, Valid: 35.36%, Test: 35.77%
Epoch: 325, Loss: 1.5442, Train: 32.22%, Valid: 31.98%, Test: 31.92%
Epoch: 350, Loss: 1.4768, Train: 36.42%, Valid: 35.90%, Test: 36.15%
Epoch: 375, Loss: 1.4529, Train: 37.72%, Valid: 37.00%, Test: 37.17%
Epoch: 400, Loss: 1.4829, Train: 36.03%, Valid: 35.37%, Test: 35.53%
Epoch: 425, Loss: 1.4632, Train: 36.85%, Valid: 36.17%, Test: 36.34%
Epoch: 450, Loss: 1.4368, Train: 38.86%, Valid: 38.06%, Test: 38.09%
Epoch: 475, Loss: 1.4466, Train: 37.48%, Valid: 36.70%, Test: 37.20%
Epoch: 500, Loss: 2.6854, Train: 23.65%, Valid: 23.65%, Test: 23.49%
Epoch: 525, Loss: 1.5274, Train: 33.47%, Valid: 32.89%, Test: 32.96%
Epoch: 550, Loss: 1.4803, Train: 36.26%, Valid: 35.45%, Test: 35.71%
Epoch: 575, Loss: 1.4715, Train: 36.61%, Valid: 35.80%, Test: 36.01%
Epoch: 600, Loss: 1.4667, Train: 36.88%, Valid: 36.00%, Test: 36.19%
Epoch: 625, Loss: 1.4630, Train: 37.00%, Valid: 36.10%, Test: 36.41%
Epoch: 650, Loss: 1.4550, Train: 37.76%, Valid: 36.73%, Test: 36.81%
Epoch: 675, Loss: 1.4453, Train: 38.04%, Valid: 36.90%, Test: 37.21%
Epoch: 700, Loss: 1.4792, Train: 35.67%, Valid: 35.45%, Test: 35.66%
Epoch: 725, Loss: 1.4880, Train: 36.54%, Valid: 36.00%, Test: 36.29%
Epoch: 750, Loss: 1.4499, Train: 37.47%, Valid: 36.57%, Test: 36.90%
Epoch: 775, Loss: 1.4951, Train: 33.65%, Valid: 32.89%, Test: 33.48%
Epoch: 800, Loss: 1.4497, Train: 37.75%, Valid: 36.83%, Test: 37.29%
Epoch: 825, Loss: 1.4384, Train: 38.97%, Valid: 38.08%, Test: 38.63%
Epoch: 850, Loss: 1.4542, Train: 37.90%, Valid: 37.19%, Test: 37.45%
Epoch: 875, Loss: 1.4493, Train: 38.74%, Valid: 38.07%, Test: 38.14%
Epoch: 900, Loss: 1.4162, Train: 38.62%, Valid: 38.05%, Test: 38.24%
Epoch: 925, Loss: 1.4393, Train: 34.85%, Valid: 34.44%, Test: 34.68%
Epoch: 950, Loss: 1.4046, Train: 39.83%, Valid: 39.06%, Test: 39.27%
Epoch: 975, Loss: 1.4701, Train: 40.37%, Valid: 39.53%, Test: 39.81%
Run 01:
Highest Train: 41.52
Highest Valid: 40.77
  Final Train: 41.52
   Final Test: 40.94
All runs:
Highest Train: 41.52, nan
Highest Valid: 40.77, nan
  Final Train: 41.52, nan
   Final Test: 40.94, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.3, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7401, Train: 18.58%, Valid: 18.60%, Test: 18.36%
Epoch: 25, Loss: 1.5268, Train: 25.77%, Valid: 25.92%, Test: 26.65%
Epoch: 50, Loss: 1.4304, Train: 37.21%, Valid: 37.04%, Test: 36.93%
Epoch: 75, Loss: 1.4683, Train: 36.76%, Valid: 36.40%, Test: 36.92%
Epoch: 100, Loss: 1.4049, Train: 39.87%, Valid: 39.33%, Test: 39.62%
Epoch: 125, Loss: 1.3959, Train: 40.10%, Valid: 39.66%, Test: 39.84%
Epoch: 150, Loss: 1.4272, Train: 38.22%, Valid: 38.10%, Test: 38.14%
Epoch: 175, Loss: 1.3873, Train: 39.42%, Valid: 39.15%, Test: 39.19%
Epoch: 200, Loss: 1.4216, Train: 39.84%, Valid: 39.29%, Test: 39.79%
Epoch: 225, Loss: 1.3823, Train: 40.67%, Valid: 39.85%, Test: 40.37%
Epoch: 250, Loss: 1.4059, Train: 36.12%, Valid: 35.77%, Test: 35.81%
Epoch: 275, Loss: 1.3810, Train: 40.65%, Valid: 40.02%, Test: 40.33%
Epoch: 300, Loss: 1.3972, Train: 40.42%, Valid: 39.76%, Test: 40.14%
Epoch: 325, Loss: 1.5559, Train: 39.10%, Valid: 38.44%, Test: 38.80%
Epoch: 350, Loss: 1.3808, Train: 38.85%, Valid: 38.31%, Test: 38.65%
Epoch: 375, Loss: 1.3702, Train: 40.40%, Valid: 39.64%, Test: 40.46%
Epoch: 400, Loss: 1.4118, Train: 40.14%, Valid: 39.32%, Test: 39.41%
Epoch: 425, Loss: 1.3717, Train: 39.76%, Valid: 39.26%, Test: 39.53%
Epoch: 450, Loss: 2.0022, Train: 24.63%, Valid: 25.10%, Test: 24.61%
Epoch: 475, Loss: 1.4834, Train: 36.82%, Valid: 36.16%, Test: 36.77%
Epoch: 500, Loss: 1.4363, Train: 37.87%, Valid: 37.56%, Test: 38.01%
Epoch: 525, Loss: 1.4027, Train: 40.38%, Valid: 39.68%, Test: 40.23%
Epoch: 550, Loss: 1.4220, Train: 39.36%, Valid: 38.70%, Test: 39.24%
Epoch: 575, Loss: 1.4308, Train: 39.48%, Valid: 38.75%, Test: 39.24%
Epoch: 600, Loss: 18.9848, Train: 24.09%, Valid: 23.92%, Test: 23.62%
Epoch: 625, Loss: 1.7433, Train: 19.01%, Valid: 18.98%, Test: 18.72%
Epoch: 650, Loss: 1.5077, Train: 37.04%, Valid: 36.75%, Test: 36.93%
Epoch: 675, Loss: 1.4423, Train: 38.69%, Valid: 38.38%, Test: 38.73%
Epoch: 700, Loss: 1.4470, Train: 39.48%, Valid: 38.91%, Test: 39.35%
Epoch: 725, Loss: 1.4331, Train: 34.40%, Valid: 34.27%, Test: 34.42%
Epoch: 750, Loss: 1.4100, Train: 40.34%, Valid: 39.57%, Test: 39.91%
Epoch: 775, Loss: 1.4012, Train: 39.14%, Valid: 38.49%, Test: 38.61%
Epoch: 800, Loss: 1.4480, Train: 38.49%, Valid: 38.16%, Test: 38.51%
Epoch: 825, Loss: 1.4001, Train: 40.51%, Valid: 39.93%, Test: 40.34%
Epoch: 850, Loss: 1.3995, Train: 40.79%, Valid: 40.05%, Test: 40.35%
Epoch: 875, Loss: 1.3746, Train: 41.74%, Valid: 41.00%, Test: 41.12%
Epoch: 900, Loss: 1.3947, Train: 40.81%, Valid: 39.97%, Test: 40.30%
Epoch: 925, Loss: 1.3629, Train: 42.09%, Valid: 41.31%, Test: 41.54%
Epoch: 950, Loss: 1.3662, Train: 42.03%, Valid: 41.20%, Test: 41.58%
Epoch: 975, Loss: 1.3606, Train: 42.09%, Valid: 41.37%, Test: 41.54%
Run 01:
Highest Train: 43.59
Highest Valid: 42.95
  Final Train: 43.59
   Final Test: 42.98
All runs:
Highest Train: 43.59, nan
Highest Valid: 42.95, nan
  Final Train: 43.59, nan
   Final Test: 42.98, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.3, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7193, Train: 17.49%, Valid: 17.45%, Test: 17.34%
Epoch: 25, Loss: 1.6126, Train: 35.03%, Valid: 34.74%, Test: 34.88%
Epoch: 50, Loss: 1.4433, Train: 35.84%, Valid: 35.49%, Test: 35.90%
Epoch: 75, Loss: 1.5434, Train: 37.40%, Valid: 37.05%, Test: 37.39%
Epoch: 100, Loss: 1.6206, Train: 32.32%, Valid: 31.78%, Test: 32.09%
Epoch: 125, Loss: 1.4533, Train: 37.02%, Valid: 36.52%, Test: 37.00%
Epoch: 150, Loss: 1.6478, Train: 31.17%, Valid: 31.08%, Test: 31.01%
Epoch: 175, Loss: 1.8108, Train: 34.81%, Valid: 34.51%, Test: 34.74%
Epoch: 200, Loss: 1.4719, Train: 37.17%, Valid: 36.80%, Test: 37.08%
Epoch: 225, Loss: 1.4405, Train: 37.06%, Valid: 36.70%, Test: 36.81%
Epoch: 250, Loss: 1.4148, Train: 39.47%, Valid: 38.95%, Test: 39.34%
Epoch: 275, Loss: 1.4094, Train: 34.62%, Valid: 34.33%, Test: 34.39%
Epoch: 300, Loss: 1.4129, Train: 40.74%, Valid: 40.05%, Test: 40.40%
Epoch: 325, Loss: 1.4231, Train: 41.23%, Valid: 40.64%, Test: 41.03%
Epoch: 350, Loss: 1.4053, Train: 40.16%, Valid: 39.80%, Test: 40.08%
Epoch: 375, Loss: 1.4028, Train: 39.71%, Valid: 39.10%, Test: 39.43%
Epoch: 400, Loss: 1.3560, Train: 41.21%, Valid: 40.73%, Test: 40.71%
Epoch: 425, Loss: 1.4422, Train: 38.86%, Valid: 38.41%, Test: 38.61%
Epoch: 450, Loss: 1.3677, Train: 41.67%, Valid: 40.67%, Test: 41.11%
Epoch: 475, Loss: 1.3822, Train: 37.25%, Valid: 36.34%, Test: 36.37%
Epoch: 500, Loss: 1.3671, Train: 40.64%, Valid: 39.84%, Test: 40.12%
Epoch: 525, Loss: 1.3868, Train: 42.21%, Valid: 41.32%, Test: 41.46%
Epoch: 550, Loss: 4.5972, Train: 25.22%, Valid: 25.21%, Test: 25.62%
Epoch: 575, Loss: 1.7448, Train: 26.92%, Valid: 27.02%, Test: 27.29%
Epoch: 600, Loss: 1.4784, Train: 35.10%, Valid: 34.12%, Test: 34.90%
Epoch: 625, Loss: 1.4667, Train: 37.67%, Valid: 36.99%, Test: 37.46%
Epoch: 650, Loss: 1.4559, Train: 35.23%, Valid: 34.73%, Test: 35.02%
Epoch: 675, Loss: 1.4176, Train: 39.83%, Valid: 39.48%, Test: 39.78%
Epoch: 700, Loss: 1.4361, Train: 38.65%, Valid: 37.99%, Test: 38.26%
Epoch: 725, Loss: 1.4048, Train: 40.23%, Valid: 39.54%, Test: 39.80%
Epoch: 750, Loss: 1.4618, Train: 37.52%, Valid: 36.48%, Test: 36.90%
Epoch: 775, Loss: 1.4229, Train: 39.63%, Valid: 38.85%, Test: 38.93%
Epoch: 800, Loss: 1.4257, Train: 38.69%, Valid: 37.90%, Test: 37.87%
Epoch: 825, Loss: 1.4102, Train: 38.33%, Valid: 37.77%, Test: 37.69%
Epoch: 850, Loss: 1.4058, Train: 33.41%, Valid: 32.88%, Test: 33.06%
Epoch: 875, Loss: 1.4120, Train: 39.23%, Valid: 38.46%, Test: 38.55%
Epoch: 900, Loss: 1.3918, Train: 40.56%, Valid: 39.70%, Test: 40.00%
Epoch: 925, Loss: 1.3915, Train: 41.59%, Valid: 40.74%, Test: 41.04%
Epoch: 950, Loss: 1.3644, Train: 42.29%, Valid: 41.10%, Test: 41.28%
Epoch: 975, Loss: 1.3662, Train: 40.38%, Valid: 39.15%, Test: 39.52%
Run 01:
Highest Train: 43.42
Highest Valid: 42.65
  Final Train: 43.42
   Final Test: 42.77
All runs:
Highest Train: 43.42, nan
Highest Valid: 42.65, nan
  Final Train: 43.42, nan
   Final Test: 42.77, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.4, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7923, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.6050, Train: 26.74%, Valid: 26.41%, Test: 27.44%
Epoch: 50, Loss: 1.5060, Train: 34.12%, Valid: 34.02%, Test: 34.13%
Epoch: 75, Loss: 1.4920, Train: 34.47%, Valid: 34.26%, Test: 34.60%
Epoch: 100, Loss: 1.4766, Train: 36.81%, Valid: 36.46%, Test: 36.50%
Epoch: 125, Loss: 1.6409, Train: 35.07%, Valid: 34.68%, Test: 35.04%
Epoch: 150, Loss: 1.6608, Train: 31.84%, Valid: 31.44%, Test: 31.93%
Epoch: 175, Loss: 1.4976, Train: 34.89%, Valid: 34.60%, Test: 34.91%
Epoch: 200, Loss: 1.4455, Train: 37.34%, Valid: 36.87%, Test: 36.99%
Epoch: 225, Loss: 1.4983, Train: 35.12%, Valid: 34.82%, Test: 34.99%
Epoch: 250, Loss: 1.4943, Train: 35.60%, Valid: 35.15%, Test: 35.47%
Epoch: 275, Loss: 1.4784, Train: 35.76%, Valid: 35.23%, Test: 35.64%
Epoch: 300, Loss: 1.4709, Train: 36.60%, Valid: 35.81%, Test: 36.20%
Epoch: 325, Loss: 1.6471, Train: 27.45%, Valid: 27.23%, Test: 27.40%
Epoch: 350, Loss: 1.4847, Train: 36.05%, Valid: 35.38%, Test: 35.59%
Epoch: 375, Loss: 1.4703, Train: 36.27%, Valid: 35.63%, Test: 36.04%
Epoch: 400, Loss: 1.4520, Train: 36.81%, Valid: 36.09%, Test: 36.53%
Epoch: 425, Loss: 1.4787, Train: 31.96%, Valid: 31.65%, Test: 31.35%
Epoch: 450, Loss: 1.4604, Train: 38.20%, Valid: 37.44%, Test: 37.75%
Epoch: 475, Loss: 1.4358, Train: 38.85%, Valid: 38.08%, Test: 38.17%
Epoch: 500, Loss: 1.4528, Train: 38.25%, Valid: 37.62%, Test: 37.83%
Epoch: 525, Loss: 1.4296, Train: 37.12%, Valid: 36.26%, Test: 36.74%
Epoch: 550, Loss: 1.4626, Train: 37.61%, Valid: 37.07%, Test: 37.57%
Epoch: 575, Loss: 1.4339, Train: 39.39%, Valid: 38.69%, Test: 38.88%
Epoch: 600, Loss: 1.4389, Train: 37.76%, Valid: 37.00%, Test: 37.26%
Epoch: 625, Loss: 1.4406, Train: 39.76%, Valid: 38.86%, Test: 39.17%
Epoch: 650, Loss: 1.5192, Train: 37.44%, Valid: 36.72%, Test: 37.05%
Epoch: 675, Loss: 1.4402, Train: 39.59%, Valid: 38.80%, Test: 39.08%
Epoch: 700, Loss: 1.4498, Train: 37.42%, Valid: 36.84%, Test: 37.02%
Epoch: 725, Loss: 1.4578, Train: 36.17%, Valid: 35.30%, Test: 36.03%
Epoch: 750, Loss: 1.4243, Train: 39.50%, Valid: 38.75%, Test: 39.01%
Epoch: 775, Loss: 1.4435, Train: 39.81%, Valid: 39.09%, Test: 39.43%
Epoch: 800, Loss: 1.5575, Train: 30.64%, Valid: 30.20%, Test: 30.38%
Epoch: 825, Loss: 1.4378, Train: 38.48%, Valid: 37.40%, Test: 37.77%
Epoch: 850, Loss: 1.4445, Train: 37.39%, Valid: 36.66%, Test: 37.05%
Epoch: 875, Loss: 1.4612, Train: 38.68%, Valid: 37.87%, Test: 38.11%
Epoch: 900, Loss: 1.4088, Train: 30.60%, Valid: 29.87%, Test: 30.00%
Epoch: 925, Loss: 1.5994, Train: 35.80%, Valid: 35.13%, Test: 35.44%
Epoch: 950, Loss: 1.4930, Train: 37.20%, Valid: 35.97%, Test: 36.41%
Epoch: 975, Loss: 1.4566, Train: 37.48%, Valid: 36.22%, Test: 36.75%
Run 01:
Highest Train: 42.28
Highest Valid: 41.54
  Final Train: 42.28
   Final Test: 41.58
All runs:
Highest Train: 42.28, nan
Highest Valid: 41.54, nan
  Final Train: 42.28, nan
   Final Test: 41.58, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.4, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.8039, Train: 22.16%, Valid: 22.17%, Test: 22.74%
Epoch: 25, Loss: 1.5993, Train: 32.92%, Valid: 32.83%, Test: 33.05%
Epoch: 50, Loss: 1.4685, Train: 32.62%, Valid: 32.70%, Test: 33.02%
Epoch: 75, Loss: 1.4443, Train: 37.30%, Valid: 36.88%, Test: 37.07%
Epoch: 100, Loss: 1.4217, Train: 38.67%, Valid: 38.14%, Test: 38.48%
Epoch: 125, Loss: 1.5115, Train: 37.36%, Valid: 36.97%, Test: 37.19%
Epoch: 150, Loss: 2.2997, Train: 19.75%, Valid: 19.80%, Test: 19.76%
Epoch: 175, Loss: 1.5784, Train: 35.39%, Valid: 35.01%, Test: 35.43%
Epoch: 200, Loss: 1.4600, Train: 29.36%, Valid: 29.47%, Test: 29.14%
Epoch: 225, Loss: 1.4459, Train: 39.10%, Valid: 38.83%, Test: 39.05%
Epoch: 250, Loss: 1.4439, Train: 33.74%, Valid: 33.63%, Test: 33.30%
Epoch: 275, Loss: 1.5352, Train: 39.26%, Valid: 38.97%, Test: 39.20%
Epoch: 300, Loss: 1.3970, Train: 31.41%, Valid: 31.42%, Test: 31.11%
Epoch: 325, Loss: 1.4060, Train: 38.95%, Valid: 38.41%, Test: 38.56%
Epoch: 350, Loss: 1.4066, Train: 37.40%, Valid: 37.17%, Test: 37.44%
Epoch: 375, Loss: 1.4161, Train: 33.37%, Valid: 33.28%, Test: 33.11%
Epoch: 400, Loss: 1.4406, Train: 40.28%, Valid: 39.39%, Test: 39.93%
Epoch: 425, Loss: 1.4010, Train: 40.18%, Valid: 39.70%, Test: 39.85%
Epoch: 450, Loss: 1.4191, Train: 37.82%, Valid: 37.40%, Test: 38.01%
Epoch: 475, Loss: 1.4884, Train: 35.49%, Valid: 35.37%, Test: 35.22%
Epoch: 500, Loss: 1.4938, Train: 36.39%, Valid: 36.43%, Test: 36.69%
Epoch: 525, Loss: 1.4557, Train: 38.26%, Valid: 37.82%, Test: 38.09%
Epoch: 550, Loss: 1.4235, Train: 38.85%, Valid: 38.30%, Test: 38.60%
Epoch: 575, Loss: 1.4084, Train: 38.30%, Valid: 38.00%, Test: 38.42%
Epoch: 600, Loss: 1.4239, Train: 39.54%, Valid: 39.19%, Test: 39.49%
Epoch: 625, Loss: 1.4325, Train: 27.89%, Valid: 27.75%, Test: 27.51%
Epoch: 650, Loss: 1.5261, Train: 41.11%, Valid: 40.53%, Test: 40.84%
Epoch: 675, Loss: 1.3778, Train: 35.56%, Valid: 35.35%, Test: 35.52%
Epoch: 700, Loss: 1.3790, Train: 37.65%, Valid: 37.33%, Test: 37.30%
Epoch: 725, Loss: 1.4191, Train: 37.22%, Valid: 36.85%, Test: 37.07%
Epoch: 750, Loss: 1.4716, Train: 38.69%, Valid: 38.27%, Test: 38.87%
Epoch: 775, Loss: 1.4267, Train: 40.04%, Valid: 39.49%, Test: 39.83%
Epoch: 800, Loss: 1.4084, Train: 39.93%, Valid: 39.41%, Test: 39.77%
Epoch: 825, Loss: 1.3835, Train: 40.75%, Valid: 40.25%, Test: 40.39%
Epoch: 850, Loss: 1.4036, Train: 39.00%, Valid: 38.46%, Test: 38.71%
Epoch: 875, Loss: 1.4831, Train: 39.73%, Valid: 39.18%, Test: 39.43%
Epoch: 900, Loss: 1.5628, Train: 32.53%, Valid: 32.13%, Test: 32.27%
Epoch: 925, Loss: 1.4127, Train: 39.58%, Valid: 38.59%, Test: 39.13%
Epoch: 950, Loss: 1.3716, Train: 41.30%, Valid: 40.26%, Test: 40.81%
Epoch: 975, Loss: 1.3724, Train: 40.80%, Valid: 39.89%, Test: 40.34%
Run 01:
Highest Train: 42.83
Highest Valid: 41.93
  Final Train: 42.83
   Final Test: 42.11
All runs:
Highest Train: 42.83, nan
Highest Valid: 41.93, nan
  Final Train: 42.83, nan
   Final Test: 42.11, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.4, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7516, Train: 22.16%, Valid: 22.17%, Test: 22.74%
Epoch: 25, Loss: 1.5458, Train: 32.81%, Valid: 32.60%, Test: 32.90%
Epoch: 50, Loss: 1.4611, Train: 36.43%, Valid: 35.73%, Test: 36.01%
Epoch: 75, Loss: 1.4076, Train: 39.75%, Valid: 39.21%, Test: 39.78%
Epoch: 100, Loss: 1.3777, Train: 40.25%, Valid: 39.60%, Test: 40.03%
Epoch: 125, Loss: 1.3949, Train: 38.64%, Valid: 38.15%, Test: 38.56%
Epoch: 150, Loss: 1.4383, Train: 39.39%, Valid: 38.78%, Test: 39.08%
Epoch: 175, Loss: 1.3629, Train: 41.60%, Valid: 41.08%, Test: 41.11%
Epoch: 200, Loss: 1.3690, Train: 40.87%, Valid: 40.16%, Test: 40.29%
Epoch: 225, Loss: 1.4132, Train: 38.93%, Valid: 38.41%, Test: 38.66%
Epoch: 250, Loss: 1.3655, Train: 40.68%, Valid: 39.88%, Test: 40.26%
Epoch: 275, Loss: 1.3622, Train: 41.81%, Valid: 41.02%, Test: 41.27%
Epoch: 300, Loss: 6.9966, Train: 23.66%, Valid: 23.52%, Test: 23.11%
Epoch: 325, Loss: 3.5592, Train: 26.08%, Valid: 26.12%, Test: 25.77%
Epoch: 350, Loss: 2.2419, Train: 21.02%, Valid: 21.30%, Test: 20.64%
Epoch: 375, Loss: 1.6731, Train: 26.79%, Valid: 27.00%, Test: 26.58%
Epoch: 400, Loss: 1.4857, Train: 34.81%, Valid: 34.39%, Test: 34.73%
Epoch: 425, Loss: 1.4430, Train: 38.16%, Valid: 37.87%, Test: 38.36%
Epoch: 450, Loss: 1.4054, Train: 39.39%, Valid: 38.89%, Test: 39.18%
Epoch: 475, Loss: 1.3968, Train: 39.70%, Valid: 39.35%, Test: 39.61%
Epoch: 500, Loss: 1.4575, Train: 31.81%, Valid: 31.27%, Test: 31.64%
Epoch: 525, Loss: 1.3871, Train: 27.84%, Valid: 27.72%, Test: 27.92%
Epoch: 550, Loss: 1.4145, Train: 39.36%, Valid: 38.73%, Test: 39.25%
Epoch: 575, Loss: 1.3786, Train: 41.63%, Valid: 40.92%, Test: 41.23%
Epoch: 600, Loss: 1.3762, Train: 42.32%, Valid: 41.47%, Test: 41.82%
Epoch: 625, Loss: 1.3677, Train: 41.38%, Valid: 40.64%, Test: 40.90%
Epoch: 650, Loss: 1.3910, Train: 34.68%, Valid: 34.27%, Test: 33.91%
Epoch: 675, Loss: 1.5642, Train: 25.07%, Valid: 25.25%, Test: 25.25%
Epoch: 700, Loss: 1.7121, Train: 29.59%, Valid: 29.35%, Test: 29.66%
Epoch: 725, Loss: 1.4945, Train: 35.89%, Valid: 35.41%, Test: 35.98%
Epoch: 750, Loss: 1.4459, Train: 37.60%, Valid: 37.37%, Test: 37.82%
Epoch: 775, Loss: 1.4660, Train: 36.34%, Valid: 35.98%, Test: 36.35%
Epoch: 800, Loss: 1.4291, Train: 38.40%, Valid: 37.89%, Test: 38.00%
Epoch: 825, Loss: 1.4134, Train: 39.70%, Valid: 39.28%, Test: 39.61%
Epoch: 850, Loss: 1.4226, Train: 38.44%, Valid: 37.82%, Test: 37.96%
Epoch: 875, Loss: 1.4016, Train: 41.26%, Valid: 40.75%, Test: 40.92%
Epoch: 900, Loss: 1.3703, Train: 41.24%, Valid: 40.53%, Test: 40.79%
Epoch: 925, Loss: 1.3766, Train: 42.97%, Valid: 42.26%, Test: 42.45%
Epoch: 950, Loss: 1.3780, Train: 41.54%, Valid: 40.53%, Test: 40.79%
Epoch: 975, Loss: 1.3646, Train: 42.15%, Valid: 41.19%, Test: 41.44%
Run 01:
Highest Train: 43.07
Highest Valid: 42.26
  Final Train: 42.97
   Final Test: 42.45
All runs:
Highest Train: 43.07, nan
Highest Valid: 42.26, nan
  Final Train: 42.97, nan
   Final Test: 42.45, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.4, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7879, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.5370, Train: 32.79%, Valid: 32.21%, Test: 32.72%
Epoch: 50, Loss: 1.5165, Train: 33.08%, Valid: 33.02%, Test: 33.49%
Epoch: 75, Loss: 1.4982, Train: 35.17%, Valid: 34.96%, Test: 35.15%
Epoch: 100, Loss: 1.4862, Train: 35.69%, Valid: 35.47%, Test: 35.69%
Epoch: 125, Loss: 1.4731, Train: 34.94%, Valid: 34.71%, Test: 34.89%
Epoch: 150, Loss: 1.5785, Train: 29.97%, Valid: 29.47%, Test: 29.98%
Epoch: 175, Loss: 1.4880, Train: 35.51%, Valid: 35.11%, Test: 35.44%
Epoch: 200, Loss: 1.4770, Train: 36.31%, Valid: 35.85%, Test: 36.15%
Epoch: 225, Loss: 1.5621, Train: 34.04%, Valid: 33.45%, Test: 34.19%
Epoch: 250, Loss: 1.4727, Train: 33.33%, Valid: 32.79%, Test: 33.53%
Epoch: 275, Loss: 1.4698, Train: 38.38%, Valid: 37.82%, Test: 38.13%
Epoch: 300, Loss: 1.5325, Train: 33.13%, Valid: 32.42%, Test: 33.08%
Epoch: 325, Loss: 1.7969, Train: 31.18%, Valid: 30.98%, Test: 31.07%
Epoch: 350, Loss: 1.4766, Train: 36.35%, Valid: 35.85%, Test: 36.23%
Epoch: 375, Loss: 1.4681, Train: 36.79%, Valid: 36.13%, Test: 36.63%
Epoch: 400, Loss: 1.4478, Train: 39.41%, Valid: 38.83%, Test: 38.98%
Epoch: 425, Loss: 1.4481, Train: 34.63%, Valid: 34.24%, Test: 34.21%
Epoch: 450, Loss: 1.5849, Train: 32.50%, Valid: 32.51%, Test: 32.89%
Epoch: 475, Loss: 1.4736, Train: 38.00%, Valid: 37.27%, Test: 37.54%
Epoch: 500, Loss: 9.1349, Train: 26.50%, Valid: 26.56%, Test: 26.07%
Epoch: 525, Loss: 2.7262, Train: 31.21%, Valid: 31.07%, Test: 31.13%
Epoch: 550, Loss: 1.5485, Train: 30.26%, Valid: 30.08%, Test: 30.48%
Epoch: 575, Loss: 1.5001, Train: 35.08%, Valid: 34.80%, Test: 35.01%
Epoch: 600, Loss: 1.4955, Train: 35.65%, Valid: 35.06%, Test: 35.65%
Epoch: 625, Loss: 1.4702, Train: 36.50%, Valid: 36.17%, Test: 36.56%
Epoch: 650, Loss: 1.4694, Train: 35.61%, Valid: 35.39%, Test: 35.56%
Epoch: 675, Loss: 1.4817, Train: 37.08%, Valid: 36.84%, Test: 37.30%
Epoch: 700, Loss: 3.1937, Train: 28.83%, Valid: 28.70%, Test: 28.93%
Epoch: 725, Loss: 1.6860, Train: 17.65%, Valid: 17.77%, Test: 17.32%
Epoch: 750, Loss: 1.5672, Train: 26.58%, Valid: 26.25%, Test: 26.57%
Epoch: 775, Loss: 1.6271, Train: 34.06%, Valid: 33.81%, Test: 34.15%
Epoch: 800, Loss: 1.4703, Train: 37.83%, Valid: 37.21%, Test: 37.74%
Epoch: 825, Loss: 1.4533, Train: 38.69%, Valid: 38.04%, Test: 38.43%
Epoch: 850, Loss: 1.4584, Train: 38.65%, Valid: 37.89%, Test: 38.21%
Epoch: 875, Loss: 1.4457, Train: 39.01%, Valid: 38.16%, Test: 38.38%
Epoch: 900, Loss: 1.4456, Train: 39.09%, Valid: 38.21%, Test: 38.55%
Epoch: 925, Loss: 1.4489, Train: 35.94%, Valid: 35.59%, Test: 35.61%
Epoch: 950, Loss: 1.5635, Train: 33.26%, Valid: 32.44%, Test: 32.89%
Epoch: 975, Loss: 1.4727, Train: 37.44%, Valid: 36.68%, Test: 37.05%
Run 01:
Highest Train: 40.59
Highest Valid: 39.75
  Final Train: 40.59
   Final Test: 40.11
All runs:
Highest Train: 40.59, nan
Highest Valid: 39.75, nan
  Final Train: 40.59, nan
   Final Test: 40.11, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.4, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7650, Train: 20.16%, Valid: 20.37%, Test: 20.03%
Epoch: 25, Loss: 1.5095, Train: 32.18%, Valid: 32.00%, Test: 32.73%
Epoch: 50, Loss: 1.5084, Train: 31.00%, Valid: 30.71%, Test: 31.42%
Epoch: 75, Loss: 1.4610, Train: 38.71%, Valid: 38.51%, Test: 38.81%
Epoch: 100, Loss: 1.4308, Train: 39.12%, Valid: 38.66%, Test: 38.92%
Epoch: 125, Loss: 1.5163, Train: 39.58%, Valid: 39.06%, Test: 39.45%
Epoch: 150, Loss: 1.4253, Train: 39.47%, Valid: 38.98%, Test: 39.16%
Epoch: 175, Loss: 1.3869, Train: 39.19%, Valid: 38.65%, Test: 39.41%
Epoch: 200, Loss: 1.7831, Train: 30.08%, Valid: 30.13%, Test: 30.09%
Epoch: 225, Loss: 1.4887, Train: 37.19%, Valid: 36.94%, Test: 37.14%
Epoch: 250, Loss: 1.4246, Train: 38.88%, Valid: 38.34%, Test: 38.51%
Epoch: 275, Loss: 1.4502, Train: 39.07%, Valid: 38.68%, Test: 39.15%
Epoch: 300, Loss: 1.4329, Train: 36.97%, Valid: 36.83%, Test: 37.15%
Epoch: 325, Loss: 1.4760, Train: 36.04%, Valid: 35.56%, Test: 35.88%
Epoch: 350, Loss: 1.4203, Train: 38.67%, Valid: 38.16%, Test: 38.46%
Epoch: 375, Loss: 1.4880, Train: 38.01%, Valid: 37.52%, Test: 37.72%
Epoch: 400, Loss: 1.4052, Train: 40.35%, Valid: 39.62%, Test: 39.98%
Epoch: 425, Loss: 1.4553, Train: 37.17%, Valid: 36.79%, Test: 36.96%
Epoch: 450, Loss: 3.6425, Train: 19.44%, Valid: 19.37%, Test: 19.23%
Epoch: 475, Loss: 1.6013, Train: 32.84%, Valid: 32.61%, Test: 32.90%
Epoch: 500, Loss: 1.4972, Train: 36.68%, Valid: 36.20%, Test: 36.41%
Epoch: 525, Loss: 1.4402, Train: 37.44%, Valid: 36.94%, Test: 37.26%
Epoch: 550, Loss: 1.4228, Train: 39.20%, Valid: 38.54%, Test: 38.79%
Epoch: 575, Loss: 1.3883, Train: 40.76%, Valid: 40.16%, Test: 40.49%
Epoch: 600, Loss: 1.3752, Train: 41.52%, Valid: 40.86%, Test: 40.96%
Epoch: 625, Loss: 1.4215, Train: 38.06%, Valid: 37.60%, Test: 37.74%
Epoch: 650, Loss: 1.4038, Train: 40.98%, Valid: 40.42%, Test: 40.47%
Epoch: 675, Loss: 1.4910, Train: 36.64%, Valid: 36.03%, Test: 36.50%
Epoch: 700, Loss: 1.4761, Train: 36.76%, Valid: 36.30%, Test: 36.66%
Epoch: 725, Loss: 1.4304, Train: 34.82%, Valid: 34.76%, Test: 34.32%
Epoch: 750, Loss: 1.3939, Train: 40.91%, Valid: 40.23%, Test: 40.37%
Epoch: 775, Loss: 1.4112, Train: 40.07%, Valid: 39.38%, Test: 39.64%
Epoch: 800, Loss: 1.3865, Train: 39.30%, Valid: 38.66%, Test: 38.88%
Epoch: 825, Loss: 1.4253, Train: 39.36%, Valid: 38.73%, Test: 38.95%
Epoch: 850, Loss: 1.6908, Train: 34.73%, Valid: 34.52%, Test: 34.55%
Epoch: 875, Loss: 1.5329, Train: 37.60%, Valid: 37.44%, Test: 37.37%
Epoch: 900, Loss: 1.4433, Train: 37.56%, Valid: 37.05%, Test: 37.12%
Epoch: 925, Loss: 1.4254, Train: 39.12%, Valid: 38.36%, Test: 38.64%
Epoch: 950, Loss: 1.4578, Train: 34.08%, Valid: 33.71%, Test: 33.94%
Epoch: 975, Loss: 1.5033, Train: 37.73%, Valid: 37.30%, Test: 37.41%
Run 01:
Highest Train: 42.52
Highest Valid: 41.74
  Final Train: 42.52
   Final Test: 41.87
All runs:
Highest Train: 42.52, nan
Highest Valid: 41.74, nan
  Final Train: 42.52, nan
   Final Test: 41.87, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.4, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7660, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.4973, Train: 29.10%, Valid: 28.82%, Test: 29.44%
Epoch: 50, Loss: 1.5022, Train: 34.48%, Valid: 34.51%, Test: 34.96%
Epoch: 75, Loss: 1.4290, Train: 37.89%, Valid: 37.62%, Test: 38.05%
Epoch: 100, Loss: 1.4625, Train: 36.68%, Valid: 36.39%, Test: 36.62%
Epoch: 125, Loss: 2.3297, Train: 26.49%, Valid: 26.38%, Test: 27.03%
Epoch: 150, Loss: 1.7936, Train: 28.14%, Valid: 28.07%, Test: 28.30%
Epoch: 175, Loss: 1.4797, Train: 32.71%, Valid: 32.51%, Test: 32.88%
Epoch: 200, Loss: 1.4594, Train: 37.30%, Valid: 37.08%, Test: 37.52%
Epoch: 225, Loss: 1.4332, Train: 37.43%, Valid: 36.73%, Test: 37.21%
Epoch: 250, Loss: 1.4606, Train: 36.23%, Valid: 35.97%, Test: 36.54%
Epoch: 275, Loss: 1.4322, Train: 36.72%, Valid: 36.37%, Test: 36.79%
Epoch: 300, Loss: 1.4199, Train: 38.73%, Valid: 38.12%, Test: 38.38%
Epoch: 325, Loss: 1.4342, Train: 39.95%, Valid: 39.31%, Test: 39.82%
Epoch: 350, Loss: 1.4085, Train: 38.02%, Valid: 37.51%, Test: 37.93%
Epoch: 375, Loss: 1.4038, Train: 41.41%, Valid: 40.60%, Test: 41.06%
Epoch: 400, Loss: 1.3903, Train: 39.23%, Valid: 38.46%, Test: 39.08%
Epoch: 425, Loss: 1.4156, Train: 36.84%, Valid: 36.07%, Test: 36.32%
Epoch: 450, Loss: 1.4100, Train: 39.40%, Valid: 38.77%, Test: 39.40%
Epoch: 475, Loss: 1.4143, Train: 39.64%, Valid: 39.24%, Test: 39.50%
Epoch: 500, Loss: 1.3737, Train: 40.64%, Valid: 39.54%, Test: 39.88%
Epoch: 525, Loss: 1.6000, Train: 34.99%, Valid: 34.39%, Test: 34.78%
Epoch: 550, Loss: 1.4648, Train: 38.89%, Valid: 38.03%, Test: 38.43%
Epoch: 575, Loss: 1.3855, Train: 39.38%, Valid: 37.96%, Test: 38.45%
Epoch: 600, Loss: 1.3531, Train: 40.29%, Valid: 39.46%, Test: 39.63%
Epoch: 625, Loss: 1.3734, Train: 42.11%, Valid: 41.11%, Test: 41.08%
Epoch: 650, Loss: 1.3588, Train: 37.49%, Valid: 36.62%, Test: 37.03%
Epoch: 675, Loss: 1.3550, Train: 42.05%, Valid: 41.09%, Test: 41.23%
Epoch: 700, Loss: 1.3303, Train: 42.91%, Valid: 41.66%, Test: 42.05%
Epoch: 725, Loss: 1.3542, Train: 43.10%, Valid: 41.89%, Test: 41.92%
Epoch: 750, Loss: 1.4481, Train: 39.20%, Valid: 38.51%, Test: 38.64%
Epoch: 775, Loss: 1.3585, Train: 41.25%, Valid: 40.10%, Test: 40.41%
Epoch: 800, Loss: 1.3596, Train: 43.15%, Valid: 41.72%, Test: 41.89%
Epoch: 825, Loss: 1.3249, Train: 42.71%, Valid: 41.50%, Test: 41.98%
Epoch: 850, Loss: 1.3688, Train: 41.84%, Valid: 40.47%, Test: 40.47%
Epoch: 875, Loss: 1.3087, Train: 44.27%, Valid: 43.04%, Test: 43.26%
Epoch: 900, Loss: 1.3449, Train: 42.04%, Valid: 40.96%, Test: 41.14%
Epoch: 925, Loss: 1.3121, Train: 44.02%, Valid: 42.64%, Test: 42.91%
Epoch: 950, Loss: 1.3479, Train: 41.98%, Valid: 41.05%, Test: 41.15%
Epoch: 975, Loss: 1.3862, Train: 38.64%, Valid: 38.31%, Test: 38.35%
Run 01:
Highest Train: 44.70
Highest Valid: 43.51
  Final Train: 44.65
   Final Test: 43.62
All runs:
Highest Train: 44.70, nan
Highest Valid: 43.51, nan
  Final Train: 44.65, nan
   Final Test: 43.62, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.4, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7977, Train: 12.51%, Valid: 12.59%, Test: 12.43%
Epoch: 25, Loss: 1.5688, Train: 31.73%, Valid: 31.51%, Test: 31.77%
Epoch: 50, Loss: 1.5073, Train: 34.59%, Valid: 34.26%, Test: 34.74%
Epoch: 75, Loss: 1.5041, Train: 33.69%, Valid: 33.48%, Test: 33.61%
Epoch: 100, Loss: 1.4583, Train: 36.79%, Valid: 36.31%, Test: 36.73%
Epoch: 125, Loss: 1.4373, Train: 38.58%, Valid: 38.06%, Test: 38.39%
Epoch: 150, Loss: 1.4793, Train: 34.49%, Valid: 34.29%, Test: 34.83%
Epoch: 175, Loss: 1.5463, Train: 34.09%, Valid: 34.11%, Test: 34.13%
Epoch: 200, Loss: 1.4788, Train: 34.90%, Valid: 34.63%, Test: 34.94%
Epoch: 225, Loss: 1.5039, Train: 33.52%, Valid: 33.55%, Test: 33.29%
Epoch: 250, Loss: 3.4414, Train: 22.20%, Valid: 22.18%, Test: 22.77%
Epoch: 275, Loss: 1.5765, Train: 35.53%, Valid: 35.05%, Test: 35.23%
Epoch: 300, Loss: 1.4736, Train: 38.08%, Valid: 37.41%, Test: 38.16%
Epoch: 325, Loss: 1.4517, Train: 38.56%, Valid: 37.91%, Test: 38.25%
Epoch: 350, Loss: 1.4455, Train: 38.99%, Valid: 38.32%, Test: 38.61%
Epoch: 375, Loss: 1.4431, Train: 38.81%, Valid: 38.14%, Test: 38.49%
Epoch: 400, Loss: 1.7333, Train: 32.99%, Valid: 32.71%, Test: 32.81%
Epoch: 425, Loss: 1.4897, Train: 34.88%, Valid: 34.33%, Test: 34.70%
Epoch: 450, Loss: 1.7039, Train: 36.25%, Valid: 35.49%, Test: 35.79%
Epoch: 475, Loss: 1.4824, Train: 36.31%, Valid: 35.53%, Test: 35.80%
Epoch: 500, Loss: 1.4656, Train: 37.07%, Valid: 36.14%, Test: 36.60%
Epoch: 525, Loss: 1.4565, Train: 37.87%, Valid: 36.74%, Test: 37.22%
Epoch: 550, Loss: 1.4536, Train: 38.92%, Valid: 37.81%, Test: 38.17%
Epoch: 575, Loss: 1.4327, Train: 38.36%, Valid: 37.07%, Test: 37.67%
Epoch: 600, Loss: 1.4461, Train: 37.96%, Valid: 36.75%, Test: 36.91%
Epoch: 625, Loss: 1.4508, Train: 38.44%, Valid: 37.08%, Test: 37.44%
Epoch: 650, Loss: 1.4310, Train: 38.69%, Valid: 37.29%, Test: 37.74%
Epoch: 675, Loss: 1.4172, Train: 40.34%, Valid: 39.36%, Test: 39.78%
Epoch: 700, Loss: 1.4621, Train: 36.39%, Valid: 35.13%, Test: 35.41%
Epoch: 725, Loss: 1.4391, Train: 40.09%, Valid: 38.84%, Test: 38.99%
Epoch: 750, Loss: 1.5214, Train: 22.44%, Valid: 22.54%, Test: 21.72%
Epoch: 775, Loss: 1.4443, Train: 38.10%, Valid: 36.79%, Test: 37.37%
Epoch: 800, Loss: 1.5448, Train: 33.03%, Valid: 32.64%, Test: 32.74%
Epoch: 825, Loss: 1.4393, Train: 38.17%, Valid: 37.03%, Test: 37.37%
Epoch: 850, Loss: 1.4202, Train: 39.76%, Valid: 38.71%, Test: 38.72%
Epoch: 875, Loss: 1.4253, Train: 40.38%, Valid: 38.83%, Test: 39.01%
Epoch: 900, Loss: 1.5163, Train: 38.18%, Valid: 36.93%, Test: 37.16%
Epoch: 925, Loss: 1.4484, Train: 39.16%, Valid: 37.53%, Test: 38.01%
Epoch: 950, Loss: 1.5031, Train: 38.36%, Valid: 37.04%, Test: 37.30%
Epoch: 975, Loss: 1.4151, Train: 35.43%, Valid: 34.64%, Test: 34.73%
Run 01:
Highest Train: 41.53
Highest Valid: 40.45
  Final Train: 40.87
   Final Test: 40.68
All runs:
Highest Train: 41.53, nan
Highest Valid: 40.45, nan
  Final Train: 40.87, nan
   Final Test: 40.68, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.4, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7732, Train: 12.51%, Valid: 12.59%, Test: 12.43%
Epoch: 25, Loss: 1.5434, Train: 33.72%, Valid: 33.43%, Test: 33.67%
Epoch: 50, Loss: 1.5359, Train: 29.92%, Valid: 29.67%, Test: 30.29%
Epoch: 75, Loss: 1.4653, Train: 31.61%, Valid: 31.36%, Test: 31.99%
Epoch: 100, Loss: 2.1116, Train: 29.90%, Valid: 29.66%, Test: 30.17%
Epoch: 125, Loss: 1.6134, Train: 29.20%, Valid: 29.01%, Test: 29.17%
Epoch: 150, Loss: 1.4413, Train: 36.73%, Valid: 36.41%, Test: 36.65%
Epoch: 175, Loss: 1.4585, Train: 35.70%, Valid: 35.52%, Test: 35.66%
Epoch: 200, Loss: 1.4467, Train: 33.40%, Valid: 33.18%, Test: 33.44%
Epoch: 225, Loss: 1.4452, Train: 37.86%, Valid: 37.66%, Test: 37.85%
Epoch: 250, Loss: 1.4305, Train: 34.17%, Valid: 34.05%, Test: 33.96%
Epoch: 275, Loss: 1.4687, Train: 32.81%, Valid: 32.77%, Test: 32.52%
Epoch: 300, Loss: 1.3818, Train: 40.82%, Valid: 40.21%, Test: 40.44%
Epoch: 325, Loss: 1.3991, Train: 38.89%, Valid: 38.67%, Test: 38.64%
Epoch: 350, Loss: 1.4279, Train: 36.66%, Valid: 36.45%, Test: 36.36%
Epoch: 375, Loss: 1.4211, Train: 41.07%, Valid: 40.52%, Test: 40.64%
Epoch: 400, Loss: 1.3925, Train: 39.00%, Valid: 38.73%, Test: 38.94%
Epoch: 425, Loss: 1.4856, Train: 33.48%, Valid: 33.44%, Test: 33.43%
Epoch: 450, Loss: 1.9070, Train: 28.80%, Valid: 28.64%, Test: 28.89%
Epoch: 475, Loss: 1.5304, Train: 32.58%, Valid: 32.47%, Test: 32.53%
Epoch: 500, Loss: 1.4545, Train: 30.11%, Valid: 29.84%, Test: 30.06%
Epoch: 525, Loss: 1.4974, Train: 38.35%, Valid: 38.31%, Test: 38.52%
Epoch: 550, Loss: 1.4203, Train: 39.31%, Valid: 38.97%, Test: 39.02%
Epoch: 575, Loss: 1.4498, Train: 37.82%, Valid: 37.35%, Test: 37.77%
Epoch: 600, Loss: 1.4191, Train: 39.24%, Valid: 39.04%, Test: 39.24%
Epoch: 625, Loss: 1.4342, Train: 39.87%, Valid: 39.53%, Test: 39.47%
Epoch: 650, Loss: 1.4045, Train: 41.04%, Valid: 40.49%, Test: 40.83%
Epoch: 675, Loss: 1.4159, Train: 41.03%, Valid: 40.53%, Test: 40.65%
Epoch: 700, Loss: 1.4556, Train: 38.60%, Valid: 38.17%, Test: 38.42%
Epoch: 725, Loss: 1.4171, Train: 39.95%, Valid: 39.47%, Test: 39.66%
Epoch: 750, Loss: 1.3967, Train: 40.64%, Valid: 40.26%, Test: 40.35%
Epoch: 775, Loss: 1.4026, Train: 38.68%, Valid: 38.37%, Test: 38.41%
Epoch: 800, Loss: 1.4385, Train: 37.63%, Valid: 37.30%, Test: 37.53%
Epoch: 825, Loss: 1.3975, Train: 40.18%, Valid: 39.64%, Test: 39.95%
Epoch: 850, Loss: 1.3988, Train: 37.50%, Valid: 37.15%, Test: 37.58%
Epoch: 875, Loss: 1.3850, Train: 41.82%, Valid: 41.19%, Test: 41.37%
Epoch: 900, Loss: 4.9521, Train: 34.32%, Valid: 34.10%, Test: 34.51%
Epoch: 925, Loss: 1.6829, Train: 32.14%, Valid: 31.95%, Test: 32.01%
Epoch: 950, Loss: 1.5009, Train: 36.02%, Valid: 35.67%, Test: 35.85%
Epoch: 975, Loss: 1.4846, Train: 32.95%, Valid: 32.53%, Test: 32.99%
Run 01:
Highest Train: 42.77
Highest Valid: 42.40
  Final Train: 42.75
   Final Test: 42.27
All runs:
Highest Train: 42.77, nan
Highest Valid: 42.40, nan
  Final Train: 42.75, nan
   Final Test: 42.27, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.4, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7825, Train: 21.02%, Valid: 21.11%, Test: 21.47%
Epoch: 25, Loss: 1.5967, Train: 30.09%, Valid: 30.11%, Test: 30.77%
Epoch: 50, Loss: 1.4667, Train: 26.61%, Valid: 26.60%, Test: 26.84%
Epoch: 75, Loss: 1.5030, Train: 38.01%, Valid: 37.57%, Test: 37.87%
Epoch: 100, Loss: 1.4066, Train: 38.51%, Valid: 38.08%, Test: 38.25%
Epoch: 125, Loss: 1.3868, Train: 40.25%, Valid: 39.46%, Test: 40.01%
Epoch: 150, Loss: 2.2599, Train: 26.48%, Valid: 26.14%, Test: 26.32%
Epoch: 175, Loss: 1.4641, Train: 37.43%, Valid: 37.23%, Test: 37.66%
Epoch: 200, Loss: 1.4383, Train: 38.43%, Valid: 37.75%, Test: 38.42%
Epoch: 225, Loss: 1.4939, Train: 38.55%, Valid: 38.08%, Test: 38.46%
Epoch: 250, Loss: 1.4233, Train: 35.91%, Valid: 35.76%, Test: 35.68%
Epoch: 275, Loss: 1.3853, Train: 41.29%, Valid: 40.49%, Test: 41.00%
Epoch: 300, Loss: 1.3949, Train: 41.27%, Valid: 40.53%, Test: 40.86%
Epoch: 325, Loss: 1.3706, Train: 41.83%, Valid: 41.15%, Test: 41.31%
Epoch: 350, Loss: 1.4249, Train: 40.57%, Valid: 39.91%, Test: 40.65%
Epoch: 375, Loss: 1.4351, Train: 38.81%, Valid: 38.43%, Test: 38.53%
Epoch: 400, Loss: 1.3739, Train: 40.81%, Valid: 40.08%, Test: 40.33%
Epoch: 425, Loss: 1.3596, Train: 42.67%, Valid: 41.82%, Test: 42.06%
Epoch: 450, Loss: 1.5880, Train: 39.59%, Valid: 38.93%, Test: 39.08%
Epoch: 475, Loss: 1.3831, Train: 38.87%, Valid: 38.17%, Test: 38.54%
Epoch: 500, Loss: 1.3678, Train: 42.44%, Valid: 41.62%, Test: 41.84%
Epoch: 525, Loss: 1.3571, Train: 42.65%, Valid: 41.77%, Test: 42.03%
Epoch: 550, Loss: 1.3546, Train: 41.86%, Valid: 40.89%, Test: 41.21%
Epoch: 575, Loss: 1.3607, Train: 41.39%, Valid: 40.84%, Test: 40.91%
Epoch: 600, Loss: 1.3633, Train: 42.96%, Valid: 41.69%, Test: 42.15%
Epoch: 625, Loss: 1.3405, Train: 41.80%, Valid: 40.92%, Test: 40.82%
Epoch: 650, Loss: 1.3232, Train: 42.01%, Valid: 41.14%, Test: 41.32%
Epoch: 675, Loss: 1.6633, Train: 37.84%, Valid: 37.05%, Test: 37.64%
Epoch: 700, Loss: 1.3744, Train: 39.61%, Valid: 38.96%, Test: 39.14%
Epoch: 725, Loss: 1.3425, Train: 43.11%, Valid: 41.94%, Test: 42.11%
Epoch: 750, Loss: 1.3233, Train: 43.43%, Valid: 42.15%, Test: 42.50%
Epoch: 775, Loss: 1.3391, Train: 43.62%, Valid: 42.48%, Test: 42.59%
Epoch: 800, Loss: 1.3099, Train: 42.41%, Valid: 41.05%, Test: 41.46%
Epoch: 825, Loss: 1.3313, Train: 43.91%, Valid: 42.61%, Test: 42.90%
Epoch: 850, Loss: 1.3437, Train: 39.83%, Valid: 38.79%, Test: 39.30%
Epoch: 875, Loss: 1.3161, Train: 43.67%, Valid: 42.46%, Test: 42.75%
Epoch: 900, Loss: 1.3409, Train: 43.76%, Valid: 42.34%, Test: 42.86%
Epoch: 925, Loss: 1.3534, Train: 42.26%, Valid: 41.15%, Test: 41.22%
Epoch: 950, Loss: 1.3700, Train: 42.08%, Valid: 40.75%, Test: 41.25%
Epoch: 975, Loss: 1.3323, Train: 42.60%, Valid: 41.55%, Test: 41.65%
Run 01:
Highest Train: 44.64
Highest Valid: 43.63
  Final Train: 44.64
   Final Test: 43.73
All runs:
Highest Train: 44.64, nan
Highest Valid: 43.63, nan
  Final Train: 44.64, nan
   Final Test: 43.73, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7947, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.5508, Train: 32.22%, Valid: 31.95%, Test: 32.30%
Epoch: 50, Loss: 1.4710, Train: 34.23%, Valid: 34.02%, Test: 34.30%
Epoch: 75, Loss: 1.5070, Train: 34.60%, Valid: 34.33%, Test: 34.48%
Epoch: 100, Loss: 1.4733, Train: 36.24%, Valid: 35.85%, Test: 36.03%
Epoch: 125, Loss: 1.4395, Train: 38.59%, Valid: 38.07%, Test: 38.57%
Epoch: 150, Loss: 1.4882, Train: 35.72%, Valid: 35.35%, Test: 35.62%
Epoch: 175, Loss: 1.4863, Train: 35.90%, Valid: 35.72%, Test: 35.87%
Epoch: 200, Loss: 1.4814, Train: 35.84%, Valid: 35.67%, Test: 35.79%
Epoch: 225, Loss: 1.4913, Train: 34.59%, Valid: 34.30%, Test: 34.79%
Epoch: 250, Loss: 1.4650, Train: 37.97%, Valid: 37.54%, Test: 37.78%
Epoch: 275, Loss: 1.5040, Train: 32.98%, Valid: 32.75%, Test: 32.70%
Epoch: 300, Loss: 1.4414, Train: 38.47%, Valid: 37.66%, Test: 38.18%
Epoch: 325, Loss: 1.5771, Train: 33.67%, Valid: 33.57%, Test: 33.85%
Epoch: 350, Loss: 1.4905, Train: 35.53%, Valid: 35.16%, Test: 35.51%
Epoch: 375, Loss: 1.4748, Train: 36.12%, Valid: 35.60%, Test: 36.11%
Epoch: 400, Loss: 1.5274, Train: 32.92%, Valid: 32.93%, Test: 33.04%
Epoch: 425, Loss: 1.4767, Train: 36.30%, Valid: 35.76%, Test: 35.99%
Epoch: 450, Loss: 1.5234, Train: 30.47%, Valid: 29.97%, Test: 30.48%
Epoch: 475, Loss: 1.4613, Train: 37.21%, Valid: 36.54%, Test: 36.84%
Epoch: 500, Loss: 1.4724, Train: 38.69%, Valid: 38.02%, Test: 38.14%
Epoch: 525, Loss: 1.4678, Train: 30.74%, Valid: 30.39%, Test: 30.63%
Epoch: 550, Loss: 1.5077, Train: 33.75%, Valid: 33.17%, Test: 33.12%
Epoch: 575, Loss: 1.5991, Train: 35.57%, Valid: 34.98%, Test: 35.16%
Epoch: 600, Loss: 1.4754, Train: 36.46%, Valid: 35.71%, Test: 35.96%
Epoch: 625, Loss: 1.4689, Train: 36.58%, Valid: 35.85%, Test: 36.01%
Epoch: 650, Loss: 1.4769, Train: 37.09%, Valid: 36.07%, Test: 36.84%
Epoch: 675, Loss: 1.4653, Train: 36.63%, Valid: 35.79%, Test: 36.14%
Epoch: 700, Loss: 1.4769, Train: 37.06%, Valid: 36.23%, Test: 36.70%
Epoch: 725, Loss: 1.4950, Train: 36.57%, Valid: 35.75%, Test: 36.21%
Epoch: 750, Loss: 1.4386, Train: 37.03%, Valid: 36.12%, Test: 36.40%
Epoch: 775, Loss: 1.4714, Train: 38.32%, Valid: 37.75%, Test: 37.95%
Epoch: 800, Loss: 1.4666, Train: 38.12%, Valid: 37.30%, Test: 37.53%
Epoch: 825, Loss: 1.4872, Train: 33.40%, Valid: 32.80%, Test: 33.11%
Epoch: 850, Loss: 1.5557, Train: 38.41%, Valid: 37.65%, Test: 37.74%
Epoch: 875, Loss: 1.5859, Train: 36.15%, Valid: 35.75%, Test: 35.77%
Epoch: 900, Loss: 1.4987, Train: 35.77%, Valid: 35.25%, Test: 35.57%
Epoch: 925, Loss: 1.4731, Train: 36.43%, Valid: 35.76%, Test: 36.04%
Epoch: 950, Loss: 1.4586, Train: 36.99%, Valid: 36.11%, Test: 36.54%
Epoch: 975, Loss: 1.4957, Train: 35.18%, Valid: 34.73%, Test: 34.86%
Run 01:
Highest Train: 40.56
Highest Valid: 39.68
  Final Train: 40.56
   Final Test: 39.83
All runs:
Highest Train: 40.56, nan
Highest Valid: 39.68, nan
  Final Train: 40.56, nan
   Final Test: 39.83, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.8433, Train: 21.69%, Valid: 21.77%, Test: 22.40%
Epoch: 25, Loss: 1.5348, Train: 25.18%, Valid: 25.13%, Test: 24.85%
Epoch: 50, Loss: 1.5155, Train: 32.40%, Valid: 32.28%, Test: 32.09%
Epoch: 75, Loss: 1.4410, Train: 34.65%, Valid: 34.34%, Test: 34.55%
Epoch: 100, Loss: 1.4647, Train: 36.16%, Valid: 35.70%, Test: 36.07%
Epoch: 125, Loss: 1.4305, Train: 39.30%, Valid: 38.78%, Test: 39.04%
Epoch: 150, Loss: 1.4111, Train: 38.47%, Valid: 37.99%, Test: 38.27%
Epoch: 175, Loss: 1.4109, Train: 37.19%, Valid: 36.86%, Test: 37.25%
Epoch: 200, Loss: 1.4109, Train: 40.33%, Valid: 39.68%, Test: 39.97%
Epoch: 225, Loss: 1.4445, Train: 38.32%, Valid: 37.90%, Test: 37.96%
Epoch: 250, Loss: 1.7768, Train: 33.10%, Valid: 33.05%, Test: 33.08%
Epoch: 275, Loss: 1.5043, Train: 34.66%, Valid: 34.45%, Test: 34.77%
Epoch: 300, Loss: 1.4757, Train: 36.22%, Valid: 35.47%, Test: 35.98%
Epoch: 325, Loss: 1.4799, Train: 38.34%, Valid: 37.77%, Test: 38.10%
Epoch: 350, Loss: 1.4425, Train: 39.13%, Valid: 38.57%, Test: 39.01%
Epoch: 375, Loss: 1.4994, Train: 36.22%, Valid: 35.96%, Test: 36.19%
Epoch: 400, Loss: 1.4663, Train: 38.35%, Valid: 37.66%, Test: 38.02%
Epoch: 425, Loss: 1.4088, Train: 39.27%, Valid: 38.90%, Test: 39.00%
Epoch: 450, Loss: 1.6511, Train: 26.38%, Valid: 26.12%, Test: 26.34%
Epoch: 475, Loss: 1.5038, Train: 32.27%, Valid: 31.87%, Test: 32.25%
Epoch: 500, Loss: 1.4928, Train: 34.92%, Valid: 34.72%, Test: 35.02%
Epoch: 525, Loss: 1.4690, Train: 34.64%, Valid: 34.32%, Test: 34.66%
Epoch: 550, Loss: 1.5190, Train: 31.59%, Valid: 31.35%, Test: 31.71%
Epoch: 575, Loss: 1.4730, Train: 37.40%, Valid: 36.94%, Test: 37.21%
Epoch: 600, Loss: 1.4942, Train: 38.61%, Valid: 37.94%, Test: 38.43%
Epoch: 625, Loss: 1.4778, Train: 38.00%, Valid: 37.36%, Test: 37.61%
Epoch: 650, Loss: 1.4355, Train: 36.07%, Valid: 35.86%, Test: 36.12%
Epoch: 675, Loss: 1.5505, Train: 30.06%, Valid: 29.78%, Test: 30.12%
Epoch: 700, Loss: 1.4614, Train: 37.42%, Valid: 36.85%, Test: 37.11%
Epoch: 725, Loss: 1.5057, Train: 38.66%, Valid: 38.17%, Test: 38.36%
Epoch: 750, Loss: 1.5473, Train: 35.16%, Valid: 34.92%, Test: 35.18%
Epoch: 775, Loss: 1.4405, Train: 36.69%, Valid: 36.24%, Test: 36.91%
Epoch: 800, Loss: 1.4804, Train: 39.67%, Valid: 39.02%, Test: 39.16%
Epoch: 825, Loss: 1.4703, Train: 32.82%, Valid: 32.14%, Test: 32.40%
Epoch: 850, Loss: 1.4313, Train: 36.18%, Valid: 35.70%, Test: 35.87%
Epoch: 875, Loss: 1.4335, Train: 39.20%, Valid: 38.49%, Test: 38.72%
Epoch: 900, Loss: 2.3388, Train: 35.22%, Valid: 34.91%, Test: 35.07%
Epoch: 925, Loss: 1.5277, Train: 37.78%, Valid: 37.40%, Test: 37.78%
Epoch: 950, Loss: 1.4596, Train: 37.63%, Valid: 37.39%, Test: 37.55%
Epoch: 975, Loss: 1.5057, Train: 38.16%, Valid: 37.51%, Test: 38.22%
Run 01:
Highest Train: 41.28
Highest Valid: 40.55
  Final Train: 41.28
   Final Test: 40.92
All runs:
Highest Train: 41.28, nan
Highest Valid: 40.55, nan
  Final Train: 41.28, nan
   Final Test: 40.92, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.8055, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.5289, Train: 31.35%, Valid: 31.30%, Test: 31.71%
Epoch: 50, Loss: 1.5157, Train: 31.57%, Valid: 31.08%, Test: 31.80%
Epoch: 75, Loss: 1.3985, Train: 39.56%, Valid: 39.20%, Test: 39.68%
Epoch: 100, Loss: 1.3901, Train: 41.16%, Valid: 40.77%, Test: 41.22%
Epoch: 125, Loss: 1.4480, Train: 34.96%, Valid: 34.45%, Test: 35.03%
Epoch: 150, Loss: 1.4116, Train: 38.85%, Valid: 38.64%, Test: 38.75%
Epoch: 175, Loss: 1.3958, Train: 37.99%, Valid: 37.77%, Test: 37.90%
Epoch: 200, Loss: 1.3827, Train: 41.44%, Valid: 40.93%, Test: 41.14%
Epoch: 225, Loss: 1.4273, Train: 32.82%, Valid: 32.58%, Test: 32.68%
Epoch: 250, Loss: 1.6120, Train: 38.61%, Valid: 38.22%, Test: 38.50%
Epoch: 275, Loss: 1.4581, Train: 33.61%, Valid: 33.28%, Test: 33.26%
Epoch: 300, Loss: 1.4204, Train: 36.60%, Valid: 36.22%, Test: 36.47%
Epoch: 325, Loss: 2.3849, Train: 28.75%, Valid: 28.54%, Test: 28.81%
Epoch: 350, Loss: 1.5171, Train: 34.35%, Valid: 33.88%, Test: 34.68%
Epoch: 375, Loss: 1.6758, Train: 28.73%, Valid: 28.55%, Test: 28.82%
Epoch: 400, Loss: 1.4728, Train: 37.36%, Valid: 36.97%, Test: 37.35%
Epoch: 425, Loss: 1.4415, Train: 35.74%, Valid: 35.42%, Test: 35.71%
Epoch: 450, Loss: 1.4460, Train: 39.14%, Valid: 38.82%, Test: 39.23%
Epoch: 475, Loss: 1.3996, Train: 40.40%, Valid: 39.62%, Test: 40.01%
Epoch: 500, Loss: 1.3718, Train: 40.43%, Valid: 39.72%, Test: 40.07%
Epoch: 525, Loss: 1.4063, Train: 38.86%, Valid: 38.16%, Test: 38.35%
Epoch: 550, Loss: 1.3797, Train: 40.06%, Valid: 39.48%, Test: 39.60%
Epoch: 575, Loss: 1.3635, Train: 40.78%, Valid: 40.34%, Test: 40.33%
Epoch: 600, Loss: 1.3484, Train: 41.89%, Valid: 41.02%, Test: 41.47%
Epoch: 625, Loss: 1.3763, Train: 41.03%, Valid: 40.42%, Test: 40.61%
Epoch: 650, Loss: 1.3557, Train: 43.23%, Valid: 42.56%, Test: 42.71%
Epoch: 675, Loss: 1.3485, Train: 40.48%, Valid: 39.83%, Test: 40.06%
Epoch: 700, Loss: 1.3663, Train: 41.41%, Valid: 40.64%, Test: 40.97%
Epoch: 725, Loss: 1.5987, Train: 35.61%, Valid: 35.21%, Test: 35.31%
Epoch: 750, Loss: 1.3744, Train: 41.48%, Valid: 40.85%, Test: 41.28%
Epoch: 775, Loss: 1.3522, Train: 42.51%, Valid: 41.63%, Test: 41.66%
Epoch: 800, Loss: 1.3585, Train: 41.82%, Valid: 40.94%, Test: 41.15%
Epoch: 825, Loss: 1.3785, Train: 39.70%, Valid: 39.03%, Test: 39.06%
Epoch: 850, Loss: 1.4722, Train: 39.48%, Valid: 38.99%, Test: 39.08%
Epoch: 875, Loss: 1.3851, Train: 39.35%, Valid: 38.84%, Test: 39.27%
Epoch: 900, Loss: 1.3542, Train: 41.48%, Valid: 40.78%, Test: 41.10%
Epoch: 925, Loss: 1.4769, Train: 35.66%, Valid: 35.04%, Test: 35.10%
Epoch: 950, Loss: 1.3606, Train: 42.27%, Valid: 41.33%, Test: 41.78%
Epoch: 975, Loss: 1.3739, Train: 42.08%, Valid: 41.07%, Test: 41.37%
Run 01:
Highest Train: 43.80
Highest Valid: 43.07
  Final Train: 43.80
   Final Test: 43.14
All runs:
Highest Train: 43.80, nan
Highest Valid: 43.07, nan
  Final Train: 43.80, nan
   Final Test: 43.14, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.8077, Train: 20.31%, Valid: 20.23%, Test: 20.37%
Epoch: 25, Loss: 1.5608, Train: 32.95%, Valid: 32.65%, Test: 33.19%
Epoch: 50, Loss: 1.4958, Train: 34.10%, Valid: 33.90%, Test: 33.95%
Epoch: 75, Loss: 1.4920, Train: 35.54%, Valid: 35.02%, Test: 35.49%
Epoch: 100, Loss: 1.4843, Train: 36.18%, Valid: 35.78%, Test: 36.13%
Epoch: 125, Loss: 1.4883, Train: 35.73%, Valid: 35.46%, Test: 35.73%
Epoch: 150, Loss: 1.4662, Train: 36.25%, Valid: 36.04%, Test: 36.63%
Epoch: 175, Loss: 1.5248, Train: 36.37%, Valid: 36.04%, Test: 36.41%
Epoch: 200, Loss: 1.5161, Train: 28.37%, Valid: 28.41%, Test: 29.06%
Epoch: 225, Loss: 1.6159, Train: 34.10%, Valid: 33.71%, Test: 34.28%
Epoch: 250, Loss: 1.4968, Train: 35.34%, Valid: 35.11%, Test: 35.33%
Epoch: 275, Loss: 1.4952, Train: 35.76%, Valid: 35.51%, Test: 35.86%
Epoch: 300, Loss: 1.4713, Train: 35.15%, Valid: 35.06%, Test: 35.46%
Epoch: 325, Loss: 1.4540, Train: 36.69%, Valid: 36.31%, Test: 36.55%
Epoch: 350, Loss: 1.8824, Train: 18.97%, Valid: 18.93%, Test: 18.69%
Epoch: 375, Loss: 1.4980, Train: 35.58%, Valid: 35.12%, Test: 35.30%
Epoch: 400, Loss: 1.4799, Train: 36.26%, Valid: 35.52%, Test: 35.93%
Epoch: 425, Loss: 1.4631, Train: 36.84%, Valid: 36.15%, Test: 36.43%
Epoch: 450, Loss: 1.4635, Train: 37.48%, Valid: 37.15%, Test: 37.14%
Epoch: 475, Loss: 1.5016, Train: 30.74%, Valid: 30.38%, Test: 30.63%
Epoch: 500, Loss: 1.4682, Train: 36.91%, Valid: 36.20%, Test: 36.47%
Epoch: 525, Loss: 1.4579, Train: 38.76%, Valid: 38.09%, Test: 38.21%
Epoch: 550, Loss: 1.4725, Train: 37.80%, Valid: 37.05%, Test: 37.40%
Epoch: 575, Loss: 1.4610, Train: 37.81%, Valid: 37.17%, Test: 37.31%
Epoch: 600, Loss: 1.4365, Train: 39.07%, Valid: 38.89%, Test: 39.06%
Epoch: 625, Loss: 1.4483, Train: 37.61%, Valid: 36.88%, Test: 37.15%
Epoch: 650, Loss: 1.4627, Train: 37.32%, Valid: 36.57%, Test: 37.09%
Epoch: 675, Loss: 1.4991, Train: 36.97%, Valid: 36.49%, Test: 36.66%
Epoch: 700, Loss: 1.4610, Train: 35.51%, Valid: 35.07%, Test: 35.36%
Epoch: 725, Loss: 1.4433, Train: 35.85%, Valid: 35.67%, Test: 35.67%
Epoch: 750, Loss: 1.5710, Train: 36.04%, Valid: 35.59%, Test: 35.85%
Epoch: 775, Loss: 1.4837, Train: 37.25%, Valid: 36.48%, Test: 36.64%
Epoch: 800, Loss: 1.4704, Train: 37.08%, Valid: 35.99%, Test: 36.50%
Epoch: 825, Loss: 2.5379, Train: 23.95%, Valid: 23.54%, Test: 23.53%
Epoch: 850, Loss: 1.4958, Train: 35.79%, Valid: 35.43%, Test: 35.82%
Epoch: 875, Loss: 1.4509, Train: 38.70%, Valid: 37.74%, Test: 38.07%
Epoch: 900, Loss: 1.5116, Train: 29.79%, Valid: 29.44%, Test: 29.86%
Epoch: 925, Loss: 1.4853, Train: 37.54%, Valid: 36.93%, Test: 37.21%
Epoch: 950, Loss: 1.5148, Train: 36.39%, Valid: 35.97%, Test: 36.20%
Epoch: 975, Loss: 1.5563, Train: 35.75%, Valid: 35.35%, Test: 35.57%
Run 01:
Highest Train: 40.58
Highest Valid: 40.07
  Final Train: 40.58
   Final Test: 40.23
All runs:
Highest Train: 40.58, nan
Highest Valid: 40.07, nan
  Final Train: 40.58, nan
   Final Test: 40.23, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.8231, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.5259, Train: 33.80%, Valid: 33.70%, Test: 34.05%
Epoch: 50, Loss: 1.4564, Train: 36.40%, Valid: 35.94%, Test: 36.29%
Epoch: 75, Loss: 1.8844, Train: 19.11%, Valid: 19.05%, Test: 18.79%
Epoch: 100, Loss: 1.6012, Train: 33.44%, Valid: 33.16%, Test: 33.46%
Epoch: 125, Loss: 1.5045, Train: 34.51%, Valid: 34.43%, Test: 34.59%
Epoch: 150, Loss: 1.4633, Train: 36.79%, Valid: 36.38%, Test: 36.78%
Epoch: 175, Loss: 1.4830, Train: 35.41%, Valid: 35.18%, Test: 35.48%
Epoch: 200, Loss: 1.4597, Train: 37.94%, Valid: 37.48%, Test: 37.74%
Epoch: 225, Loss: 1.4464, Train: 39.69%, Valid: 39.29%, Test: 39.67%
Epoch: 250, Loss: 1.4541, Train: 38.32%, Valid: 38.01%, Test: 38.11%
Epoch: 275, Loss: 1.4460, Train: 36.08%, Valid: 35.85%, Test: 36.01%
Epoch: 300, Loss: 1.4179, Train: 39.90%, Valid: 39.45%, Test: 39.78%
Epoch: 325, Loss: 1.6571, Train: 21.45%, Valid: 21.59%, Test: 21.41%
Epoch: 350, Loss: 1.4745, Train: 37.30%, Valid: 36.70%, Test: 37.03%
Epoch: 375, Loss: 1.4559, Train: 38.45%, Valid: 38.02%, Test: 38.25%
Epoch: 400, Loss: 1.4418, Train: 35.86%, Valid: 35.53%, Test: 36.00%
Epoch: 425, Loss: 1.4258, Train: 38.84%, Valid: 38.24%, Test: 38.44%
Epoch: 450, Loss: 1.4343, Train: 40.18%, Valid: 39.73%, Test: 39.91%
Epoch: 475, Loss: 1.4605, Train: 36.98%, Valid: 36.34%, Test: 36.37%
Epoch: 500, Loss: 1.4764, Train: 36.35%, Valid: 36.05%, Test: 36.41%
Epoch: 525, Loss: 1.4387, Train: 39.09%, Valid: 38.25%, Test: 38.43%
Epoch: 550, Loss: 1.4738, Train: 37.62%, Valid: 37.10%, Test: 37.36%
Epoch: 575, Loss: 1.4144, Train: 38.02%, Valid: 37.51%, Test: 37.67%
Epoch: 600, Loss: 2.7531, Train: 36.25%, Valid: 35.93%, Test: 36.03%
Epoch: 625, Loss: 1.4759, Train: 33.78%, Valid: 33.43%, Test: 33.29%
Epoch: 650, Loss: 3.0226, Train: 31.49%, Valid: 31.28%, Test: 31.34%
Epoch: 675, Loss: 1.7329, Train: 34.97%, Valid: 34.41%, Test: 34.77%
Epoch: 700, Loss: 1.4756, Train: 36.38%, Valid: 35.86%, Test: 36.25%
Epoch: 725, Loss: 1.4471, Train: 36.64%, Valid: 36.20%, Test: 36.62%
Epoch: 750, Loss: 1.4345, Train: 37.59%, Valid: 36.99%, Test: 37.39%
Epoch: 775, Loss: 1.4613, Train: 39.03%, Valid: 38.32%, Test: 38.60%
Epoch: 800, Loss: 1.4351, Train: 38.24%, Valid: 37.74%, Test: 38.08%
Epoch: 825, Loss: 1.4536, Train: 36.04%, Valid: 35.90%, Test: 36.08%
Epoch: 850, Loss: 1.4121, Train: 39.69%, Valid: 39.00%, Test: 39.50%
Epoch: 875, Loss: 1.4365, Train: 36.11%, Valid: 35.69%, Test: 36.01%
Epoch: 900, Loss: 1.4286, Train: 39.69%, Valid: 38.99%, Test: 39.15%
Epoch: 925, Loss: 1.4049, Train: 39.87%, Valid: 39.44%, Test: 39.69%
Epoch: 950, Loss: 1.4985, Train: 31.26%, Valid: 31.02%, Test: 31.18%
Epoch: 975, Loss: 1.4084, Train: 38.30%, Valid: 37.71%, Test: 37.95%
Run 01:
Highest Train: 41.84
Highest Valid: 41.07
  Final Train: 41.84
   Final Test: 41.39
All runs:
Highest Train: 41.84, nan
Highest Valid: 41.07, nan
  Final Train: 41.84, nan
   Final Test: 41.39, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.8276, Train: 17.34%, Valid: 17.23%, Test: 16.99%
Epoch: 25, Loss: 1.5123, Train: 30.81%, Valid: 30.21%, Test: 30.47%
Epoch: 50, Loss: 1.4428, Train: 37.40%, Valid: 37.28%, Test: 37.79%
Epoch: 75, Loss: 1.4357, Train: 38.78%, Valid: 38.40%, Test: 38.65%
Epoch: 100, Loss: 1.4133, Train: 40.12%, Valid: 39.71%, Test: 39.98%
Epoch: 125, Loss: 1.4288, Train: 38.41%, Valid: 37.84%, Test: 38.09%
Epoch: 150, Loss: 1.6332, Train: 28.84%, Valid: 28.69%, Test: 28.93%
Epoch: 175, Loss: 1.5167, Train: 35.05%, Valid: 34.83%, Test: 35.12%
Epoch: 200, Loss: 1.3914, Train: 34.74%, Valid: 34.18%, Test: 34.08%
Epoch: 225, Loss: 1.3853, Train: 41.63%, Valid: 40.88%, Test: 41.29%
Epoch: 250, Loss: 1.4031, Train: 36.25%, Valid: 35.55%, Test: 36.23%
Epoch: 275, Loss: 1.3675, Train: 39.58%, Valid: 39.22%, Test: 39.29%
Epoch: 300, Loss: 1.3572, Train: 41.68%, Valid: 41.16%, Test: 41.30%
Epoch: 325, Loss: 1.3945, Train: 39.14%, Valid: 38.07%, Test: 38.28%
Epoch: 350, Loss: 1.4230, Train: 41.83%, Valid: 41.26%, Test: 41.48%
Epoch: 375, Loss: 1.3737, Train: 41.81%, Valid: 41.21%, Test: 41.09%
Epoch: 400, Loss: 1.3671, Train: 41.84%, Valid: 41.21%, Test: 41.43%
Epoch: 425, Loss: 1.3766, Train: 41.23%, Valid: 40.49%, Test: 40.58%
Epoch: 450, Loss: 1.3449, Train: 42.24%, Valid: 41.55%, Test: 41.63%
Epoch: 475, Loss: 1.3592, Train: 41.85%, Valid: 40.98%, Test: 41.19%
Epoch: 500, Loss: 1.5915, Train: 22.39%, Valid: 22.06%, Test: 22.19%
Epoch: 525, Loss: 1.5268, Train: 36.13%, Valid: 35.80%, Test: 36.20%
Epoch: 550, Loss: 1.4344, Train: 38.71%, Valid: 38.28%, Test: 38.65%
Epoch: 575, Loss: 1.3864, Train: 40.50%, Valid: 39.90%, Test: 40.21%
Epoch: 600, Loss: 1.3632, Train: 38.68%, Valid: 38.43%, Test: 38.51%
Epoch: 625, Loss: 1.3959, Train: 40.89%, Valid: 40.03%, Test: 40.37%
Epoch: 650, Loss: 1.4922, Train: 32.60%, Valid: 32.72%, Test: 32.57%
Epoch: 675, Loss: 1.3949, Train: 39.46%, Valid: 38.69%, Test: 39.28%
Epoch: 700, Loss: 1.3527, Train: 40.91%, Valid: 40.25%, Test: 40.53%
Epoch: 725, Loss: 1.3739, Train: 41.57%, Valid: 40.59%, Test: 40.91%
Epoch: 750, Loss: 1.3598, Train: 42.84%, Valid: 41.68%, Test: 42.21%
Epoch: 775, Loss: 1.3489, Train: 42.77%, Valid: 41.91%, Test: 42.38%
Epoch: 800, Loss: 1.3501, Train: 41.34%, Valid: 40.62%, Test: 40.72%
Epoch: 825, Loss: 1.3832, Train: 40.17%, Valid: 39.79%, Test: 40.03%
Epoch: 850, Loss: 1.4255, Train: 36.78%, Valid: 36.79%, Test: 36.48%
Epoch: 875, Loss: 1.5267, Train: 29.60%, Valid: 29.37%, Test: 29.66%
Epoch: 900, Loss: 1.3745, Train: 40.81%, Valid: 40.07%, Test: 40.28%
Epoch: 925, Loss: 1.3639, Train: 42.88%, Valid: 42.00%, Test: 42.17%
Epoch: 950, Loss: 1.3685, Train: 42.27%, Valid: 41.45%, Test: 41.84%
Epoch: 975, Loss: 1.3357, Train: 43.24%, Valid: 42.61%, Test: 42.43%
Run 01:
Highest Train: 43.63
Highest Valid: 42.72
  Final Train: 43.63
   Final Test: 42.83
All runs:
Highest Train: 43.63, nan
Highest Valid: 42.72, nan
  Final Train: 43.63, nan
   Final Test: 42.83, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7791, Train: 12.51%, Valid: 12.57%, Test: 12.42%
Epoch: 25, Loss: 1.5527, Train: 33.46%, Valid: 33.17%, Test: 33.67%
Epoch: 50, Loss: 1.5071, Train: 34.63%, Valid: 34.36%, Test: 34.49%
Epoch: 75, Loss: 1.4782, Train: 34.49%, Valid: 34.22%, Test: 34.61%
Epoch: 100, Loss: 1.4801, Train: 36.96%, Valid: 36.49%, Test: 36.84%
Epoch: 125, Loss: 1.5311, Train: 35.12%, Valid: 34.73%, Test: 34.99%
Epoch: 150, Loss: 1.4873, Train: 35.52%, Valid: 35.11%, Test: 35.43%
Epoch: 175, Loss: 1.4741, Train: 35.93%, Valid: 35.48%, Test: 35.74%
Epoch: 200, Loss: 1.4951, Train: 35.93%, Valid: 35.39%, Test: 35.63%
Epoch: 225, Loss: 1.4737, Train: 36.32%, Valid: 35.80%, Test: 36.09%
Epoch: 250, Loss: 1.4840, Train: 35.54%, Valid: 34.94%, Test: 35.16%
Epoch: 275, Loss: 1.4600, Train: 36.60%, Valid: 36.13%, Test: 36.20%
Epoch: 300, Loss: 1.4558, Train: 36.27%, Valid: 35.84%, Test: 35.98%
Epoch: 325, Loss: 1.6335, Train: 35.54%, Valid: 35.24%, Test: 35.39%
Epoch: 350, Loss: 1.4835, Train: 36.24%, Valid: 35.58%, Test: 36.17%
Epoch: 375, Loss: 1.4730, Train: 37.18%, Valid: 36.26%, Test: 36.44%
Epoch: 400, Loss: 1.4532, Train: 35.64%, Valid: 35.26%, Test: 35.58%
Epoch: 425, Loss: 1.5069, Train: 35.20%, Valid: 34.67%, Test: 34.91%
Epoch: 450, Loss: 1.4828, Train: 36.18%, Valid: 35.62%, Test: 36.04%
Epoch: 475, Loss: 1.4726, Train: 36.86%, Valid: 36.07%, Test: 36.52%
Epoch: 500, Loss: 1.4630, Train: 37.27%, Valid: 36.45%, Test: 36.66%
Epoch: 525, Loss: 1.4827, Train: 36.34%, Valid: 35.76%, Test: 36.12%
Epoch: 550, Loss: 1.4789, Train: 37.55%, Valid: 36.50%, Test: 36.84%
Epoch: 575, Loss: 1.4905, Train: 36.58%, Valid: 35.95%, Test: 36.39%
Epoch: 600, Loss: 1.5003, Train: 33.64%, Valid: 33.22%, Test: 33.40%
Epoch: 625, Loss: 1.4654, Train: 38.67%, Valid: 37.74%, Test: 38.15%
Epoch: 650, Loss: 1.4812, Train: 38.35%, Valid: 37.93%, Test: 38.02%
Epoch: 675, Loss: 1.4868, Train: 36.59%, Valid: 36.07%, Test: 36.35%
Epoch: 700, Loss: 1.4813, Train: 28.74%, Valid: 28.48%, Test: 27.91%
Epoch: 725, Loss: 1.5517, Train: 33.68%, Valid: 33.27%, Test: 33.56%
Epoch: 750, Loss: 1.5604, Train: 35.28%, Valid: 35.08%, Test: 35.29%
Epoch: 775, Loss: 1.5193, Train: 34.88%, Valid: 34.59%, Test: 34.94%
Epoch: 800, Loss: 1.4708, Train: 35.87%, Valid: 35.48%, Test: 35.87%
Epoch: 825, Loss: 1.4691, Train: 37.28%, Valid: 36.73%, Test: 37.01%
Epoch: 850, Loss: 1.4590, Train: 38.85%, Valid: 38.45%, Test: 38.75%
Epoch: 875, Loss: 1.5039, Train: 37.34%, Valid: 36.86%, Test: 37.10%
Epoch: 900, Loss: 1.5040, Train: 37.13%, Valid: 36.87%, Test: 36.99%
Epoch: 925, Loss: 1.4549, Train: 38.45%, Valid: 37.75%, Test: 37.87%
Epoch: 950, Loss: 1.5314, Train: 32.66%, Valid: 32.36%, Test: 32.49%
Epoch: 975, Loss: 1.4765, Train: 38.11%, Valid: 37.54%, Test: 37.82%
Run 01:
Highest Train: 40.22
Highest Valid: 39.62
  Final Train: 40.22
   Final Test: 39.70
All runs:
Highest Train: 40.22, nan
Highest Valid: 39.62, nan
  Final Train: 40.22, nan
   Final Test: 39.70, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7830, Train: 27.83%, Valid: 27.71%, Test: 27.96%
Epoch: 25, Loss: 1.5315, Train: 32.55%, Valid: 32.23%, Test: 32.71%
Epoch: 50, Loss: 1.4868, Train: 33.26%, Valid: 33.11%, Test: 33.33%
Epoch: 75, Loss: 1.4808, Train: 36.75%, Valid: 36.36%, Test: 36.56%
Epoch: 100, Loss: 1.4268, Train: 38.73%, Valid: 38.16%, Test: 38.60%
Epoch: 125, Loss: 1.4209, Train: 39.63%, Valid: 38.93%, Test: 39.32%
Epoch: 150, Loss: 1.4520, Train: 33.65%, Valid: 33.44%, Test: 33.66%
Epoch: 175, Loss: 1.4349, Train: 39.56%, Valid: 39.01%, Test: 39.34%
Epoch: 200, Loss: 1.4082, Train: 38.82%, Valid: 38.41%, Test: 38.58%
Epoch: 225, Loss: 1.3998, Train: 39.48%, Valid: 39.18%, Test: 39.13%
Epoch: 250, Loss: 2.5843, Train: 29.91%, Valid: 29.82%, Test: 30.34%
Epoch: 275, Loss: 1.5769, Train: 33.20%, Valid: 32.98%, Test: 33.23%
Epoch: 300, Loss: 1.4587, Train: 29.65%, Valid: 29.43%, Test: 29.87%
Epoch: 325, Loss: 1.4377, Train: 37.34%, Valid: 36.84%, Test: 37.33%
Epoch: 350, Loss: 1.4436, Train: 36.11%, Valid: 35.39%, Test: 36.28%
Epoch: 375, Loss: 1.4221, Train: 39.29%, Valid: 38.85%, Test: 39.08%
Epoch: 400, Loss: 4.1776, Train: 24.00%, Valid: 24.03%, Test: 23.83%
Epoch: 425, Loss: 1.5663, Train: 34.34%, Valid: 34.23%, Test: 34.69%
Epoch: 450, Loss: 1.4945, Train: 35.48%, Valid: 35.25%, Test: 35.46%
Epoch: 475, Loss: 1.4929, Train: 36.64%, Valid: 36.11%, Test: 36.55%
Epoch: 500, Loss: 1.4641, Train: 38.05%, Valid: 37.46%, Test: 37.63%
Epoch: 525, Loss: 1.4308, Train: 39.72%, Valid: 39.28%, Test: 39.28%
Epoch: 550, Loss: 1.4109, Train: 39.14%, Valid: 38.31%, Test: 38.83%
Epoch: 575, Loss: 1.3985, Train: 39.49%, Valid: 38.97%, Test: 39.14%
Epoch: 600, Loss: 1.4104, Train: 39.52%, Valid: 38.84%, Test: 39.22%
Epoch: 625, Loss: 1.3891, Train: 41.45%, Valid: 40.49%, Test: 40.92%
Epoch: 650, Loss: 1.4038, Train: 41.27%, Valid: 40.63%, Test: 40.72%
Epoch: 675, Loss: 1.3681, Train: 40.78%, Valid: 40.07%, Test: 40.23%
Epoch: 700, Loss: 1.3927, Train: 42.22%, Valid: 41.46%, Test: 41.71%
Epoch: 725, Loss: 1.3394, Train: 41.35%, Valid: 40.30%, Test: 40.69%
Epoch: 750, Loss: 1.3731, Train: 41.03%, Valid: 40.28%, Test: 40.86%
Epoch: 775, Loss: 1.3978, Train: 41.33%, Valid: 40.52%, Test: 41.02%
Epoch: 800, Loss: 1.4308, Train: 41.93%, Valid: 41.28%, Test: 41.33%
Epoch: 825, Loss: 1.3782, Train: 41.69%, Valid: 41.10%, Test: 41.20%
Epoch: 850, Loss: 1.3651, Train: 42.65%, Valid: 41.81%, Test: 42.02%
Epoch: 875, Loss: 1.3713, Train: 41.76%, Valid: 41.09%, Test: 41.28%
Epoch: 900, Loss: 1.4744, Train: 34.02%, Valid: 33.83%, Test: 33.24%
Epoch: 925, Loss: 1.4417, Train: 38.79%, Valid: 38.30%, Test: 38.25%
Epoch: 950, Loss: 1.3868, Train: 41.43%, Valid: 40.70%, Test: 40.79%
Epoch: 975, Loss: 1.3706, Train: 41.96%, Valid: 41.22%, Test: 41.38%
Run 01:
Highest Train: 43.68
Highest Valid: 42.71
  Final Train: 43.68
   Final Test: 42.99
All runs:
Highest Train: 43.68, nan
Highest Valid: 42.71, nan
  Final Train: 43.68, nan
   Final Test: 42.99, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.5, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=2, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7891, Train: 17.64%, Valid: 17.79%, Test: 17.32%
Epoch: 25, Loss: 1.5154, Train: 29.61%, Valid: 29.63%, Test: 29.70%
Epoch: 50, Loss: 1.4450, Train: 37.01%, Valid: 36.67%, Test: 36.87%
Epoch: 75, Loss: 1.4700, Train: 36.44%, Valid: 36.11%, Test: 36.37%
Epoch: 100, Loss: 1.4022, Train: 39.65%, Valid: 39.40%, Test: 39.51%
Epoch: 125, Loss: 1.4257, Train: 38.91%, Valid: 38.27%, Test: 38.57%
Epoch: 150, Loss: 1.6802, Train: 36.40%, Valid: 35.97%, Test: 36.25%
Epoch: 175, Loss: 1.4896, Train: 35.18%, Valid: 34.86%, Test: 35.39%
Epoch: 200, Loss: 1.4494, Train: 36.10%, Valid: 35.82%, Test: 36.10%
Epoch: 225, Loss: 1.4482, Train: 39.38%, Valid: 39.20%, Test: 39.46%
Epoch: 250, Loss: 1.3963, Train: 39.56%, Valid: 39.09%, Test: 39.50%
Epoch: 275, Loss: 1.4046, Train: 36.06%, Valid: 35.38%, Test: 35.97%
Epoch: 300, Loss: 1.3704, Train: 42.01%, Valid: 41.62%, Test: 41.81%
Epoch: 325, Loss: 1.3937, Train: 38.60%, Valid: 38.22%, Test: 38.71%
Epoch: 350, Loss: 1.3631, Train: 41.91%, Valid: 41.32%, Test: 41.48%
Epoch: 375, Loss: 1.3786, Train: 41.35%, Valid: 40.76%, Test: 41.11%
Epoch: 400, Loss: 1.5944, Train: 34.60%, Valid: 34.41%, Test: 34.96%
Epoch: 425, Loss: 1.3947, Train: 40.83%, Valid: 39.98%, Test: 40.38%
Epoch: 450, Loss: 1.3682, Train: 40.38%, Valid: 39.79%, Test: 40.05%
Epoch: 475, Loss: 1.4220, Train: 38.42%, Valid: 38.22%, Test: 38.41%
Epoch: 500, Loss: 1.3543, Train: 42.05%, Valid: 41.39%, Test: 41.70%
Epoch: 525, Loss: 1.3608, Train: 40.53%, Valid: 39.82%, Test: 40.17%
Epoch: 550, Loss: 1.3449, Train: 42.95%, Valid: 42.27%, Test: 42.30%
Epoch: 575, Loss: 1.3568, Train: 40.75%, Valid: 40.13%, Test: 40.30%
Epoch: 600, Loss: 1.4017, Train: 41.46%, Valid: 40.72%, Test: 40.94%
Epoch: 625, Loss: 3.7289, Train: 19.11%, Valid: 19.10%, Test: 18.82%
Epoch: 650, Loss: 1.7626, Train: 32.51%, Valid: 32.40%, Test: 33.10%
Epoch: 675, Loss: 1.4595, Train: 38.46%, Valid: 38.12%, Test: 38.62%
Epoch: 700, Loss: 1.4384, Train: 38.15%, Valid: 37.69%, Test: 37.96%
Epoch: 725, Loss: 1.3945, Train: 40.48%, Valid: 39.83%, Test: 39.93%
Epoch: 750, Loss: 1.3732, Train: 41.02%, Valid: 40.36%, Test: 40.71%
Epoch: 775, Loss: 1.3713, Train: 40.49%, Valid: 39.64%, Test: 39.97%
Epoch: 800, Loss: 1.3666, Train: 41.74%, Valid: 40.95%, Test: 41.36%
Epoch: 825, Loss: 1.4098, Train: 40.71%, Valid: 39.76%, Test: 40.20%
Epoch: 850, Loss: 1.3602, Train: 40.56%, Valid: 40.09%, Test: 40.58%
Epoch: 875, Loss: 1.5516, Train: 32.84%, Valid: 32.67%, Test: 32.76%
Epoch: 900, Loss: 1.5447, Train: 35.49%, Valid: 35.16%, Test: 35.41%
Epoch: 925, Loss: 1.4305, Train: 36.95%, Valid: 36.49%, Test: 37.07%
Epoch: 950, Loss: 1.4157, Train: 38.15%, Valid: 37.55%, Test: 37.73%
Epoch: 975, Loss: 1.3861, Train: 40.15%, Valid: 39.65%, Test: 39.95%
Run 01:
Highest Train: 43.26
Highest Valid: 42.49
  Final Train: 43.26
   Final Test: 42.57
All runs:
Highest Train: 43.26, nan
Highest Valid: 42.49, nan
  Final Train: 43.26, nan
   Final Test: 42.57, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.1, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.6839, Train: 17.64%, Valid: 17.78%, Test: 17.32%
Epoch: 25, Loss: 1.5715, Train: 20.25%, Valid: 20.26%, Test: 20.05%
Epoch: 50, Loss: 1.5447, Train: 28.00%, Valid: 27.93%, Test: 28.03%
Epoch: 75, Loss: 1.4912, Train: 34.76%, Valid: 34.49%, Test: 34.56%
Epoch: 100, Loss: 1.4608, Train: 36.79%, Valid: 36.29%, Test: 36.51%
Epoch: 125, Loss: 1.5313, Train: 31.09%, Valid: 30.40%, Test: 31.07%
Epoch: 150, Loss: 1.5276, Train: 35.34%, Valid: 35.06%, Test: 35.22%
Epoch: 175, Loss: 1.4719, Train: 36.57%, Valid: 35.84%, Test: 36.21%
Epoch: 200, Loss: 1.4917, Train: 35.22%, Valid: 34.87%, Test: 34.99%
Epoch: 225, Loss: 1.4384, Train: 39.01%, Valid: 38.40%, Test: 38.89%
Epoch: 250, Loss: 1.5403, Train: 21.82%, Valid: 21.43%, Test: 21.52%
Epoch: 275, Loss: 1.4753, Train: 35.62%, Valid: 34.78%, Test: 35.23%
Epoch: 300, Loss: 1.4585, Train: 36.57%, Valid: 35.72%, Test: 36.40%
Epoch: 325, Loss: 1.4578, Train: 36.84%, Valid: 36.14%, Test: 36.49%
Epoch: 350, Loss: 1.4473, Train: 37.18%, Valid: 36.25%, Test: 36.70%
Epoch: 375, Loss: 1.4909, Train: 35.63%, Valid: 34.88%, Test: 35.43%
Epoch: 400, Loss: 1.7845, Train: 32.42%, Valid: 31.93%, Test: 32.20%
Epoch: 425, Loss: 1.4735, Train: 36.97%, Valid: 35.87%, Test: 36.33%
Epoch: 450, Loss: 1.4422, Train: 37.66%, Valid: 36.29%, Test: 36.72%
Epoch: 475, Loss: 1.4215, Train: 39.33%, Valid: 37.86%, Test: 38.18%
Epoch: 500, Loss: 1.5015, Train: 36.87%, Valid: 35.50%, Test: 35.79%
Epoch: 525, Loss: 1.4445, Train: 37.51%, Valid: 35.93%, Test: 36.40%
Epoch: 550, Loss: 1.4340, Train: 37.65%, Valid: 36.12%, Test: 36.51%
Epoch: 575, Loss: 1.4547, Train: 37.73%, Valid: 36.27%, Test: 36.75%
Epoch: 600, Loss: 1.4527, Train: 38.15%, Valid: 36.74%, Test: 36.99%
Epoch: 625, Loss: 1.3990, Train: 40.42%, Valid: 38.91%, Test: 39.29%
Epoch: 650, Loss: 1.4065, Train: 40.12%, Valid: 38.46%, Test: 38.76%
Epoch: 675, Loss: 1.3947, Train: 37.98%, Valid: 36.30%, Test: 37.01%
Epoch: 700, Loss: 1.3822, Train: 40.24%, Valid: 38.62%, Test: 38.84%
Epoch: 725, Loss: 1.4332, Train: 39.62%, Valid: 38.50%, Test: 38.56%
Epoch: 750, Loss: 7.1186, Train: 20.23%, Valid: 20.05%, Test: 19.69%
Epoch: 775, Loss: 2.0285, Train: 22.23%, Valid: 22.18%, Test: 22.76%
Epoch: 800, Loss: 1.4850, Train: 36.45%, Valid: 35.50%, Test: 36.03%
Epoch: 825, Loss: 1.4591, Train: 38.14%, Valid: 37.12%, Test: 37.38%
Epoch: 850, Loss: 1.6532, Train: 25.93%, Valid: 25.55%, Test: 25.95%
Epoch: 875, Loss: 1.4882, Train: 36.30%, Valid: 35.38%, Test: 35.58%
Epoch: 900, Loss: 1.4603, Train: 37.07%, Valid: 36.04%, Test: 36.41%
Epoch: 925, Loss: 1.4510, Train: 37.75%, Valid: 36.40%, Test: 36.64%
Epoch: 950, Loss: 1.4447, Train: 38.09%, Valid: 36.76%, Test: 36.93%
Epoch: 975, Loss: 1.4379, Train: 38.19%, Valid: 36.63%, Test: 37.11%
Run 01:
Highest Train: 41.94
Highest Valid: 40.41
  Final Train: 41.84
   Final Test: 40.86
All runs:
Highest Train: 41.94, nan
Highest Valid: 40.41, nan
  Final Train: 41.84, nan
   Final Test: 40.86, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.1, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7255, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.5953, Train: 29.36%, Valid: 29.39%, Test: 29.54%
Epoch: 50, Loss: 1.8244, Train: 28.22%, Valid: 28.18%, Test: 27.94%
Epoch: 75, Loss: 1.4699, Train: 35.68%, Valid: 35.34%, Test: 35.98%
Epoch: 100, Loss: 1.5070, Train: 26.58%, Valid: 26.46%, Test: 26.18%
Epoch: 125, Loss: 1.4419, Train: 38.25%, Valid: 37.72%, Test: 37.95%
Epoch: 150, Loss: 1.3976, Train: 40.43%, Valid: 39.64%, Test: 39.99%
Epoch: 175, Loss: 1.4415, Train: 36.53%, Valid: 35.87%, Test: 36.30%
Epoch: 200, Loss: 1.3740, Train: 41.21%, Valid: 40.62%, Test: 40.53%
Epoch: 225, Loss: 1.3798, Train: 38.63%, Valid: 38.12%, Test: 38.46%
Epoch: 250, Loss: 4.0598, Train: 32.16%, Valid: 31.89%, Test: 31.99%
Epoch: 275, Loss: 1.5503, Train: 36.49%, Valid: 36.56%, Test: 36.90%
Epoch: 300, Loss: 1.4830, Train: 38.16%, Valid: 37.88%, Test: 38.11%
Epoch: 325, Loss: 1.4470, Train: 37.09%, Valid: 36.51%, Test: 37.15%
Epoch: 350, Loss: 1.6081, Train: 37.34%, Valid: 36.77%, Test: 37.17%
Epoch: 375, Loss: 1.4598, Train: 38.17%, Valid: 37.72%, Test: 37.90%
Epoch: 400, Loss: 1.4248, Train: 39.07%, Valid: 38.67%, Test: 38.94%
Epoch: 425, Loss: 1.4000, Train: 39.66%, Valid: 39.17%, Test: 39.52%
Epoch: 450, Loss: 2.0097, Train: 32.10%, Valid: 32.05%, Test: 32.17%
Epoch: 475, Loss: 1.5082, Train: 35.10%, Valid: 34.70%, Test: 34.96%
Epoch: 500, Loss: 1.4830, Train: 35.47%, Valid: 35.09%, Test: 35.51%
Epoch: 525, Loss: 1.4545, Train: 36.42%, Valid: 35.96%, Test: 36.28%
Epoch: 550, Loss: 1.4356, Train: 37.68%, Valid: 37.30%, Test: 37.61%
Epoch: 575, Loss: 1.3914, Train: 40.37%, Valid: 39.91%, Test: 39.94%
Epoch: 600, Loss: 1.3970, Train: 40.99%, Valid: 40.37%, Test: 40.93%
Epoch: 625, Loss: 1.3872, Train: 41.03%, Valid: 40.51%, Test: 40.72%
Epoch: 650, Loss: 1.3733, Train: 41.01%, Valid: 40.54%, Test: 40.80%
Epoch: 675, Loss: 1.3782, Train: 41.52%, Valid: 41.02%, Test: 41.17%
Epoch: 700, Loss: 1.3922, Train: 38.90%, Valid: 38.44%, Test: 38.73%
Epoch: 725, Loss: 1.3794, Train: 41.77%, Valid: 40.99%, Test: 41.42%
Epoch: 750, Loss: 1.3657, Train: 40.10%, Valid: 39.50%, Test: 39.66%
Epoch: 775, Loss: 1.5210, Train: 29.82%, Valid: 29.16%, Test: 29.30%
Epoch: 800, Loss: 1.3825, Train: 41.29%, Valid: 40.04%, Test: 40.51%
Epoch: 825, Loss: 1.3593, Train: 42.83%, Valid: 41.75%, Test: 42.00%
Epoch: 850, Loss: 1.6364, Train: 24.29%, Valid: 24.40%, Test: 24.13%
Epoch: 875, Loss: 1.4809, Train: 36.86%, Valid: 36.10%, Test: 36.41%
Epoch: 900, Loss: 1.4191, Train: 39.97%, Valid: 38.94%, Test: 39.45%
Epoch: 925, Loss: 1.3685, Train: 39.75%, Valid: 38.88%, Test: 39.46%
Epoch: 950, Loss: 1.3809, Train: 40.04%, Valid: 39.03%, Test: 39.17%
Epoch: 975, Loss: 1.4048, Train: 37.52%, Valid: 36.70%, Test: 37.15%
Run 01:
Highest Train: 42.93
Highest Valid: 41.90
  Final Train: 42.87
   Final Test: 41.89
All runs:
Highest Train: 42.93, nan
Highest Valid: 41.90, nan
  Final Train: 42.87, nan
   Final Test: 41.89, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.1, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7313, Train: 28.72%, Valid: 28.54%, Test: 28.82%
Epoch: 25, Loss: 1.5673, Train: 29.93%, Valid: 29.80%, Test: 29.98%
Epoch: 50, Loss: 1.4606, Train: 34.86%, Valid: 34.63%, Test: 35.25%
Epoch: 75, Loss: 1.4235, Train: 36.17%, Valid: 35.89%, Test: 35.93%
Epoch: 100, Loss: 1.7826, Train: 33.23%, Valid: 33.09%, Test: 33.25%
Epoch: 125, Loss: 1.4897, Train: 36.60%, Valid: 36.34%, Test: 36.55%
Epoch: 150, Loss: 1.4749, Train: 38.14%, Valid: 37.84%, Test: 38.26%
Epoch: 175, Loss: 1.4184, Train: 38.99%, Valid: 38.42%, Test: 38.65%
Epoch: 200, Loss: 1.4177, Train: 37.66%, Valid: 37.25%, Test: 37.36%
Epoch: 225, Loss: 1.4086, Train: 38.67%, Valid: 38.19%, Test: 38.18%
Epoch: 250, Loss: 1.4125, Train: 39.29%, Valid: 38.57%, Test: 38.95%
Epoch: 275, Loss: 1.3708, Train: 40.93%, Valid: 40.39%, Test: 40.43%
Epoch: 300, Loss: 1.3660, Train: 41.17%, Valid: 40.39%, Test: 40.73%
Epoch: 325, Loss: 1.3350, Train: 42.26%, Valid: 41.68%, Test: 41.74%
Epoch: 350, Loss: 6.8318, Train: 28.90%, Valid: 28.91%, Test: 28.93%
Epoch: 375, Loss: 1.7446, Train: 28.40%, Valid: 28.49%, Test: 28.30%
Epoch: 400, Loss: 1.4637, Train: 35.69%, Valid: 35.11%, Test: 35.70%
Epoch: 425, Loss: 1.4278, Train: 37.79%, Valid: 37.29%, Test: 37.45%
Epoch: 450, Loss: 1.3980, Train: 39.64%, Valid: 39.02%, Test: 39.30%
Epoch: 475, Loss: 1.3813, Train: 40.52%, Valid: 39.94%, Test: 39.95%
Epoch: 500, Loss: 1.3760, Train: 39.11%, Valid: 38.26%, Test: 38.70%
Epoch: 525, Loss: 1.3654, Train: 38.07%, Valid: 37.34%, Test: 37.97%
Epoch: 550, Loss: 1.3739, Train: 40.39%, Valid: 39.72%, Test: 39.75%
Epoch: 575, Loss: 1.3457, Train: 41.66%, Valid: 40.90%, Test: 40.97%
Epoch: 600, Loss: 1.4037, Train: 39.92%, Valid: 39.31%, Test: 39.48%
Epoch: 625, Loss: 1.3362, Train: 42.47%, Valid: 41.70%, Test: 41.69%
Epoch: 650, Loss: 6.6848, Train: 22.27%, Valid: 22.27%, Test: 22.80%
Epoch: 675, Loss: 1.9537, Train: 32.96%, Valid: 32.85%, Test: 32.78%
Epoch: 700, Loss: 1.4969, Train: 34.83%, Valid: 34.47%, Test: 34.94%
Epoch: 725, Loss: 1.4309, Train: 39.42%, Valid: 39.08%, Test: 39.33%
Epoch: 750, Loss: 1.4123, Train: 37.42%, Valid: 36.92%, Test: 37.38%
Epoch: 775, Loss: 1.4103, Train: 40.65%, Valid: 39.88%, Test: 40.33%
Epoch: 800, Loss: 1.3862, Train: 40.31%, Valid: 39.59%, Test: 39.95%
Epoch: 825, Loss: 1.4096, Train: 40.52%, Valid: 39.87%, Test: 40.14%
Epoch: 850, Loss: 1.3855, Train: 40.25%, Valid: 39.47%, Test: 39.83%
Epoch: 875, Loss: 1.5200, Train: 40.67%, Valid: 40.14%, Test: 40.23%
Epoch: 900, Loss: 1.3905, Train: 40.92%, Valid: 40.00%, Test: 40.45%
Epoch: 925, Loss: 1.3873, Train: 39.96%, Valid: 39.54%, Test: 39.51%
Epoch: 950, Loss: 1.3781, Train: 40.74%, Valid: 39.95%, Test: 40.23%
Epoch: 975, Loss: 1.3751, Train: 40.18%, Valid: 39.35%, Test: 39.43%
Run 01:
Highest Train: 42.70
Highest Valid: 41.99
  Final Train: 42.70
   Final Test: 41.92
All runs:
Highest Train: 42.70, nan
Highest Valid: 41.99, nan
  Final Train: 42.70, nan
   Final Test: 41.92, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.1, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7139, Train: 22.16%, Valid: 22.17%, Test: 22.74%
Epoch: 25, Loss: 1.5439, Train: 31.05%, Valid: 30.58%, Test: 31.21%
Epoch: 50, Loss: 1.4965, Train: 30.73%, Valid: 30.69%, Test: 30.79%
Epoch: 75, Loss: 1.5581, Train: 26.85%, Valid: 26.99%, Test: 27.08%
Epoch: 100, Loss: 1.4826, Train: 35.22%, Valid: 34.93%, Test: 34.97%
Epoch: 125, Loss: 1.4383, Train: 36.95%, Valid: 36.70%, Test: 36.81%
Epoch: 150, Loss: 1.4824, Train: 36.21%, Valid: 35.69%, Test: 36.11%
Epoch: 175, Loss: 1.4239, Train: 39.29%, Valid: 38.68%, Test: 38.85%
Epoch: 200, Loss: 7.3351, Train: 26.17%, Valid: 26.27%, Test: 26.27%
Epoch: 225, Loss: 1.9550, Train: 21.18%, Valid: 21.08%, Test: 20.89%
Epoch: 250, Loss: 1.5349, Train: 34.17%, Valid: 34.00%, Test: 34.27%
Epoch: 275, Loss: 1.4765, Train: 35.73%, Valid: 35.35%, Test: 35.81%
Epoch: 300, Loss: 1.4855, Train: 36.06%, Valid: 35.60%, Test: 35.88%
Epoch: 325, Loss: 1.4605, Train: 37.01%, Valid: 36.20%, Test: 36.63%
Epoch: 350, Loss: 1.4342, Train: 37.92%, Valid: 37.16%, Test: 37.63%
Epoch: 375, Loss: 1.4201, Train: 39.64%, Valid: 39.08%, Test: 39.15%
Epoch: 400, Loss: 1.4646, Train: 36.61%, Valid: 35.96%, Test: 36.10%
Epoch: 425, Loss: 1.4590, Train: 37.86%, Valid: 36.80%, Test: 37.10%
Epoch: 450, Loss: 1.4337, Train: 39.09%, Valid: 38.01%, Test: 38.24%
Epoch: 475, Loss: 1.4081, Train: 39.54%, Valid: 38.17%, Test: 38.46%
Epoch: 500, Loss: 1.4487, Train: 38.48%, Valid: 37.57%, Test: 37.43%
Epoch: 525, Loss: 1.3912, Train: 40.59%, Valid: 39.56%, Test: 39.58%
Epoch: 550, Loss: 1.4375, Train: 36.49%, Valid: 35.62%, Test: 35.93%
Epoch: 575, Loss: 1.4041, Train: 40.41%, Valid: 39.42%, Test: 39.53%
Epoch: 600, Loss: 1.5593, Train: 29.69%, Valid: 29.44%, Test: 29.78%
Epoch: 625, Loss: 1.4239, Train: 38.33%, Valid: 37.10%, Test: 37.54%
Epoch: 650, Loss: 1.4125, Train: 39.26%, Valid: 38.11%, Test: 38.06%
Epoch: 675, Loss: 1.3905, Train: 41.01%, Valid: 39.91%, Test: 39.87%
Epoch: 700, Loss: 1.6324, Train: 30.32%, Valid: 29.85%, Test: 30.18%
Epoch: 725, Loss: 1.4591, Train: 36.08%, Valid: 35.19%, Test: 35.54%
Epoch: 750, Loss: 1.4348, Train: 36.35%, Valid: 34.99%, Test: 34.97%
Epoch: 775, Loss: 2.4818, Train: 21.11%, Valid: 20.63%, Test: 20.58%
Epoch: 800, Loss: 1.5174, Train: 27.06%, Valid: 27.06%, Test: 26.46%
Epoch: 825, Loss: 1.4486, Train: 37.15%, Valid: 36.43%, Test: 36.85%
Epoch: 850, Loss: 1.4825, Train: 36.01%, Valid: 34.97%, Test: 35.25%
Epoch: 875, Loss: 1.4148, Train: 39.61%, Valid: 38.59%, Test: 38.61%
Epoch: 900, Loss: 1.5000, Train: 34.23%, Valid: 33.63%, Test: 34.02%
Epoch: 925, Loss: 1.4401, Train: 37.32%, Valid: 36.18%, Test: 36.60%
Epoch: 950, Loss: 1.4282, Train: 37.72%, Valid: 36.52%, Test: 36.78%
Epoch: 975, Loss: 1.4439, Train: 38.22%, Valid: 36.78%, Test: 37.25%
Run 01:
Highest Train: 42.29
Highest Valid: 41.36
  Final Train: 42.20
   Final Test: 41.33
All runs:
Highest Train: 42.29, nan
Highest Valid: 41.36, nan
  Final Train: 42.20, nan
   Final Test: 41.33, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.1, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.6907, Train: 18.12%, Valid: 18.19%, Test: 17.87%
Epoch: 25, Loss: 1.5134, Train: 30.95%, Valid: 31.24%, Test: 31.34%
Epoch: 50, Loss: 1.4526, Train: 35.37%, Valid: 35.20%, Test: 35.23%
Epoch: 75, Loss: 1.5173, Train: 34.76%, Valid: 34.47%, Test: 34.87%
Epoch: 100, Loss: 1.4607, Train: 36.16%, Valid: 35.87%, Test: 36.38%
Epoch: 125, Loss: 1.9643, Train: 32.91%, Valid: 32.70%, Test: 32.99%
Epoch: 150, Loss: 2.1115, Train: 18.03%, Valid: 18.11%, Test: 17.75%
Epoch: 175, Loss: 1.5153, Train: 34.76%, Valid: 34.59%, Test: 34.74%
Epoch: 200, Loss: 1.4806, Train: 35.34%, Valid: 34.70%, Test: 35.29%
Epoch: 225, Loss: 1.4609, Train: 35.91%, Valid: 35.13%, Test: 35.56%
Epoch: 250, Loss: 1.4542, Train: 36.21%, Valid: 35.62%, Test: 35.80%
Epoch: 275, Loss: 1.4431, Train: 36.28%, Valid: 35.67%, Test: 35.88%
Epoch: 300, Loss: 1.4304, Train: 37.44%, Valid: 36.48%, Test: 36.99%
Epoch: 325, Loss: 1.3727, Train: 40.45%, Valid: 39.56%, Test: 39.82%
Epoch: 350, Loss: 2.3975, Train: 24.86%, Valid: 24.76%, Test: 24.88%
Epoch: 375, Loss: 1.5248, Train: 35.80%, Valid: 35.38%, Test: 35.50%
Epoch: 400, Loss: 1.4684, Train: 37.88%, Valid: 37.25%, Test: 37.28%
Epoch: 425, Loss: 1.4099, Train: 39.78%, Valid: 38.66%, Test: 39.16%
Epoch: 450, Loss: 1.3629, Train: 40.25%, Valid: 39.15%, Test: 39.45%
Epoch: 475, Loss: 1.4037, Train: 38.18%, Valid: 37.33%, Test: 37.70%
Epoch: 500, Loss: 1.3817, Train: 39.52%, Valid: 38.59%, Test: 39.12%
Epoch: 525, Loss: 1.3686, Train: 40.16%, Valid: 38.84%, Test: 39.15%
Epoch: 550, Loss: 1.3448, Train: 41.41%, Valid: 40.35%, Test: 40.78%
Epoch: 575, Loss: 5.9489, Train: 33.46%, Valid: 33.40%, Test: 33.50%
Epoch: 600, Loss: 1.8128, Train: 29.59%, Valid: 29.54%, Test: 29.47%
Epoch: 625, Loss: 1.5015, Train: 36.24%, Valid: 35.16%, Test: 35.90%
Epoch: 650, Loss: 1.4350, Train: 37.84%, Valid: 37.00%, Test: 37.70%
Epoch: 675, Loss: 1.4152, Train: 39.51%, Valid: 38.74%, Test: 39.04%
Epoch: 700, Loss: 3.7946, Train: 18.93%, Valid: 19.06%, Test: 18.72%
Epoch: 725, Loss: 2.9628, Train: 19.10%, Valid: 19.03%, Test: 18.83%
Epoch: 750, Loss: 1.5786, Train: 37.03%, Valid: 36.59%, Test: 36.94%
Epoch: 775, Loss: 1.4807, Train: 36.16%, Valid: 35.57%, Test: 36.07%
Epoch: 800, Loss: 1.4609, Train: 37.32%, Valid: 36.89%, Test: 37.05%
Epoch: 825, Loss: 1.4509, Train: 38.06%, Valid: 37.52%, Test: 37.55%
Epoch: 850, Loss: 1.4538, Train: 38.81%, Valid: 38.12%, Test: 38.39%
Epoch: 875, Loss: 1.4928, Train: 36.73%, Valid: 36.27%, Test: 36.46%
Epoch: 900, Loss: 1.4298, Train: 38.58%, Valid: 37.55%, Test: 38.13%
Epoch: 925, Loss: 1.4471, Train: 37.65%, Valid: 36.91%, Test: 37.10%
Epoch: 950, Loss: 1.4327, Train: 39.14%, Valid: 38.16%, Test: 38.36%
Epoch: 975, Loss: 1.4035, Train: 40.26%, Valid: 39.61%, Test: 39.81%
Run 01:
Highest Train: 43.43
Highest Valid: 42.31
  Final Train: 43.43
   Final Test: 42.60
All runs:
Highest Train: 43.43, nan
Highest Valid: 42.31, nan
  Final Train: 43.43, nan
   Final Test: 42.60, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.1, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7268, Train: 12.52%, Valid: 12.59%, Test: 12.45%
Epoch: 25, Loss: 1.5149, Train: 25.95%, Valid: 26.20%, Test: 26.29%
Epoch: 50, Loss: 1.5437, Train: 31.94%, Valid: 31.79%, Test: 32.17%
Epoch: 75, Loss: 1.4439, Train: 36.96%, Valid: 36.70%, Test: 36.84%
Epoch: 100, Loss: 1.3977, Train: 39.35%, Valid: 39.01%, Test: 39.27%
Epoch: 125, Loss: 1.3914, Train: 39.50%, Valid: 38.82%, Test: 39.33%
Epoch: 150, Loss: 1.3979, Train: 39.42%, Valid: 38.88%, Test: 39.21%
Epoch: 175, Loss: 1.3798, Train: 40.46%, Valid: 39.67%, Test: 40.27%
Epoch: 200, Loss: 1.3673, Train: 41.09%, Valid: 40.44%, Test: 40.55%
Epoch: 225, Loss: 1.3588, Train: 41.01%, Valid: 40.42%, Test: 40.84%
Epoch: 250, Loss: 1.4059, Train: 38.02%, Valid: 37.75%, Test: 37.65%
Epoch: 275, Loss: 1.4048, Train: 38.95%, Valid: 38.58%, Test: 38.69%
Epoch: 300, Loss: 1.4260, Train: 40.02%, Valid: 39.44%, Test: 39.82%
Epoch: 325, Loss: 1.3579, Train: 40.42%, Valid: 39.86%, Test: 40.05%
Epoch: 350, Loss: 1.3465, Train: 41.74%, Valid: 40.81%, Test: 41.45%
Epoch: 375, Loss: 1.3965, Train: 32.18%, Valid: 32.07%, Test: 31.90%
Epoch: 400, Loss: 1.3638, Train: 41.36%, Valid: 40.77%, Test: 40.89%
Epoch: 425, Loss: 1.3481, Train: 40.87%, Valid: 39.93%, Test: 40.21%
Epoch: 450, Loss: 1.3865, Train: 35.17%, Valid: 34.75%, Test: 34.65%
Epoch: 475, Loss: 1.5671, Train: 32.70%, Valid: 32.61%, Test: 32.69%
Epoch: 500, Loss: 1.3813, Train: 39.05%, Valid: 38.22%, Test: 38.56%
Epoch: 525, Loss: 1.3468, Train: 41.23%, Valid: 40.36%, Test: 40.44%
Epoch: 550, Loss: 1.5926, Train: 40.90%, Valid: 40.35%, Test: 40.43%
Epoch: 575, Loss: 1.3534, Train: 41.61%, Valid: 40.36%, Test: 40.81%
Epoch: 600, Loss: 1.4272, Train: 35.77%, Valid: 35.69%, Test: 35.68%
Epoch: 625, Loss: 1.3390, Train: 41.88%, Valid: 41.14%, Test: 41.35%
Epoch: 650, Loss: 1.3353, Train: 42.60%, Valid: 41.34%, Test: 41.67%
Epoch: 675, Loss: 1.3514, Train: 41.46%, Valid: 40.26%, Test: 40.48%
Epoch: 700, Loss: 1.6750, Train: 37.83%, Valid: 36.97%, Test: 37.43%
Epoch: 725, Loss: 1.3860, Train: 39.27%, Valid: 38.38%, Test: 38.35%
Epoch: 750, Loss: 1.3486, Train: 42.07%, Valid: 40.86%, Test: 41.05%
Epoch: 775, Loss: 1.3932, Train: 38.57%, Valid: 37.66%, Test: 37.89%
Epoch: 800, Loss: 1.5764, Train: 35.08%, Valid: 34.52%, Test: 34.37%
Epoch: 825, Loss: 1.3684, Train: 42.14%, Valid: 41.03%, Test: 41.20%
Epoch: 850, Loss: 1.4481, Train: 37.55%, Valid: 36.94%, Test: 37.19%
Epoch: 875, Loss: 1.3592, Train: 39.88%, Valid: 38.76%, Test: 38.93%
Epoch: 900, Loss: 1.3298, Train: 42.80%, Valid: 41.46%, Test: 41.83%
Epoch: 925, Loss: 1.3220, Train: 42.91%, Valid: 41.66%, Test: 41.65%
Epoch: 950, Loss: 1.4372, Train: 38.05%, Valid: 37.12%, Test: 37.13%
Epoch: 975, Loss: 1.3767, Train: 39.13%, Valid: 38.48%, Test: 38.43%
Run 01:
Highest Train: 43.35
Highest Valid: 42.09
  Final Train: 43.17
   Final Test: 42.34
All runs:
Highest Train: 43.35, nan
Highest Valid: 42.09, nan
  Final Train: 43.17, nan
   Final Test: 42.34, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.1, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.6989, Train: 12.51%, Valid: 12.59%, Test: 12.43%
Epoch: 25, Loss: 1.5637, Train: 33.35%, Valid: 33.28%, Test: 33.43%
Epoch: 50, Loss: 1.4977, Train: 34.70%, Valid: 34.35%, Test: 34.62%
Epoch: 75, Loss: 1.4925, Train: 34.79%, Valid: 34.30%, Test: 34.90%
Epoch: 100, Loss: 1.4569, Train: 38.53%, Valid: 38.29%, Test: 38.47%
Epoch: 125, Loss: 1.5007, Train: 29.88%, Valid: 29.66%, Test: 30.21%
Epoch: 150, Loss: 1.8138, Train: 20.84%, Valid: 21.04%, Test: 21.04%
Epoch: 175, Loss: 1.5140, Train: 35.12%, Valid: 34.66%, Test: 35.01%
Epoch: 200, Loss: 1.4697, Train: 36.19%, Valid: 35.39%, Test: 35.92%
Epoch: 225, Loss: 3.2431, Train: 19.91%, Valid: 20.01%, Test: 19.72%
Epoch: 250, Loss: 1.6169, Train: 30.95%, Valid: 30.28%, Test: 30.89%
Epoch: 275, Loss: 1.4746, Train: 36.32%, Valid: 35.45%, Test: 35.65%
Epoch: 300, Loss: 1.4609, Train: 36.77%, Valid: 35.70%, Test: 36.11%
Epoch: 325, Loss: 1.4377, Train: 38.37%, Valid: 37.17%, Test: 37.52%
Epoch: 350, Loss: 1.4624, Train: 36.72%, Valid: 35.64%, Test: 35.80%
Epoch: 375, Loss: 1.4483, Train: 37.40%, Valid: 35.93%, Test: 36.41%
Epoch: 400, Loss: 1.4440, Train: 37.70%, Valid: 36.02%, Test: 36.46%
Epoch: 425, Loss: 1.4385, Train: 37.92%, Valid: 36.19%, Test: 36.57%
Epoch: 450, Loss: 1.4317, Train: 38.15%, Valid: 36.35%, Test: 36.64%
Epoch: 475, Loss: 1.4256, Train: 38.28%, Valid: 36.47%, Test: 36.76%
Epoch: 500, Loss: 1.4343, Train: 38.16%, Valid: 36.19%, Test: 36.46%
Epoch: 525, Loss: 1.4264, Train: 38.50%, Valid: 36.26%, Test: 36.57%
Epoch: 550, Loss: 1.4205, Train: 38.72%, Valid: 36.42%, Test: 36.67%
Epoch: 575, Loss: 1.4134, Train: 39.17%, Valid: 36.39%, Test: 36.55%
Epoch: 600, Loss: 1.4071, Train: 39.68%, Valid: 36.56%, Test: 36.77%
Epoch: 625, Loss: 1.4146, Train: 37.92%, Valid: 35.55%, Test: 35.80%
Epoch: 650, Loss: 1.4114, Train: 39.28%, Valid: 36.29%, Test: 36.58%
Epoch: 675, Loss: 1.3952, Train: 39.81%, Valid: 36.58%, Test: 36.88%
Epoch: 700, Loss: 1.3970, Train: 40.06%, Valid: 36.72%, Test: 36.91%
Epoch: 725, Loss: 1.4032, Train: 40.47%, Valid: 37.36%, Test: 37.38%
Epoch: 750, Loss: 1.3723, Train: 39.94%, Valid: 36.68%, Test: 36.48%
Epoch: 775, Loss: 1.7099, Train: 22.37%, Valid: 22.23%, Test: 22.88%
Epoch: 800, Loss: 1.4972, Train: 36.12%, Valid: 35.26%, Test: 35.43%
Epoch: 825, Loss: 1.4524, Train: 37.22%, Valid: 35.71%, Test: 36.14%
Epoch: 850, Loss: 1.4339, Train: 37.91%, Valid: 36.18%, Test: 36.50%
Epoch: 875, Loss: 1.4207, Train: 38.04%, Valid: 35.78%, Test: 36.29%
Epoch: 900, Loss: 1.4289, Train: 38.50%, Valid: 36.06%, Test: 36.56%
Epoch: 925, Loss: 1.4207, Train: 39.06%, Valid: 36.30%, Test: 36.70%
Epoch: 950, Loss: 1.4130, Train: 39.35%, Valid: 36.26%, Test: 36.75%
Epoch: 975, Loss: 1.4036, Train: 39.93%, Valid: 36.62%, Test: 37.26%
Run 01:
Highest Train: 41.95
Highest Valid: 38.29
  Final Train: 38.53
   Final Test: 38.47
All runs:
Highest Train: 41.95, nan
Highest Valid: 38.29, nan
  Final Train: 38.53, nan
   Final Test: 38.47, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.1, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.6994, Train: 16.60%, Valid: 16.38%, Test: 16.66%
Epoch: 25, Loss: 1.5354, Train: 32.00%, Valid: 31.86%, Test: 32.01%
Epoch: 50, Loss: 1.4919, Train: 37.33%, Valid: 37.10%, Test: 37.14%
Epoch: 75, Loss: 1.4581, Train: 36.10%, Valid: 35.83%, Test: 35.98%
Epoch: 100, Loss: 1.4209, Train: 35.50%, Valid: 35.18%, Test: 35.82%
Epoch: 125, Loss: 1.3638, Train: 40.41%, Valid: 39.76%, Test: 40.08%
Epoch: 150, Loss: 1.4180, Train: 40.15%, Valid: 39.36%, Test: 39.78%
Epoch: 175, Loss: 1.6382, Train: 19.07%, Valid: 19.04%, Test: 18.75%
Epoch: 200, Loss: 1.6159, Train: 23.22%, Valid: 22.97%, Test: 23.75%
Epoch: 225, Loss: 1.4433, Train: 34.53%, Valid: 34.11%, Test: 34.75%
Epoch: 250, Loss: 1.4407, Train: 36.93%, Valid: 36.28%, Test: 37.01%
Epoch: 275, Loss: 1.4244, Train: 38.45%, Valid: 37.48%, Test: 37.92%
Epoch: 300, Loss: 1.8853, Train: 36.05%, Valid: 35.69%, Test: 35.92%
Epoch: 325, Loss: 1.5981, Train: 27.96%, Valid: 27.35%, Test: 27.83%
Epoch: 350, Loss: 1.4839, Train: 35.87%, Valid: 35.26%, Test: 35.78%
Epoch: 375, Loss: 1.4625, Train: 36.57%, Valid: 35.90%, Test: 36.09%
Epoch: 400, Loss: 1.4508, Train: 37.16%, Valid: 36.14%, Test: 36.52%
Epoch: 425, Loss: 1.4335, Train: 38.05%, Valid: 37.05%, Test: 37.37%
Epoch: 450, Loss: 1.4552, Train: 37.07%, Valid: 36.41%, Test: 36.91%
Epoch: 475, Loss: 1.5349, Train: 38.62%, Valid: 37.75%, Test: 38.18%
Epoch: 500, Loss: 1.4068, Train: 39.49%, Valid: 38.45%, Test: 38.58%
Epoch: 525, Loss: 2.5948, Train: 25.97%, Valid: 25.96%, Test: 25.43%
Epoch: 550, Loss: 1.7182, Train: 22.89%, Valid: 22.78%, Test: 23.31%
Epoch: 575, Loss: 1.4872, Train: 36.66%, Valid: 35.90%, Test: 36.16%
Epoch: 600, Loss: 1.4331, Train: 33.57%, Valid: 32.70%, Test: 33.15%
Epoch: 625, Loss: 1.4257, Train: 38.90%, Valid: 37.91%, Test: 38.06%
Epoch: 650, Loss: 1.4097, Train: 39.60%, Valid: 38.45%, Test: 38.71%
Epoch: 675, Loss: 1.4214, Train: 39.71%, Valid: 38.41%, Test: 38.71%
Epoch: 700, Loss: 1.5039, Train: 35.75%, Valid: 34.74%, Test: 35.47%
Epoch: 725, Loss: 1.3894, Train: 40.11%, Valid: 38.93%, Test: 39.23%
Epoch: 750, Loss: 1.4528, Train: 37.94%, Valid: 36.83%, Test: 36.97%
Epoch: 775, Loss: 1.3920, Train: 40.30%, Valid: 38.92%, Test: 39.20%
Epoch: 800, Loss: 1.3634, Train: 40.81%, Valid: 39.54%, Test: 39.75%
Epoch: 825, Loss: 2.3700, Train: 30.14%, Valid: 29.66%, Test: 30.15%
Epoch: 850, Loss: 7.0680, Train: 12.46%, Valid: 12.51%, Test: 12.38%
Epoch: 875, Loss: 2.6683, Train: 32.18%, Valid: 31.69%, Test: 31.90%
Epoch: 900, Loss: 1.6811, Train: 32.84%, Valid: 32.39%, Test: 32.44%
Epoch: 925, Loss: 1.4799, Train: 35.44%, Valid: 34.80%, Test: 35.31%
Epoch: 950, Loss: 1.4373, Train: 38.53%, Valid: 37.68%, Test: 38.25%
Epoch: 975, Loss: 1.7460, Train: 28.28%, Valid: 28.04%, Test: 28.47%
Run 01:
Highest Train: 42.24
Highest Valid: 40.85
  Final Train: 42.24
   Final Test: 41.06
All runs:
Highest Train: 42.24, nan
Highest Valid: 40.85, nan
  Final Train: 42.24, nan
   Final Test: 41.06, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.1, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7245, Train: 26.09%, Valid: 25.92%, Test: 26.29%
Epoch: 25, Loss: 1.5516, Train: 29.00%, Valid: 28.86%, Test: 29.11%
Epoch: 50, Loss: 1.4329, Train: 38.29%, Valid: 38.38%, Test: 38.41%
Epoch: 75, Loss: 1.6819, Train: 24.31%, Valid: 24.42%, Test: 24.63%
Epoch: 100, Loss: 1.4734, Train: 36.62%, Valid: 36.44%, Test: 37.01%
Epoch: 125, Loss: 1.4334, Train: 37.73%, Valid: 37.40%, Test: 37.84%
Epoch: 150, Loss: 1.4262, Train: 38.14%, Valid: 37.81%, Test: 38.27%
Epoch: 175, Loss: 1.4226, Train: 39.00%, Valid: 38.59%, Test: 39.02%
Epoch: 200, Loss: 1.4562, Train: 36.58%, Valid: 36.00%, Test: 36.53%
Epoch: 225, Loss: 1.4206, Train: 38.99%, Valid: 38.50%, Test: 38.63%
Epoch: 250, Loss: 1.4087, Train: 39.67%, Valid: 39.01%, Test: 39.17%
Epoch: 275, Loss: 1.3971, Train: 40.80%, Valid: 40.15%, Test: 40.23%
Epoch: 300, Loss: 1.3997, Train: 40.46%, Valid: 39.72%, Test: 39.89%
Epoch: 325, Loss: 1.3783, Train: 41.71%, Valid: 41.06%, Test: 41.28%
Epoch: 350, Loss: 1.4868, Train: 35.28%, Valid: 34.96%, Test: 35.34%
Epoch: 375, Loss: 1.4077, Train: 39.37%, Valid: 38.72%, Test: 38.99%
Epoch: 400, Loss: 1.3908, Train: 39.59%, Valid: 38.78%, Test: 39.04%
Epoch: 425, Loss: 1.3831, Train: 39.60%, Valid: 38.72%, Test: 38.64%
Epoch: 450, Loss: 1.3927, Train: 36.93%, Valid: 35.84%, Test: 36.41%
Epoch: 475, Loss: 1.3856, Train: 38.91%, Valid: 37.89%, Test: 38.22%
Epoch: 500, Loss: 1.4210, Train: 39.91%, Valid: 39.03%, Test: 39.11%
Epoch: 525, Loss: 1.3885, Train: 41.64%, Valid: 40.65%, Test: 41.03%
Epoch: 550, Loss: 1.3898, Train: 41.04%, Valid: 39.75%, Test: 39.99%
Epoch: 575, Loss: 1.3567, Train: 39.71%, Valid: 39.12%, Test: 39.21%
Epoch: 600, Loss: 1.4396, Train: 38.96%, Valid: 38.06%, Test: 38.56%
Epoch: 625, Loss: 1.3775, Train: 40.28%, Valid: 39.00%, Test: 39.22%
Epoch: 650, Loss: 1.3727, Train: 40.01%, Valid: 38.77%, Test: 39.16%
Epoch: 675, Loss: 1.3609, Train: 40.71%, Valid: 39.68%, Test: 39.50%
Epoch: 700, Loss: 1.4080, Train: 37.74%, Valid: 36.70%, Test: 36.75%
Epoch: 725, Loss: 1.4155, Train: 38.81%, Valid: 38.09%, Test: 38.31%
Epoch: 750, Loss: 1.3282, Train: 42.05%, Valid: 41.07%, Test: 41.24%
Epoch: 775, Loss: 1.3644, Train: 41.36%, Valid: 40.16%, Test: 40.51%
Epoch: 800, Loss: 1.3643, Train: 41.02%, Valid: 39.82%, Test: 39.87%
Epoch: 825, Loss: 11.2148, Train: 23.65%, Valid: 23.67%, Test: 24.05%
Epoch: 850, Loss: 2.0734, Train: 24.31%, Valid: 24.34%, Test: 24.45%
Epoch: 875, Loss: 1.4788, Train: 33.45%, Valid: 32.82%, Test: 33.61%
Epoch: 900, Loss: 1.4073, Train: 38.52%, Valid: 38.06%, Test: 38.37%
Epoch: 925, Loss: 1.4217, Train: 37.96%, Valid: 37.40%, Test: 37.85%
Epoch: 950, Loss: 1.4133, Train: 38.81%, Valid: 38.32%, Test: 38.62%
Epoch: 975, Loss: 1.3831, Train: 39.41%, Valid: 38.76%, Test: 39.15%
Run 01:
Highest Train: 43.87
Highest Valid: 42.35
  Final Train: 43.87
   Final Test: 42.51
All runs:
Highest Train: 43.87, nan
Highest Valid: 42.35, nan
  Final Train: 43.87, nan
   Final Test: 42.51, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.2, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7184, Train: 12.51%, Valid: 12.59%, Test: 12.43%
Epoch: 25, Loss: 1.5567, Train: 32.57%, Valid: 32.26%, Test: 32.53%
Epoch: 50, Loss: 1.6226, Train: 34.66%, Valid: 34.42%, Test: 34.60%
Epoch: 75, Loss: 1.4967, Train: 35.26%, Valid: 34.73%, Test: 35.41%
Epoch: 100, Loss: 1.4465, Train: 38.36%, Valid: 38.07%, Test: 38.32%
Epoch: 125, Loss: 1.5836, Train: 35.13%, Valid: 34.78%, Test: 35.01%
Epoch: 150, Loss: 1.4811, Train: 35.83%, Valid: 35.41%, Test: 35.72%
Epoch: 175, Loss: 1.5346, Train: 35.19%, Valid: 34.52%, Test: 34.86%
Epoch: 200, Loss: 1.4668, Train: 36.53%, Valid: 35.91%, Test: 36.33%
Epoch: 225, Loss: 1.6217, Train: 35.85%, Valid: 35.49%, Test: 35.78%
Epoch: 250, Loss: 1.4631, Train: 37.32%, Valid: 36.49%, Test: 36.97%
Epoch: 275, Loss: 1.4559, Train: 37.65%, Valid: 36.62%, Test: 37.18%
Epoch: 300, Loss: 1.4899, Train: 37.28%, Valid: 36.10%, Test: 36.76%
Epoch: 325, Loss: 1.4229, Train: 38.97%, Valid: 38.13%, Test: 38.47%
Epoch: 350, Loss: 1.4278, Train: 36.11%, Valid: 35.24%, Test: 36.13%
Epoch: 375, Loss: 1.4376, Train: 38.60%, Valid: 37.69%, Test: 38.06%
Epoch: 400, Loss: 1.4578, Train: 37.04%, Valid: 36.25%, Test: 36.43%
Epoch: 425, Loss: 1.4229, Train: 38.42%, Valid: 37.47%, Test: 37.91%
Epoch: 450, Loss: 1.4203, Train: 38.74%, Valid: 37.65%, Test: 38.08%
Epoch: 475, Loss: 1.4647, Train: 36.82%, Valid: 35.99%, Test: 36.25%
Epoch: 500, Loss: 1.5134, Train: 28.86%, Valid: 28.47%, Test: 27.89%
Epoch: 525, Loss: 1.4249, Train: 38.50%, Valid: 37.43%, Test: 37.74%
Epoch: 550, Loss: 1.4351, Train: 39.66%, Valid: 38.36%, Test: 38.79%
Epoch: 575, Loss: 1.4765, Train: 38.24%, Valid: 37.08%, Test: 37.52%
Epoch: 600, Loss: 1.3997, Train: 37.80%, Valid: 36.45%, Test: 36.51%
Epoch: 625, Loss: 1.4028, Train: 40.40%, Valid: 38.81%, Test: 39.07%
Epoch: 650, Loss: 1.4498, Train: 39.21%, Valid: 37.91%, Test: 38.36%
Epoch: 675, Loss: 1.5524, Train: 36.37%, Valid: 35.28%, Test: 35.54%
Epoch: 700, Loss: 1.4423, Train: 37.62%, Valid: 36.16%, Test: 36.45%
Epoch: 725, Loss: 1.4155, Train: 39.79%, Valid: 38.02%, Test: 38.48%
Epoch: 750, Loss: 1.4351, Train: 37.26%, Valid: 35.89%, Test: 36.63%
Epoch: 775, Loss: 1.3973, Train: 41.77%, Valid: 39.78%, Test: 40.04%
Epoch: 800, Loss: 1.4238, Train: 39.63%, Valid: 38.03%, Test: 38.56%
Epoch: 825, Loss: 4.6034, Train: 17.65%, Valid: 17.77%, Test: 17.33%
Epoch: 850, Loss: 1.9650, Train: 22.74%, Valid: 22.60%, Test: 22.73%
Epoch: 875, Loss: 1.5386, Train: 35.03%, Valid: 34.48%, Test: 34.68%
Epoch: 900, Loss: 1.4804, Train: 36.04%, Valid: 35.38%, Test: 35.65%
Epoch: 925, Loss: 1.4688, Train: 36.65%, Valid: 35.81%, Test: 36.21%
Epoch: 950, Loss: 1.4624, Train: 36.95%, Valid: 36.11%, Test: 36.40%
Epoch: 975, Loss: 1.4353, Train: 38.01%, Valid: 36.82%, Test: 37.32%
Run 01:
Highest Train: 43.12
Highest Valid: 41.17
  Final Train: 43.12
   Final Test: 41.17
All runs:
Highest Train: 43.12, nan
Highest Valid: 41.17, nan
  Final Train: 43.12, nan
   Final Test: 41.17, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.2, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7402, Train: 22.22%, Valid: 22.07%, Test: 22.09%
Epoch: 25, Loss: 1.4926, Train: 25.54%, Valid: 25.63%, Test: 26.15%
Epoch: 50, Loss: 1.5311, Train: 28.14%, Valid: 28.03%, Test: 27.85%
Epoch: 75, Loss: 1.4578, Train: 37.15%, Valid: 37.20%, Test: 37.31%
Epoch: 100, Loss: 1.4611, Train: 37.10%, Valid: 36.60%, Test: 37.03%
Epoch: 125, Loss: 1.6200, Train: 35.93%, Valid: 35.58%, Test: 35.75%
Epoch: 150, Loss: 1.4318, Train: 38.34%, Valid: 37.86%, Test: 38.05%
Epoch: 175, Loss: 1.4203, Train: 39.67%, Valid: 39.01%, Test: 39.52%
Epoch: 200, Loss: 1.4279, Train: 36.46%, Valid: 35.78%, Test: 36.06%
Epoch: 225, Loss: 1.3850, Train: 40.05%, Valid: 39.52%, Test: 39.84%
Epoch: 250, Loss: 3.8038, Train: 22.32%, Valid: 22.30%, Test: 22.88%
Epoch: 275, Loss: 1.6256, Train: 32.83%, Valid: 32.68%, Test: 32.65%
Epoch: 300, Loss: 1.4426, Train: 39.05%, Valid: 38.38%, Test: 38.72%
Epoch: 325, Loss: 1.4452, Train: 38.06%, Valid: 37.36%, Test: 37.61%
Epoch: 350, Loss: 1.4034, Train: 39.86%, Valid: 39.18%, Test: 39.32%
Epoch: 375, Loss: 1.5697, Train: 32.87%, Valid: 32.61%, Test: 32.86%
Epoch: 400, Loss: 1.4480, Train: 38.14%, Valid: 37.66%, Test: 37.93%
Epoch: 425, Loss: 1.3919, Train: 40.71%, Valid: 39.86%, Test: 40.27%
Epoch: 450, Loss: 1.8781, Train: 17.73%, Valid: 17.79%, Test: 17.36%
Epoch: 475, Loss: 1.5484, Train: 38.96%, Valid: 38.35%, Test: 38.81%
Epoch: 500, Loss: 1.7272, Train: 35.39%, Valid: 34.97%, Test: 35.02%
Epoch: 525, Loss: 1.4736, Train: 35.62%, Valid: 35.34%, Test: 35.58%
Epoch: 550, Loss: 1.4168, Train: 39.75%, Valid: 38.89%, Test: 39.09%
Epoch: 575, Loss: 1.4085, Train: 39.99%, Valid: 39.19%, Test: 39.45%
Epoch: 600, Loss: 1.4514, Train: 40.41%, Valid: 39.82%, Test: 40.06%
Epoch: 625, Loss: 1.4698, Train: 31.00%, Valid: 30.41%, Test: 30.30%
Epoch: 650, Loss: 1.3789, Train: 40.29%, Valid: 39.46%, Test: 39.54%
Epoch: 675, Loss: 15.1830, Train: 31.25%, Valid: 31.04%, Test: 31.09%
Epoch: 700, Loss: 3.1335, Train: 30.94%, Valid: 30.85%, Test: 30.97%
Epoch: 725, Loss: 1.9135, Train: 34.40%, Valid: 34.23%, Test: 34.34%
Epoch: 750, Loss: 1.5785, Train: 35.44%, Valid: 35.15%, Test: 35.26%
Epoch: 775, Loss: 1.4860, Train: 36.67%, Valid: 36.19%, Test: 36.86%
Epoch: 800, Loss: 1.4616, Train: 37.31%, Valid: 36.81%, Test: 37.04%
Epoch: 825, Loss: 1.4333, Train: 39.19%, Valid: 38.61%, Test: 38.74%
Epoch: 850, Loss: 1.4408, Train: 38.45%, Valid: 37.94%, Test: 37.83%
Epoch: 875, Loss: 1.4536, Train: 37.23%, Valid: 36.83%, Test: 36.88%
Epoch: 900, Loss: 1.4298, Train: 37.50%, Valid: 37.13%, Test: 37.30%
Epoch: 925, Loss: 1.4106, Train: 38.50%, Valid: 37.88%, Test: 38.05%
Epoch: 950, Loss: 1.5198, Train: 32.54%, Valid: 32.48%, Test: 32.29%
Epoch: 975, Loss: 1.4567, Train: 38.54%, Valid: 37.88%, Test: 38.25%
Run 01:
Highest Train: 42.65
Highest Valid: 41.69
  Final Train: 42.51
   Final Test: 41.78
All runs:
Highest Train: 42.65, nan
Highest Valid: 41.69, nan
  Final Train: 42.51, nan
   Final Test: 41.78, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.2, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-07)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7107, Train: 11.85%, Valid: 11.76%, Test: 11.60%
Epoch: 25, Loss: 1.5985, Train: 31.17%, Valid: 30.83%, Test: 30.97%
Epoch: 50, Loss: 1.3960, Train: 35.20%, Valid: 34.79%, Test: 35.06%
Epoch: 75, Loss: 1.4598, Train: 36.25%, Valid: 35.92%, Test: 36.01%
Epoch: 100, Loss: 1.3867, Train: 40.82%, Valid: 40.22%, Test: 40.65%
Epoch: 125, Loss: 1.4896, Train: 34.64%, Valid: 34.46%, Test: 34.48%
Epoch: 150, Loss: 1.3836, Train: 40.35%, Valid: 39.92%, Test: 40.08%
Epoch: 175, Loss: 1.4228, Train: 34.72%, Valid: 34.32%, Test: 34.71%
Epoch: 200, Loss: 1.3726, Train: 40.58%, Valid: 39.68%, Test: 40.12%
Epoch: 225, Loss: 1.3747, Train: 39.85%, Valid: 39.14%, Test: 39.70%
Epoch: 250, Loss: 1.3539, Train: 41.82%, Valid: 40.91%, Test: 41.23%
Epoch: 275, Loss: 1.3439, Train: 42.65%, Valid: 41.87%, Test: 41.96%
Epoch: 300, Loss: 1.3644, Train: 41.31%, Valid: 40.49%, Test: 40.66%
Epoch: 325, Loss: 1.3468, Train: 41.99%, Valid: 41.02%, Test: 41.35%
Epoch: 350, Loss: 1.3238, Train: 43.31%, Valid: 42.51%, Test: 42.56%
Epoch: 375, Loss: 1.3384, Train: 42.85%, Valid: 41.70%, Test: 42.06%
Epoch: 400, Loss: 1.3174, Train: 43.43%, Valid: 42.63%, Test: 42.67%
Epoch: 425, Loss: 1.3130, Train: 43.99%, Valid: 42.91%, Test: 43.34%
Epoch: 450, Loss: 1.2907, Train: 44.91%, Valid: 44.18%, Test: 44.27%
Epoch: 475, Loss: 1.3301, Train: 43.07%, Valid: 41.95%, Test: 42.47%
Epoch: 500, Loss: 1.4700, Train: 36.11%, Valid: 35.62%, Test: 35.60%
Epoch: 525, Loss: 1.3929, Train: 40.66%, Valid: 40.29%, Test: 40.14%
Epoch: 550, Loss: 1.3877, Train: 39.47%, Valid: 38.57%, Test: 38.73%
Epoch: 575, Loss: 1.3553, Train: 42.19%, Valid: 41.02%, Test: 41.35%
Epoch: 600, Loss: 1.3306, Train: 43.04%, Valid: 42.25%, Test: 42.68%
Epoch: 625, Loss: 1.3688, Train: 39.64%, Valid: 38.69%, Test: 38.91%
Epoch: 650, Loss: 1.3147, Train: 38.44%, Valid: 37.60%, Test: 38.04%
Epoch: 675, Loss: 1.3247, Train: 41.56%, Valid: 40.38%, Test: 40.91%
Epoch: 700, Loss: 1.3081, Train: 42.05%, Valid: 41.03%, Test: 41.25%
Epoch: 725, Loss: 1.2919, Train: 45.54%, Valid: 44.24%, Test: 44.23%
Epoch: 750, Loss: 1.3148, Train: 42.51%, Valid: 41.67%, Test: 41.61%
Epoch: 775, Loss: 1.3169, Train: 42.58%, Valid: 41.58%, Test: 41.86%
Epoch: 800, Loss: 1.2891, Train: 44.48%, Valid: 43.25%, Test: 43.67%
Epoch: 825, Loss: 1.3189, Train: 44.42%, Valid: 43.18%, Test: 43.52%
Epoch: 850, Loss: 1.2942, Train: 45.38%, Valid: 44.11%, Test: 44.13%
Epoch: 875, Loss: 1.2645, Train: 45.53%, Valid: 44.02%, Test: 44.29%
Epoch: 900, Loss: 1.3042, Train: 43.00%, Valid: 42.12%, Test: 42.15%
Epoch: 925, Loss: 1.3667, Train: 43.62%, Valid: 42.60%, Test: 42.74%
Epoch: 950, Loss: 1.2964, Train: 44.49%, Valid: 43.09%, Test: 43.49%
Epoch: 975, Loss: 1.3178, Train: 43.08%, Valid: 41.85%, Test: 42.11%
Run 01:
Highest Train: 46.86
Highest Valid: 45.55
  Final Train: 46.86
   Final Test: 45.75
All runs:
Highest Train: 46.86, nan
Highest Valid: 45.55, nan
  Final Train: 46.86, nan
   Final Test: 45.75, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.2, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7273, Train: 17.64%, Valid: 17.78%, Test: 17.32%
Epoch: 25, Loss: 1.5385, Train: 29.62%, Valid: 29.13%, Test: 29.95%
Epoch: 50, Loss: 1.5053, Train: 35.30%, Valid: 34.90%, Test: 35.15%
Epoch: 75, Loss: 1.5117, Train: 31.12%, Valid: 30.70%, Test: 31.51%
Epoch: 100, Loss: 1.5138, Train: 33.33%, Valid: 33.13%, Test: 33.10%
Epoch: 125, Loss: 1.4760, Train: 33.23%, Valid: 33.09%, Test: 33.33%
Epoch: 150, Loss: 1.4481, Train: 37.17%, Valid: 36.63%, Test: 37.21%
Epoch: 175, Loss: 1.4774, Train: 35.10%, Valid: 34.84%, Test: 35.21%
Epoch: 200, Loss: 1.5233, Train: 31.79%, Valid: 31.59%, Test: 31.91%
Epoch: 225, Loss: 1.4906, Train: 36.50%, Valid: 35.77%, Test: 36.09%
Epoch: 250, Loss: 1.4168, Train: 39.55%, Valid: 39.15%, Test: 39.24%
Epoch: 275, Loss: 1.4700, Train: 36.12%, Valid: 35.71%, Test: 35.86%
Epoch: 300, Loss: 1.4657, Train: 32.76%, Valid: 32.18%, Test: 32.87%
Epoch: 325, Loss: 1.4224, Train: 39.22%, Valid: 38.70%, Test: 38.86%
Epoch: 350, Loss: 1.5386, Train: 34.71%, Valid: 34.28%, Test: 34.66%
Epoch: 375, Loss: 1.4751, Train: 36.80%, Valid: 35.89%, Test: 36.24%
Epoch: 400, Loss: 1.4598, Train: 37.34%, Valid: 36.51%, Test: 36.80%
Epoch: 425, Loss: 1.4382, Train: 38.56%, Valid: 37.79%, Test: 38.00%
Epoch: 450, Loss: 1.4217, Train: 33.73%, Valid: 33.34%, Test: 33.26%
Epoch: 475, Loss: 1.4179, Train: 36.81%, Valid: 36.00%, Test: 36.28%
Epoch: 500, Loss: 1.4491, Train: 38.26%, Valid: 37.55%, Test: 37.68%
Epoch: 525, Loss: 1.4306, Train: 37.75%, Valid: 36.68%, Test: 37.25%
Epoch: 550, Loss: 1.4580, Train: 37.08%, Valid: 36.18%, Test: 36.65%
Epoch: 575, Loss: 1.5509, Train: 25.43%, Valid: 25.38%, Test: 25.30%
Epoch: 600, Loss: 1.5421, Train: 35.29%, Valid: 34.86%, Test: 34.92%
Epoch: 625, Loss: 1.4555, Train: 37.42%, Valid: 36.37%, Test: 36.59%
Epoch: 650, Loss: 1.4191, Train: 39.43%, Valid: 38.08%, Test: 38.54%
Epoch: 675, Loss: 18.5989, Train: 21.46%, Valid: 21.60%, Test: 21.89%
Epoch: 700, Loss: 2.9607, Train: 31.21%, Valid: 30.89%, Test: 31.20%
Epoch: 725, Loss: 1.8634, Train: 22.93%, Valid: 23.06%, Test: 22.54%
Epoch: 750, Loss: 1.6059, Train: 26.60%, Valid: 26.73%, Test: 26.30%
Epoch: 775, Loss: 1.4964, Train: 35.45%, Valid: 34.97%, Test: 35.23%
Epoch: 800, Loss: 1.4904, Train: 34.88%, Valid: 34.24%, Test: 34.83%
Epoch: 825, Loss: 1.4753, Train: 36.90%, Valid: 36.10%, Test: 36.54%
Epoch: 850, Loss: 1.4900, Train: 36.41%, Valid: 35.72%, Test: 36.11%
Epoch: 875, Loss: 1.4716, Train: 36.86%, Valid: 36.24%, Test: 36.46%
Epoch: 900, Loss: 1.4541, Train: 37.68%, Valid: 36.75%, Test: 37.12%
Epoch: 925, Loss: 1.4596, Train: 37.69%, Valid: 36.66%, Test: 37.20%
Epoch: 950, Loss: 1.5191, Train: 26.47%, Valid: 25.85%, Test: 26.46%
Epoch: 975, Loss: 1.4754, Train: 38.35%, Valid: 37.42%, Test: 38.07%
Run 01:
Highest Train: 41.36
Highest Valid: 40.39
  Final Train: 41.36
   Final Test: 40.48
All runs:
Highest Train: 41.36, nan
Highest Valid: 40.39, nan
  Final Train: 41.36, nan
   Final Test: 40.48, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.2, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7153, Train: 12.51%, Valid: 12.59%, Test: 12.43%
Epoch: 25, Loss: 1.5160, Train: 30.63%, Valid: 30.63%, Test: 30.91%
Epoch: 50, Loss: 1.5286, Train: 33.04%, Valid: 32.54%, Test: 33.05%
Epoch: 75, Loss: 1.4090, Train: 38.98%, Valid: 38.79%, Test: 38.75%
Epoch: 100, Loss: 1.4009, Train: 36.81%, Valid: 36.79%, Test: 36.60%
Epoch: 125, Loss: 1.4276, Train: 38.67%, Valid: 38.25%, Test: 38.36%
Epoch: 150, Loss: 1.4015, Train: 40.83%, Valid: 40.24%, Test: 40.48%
Epoch: 175, Loss: 1.3661, Train: 41.43%, Valid: 41.04%, Test: 40.89%
Epoch: 200, Loss: 1.3520, Train: 42.22%, Valid: 41.62%, Test: 41.93%
Epoch: 225, Loss: 1.3412, Train: 42.45%, Valid: 41.77%, Test: 42.05%
Epoch: 250, Loss: 1.4836, Train: 38.47%, Valid: 37.69%, Test: 38.02%
Epoch: 275, Loss: 1.4311, Train: 40.12%, Valid: 39.34%, Test: 39.68%
Epoch: 300, Loss: 1.4507, Train: 25.57%, Valid: 25.08%, Test: 24.82%
Epoch: 325, Loss: 1.6135, Train: 39.18%, Valid: 38.52%, Test: 38.84%
Epoch: 350, Loss: 1.3903, Train: 40.94%, Valid: 40.08%, Test: 40.30%
Epoch: 375, Loss: 1.3630, Train: 40.64%, Valid: 40.02%, Test: 40.00%
Epoch: 400, Loss: 1.3141, Train: 40.59%, Valid: 39.34%, Test: 40.10%
Epoch: 425, Loss: 1.3353, Train: 43.45%, Valid: 42.47%, Test: 42.60%
Epoch: 450, Loss: 1.3453, Train: 40.13%, Valid: 39.31%, Test: 39.05%
Epoch: 475, Loss: 1.3312, Train: 43.35%, Valid: 42.05%, Test: 42.58%
Epoch: 500, Loss: 1.3404, Train: 43.26%, Valid: 42.11%, Test: 42.42%
Epoch: 525, Loss: 1.3427, Train: 43.90%, Valid: 43.04%, Test: 42.94%
Epoch: 550, Loss: 1.4223, Train: 39.69%, Valid: 38.80%, Test: 39.48%
Epoch: 575, Loss: 1.2955, Train: 44.03%, Valid: 42.74%, Test: 43.08%
Epoch: 600, Loss: 1.3203, Train: 43.45%, Valid: 42.55%, Test: 42.50%
Epoch: 625, Loss: 1.4753, Train: 33.98%, Valid: 33.35%, Test: 34.14%
Epoch: 650, Loss: 1.3405, Train: 44.07%, Valid: 42.93%, Test: 43.13%
Epoch: 675, Loss: 1.4938, Train: 29.38%, Valid: 28.77%, Test: 28.66%
Epoch: 700, Loss: 1.3864, Train: 40.33%, Valid: 39.19%, Test: 39.49%
Epoch: 725, Loss: 1.3898, Train: 40.82%, Valid: 39.35%, Test: 39.91%
Epoch: 750, Loss: 1.3739, Train: 43.01%, Valid: 41.63%, Test: 41.96%
Epoch: 775, Loss: 1.4050, Train: 41.96%, Valid: 40.81%, Test: 41.02%
Epoch: 800, Loss: 1.3513, Train: 43.03%, Valid: 41.46%, Test: 41.69%
Epoch: 825, Loss: 1.3411, Train: 42.37%, Valid: 40.62%, Test: 40.81%
Epoch: 850, Loss: 1.3324, Train: 42.60%, Valid: 40.90%, Test: 41.33%
Epoch: 875, Loss: 1.3686, Train: 40.96%, Valid: 39.52%, Test: 39.80%
Epoch: 900, Loss: 1.4112, Train: 42.88%, Valid: 41.30%, Test: 41.33%
Epoch: 925, Loss: 1.3278, Train: 43.05%, Valid: 41.49%, Test: 41.47%
Epoch: 950, Loss: 1.3172, Train: 40.14%, Valid: 38.86%, Test: 38.72%
Epoch: 975, Loss: 1.5905, Train: 31.64%, Valid: 30.96%, Test: 31.55%
Run 01:
Highest Train: 45.61
Highest Valid: 44.46
  Final Train: 45.61
   Final Test: 44.84
All runs:
Highest Train: 45.61, nan
Highest Valid: 44.46, nan
  Final Train: 45.61, nan
   Final Test: 44.84, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.2, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=1e-05)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7244, Train: 28.71%, Valid: 28.53%, Test: 28.81%
Epoch: 25, Loss: 1.5192, Train: 34.70%, Valid: 34.37%, Test: 34.90%
Epoch: 50, Loss: 1.5622, Train: 33.06%, Valid: 33.05%, Test: 33.51%
Epoch: 75, Loss: 1.4488, Train: 36.17%, Valid: 35.87%, Test: 36.11%
Epoch: 100, Loss: 1.4117, Train: 37.69%, Valid: 37.12%, Test: 37.53%
Epoch: 125, Loss: 1.3862, Train: 39.63%, Valid: 39.20%, Test: 39.29%
Epoch: 150, Loss: 1.3677, Train: 39.89%, Valid: 39.29%, Test: 39.49%
Epoch: 175, Loss: 2.7981, Train: 28.98%, Valid: 28.82%, Test: 28.96%
Epoch: 200, Loss: 1.6088, Train: 32.29%, Valid: 32.10%, Test: 32.26%
Epoch: 225, Loss: 1.4603, Train: 31.94%, Valid: 31.50%, Test: 31.55%
Epoch: 250, Loss: 1.4821, Train: 35.02%, Valid: 34.62%, Test: 35.15%
Epoch: 275, Loss: 1.4468, Train: 36.95%, Valid: 36.57%, Test: 36.80%
Epoch: 300, Loss: 1.4359, Train: 37.35%, Valid: 36.84%, Test: 37.13%
Epoch: 325, Loss: 1.4402, Train: 32.96%, Valid: 32.61%, Test: 32.80%
Epoch: 350, Loss: 1.4194, Train: 38.33%, Valid: 37.62%, Test: 38.18%
Epoch: 375, Loss: 1.4127, Train: 38.31%, Valid: 37.57%, Test: 37.95%
Epoch: 400, Loss: 1.4384, Train: 36.11%, Valid: 35.61%, Test: 35.82%
Epoch: 425, Loss: 1.4120, Train: 36.67%, Valid: 36.45%, Test: 36.71%
Epoch: 450, Loss: 1.4199, Train: 37.28%, Valid: 36.53%, Test: 37.05%
Epoch: 475, Loss: 1.3928, Train: 37.48%, Valid: 36.53%, Test: 37.01%
Epoch: 500, Loss: 1.3917, Train: 31.48%, Valid: 31.05%, Test: 30.92%
Epoch: 525, Loss: 1.3978, Train: 40.55%, Valid: 39.74%, Test: 40.00%
Epoch: 550, Loss: 1.3762, Train: 40.61%, Valid: 39.69%, Test: 39.67%
Epoch: 575, Loss: 1.3746, Train: 41.41%, Valid: 40.81%, Test: 40.96%
Epoch: 600, Loss: 1.4451, Train: 33.73%, Valid: 33.65%, Test: 33.73%
Epoch: 625, Loss: 1.5326, Train: 38.62%, Valid: 38.00%, Test: 38.20%
Epoch: 650, Loss: 1.4339, Train: 38.35%, Valid: 37.79%, Test: 38.09%
Epoch: 675, Loss: 1.3918, Train: 40.18%, Valid: 39.25%, Test: 39.54%
Epoch: 700, Loss: 1.6452, Train: 37.15%, Valid: 36.57%, Test: 36.77%
Epoch: 725, Loss: 1.4171, Train: 39.62%, Valid: 38.73%, Test: 38.98%
Epoch: 750, Loss: 1.5106, Train: 28.29%, Valid: 27.92%, Test: 28.40%
Epoch: 775, Loss: 1.4476, Train: 36.71%, Valid: 35.77%, Test: 35.94%
Epoch: 800, Loss: 1.3778, Train: 41.11%, Valid: 40.11%, Test: 40.45%
Epoch: 825, Loss: 1.4279, Train: 38.83%, Valid: 38.31%, Test: 38.29%
Epoch: 850, Loss: 1.3789, Train: 36.20%, Valid: 35.38%, Test: 35.39%
Epoch: 875, Loss: 1.3443, Train: 42.62%, Valid: 41.42%, Test: 41.69%
Epoch: 900, Loss: 1.3880, Train: 41.90%, Valid: 40.69%, Test: 41.00%
Epoch: 925, Loss: 1.3383, Train: 43.09%, Valid: 41.69%, Test: 41.90%
Epoch: 950, Loss: 1.3999, Train: 39.64%, Valid: 38.62%, Test: 39.04%
Epoch: 975, Loss: 1.3663, Train: 42.13%, Valid: 40.86%, Test: 40.96%
Run 01:
Highest Train: 43.87
Highest Valid: 42.53
  Final Train: 43.87
   Final Test: 42.73
All runs:
Highest Train: 43.87, nan
Highest Valid: 42.53, nan
  Final Train: 43.87, nan
   Final Test: 42.73, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.2, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=1, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7337, Train: 18.96%, Valid: 18.92%, Test: 18.68%
Epoch: 25, Loss: 1.5581, Train: 31.83%, Valid: 31.38%, Test: 31.69%
Epoch: 50, Loss: 1.6299, Train: 34.08%, Valid: 33.55%, Test: 34.16%
Epoch: 75, Loss: 1.4803, Train: 35.41%, Valid: 35.13%, Test: 35.25%
Epoch: 100, Loss: 1.4824, Train: 35.73%, Valid: 35.32%, Test: 35.53%
Epoch: 125, Loss: 2.8421, Train: 12.67%, Valid: 12.74%, Test: 12.59%
Epoch: 150, Loss: 1.5426, Train: 34.02%, Valid: 33.69%, Test: 34.06%
Epoch: 175, Loss: 1.4874, Train: 35.18%, Valid: 34.81%, Test: 35.28%
Epoch: 200, Loss: 1.4569, Train: 36.57%, Valid: 35.95%, Test: 36.40%
Epoch: 225, Loss: 1.4327, Train: 39.20%, Valid: 38.80%, Test: 38.95%
Epoch: 250, Loss: 4.4806, Train: 21.75%, Valid: 21.72%, Test: 22.11%
Epoch: 275, Loss: 1.9115, Train: 18.97%, Valid: 18.93%, Test: 18.68%
Epoch: 300, Loss: 1.5079, Train: 32.57%, Valid: 32.19%, Test: 32.34%
Epoch: 325, Loss: 1.4793, Train: 36.20%, Valid: 35.83%, Test: 36.07%
Epoch: 350, Loss: 1.4290, Train: 39.49%, Valid: 39.01%, Test: 39.19%
Epoch: 375, Loss: 1.4762, Train: 36.72%, Valid: 36.27%, Test: 36.43%
Epoch: 400, Loss: 1.4783, Train: 30.30%, Valid: 30.04%, Test: 30.51%
Epoch: 425, Loss: 1.4950, Train: 32.90%, Valid: 32.49%, Test: 32.53%
Epoch: 450, Loss: 1.4671, Train: 36.46%, Valid: 35.72%, Test: 36.06%
Epoch: 475, Loss: 1.4672, Train: 37.55%, Valid: 36.77%, Test: 36.78%
Epoch: 500, Loss: 1.4503, Train: 37.50%, Valid: 36.47%, Test: 36.83%
Epoch: 525, Loss: 1.4966, Train: 32.38%, Valid: 31.38%, Test: 32.10%
Epoch: 550, Loss: 1.4707, Train: 36.85%, Valid: 35.81%, Test: 36.09%
Epoch: 575, Loss: 1.4639, Train: 37.16%, Valid: 36.01%, Test: 36.25%
Epoch: 600, Loss: 1.4616, Train: 37.35%, Valid: 35.97%, Test: 36.31%
Epoch: 625, Loss: 1.4556, Train: 37.51%, Valid: 36.15%, Test: 36.49%
Epoch: 650, Loss: 1.4511, Train: 37.52%, Valid: 36.22%, Test: 36.46%
Epoch: 675, Loss: 1.4479, Train: 37.86%, Valid: 36.34%, Test: 36.53%
Epoch: 700, Loss: 1.4435, Train: 38.04%, Valid: 36.42%, Test: 36.85%
Epoch: 725, Loss: 1.4298, Train: 38.64%, Valid: 37.12%, Test: 37.37%
Epoch: 750, Loss: 1.5140, Train: 31.71%, Valid: 30.73%, Test: 30.36%
Epoch: 775, Loss: 1.7473, Train: 30.63%, Valid: 30.13%, Test: 29.87%
Epoch: 800, Loss: 1.4830, Train: 37.15%, Valid: 35.87%, Test: 36.33%
Epoch: 825, Loss: 1.4533, Train: 37.58%, Valid: 36.12%, Test: 36.54%
Epoch: 850, Loss: 1.4437, Train: 38.47%, Valid: 36.57%, Test: 37.08%
Epoch: 875, Loss: 1.4403, Train: 38.17%, Valid: 36.42%, Test: 36.83%
Epoch: 900, Loss: 1.4380, Train: 38.52%, Valid: 36.69%, Test: 37.17%
Epoch: 925, Loss: 1.4364, Train: 38.61%, Valid: 36.72%, Test: 37.22%
Epoch: 950, Loss: 1.4316, Train: 38.65%, Valid: 36.63%, Test: 37.23%
Epoch: 975, Loss: 1.4253, Train: 39.06%, Valid: 37.01%, Test: 37.41%
Run 01:
Highest Train: 41.11
Highest Valid: 39.63
  Final Train: 41.11
   Final Test: 39.91
All runs:
Highest Train: 41.11, nan
Highest Valid: 39.63, nan
  Final Train: 41.11, nan
   Final Test: 39.91, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.2, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=2, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7310, Train: 12.51%, Valid: 12.59%, Test: 12.43%
Epoch: 25, Loss: 1.5273, Train: 27.59%, Valid: 28.21%, Test: 27.67%
Epoch: 50, Loss: 1.4251, Train: 33.64%, Valid: 33.38%, Test: 33.03%
Epoch: 75, Loss: 1.6520, Train: 31.70%, Valid: 31.34%, Test: 31.92%
Epoch: 100, Loss: 1.4483, Train: 36.53%, Valid: 36.49%, Test: 37.11%
Epoch: 125, Loss: 1.4205, Train: 39.42%, Valid: 39.06%, Test: 39.37%
Epoch: 150, Loss: 1.4217, Train: 37.01%, Valid: 36.79%, Test: 36.89%
Epoch: 175, Loss: 1.3716, Train: 40.46%, Valid: 39.87%, Test: 40.18%
Epoch: 200, Loss: 1.3773, Train: 41.62%, Valid: 41.08%, Test: 41.16%
Epoch: 225, Loss: 1.4171, Train: 33.97%, Valid: 33.60%, Test: 33.88%
Epoch: 250, Loss: 1.3963, Train: 39.13%, Valid: 38.56%, Test: 38.58%
Epoch: 275, Loss: 1.4727, Train: 39.48%, Valid: 38.86%, Test: 39.04%
Epoch: 300, Loss: 1.4927, Train: 34.20%, Valid: 33.97%, Test: 34.04%
Epoch: 325, Loss: 1.4215, Train: 39.02%, Valid: 38.23%, Test: 38.66%
Epoch: 350, Loss: 1.4553, Train: 37.47%, Valid: 36.66%, Test: 37.10%
Epoch: 375, Loss: 1.4245, Train: 38.42%, Valid: 37.49%, Test: 37.87%
Epoch: 400, Loss: 1.4093, Train: 39.31%, Valid: 38.52%, Test: 38.85%
Epoch: 425, Loss: 1.4403, Train: 31.46%, Valid: 31.45%, Test: 31.73%
Epoch: 450, Loss: 1.4187, Train: 37.33%, Valid: 36.45%, Test: 37.01%
Epoch: 475, Loss: 3.1368, Train: 32.79%, Valid: 32.03%, Test: 32.56%
Epoch: 500, Loss: 1.6100, Train: 22.95%, Valid: 22.90%, Test: 22.39%
Epoch: 525, Loss: 1.4770, Train: 36.63%, Valid: 35.66%, Test: 36.14%
Epoch: 550, Loss: 1.4573, Train: 38.02%, Valid: 36.99%, Test: 37.39%
Epoch: 575, Loss: 1.4347, Train: 37.27%, Valid: 36.47%, Test: 36.87%
Epoch: 600, Loss: 1.4029, Train: 38.40%, Valid: 37.37%, Test: 37.85%
Epoch: 625, Loss: 1.3607, Train: 42.00%, Valid: 40.88%, Test: 40.96%
Epoch: 650, Loss: 1.4353, Train: 38.79%, Valid: 37.82%, Test: 38.17%
Epoch: 675, Loss: 1.4029, Train: 40.57%, Valid: 39.60%, Test: 39.90%
Epoch: 700, Loss: 1.4019, Train: 40.29%, Valid: 39.07%, Test: 39.26%
Epoch: 725, Loss: 1.9090, Train: 34.68%, Valid: 34.26%, Test: 34.36%
Epoch: 750, Loss: 1.4583, Train: 36.47%, Valid: 35.68%, Test: 35.99%
Epoch: 775, Loss: 1.4625, Train: 37.38%, Valid: 36.15%, Test: 36.79%
Epoch: 800, Loss: 1.4535, Train: 38.86%, Valid: 37.58%, Test: 37.94%
Epoch: 825, Loss: 1.4138, Train: 39.37%, Valid: 37.96%, Test: 38.37%
Epoch: 850, Loss: 1.4511, Train: 37.39%, Valid: 36.51%, Test: 36.77%
Epoch: 875, Loss: 1.4082, Train: 40.38%, Valid: 39.09%, Test: 39.53%
Epoch: 900, Loss: 1.4117, Train: 40.72%, Valid: 39.29%, Test: 39.44%
Epoch: 925, Loss: 1.4326, Train: 35.13%, Valid: 33.94%, Test: 34.65%
Epoch: 950, Loss: 1.4511, Train: 38.03%, Valid: 37.15%, Test: 37.37%
Epoch: 975, Loss: 1.4043, Train: 40.23%, Valid: 38.79%, Test: 39.10%
Run 01:
Highest Train: 42.37
Highest Valid: 41.63
  Final Train: 42.22
   Final Test: 41.54
All runs:
Highest Train: 42.37, nan
Highest Valid: 41.63, nan
  Final Train: 42.22, nan
   Final Test: 41.54, nan
Saving results to results/arxiv-year.csv
Using backend: pytorch
Namespace(SGD=False, adam=False, alpha=1.0, beta=10.0, cached=False, cpu=False, dataset='arxiv-year', directed=True, display_step=25, dropout=0.2, epochs=1000, gamma=0.9, gat_heads=8, gcn2_alpha=0.1, gpr_alpha=0.1, hidden_channels=256, hops=1, inner_activation=False, inner_dropout=False, jk_type='max', link_init_layers_A=1, link_init_layers_X=1, lp_alpha=0.1, lr=0.1, method='mlpnorm', no_bn=False, norm_func_id=2, norm_layers=3, num_layers=2, num_mlp_layers=1, orders=3, orders_func_id=2, print_prop=False, rand_split=False, rocauc=False, runs=1, sampling=False, sub_dataset='', theta=0.5, train_prop=0.5, valid_prop=0.25, weight_decay=0.001)
num nodes 169343 | num classes 5 | num node feats 128
MODEL: MLPNORM(
  (fc1): Linear(in_features=128, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=5, bias=True)
  (fc3): Linear(in_features=256, out_features=256, bias=True)
  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Epoch: 00, Loss: 1.7223, Train: 26.47%, Valid: 26.34%, Test: 26.37%
Epoch: 25, Loss: 1.5047, Train: 29.84%, Valid: 29.33%, Test: 30.01%
Epoch: 50, Loss: 1.5266, Train: 36.02%, Valid: 35.63%, Test: 35.97%
Epoch: 75, Loss: 1.4172, Train: 37.27%, Valid: 36.96%, Test: 37.18%
Epoch: 100, Loss: 1.4299, Train: 29.86%, Valid: 29.75%, Test: 30.00%
Epoch: 125, Loss: 1.4040, Train: 38.09%, Valid: 37.67%, Test: 37.97%
Epoch: 150, Loss: 1.3731, Train: 40.78%, Valid: 40.36%, Test: 40.62%
Epoch: 175, Loss: 1.3619, Train: 41.19%, Valid: 40.62%, Test: 40.80%
Epoch: 200, Loss: 1.4890, Train: 37.51%, Valid: 37.25%, Test: 37.53%
Epoch: 225, Loss: 1.3864, Train: 39.79%, Valid: 39.13%, Test: 39.37%
