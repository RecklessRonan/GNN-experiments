{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def load_data(path=\"./data/cora/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    print('adj', adj.shape)\n",
    "    print('features', features.shape)\n",
    "    print('labels', labels.shape)\n",
    "    print('idx_train', idx_train.shape)\n",
    "    print('idx_val', idx_val.shape)\n",
    "    print('idx_test', idx_test.shape)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "adj torch.Size([2708, 2708])\n",
      "features torch.Size([2708, 1433])\n",
      "labels torch.Size([2708])\n",
      "idx_train torch.Size([140])\n",
      "idx_val torch.Size([300])\n",
      "idx_test torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
       "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
       "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "def load_data_new(dataset_str, split_part):\n",
    "    \"\"\"\n",
    "    Loads input data from gcn/data directory\n",
    "\n",
    "    ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances\n",
    "        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;\n",
    "    ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;\n",
    "    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
    "    ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict\n",
    "        object;\n",
    "    ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.\n",
    "\n",
    "    All objects above must be saved using python pickle module.\n",
    "\n",
    "    :param dataset_str: Dataset name\n",
    "    :return: All data input files loaded (as well the training/test data).\n",
    "    \"\"\"\n",
    "    print('dataset_str', dataset_str)\n",
    "    print('split_part', split_part)\n",
    "    if dataset_str in ['citeseer', 'cora', 'pubmed']:\n",
    "        names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "        objects = []\n",
    "        for i in range(len(names)):\n",
    "            with open(\"data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "                if sys.version_info > (3, 0):\n",
    "                    objects.append(pkl.load(f, encoding='latin1'))\n",
    "                else:\n",
    "                    objects.append(pkl.load(f))\n",
    "\n",
    "        x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "        test_idx_reorder = parse_index_file(\n",
    "            \"data/ind.{}.test.index\".format(dataset_str))\n",
    "        test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "        if dataset_str == 'citeseer':\n",
    "            # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "            # Find isolated nodes, add them as zero-vecs into the right position\n",
    "            test_idx_range_full = range(\n",
    "                min(test_idx_reorder), max(test_idx_reorder)+1)\n",
    "            tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "            tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
    "            tx = tx_extended\n",
    "            ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
    "            ty_extended[test_idx_range-min(test_idx_range), :] = ty\n",
    "            ty = ty_extended\n",
    "\n",
    "        features = sp.vstack((allx, tx)).tolil()\n",
    "        features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "        adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "\n",
    "        labels = np.vstack((ally, ty))\n",
    "        labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "\n",
    "        splits_file_path = 'splits/' + dataset_str + \\\n",
    "            '_split_0.6_0.2_' + str(split_part) + '.npz'\n",
    "\n",
    "        with np.load(splits_file_path) as splits_file:\n",
    "            train_mask = splits_file['train_mask']\n",
    "            val_mask = splits_file['val_mask']\n",
    "            test_mask = splits_file['test_mask']\n",
    "\n",
    "        idx_train = np.where(train_mask==1)[0]\n",
    "        idx_val = np.where(val_mask==1)[0]\n",
    "        idx_test = np.where(test_mask==1)[0]\n",
    "\n",
    "        \n",
    "    elif dataset_str in ['chameleon', 'cornell', 'film', 'squirrel', 'texas', 'wisconsin']:\n",
    "        graph_adjacency_list_file_path = os.path.join(\n",
    "            'new_data', dataset_str, 'out1_graph_edges.txt')\n",
    "        graph_node_features_and_labels_file_path = os.path.join('new_data', dataset_str,\n",
    "                                                                f'out1_node_feature_label.txt')\n",
    "        graph_dict = defaultdict(list)\n",
    "        with open(graph_adjacency_list_file_path) as graph_adjacency_list_file:\n",
    "            graph_adjacency_list_file.readline()\n",
    "            for line in graph_adjacency_list_file:\n",
    "                line = line.rstrip().split('\\t')\n",
    "                assert (len(line) == 2)\n",
    "                graph_dict[int(line[0])].append(int(line[1]))\n",
    "                graph_dict[int(line[1])].append(int(line[0]))\n",
    "\n",
    "        # print(sorted(graph_dict))\n",
    "        graph_dict_ordered = defaultdict(list)\n",
    "        for key in sorted(graph_dict):\n",
    "            graph_dict_ordered[key] = graph_dict[key]\n",
    "            graph_dict_ordered[key].sort()\n",
    "\n",
    "        adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph_dict_ordered))\n",
    "        # adj = sp.csr_matrix(adj)\n",
    "\n",
    "        graph_node_features_dict = {}\n",
    "        graph_labels_dict = {}\n",
    "\n",
    "        if dataset_str == 'film':\n",
    "            with open(graph_node_features_and_labels_file_path) as graph_node_features_and_labels_file:\n",
    "                graph_node_features_and_labels_file.readline()\n",
    "                for line in graph_node_features_and_labels_file:\n",
    "                    line = line.rstrip().split('\\t')\n",
    "                    assert (len(line) == 3)\n",
    "                    assert (int(line[0]) not in graph_node_features_dict and int(\n",
    "                        line[0]) not in graph_labels_dict)\n",
    "                    feature_blank = np.zeros(932, dtype=np.uint8)\n",
    "                    feature_blank[np.array(\n",
    "                        line[1].split(','), dtype=np.uint16)] = 1\n",
    "                    graph_node_features_dict[int(line[0])] = feature_blank\n",
    "                    graph_labels_dict[int(line[0])] = int(line[2])\n",
    "        else:\n",
    "            with open(graph_node_features_and_labels_file_path) as graph_node_features_and_labels_file:\n",
    "                graph_node_features_and_labels_file.readline()\n",
    "                for line in graph_node_features_and_labels_file:\n",
    "                    line = line.rstrip().split('\\t')\n",
    "                    assert (len(line) == 3)\n",
    "                    assert (int(line[0]) not in graph_node_features_dict and int(\n",
    "                        line[0]) not in graph_labels_dict)\n",
    "                    graph_node_features_dict[int(line[0])] = np.array(\n",
    "                        line[1].split(','), dtype=np.uint8)\n",
    "                    graph_labels_dict[int(line[0])] = int(line[2])\n",
    "\n",
    "        features_list = []\n",
    "        for key in sorted(graph_node_features_dict):\n",
    "            features_list.append(graph_node_features_dict[key])\n",
    "        features = np.vstack(features_list)\n",
    "        features = sp.csr_matrix(features)        \n",
    "\n",
    "        labels_list = []\n",
    "        for key in sorted(graph_labels_dict):\n",
    "            labels_list.append(graph_labels_dict[key])\n",
    "\n",
    "        label_classes = max(labels_list) + 1\n",
    "        labels = np.eye(label_classes)[labels_list]\n",
    "\n",
    "\n",
    "        splits_file_path = 'splits/' + dataset_str + \\\n",
    "            '_split_0.6_0.2_' + str(split_part) + '.npz'\n",
    "\n",
    "        with np.load(splits_file_path) as splits_file:\n",
    "            train_mask = splits_file['train_mask']\n",
    "            val_mask = splits_file['val_mask']\n",
    "            test_mask = splits_file['test_mask']\n",
    "\n",
    "        idx_train = np.where(train_mask==1)[0]\n",
    "        idx_val = np.where(val_mask==1)[0]\n",
    "        idx_test = np.where(test_mask==1)[0]\n",
    "\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    \n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    print('adj', adj.shape)\n",
    "    print('features', features.shape)\n",
    "    print('labels', labels.shape)\n",
    "    print('idx_train', idx_train.shape)\n",
    "    print('idx_val', idx_val.shape)\n",
    "    print('idx_test', idx_test.shape)\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_str texas\n",
      "split_part 0\n",
      "adj torch.Size([183, 183])\n",
      "features torch.Size([183, 1703])\n",
      "labels torch.Size([183])\n",
      "idx_train torch.Size([87])\n",
      "idx_val torch.Size([59])\n",
      "idx_test torch.Size([37])\n"
     ]
    }
   ],
   "source": [
    "adj, features, labels, idx_train, idx_val, idx_test = load_data_new('pubmed', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   2,   4,   8,  11,  13,  15,  19,  21,  26,  32,  35,  37,  39,\n",
       "         41,  44,  45,  46,  50,  53,  58,  59,  60,  61,  63,  65,  66,  70,\n",
       "         71,  72,  73,  74,  75,  76,  77,  78,  83,  85,  86,  88,  89,  92,\n",
       "         94,  95,  96, 101, 103, 104, 105, 107, 108, 110, 113, 115, 116, 119,\n",
       "        120, 121, 123, 124, 125, 126, 130, 131, 133, 135, 137, 139, 140, 141,\n",
       "        142, 144, 145, 146, 148, 152, 153, 156, 161, 168, 170, 171, 173, 177,\n",
       "        179, 181, 182])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
